At Facebook, Hadoop has traditionally been used in conjunction with Hive for storage and analysis of large data sets.
Most of this analysis occurs in offline batch jobs and the emphasis has been on maximizing throughput and efficiency.
These workloads typically read and write large amounts of data from disk sequentially.
As such, there has been less emphasis on making Hadoop performant for random access workloads by providing low latency access to HDFS.
Instead, we have used a combination of large clusters of MySQL databases and caching tiers built using memcached[9]
In many cases, results from Hadoop are uploaded into MySQL or memcached for consumption by the web tier.
Recently, a new generation of applications has arisen at Facebook that require very high write throughput and cheap and elastic storage, while simultaneously requiring low latency and disk efficient sequential and random read performance.
MySQL storage engines are proven and have very good random read performance, but typically suffer from low random write throughput.
It is difficult to scale up our MySQL clusters rapidly while maintaining good load balancing and high uptime.
Administration of MySQL clusters requires a relatively high management overhead and they typically use more expensive hardware.
Given our high confidence in the reliability and scalability of HDFS, we began to explore Hadoop and HBase for such applications.
The first set of applications requires realtime concurrent, but sequential, read access to a very large stream of realtime data being stored in HDFS.
An example system generating and storing such data is Scribe [10], an open source distributed log aggregation service created by and used extensively at Facebook.
Previously, data generated by Scribe was stored in expensive and hard to manage NFS servers.
Two main applications that fall into this category are Realtime Analytics [11] and MySQL backups.
We have enhanced HDFS to become a high performance low latency file system and have been able to reduce our use of expensive file servers.
The second generation of non-MapReduce Hadoop applications needed to dynamically index a rapidly growing data set for fast random lookups.
One primary example of such an application is Facebook Messages [12]
Facebook Messages gives every Facebook user a facebook.com email address, integrates the display of all e-mail, SMS and chat messages between a pair or group of users, has strong controls over who users receive messages from, and is the foundation of a Social Inbox.
In addition, this new application had to be suited for production use by more than 500 million people immediately after launch and needed to scale to many petabytes of data with stringent uptime requirements.
HBase in turn leverages HDFS for scalable and fault tolerant storage and ZooKeeper for distributed consensus.
Before deciding on a particular software stack and whether or not to move away from our MySQL-based architecture, we looked at a few specific applications where existing solutions may be problematic.
These use cases would have workloads that are challenging to scale because of very high write throughput, massive datasets, unpredictable growth, or other patterns that may be difficult or suboptimal in a sharded RDBMS environment.
The latest generation of Facebook Messaging combines existing Facebook messages with e-mail, chat, and SMS.
In addition to persisting all of these messages, a new threading model also requires messages to be stored for each participating user.
As part of the application server requirements, each user will be sticky to a single data center.
With an existing rate of millions of messages and billions of instant messages every day, the volume of ingested data would be very large from day one and only continue to grow.
The denormalized requirement would further increase the number of writes to the system as each message could be written several times.
As part of the product requirements, messages would not be deleted unless explicitly done so by the user, so each mailbox would grow indefinitely.
As is typical with most messaging applications, messages are read only a handful of times when they are recent, and then are rarely looked at again.
As such, a vast majority would not be read from the database but must be available at all times and with low latency, so archiving would be difficult.
The sheer number of new messages would also mean a heavy write workload, which could translate to a high number of random IO operations in this type of system.
Facebook Insights provides developers and website owners with access to real-time analytics related to Facebook activity across websites with social plugins, Facebook Pages, and Facebook Ads.
Using anonymized data, Facebook surfaces activity such as impressions, click through rates and website visits.
These analytics can help everyone from businesses to bloggers gain insights into how people are interacting with their content so they can optimize their services.
Domain and URL analytics were previously generated in a periodic, offline fashion through our Hadoop and Hive data warehouse.
However, this yields a poor user experience as the data is only available several hours after it has occurred.
The insights teams wanted to make statistics available to their users within seconds of user actions rather than the hours previously supported.
This would require a large-scale, asynchronous queuing system for user actions as well as systems to process, aggregate, and persist these events.
All of these systems need to be fault-tolerant and support more than a million events per second.
To support the existing insights functionality, time and demographic-based aggregations would be necessary.
However, these aggregations must be kept up-to-date and thus processed on the fly, one event at a time, through numeric counters.
With millions of unique aggregates and billions of events, this meant a very large number of counters with an even larger number of operations against them.
For example, we may collect the amount of CPU usage on a given server or tier of servers, or we may track the number of write operations to an HBase cluster.
For each node or group of nodes we track hundreds or thousands of different metrics, and engineers will ask to plot them over time at various granularities.
While this application has hefty requirements for write throughput, some of the bigger pain points with the existing MySQL-based system are around the resharding of data and the ability to do table scans for analysis and time roll-ups.
The massive number of indexed and time-series writes and the unpredictable growth patterns are difficult to reconcile on a sharded MySQL setup.
For example, a given product may only collect ten metrics over a long period of time, but following a large rollout or product launch, the same product may produce thousands of metrics.
With the existing system, a single MySQL server may suddenly be handling much more load than it can handle, forcing the team to manually re-shard data from this server onto multiple servers.
A vast majority of reads to the metrics system is for very recent, raw data, however all historical data must also be available.
Recently written data should be available quickly, but the entire dataset will also be periodically scanned in order to perform timebased rollups.
The requirements for the storage system from the workloads presented above can be summarized as follows (in no particular order):
Elasticity: We need to be able to add incremental capacity to our storage systems with minimal overhead and no downtime.
In some cases we may want to add capacity rapidly and the system should automatically balance load and utilization across new hardware.
High write throughput: Most of the applications store (and optionally index) tremendous amounts of data and require high aggregate write throughput.
Efficient random reads from disk: In spite of the widespread use of application level caches (whether embedded or via memcached), at   Facebook scale, a lot of accesses miss the cache and hit the back-end storage system.
MySQL is very efficient at performing random reads from disk and any new system would have to be comparable.
High Availability and Disaster Recovery: We need to provide a service with very high uptime to users that covers both planned and unplanned events (examples of the former being events like software upgrades and addition of hardware/capacity.
We also need to be able to tolerate the loss of a data center with minimal data loss and be able to serve data out of another data center in a reasonable time frame.
MySQL databases has shown us that fault isolation is critical.
Individual databases can and do go down, but only a small fraction of users are affected by any such event.
Similarly, in our warehouse usage of Hadoop, individual disk failures affect only a small part of the data and the system quickly recovers from such faults.
Atomic read-modify-write primitives: Atomic increments and compare-and-swap APIs have been very useful in building lockless concurrent applications and are a must have from the underlying storage system.
Range Scans: Several applications require efficient retrieval of a set of rows in a particular range.
Tolerance of network partitions within a single data center: Different system components are often inherently centralized.
For example, MySQL servers may all be located within a few racks, and network partitions within a data center would cause major loss in serving capabilities therein.
Hence every effort is made to eliminate the possibility of such events at the hardware level by having a highly redundant network design.
Zero Downtime in case of individual data center failure: In our experience such failures are very rare, though not impossible.
In a less than ideal world where the choice of system design boils down to the choice of compromises that are acceptable, this is one compromise that we are willing to make given the low occurrence rate of such events.
After considerable research and experimentation, we chose Hadoop and HBase as the foundational storage technology for these next generation applications.
The decision was based on the state of HBase at the point of evaluation as well as our confidence in addressing the features that were lacking at that point via inhouse engineering.
HBase already provided a highly consistent, high write-throughput key-value store.
HBase is massively scalable and delivers fast random writes as well as random and streaming reads.
It also provides row-level atomicity guarantees, but no native cross-row transactional support.
From a data model perspective, column-orientation gives extreme flexibility in storing data and wide rows allow the creation of billions of indexed values within a single table.
HBase is ideal for workloads that are write-intensive, need to maintain a large amount of data, large indices, and maintain the flexibility to scale out quickly.
We have seen the advantages of using HDFS: its linear scalability and fault tolerance results in huge cost savings across the enterprise.
The new, more realtime and online usage of HDFS push new requirements and now use HDFS as a general-purpose low-latency file system.
In this section, we describe some of the core changes we have made to HDFS to support these new applications.
At startup time, the HDFS NameNode reads filesystem metadata from a file called the fsimage file.
This metadata contains the names and metadata of every file and directory in HDFS.
However, the NameNode does not persistently store the locations of each block.
Thus, the time to cold-start a NameNode consists of two main parts: firstly, the reading of the file system image, applying the transaction log and saving the new file system image back to disk; and secondly, the processing of block reports from a majority of DataNodes to recover all known block locations of.
The BackupNode available in Apache HDFS avoids reading the fsimage from disk on a failover, but it still needs to gather block reports from all DataNodes.
Thus, the failover times for the BackupNode solution can be as high as 20 minutes.
Our goal is to do a failover within seconds; thus, the BackupNode solution does not meet our goals for fast failover.
Another problem is that the NameNode synchronously updates the BackupNode on every transaction, thus the reliability of the entire system could now be lower than the reliability of the standalone NameNode.
All HDFS clusters at Facebook use NFS to store one copy of the filesystem image and one copy of the transaction log.
The Active AvatarNode writes its transactions to the transaction log stored in a NFS filesystem.
At the same time, the Standby opens the same transaction log for reading from the NFS file system and starts applying transactions to its own namespace thus keeping its namespace as close to the primary as possible.
The Standby AvatarNode also takes care of check-pointing the primary and creating a new filesystem image so there is no separate SecondaryNameNode anymore.
The DataNodes talk to both Active AvatarNode and Standby AvatarNode instead of just talking to a single NameNode.
That means that the Standby AvatarNode has the most recent state about block locations as well and can become Active in well under a minute.
The Avatar DataNode sends heartbeats, block reports and block received to both AvatarNodes.
Replication or deletion requests coming from the Standby AvatarNode are ignored.
Since we wanted to make the failover as transparent as possible, the Standby has to know of each block allocation as it happens, so we write a new transaction to the edits log on each block allocation.
This allows a client to continue writing to files that it was writing at the moment just before the failover.
When the Standby reads transactions from the transaction log that is being written by the Active AvatarNode, there is a possibility that it reads a partial transaction.
To avoid this problem we had to change the format of the edits log to have a transaction length, transaction id and the checksum per each transaction written to the file.
ZooKeeper holds a zNode with the physical address of the Primary AvatarNode for a given cluster.
When the client is trying to connect to the HDFS cluster (e.g.
If a call encounters a network error, DAFS checks with ZooKeeper for a change of the primary node.
In case there was a failover event, the zNone will now contain the name of the new Primary AvatarNode.
We do not use the ZooKeeper subscription model because it would require much more resources dedicated on ZooKeeper servers.
If a failover is in progress, then DAFS will automatically block till the failover is complete.
A failover event is completely transparent to an application that is accessing data from HDFS.
Early on, we were pretty clear that we will be running multiple Hadoop clusters for our Messages application.
We needed the capability to deploy newer versions of the software on different clusters at different points in time.
This required that we enhance the Hadoop clients to be able to interoperate with Hadoop servers running different versions of the Hadoop software.
The various server process within the same cluster run the same version of the software.
We enhanced the Hadoop RPC software to automatically determine the version of the software running on the server that it is communicating with, and then talk the appropriate protocol while talking to that server.
The default HDFS block placement policy, while rack aware, is still minimally constrained.
Placement decision for non-local replicas is random, it can be on any rack and within any node of the rack.
To reduce the probability of data loss when multiple simultaneous nodes fail, we implemented a pluggable block placement policy that constrains the placement of block replicas into smaller, configurable node groups.
This allows us to reduce the probability of data loss by orders of magnitude, depending on the size chosen for the groups.
Our strategy is to define a window of racks and machines where replicas can be placed around the original block, using a logical ring of racks, each one containing a.
More details, the math, and the scripts used to calculate these numbers can be found at HDFS-1094[14]
We found that the probability of losing a random block increases with the size of the node group.
We picked this choice because the probability of data loss is about a hundred times lesser than the default block placement policy.
Realtime Workload HDFS is originally designed for high-throughput systems like MapReduce.
Many of its original design principles are to improve its throughput but do not focus much on response time.
For example, when dealing with errors, it favors retries or wait over fast failures.
To support realtime applications, offering reasonable response time even in case of errors becomes the major challenge for HDFS.
When a RPC client detects a tcp-socket timeout, instead of declaring a RPC timeout, it sends a ping to the RPC server.
If the server is still alive, the client continues to wait for a response.
The idea is that if a RPC server is experiencing a communication burst, a temporary high load, or a stop the world GC, the client should wait and throttles its traffic to the server.
On the contrary, throwing a timeout exception or retrying the RPC request causes tasks to fail unnecessarily or add additional load to a RPC server.
However, infinite wait adversely impacts any application that has a real time requirement.
An HDFS client occasionally makes an RPC to some Dataode, and it is bad when the DataNode fails to respond back in time and the client is stuck in an RPC.
A better strategy is to fail fast and try a different DataNode for either reading or writing.
Hence, we added the ability for specifying an RPC-timeout when starting a RPC session with a server.
There are times when an application wants to store data in HDFS for scalability and performance reasons.
However, the latency of reads and writes to an HDFS file is an order of magnitude greater than reading or writing to a local file on the machine.
To alleviate this problem, we implemented an enhancement to the HDFS client that detects that there is a local replica of the data and then transparently reads data from the local replica without transferring the data via the DataNode.
This has resulted in  doubling the performance profile of a certain workload that uses HBase.
Hflush/sync is an important operation for both HBase and Scribe.
It pushes the written data buffered at the client side to the write pipeline, making the data visible to any new reader and increasing the data durability when either the client or any DataNode on the pipeline fails.
Hflush/sync is a synchronous operation, meaning that it does not return until an acknowledgement from the write pipeline is received.
Since the operation is frequently invoked, increasing its efficiency is important.
One optimization we have is to allow following writes to proceed while an Hflush/sync operation is waiting for a reply.
This greatly increases the write throughput in both HBase and Scribe where a designated thread invokes Hflush/sync periodically.
Application developers have come to expect ACID compliance, or some approximation of it, from their database systems.
Indeed, strong consistency guarantees was one of the benefits of HBase in our early evaluations.
The existing MVCC-like read-write consistency control (RWCC) provided sufficient isolation guarantees and the HLog (write ahead log) on HDFS provided sufficient durability.
However, some modifications were necessary to make sure that HBase adhered to the row-level atomicity and consistency of ACID compliance we needed.
Originally, multiple entries in a single row transaction would be written in sequence to the HLog.
With a new concept of a log transaction (WALEdit), each write transaction will now be fully completed or not written at all.
During writes, HDFS sets up a pipeline connection to each replica and all replicas must ACK any data sent to them.
HBase will not continue until it gets a response or failure notification.
Through the use of sequence numbers, the NameNode is able to identify any misbehaving replicas and exclude them.
While functional, it takes time for the NameNode to do this file recovery.
In the case of the HLog, where forward progress while maintaining consistency and durability are an absolute must, HBase will immediately roll the log and obtain new blocks if it detects that even a single HDFS replica has failed to write data.
Upon reading an HDFS block, checksum validation is performed and the entire block is discarded upon a checksum failure.
Data discard is rarely problematic because two other replicas exist for this data.
Additional functionality was added to ensure that if all 3 replicas contain corrupt data the blocks are quarantined for postmortem analysis.
The largest cause of cluster downtime was not random server deaths, but rather system maintenance.
We had a number of problems to solve to minimize this downtime.
First, we discovered over time that RegionServers would intermittently require minutes to shutdown after issuing a stop request.
To address this, we made compactions interruptible to favor responsiveness over completion.
This reduced RegionServer downtime to seconds and gave us a reasonable bound on cluster shutdown time.
Originally, HBase only supported full cluster stop and start for upgrades.
We added rolling restarts script to perform software upgrades one server at a time.
Since the master automatically reassigns regions on a RegionServer stop, this minimizes the amount of downtime that our users experience.
We fixed numerous edge case issues that resulted from this new restart.
Incidentally, numerous bugs during rolling restarts were related to region offlining and reassignment, so our master rewrite with ZooKeeper integration helped address a number of issues here as well.
When a RegionServer dies, the HLogs of that server must be split and replayed before its regions can be reopened and made available for reads and writes.
Previously, the Master would split the logs before they were replayed across the remaining RegionServers.
This was the slowest part of the recovery process and because there are many HLogs per server, it could be parallelized.
Utilizing ZooKeeper to manage the split tasks across RegionServers, the Master now coordinates a distributed log split.
This cut recovery times by an order of magnitude and allows RegionServers to retain more HLogs without severely impacting failover performance.
Data insertion in HBase is optimized for write performance by focusing on sequential writes at the occasional expense of redundant reads.
A data transaction first gets written to a commit log and then applied to an in-memory cache called MemStore.
When the MemStore reaches a certain threshold it is written out as an HFile.
HFiles are immutable HDFS files containing key/value pairs in sorted order.
Instead of editing an existing HFile, new HFiles are written on every flush and added to a perregion list.
Read requests are issued on these multiple HFiles in parallel & aggregated for a final result.
For efficiency, these HFiles need to be periodically compacted, or merged together, to avoid degrading read performance.
Read performance is correlated with the number of files in a region and thus critically hinges on a well-tuned compaction algorithm.
More subtly, network IO efficiency can also be drastically affected if a compaction algorithm is improperly tuned.
Significant effort went into making sure we had an efficient compaction algorithm for our use case.
Compactions were initially separated into two distinct code paths depending upon whether they were minor or major.
Minor compactions select a subset of all files based on size metrics whereas time-based major compactions unconditionally compact all HFiles.
Previously, only major compactions processed deletes, overwrites, and purging of expired data, which meant that minor compactions resulted in larger HFiles than necessary, which decreases block cache efficiency and penalizes future compactions.
By unifying the code paths, the codebase was simplified and files were kept as small as possible.
After launching to employees, we noticed that our put and sync latencies were very high.
This network IO waste would continue until the compaction queue started to backlog.
This problem occurred because the existing algorithm would unconditionally minor compact the first four HFiles, while triggering a minor compaction after 3 HFiles had been reached.
The solution was to stop unconditionally compacting files above a certain size and skip compactions if enough candidate files could not be found.
We also worked on improving the size ratio decision of the compaction algorithm.
Originally, the compaction algorithm would sort by file age and compare adjacent files.
However, this algorithm had suboptimal behavior as the number and size of HFiles increased significantly.
To improve, we now include an older file if it is within 2x the aggregate size of all newer HFiles.
As discussed, read performance hinges on keeping the number of files in a region low thus reducing random IO operations.
In addition to utilizing comapctions to keep the number of files on disk low, it is also possible to skip certain files for some queries, similarly reducing IO operations.
Bloom filters provide a space-efficient and constant-time method for checking if a given row or row and column exists in a given HFile.
As each HFile is written sequentially with optional metadata blocks at the end, the addition of bloom filters fit in without significant changes.
Through the use of folding, each bloom filter is kept as small as possible when written to disk and cached in memory.
For queries that ask for specific rows and/or columns, a check of the cached bloom filter for each HFile can allow some files to be completely skipped.
For data stored in HBase that is time-series or contains a specific, known timestamp, a special timestamp file selection algorithm was added.
Since time moves forward and data is rarely inserted at a significantly later time than its timestamp, each HFile will generally contain values for a fixed range of time.
This information is stored as metadata in each HFile and queries that ask for a specific timestamp or range of timestamps will check if the request intersects with the ranges of each file, skipping those which do not overlap.
As read performance improved significantly with HDFS local file reads, it is critical that regions are hosted on the same physical nodes as their files.
Changes have been made to retain the assignment of regions across cluster and node restarts to ensure that locality is maintained.
These deployments are already serving live production traffic to millions of users.
During the same time frame, we have iterated rapidly on the core software (HBase/HDFS) as well as the application logic running against HBase.
In such a fluid environment, our ability to ship high quality software, deploy it correctly, monitor running systems and detect and fix any anomalies with minimal downtime are critical.
This section goes into some of the practices and tools that we have used during this evolution.
From early on in our design of an HBase solution, we were worried about code stability.
We first needed to test the stability and durability of the open source HBase code and additionally ensure the stability of our future changes.
The testing program generated data to write into HBase, both deterministically and randomly.
The tester will write data into the HBase cluster and simultaneously read and verify all the data it has added.
We further enhanced the tester to randomly select and kill processes in the cluster and verify that successfully returned database transactions were indeed written.
This helped catch a lot of issues, and is still our first method of testing changes.
Although our common cluster contains many servers operating in a distributed fashion, our local development verification commonly consists of unit tests and single-server setups.
We were concerned about discrepancies between single-server setups and truly distributed scenarios.
We created a utility called HBase Verify to run simple CRUD workloads on a live server.
This allows us to exercise simple API calls and run load tests in a couple of minutes.
This utility is even more important for our dark launch clusters, where algorithms are first evaluated at a large scale.
In particular, RegionServer metrics are far more useful for evaluating the health of the cluster than HMaster or ZooKeeper metrcs.
HBase already had a number of metrics exported through JMX.
However, all the metrics were for short-running operations such as log writes and RPC requests.
We needed to add metrics to monitor long-running events such as compactions, flushes, and log splits.
A slightly innocuous metric that ended up being critical for monitoring was version information.
If a cluster crash happens, we need to understand if any functionality was specific to that cluster.
Also, rolling upgrades mean that the running version and the installed version are not necessarily the same.
We therefore keep track of both versions and signify when they are different.
When learning a new system, we needed to determine which features we should utilize immediately and which features we could postpone adopting.
HBase offers a feature called automatic splitting, which partitions a single region into 2 regions when its size grows too large.
We decided that automatic splitting was an optional feature for our use case and developed manual splitting.
On table creation, we pre-split a table into a specific number of equally sized regions.
When the average region size becomes too large, we initiate rolling splits of these regions.
Since our data grows roughly uniform across all regions, it's easy for automatic splitting to cause split and compaction storms as the regions all roughly hit the same data size at the same time.
With manual splits, we can stagger splits across time and thereby spread out the network IO load typically generated by the splitting process.
Since the number of regions is known at any given point in time, long-term debugging and profiling is much easier.
It is hard to trace the logs to understand region level problems if regions keep splitting and getting renamed.
When we first started using HBase, we would occasionally run into problems with Log Recovery where some log files may be left unprocessed on region failover.
Manual post-mortem recovery from such unexpected events is much easier if the regions have not been split (automatically) since then.
We can go back to the affected region and replay unprocessed logs.
In doing this, we also leverage the Trash facility in HDFS that retains deleted files for a configurable time period.
We have metrics being exported by JMX, but we needed an easy way to visualize these metrics and analyze cluster health over time.
We decided to utilize ODS, an internal tool similar to Ganglia, to visualize important metrics as line graphs.
Graphing min/max is vital because it identifies misbehaving RegionServers, which may cause the application server processing queue to congest.
The greatest benefit is that we can observe statistics in realtime to observe how the cluster reacts to any changes in the workload (for example, running a Hadoop MapReduce job or splitting regions)
Additionally, we have a couple different cross-cluster dashboards that we use for high-level analysis.
We place vital stats of all clusters in a single overview dashboard to provide a broad health snapshot.
We also realized after exceeding a half-dozen clusters that we needed some way to visualize our version differences.
We display the HMaster version, HDFS Client version, NameNode version, and JobTracker version for each cluster on 4 different heat maps.
This allows us to scan our versions for consistency and sorting allows us to identify legacy builds that may have known bugs.
How do we take regular backups of this large dataset? One option is to copy and replicate the data from one HDFS cluster to another.
Since this approach is not continuous, there is a possibility that data is already corrupt in HDFS before the next backup event.
Instead, we decided to enhance the application to continuously generate an alternate application log.
This log is transported via Scribe and stored in a separate HDFS cluster that is used for web analytics.
This is a reliable and time-tested data capture pipeline, especially because we have been using the same software stack to capture and transport huge volumes of click-logs from our web application to our Hive analytics storage.
The records in this application log are idempotent, and can be applied multiple times without any data loss.
In the event of a data loss problem in HBase, we can replay this log-stream and regenerate the data in HBase.
HBase currently does not support online schema changes to an existing table.
This means that if we need to add a new column family to an existing table, we have to stop access to the table, disable the table, add new column families, bring the table back online and then restart the load.
This is a serious drawback because we do not have the luxury of stopping our workload.
Instead, we have pre-created a few additional column families for some our core HBase tables.
The application currently does not store any data into these column families, but can use them in the future.
The main cause of network traffic would then be the shuffle of the map output.
This problem was solved by GZIP compressing the intermediate map output.
After running in production for a couple months, we quickly realized from our dashboards that we were network IO bound.
We needed some way to analyze where our network IO traffic was coming from.
We utilized a combination of JMX statistics and log scraping to estimate total network IO on a single RegionServer for a 24-hour period.
We found a lot of lowhanging optimizations by observing these ratios.
We were able to get 40% network IO reduction by simply increasing our major compaction interval from every day to every week.
We also got big gains by excluding certain column families from being logged to the HLog.
Best effort durability sufficed for data stored in these column families.
The current state of the Hadoop Realtime Infrastructure has been the result of ongoing work over the last couple of years.
During this time a number of people at Facebook have made significant contributions and enhancements to these systems.
Also thanks are due to Andrew Ryan, Matthew Welty and Paul Tuckfield for doing a lot of work on operations, monitoring and the statistics setup that makes these tasks easy.
Thanks are also due to Gautam Roy, Aron Rivin, Prakash Khemani and Zheng Shao for their continued support and enhancements to various pieces of the storage stack.
Acknowledgements are also due to Patrick Kling for implementing a test suite for HDFS HA as part of his internship at Facebook.
Last but not the least, thanks are also due to the users of our infrastructure who have patiently dealt with periods of instability during its evolution and have provided valuable.
