For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.
When I was an undergrad in electrical engineering, I discovered digital signal processing and gravitated toward it.
I found out that music, video, photos, and lots of other stuff could be viewed as data.
Over time, I continued to get excited by new aspects of data.
The last few years had exposed me to social and big data.
Previously I had learned to look at data from a statistician’s point of view, and new types of data had “only” asked for new mathematical methods.
It wasn’t simple, but at least I had been trained for that, and there was also a wealth of resources to tap into.
Big data, on the other hand, was about system-level innovations and new ways of programming.
I wasn’t trained for it, and more importantly, I wasn’t alone.
Knowledge about handling big data in practice was somewhat of a black art.
This was true of many tools and techniques for scaling data processing, including caching (for example, memcached), replication, sharding, and, of course, MapReduce/ Hadoop.
I had spent the last few years getting up to speed on many of these skills.
Personally I have found the biggest hurdle to learning these techniques is in the middle of the learning curve.
And when you’re sufficiently well-versed, you’ll know how to ask additional questions to the mailing lists, meet experts at meetups or conferences, and even read the source code yourself.
But there’s a huge knowledge gap in the middle, when your.
This problem is especially acute for the newest technologies, like Hadoop.
Fortunately I’ve found Manning’s In Action series to be consistent with this objective, and they have excellent editors that helped me along the way.
I had a fun time writing this book, and I hope this is the beginning of your wonderful journey with Hadoop.
He led analytics at RockYou, and together we evangelized the use of Hadoop throughout the organization.
I learned a lot from him, and he even lent a hand in some of the initial writings.
I’ve been lucky to have many people contribute interesting case studies from outside the Web 2.0 industry.
I also want to thank the many reviewers of this book.
In particular, Paul O’Rorke was the technical reviewer that went beyond his call of duty and made some wonderful suggestions on how to make the manuscript better.
I look forward to seeing him author his own book at some point.
His expertise in databases and large-scale systems provided a broad perspective to understanding the capabilities of Hadoop.
The other reviewers who read the manuscript numerous times during development and whom I’d like to thank for their invaluable feedback include the following: Paul Stusiak, Philipp K.
I’ve been blessed to work with a wonderful group of people at Manning.
Special thanks go out to Troy Mott, who got me started on this writing project and has been patient enough to see me finish it.
I couldn’t imagine a better group of people to work with.
Needless to say, all the people who contribute to Hadoop and help grow its ecosystem deserve praise.
Doug Cutting got it all started, and Yahoo had the foresight to support it early on.
Cloudera is now bringing Hadoop to a broader enterprise audience.
It’s an exciting time to be part of this growing Hadoop community.
Last, but not least, I want to thank all my friends, family, and colleagues for supporting me throughout the writing of this book.
Hadoop is an open source framework implementing the MapReduce algorithm behind Google’s approach to querying the distributed data sets that constitute the internet.
This definition naturally leads to an obvious question: What are maps and why do they need to be reduced  ? Massive data sets can be extremely difficult to analyze and query using traditional mechanisms, especially when the queries themselves are quite complicated.
In effect, the MapReduce algorithm breaks up both the query and the data set into constituent parts—that’s the mapping.
This book teaches readers how to use Hadoop and write MapReduce programs.
The intended readers are programmers, architects, and project managers who have to process large amounts of data offline.
This book guides the reader from obtaining a copy of Hadoop to setting it up in a cluster and writing data analytic programs.
The book begins by making the basic idea of Hadoop and MapReduce easier to grasp by applying the default Hadoop installation to a few easy-to-follow tasks, such as analyzing changes in word frequency across a body of documents.
The book continues through the basic concepts of MapReduce applications developed using Hadoop, including a close look at framework components, use of Hadoop for a variety of data analysis tasks, and numerous examples of Hadoop in action.
MapReduce is a complex idea both conceptually and in its implementation, and Hadoop users are challenged to learn all the knobs and levers for running Hadoop.
This book takes you beyond the mechanics of running Hadoop, teaching you to write meaningful programs in a MapReduce framework.
This book assumes the reader will have a basic familiarity with Java, as most code examples will be written in Java.
Familiarity with basic statistical concepts (e.g., histogram, correlation) will help the reader appreciate the more advanced data processing examples.
Roadmap The book has 12 chapters divided into three parts.
Part 1 consists of three chapters which introduce the Hadoop framework, covering the basics you’ll need to understand and use Hadoop.
The chapters describe the hardware components that make up a Hadoop cluster, as well as the installation and configuration to create a working system.
Part 1 also covers the MapReduce framework at a high level and gets your first MapReduce program up and running.
In these chapters we explore various examples of using Hadoop to analyze a patent data set, including advanced algorithms such as the Bloom filter.
We also cover programming and administration techniques that are uniquely useful to working with Hadoop in production.
Cloud services provide an alternative to buying and hosting your own hardware to create a Hadoop cluster and any add-on packages provide higher-level programming abstractions over MapReduce.
Finally, we look at several case studies where Hadoop solves real business problems in practice.
An appendix contains a listing of HDFS commands along with their descriptions and usage.
Code conventions and downloads All source code in listings or in text is in a fixed-width font like this to separate it from ordinary text.
Code annotations accompany many of the listings, highlighting the listing.
The code for the examples in this book is available for download from the publisher’s website at www.manning.com/HadoopinAction.
Author Online The purchase of Hadoop in Action includes free access to a private forum run by Manning Publications where you can make comments about the book, ask technical questions, and receive help from the author and other users.
You can access and subscribe to the forum at www.manning.com/HadoopinAction.
This page provides information on how to get on the forum once you’re registered, what kind of help is available, and the rules of conduct in the forum.
Manning’s commitment to our readers is to provide a venue where a meaningful dialogue between individual readers and between readers and the author can take place.
It isn’t a commitment to any specific amount of participation on the part of the author, whose contribution to the book’s forum remains voluntary (and unpaid)
We suggest you try asking the author some challenging questions, lest his interest stray!
The Author Online forum and the archives of previous discussions will be accessible from the publisher’s website as long as the book is in print.
About the author Chuck Lam is currently founding a mobile social networking startup called RollCall.
There he developed social applications and data processing infrastructure handling hundreds of millions of users.
He applied A/B testing and statistical analysis to tune the virality of social apps.
He also optimized RockYou’s ad network with the use of real-time and social data.
He was able to improve response rates dramatically, sometimes by an order of magnitude.
Chuck first got interested in big data when he began his PhD study at Stanford.
He learned of the significant effect big data has on machine learning and began to explore its consequences.
The illustrations were obtained from a helpful librarian at the Ethnographic Museum in Split, itself situated in the Roman core of the medieval center of the town: the ruins of Emperor Diocletian’s retirement palace from around AD 304
The book includes finely colored illustrations of figures from different regions of Croatia, accompanied by descriptions of the costumes and of everyday life.
Kistanja is a small town located in Bukovica, a geographical region in Croatia.
It is situated in northern Dalmatia, an area rich in Roman and Venetian history.
Dress codes and lifestyles have changed over the last 200 years, and the diversity by region, so rich at the time, has faded away.
It is now hard to tell apart the inhabitants of different continents, let alone of different hamlets or towns separated by only a few miles.
Perhaps we have traded cultural diversity for a more varied personal life—certainly for a more varied and fast-paced technological life.
Manning celebrates the inventiveness and initiative of the computer business with book covers based on the rich diversity of regional life of two centuries ago, brought back to life by illustrations from old books and collections like this one.
Part 1 of this book introduces the basics for understanding and using Hadoop.
We describe the hardware components that make up a Hadoop cluster, as well as the installation and configuration to create a working system.
We cover the MapReduce framework at a high level and get your first MapReduce program up and running.
This chapter covers ■ The basics of writing a scalable,
People upload videos, take pictures on their cell phones, text friends, update their Facebook status, leave comments around the web, click on ads, and so forth.
Machines, too, are generating and keeping more and more data.
You may even be reading this book as digital data on your computer screen, and certainly your purchase of this book is recorded as data with some retailer.1
The exponential growth of data first presented challenges to cutting-edge businesses such as Google, Yahoo, Amazon, and Microsoft.
They needed to go through terabytes   and petabytes   of data to figure out which websites were popular, what books were in demand, and what kinds of ads appealed to people.
Existing tools were becoming inadequate to process such large data sets.
Google was the first to publicize MapReduce—a system they had used to scale their data processing needs.
Of course, you’re reading a legitimate copy of this, right?
This system aroused a lot of interest because many other businesses were facing similar scaling challenges, and it wasn’t feasible for everyone to reinvent their own proprietary tool.
Doug Cutting   saw an opportunity and led the charge to develop an open source version of this MapReduce system called Hadoop.
Soon after, Yahoo and others rallied around to support this effort.
Today, Hadoop is a core part of the computing infrastructure for many web companies, such as Yahoo , Facebook , LinkedIn , and Twitter.
Many more traditional businesses, such as media and telecom, are beginning to adopt this system too.
Our case studies in chapter 12 will describe how companies including New York Times , China Mobile , and IBM are using Hadoop.
Hadoop, and large-scale distributed data processing in general, is rapidly becoming an important skill set for many programmers.
An effective programmer, today, must have knowledge of relational databases, networking, and security, all of which were considered optional skills a couple decades ago.
Similarly, basic understanding of distributed data processing will soon become an essential part of every programmer’s toolbox.
Leading universities, such as Stanford   and CMU,   have already started introducing Hadoop into their computer science curriculum.
This book will help you, the practicing programmer, get up to speed on Hadoop quickly and start using it to process your data sets.
This chapter introduces Hadoop more formally, positioning it in terms of distributed systems and data processing systems.
A simple word counting example with existing tools highlights the challenges around processing data at large scale.
You’ll implement that example using Hadoop to gain a deeper appreciation of Hadoop’s simplicity.
We’ll also discuss the history of Hadoop and some perspectives on the MapReduce paradigm.
But let me first briefly explain why I wrote this book and why it’s useful to you.
The documentation at the official Hadoop site is fairly comprehensive, but it isn’t always easy to find straightforward answers to straightforward questions.
The purpose of writing the book is to address this problem.
Instead I will provide the information that will allow you to quickly create useful code, along with more advanced topics most often encountered in practice.
Distributed computing is a wide and varied field, but the key distinctions of Hadoop are that it is.
Figure 1.1 A Hadoop cluster has many parallel machines that store and process large data sets.
Client computers send jobs into this computer cloud and obtain results.
Robust—Because it is intended to run on commodity hardware, Hadoop is architected with the assumption of frequent hardware malfunctions.
Scalable—Hadoop scales linearly to handle larger data by adding more nodes to the cluster.
Hadoop’s accessibility and simplicity give it an edge over writing and running large distributed programs.
Even college students can quickly and cheaply create their own Hadoop cluster.
On the other hand, its robustness and scalability make it suitable for even the most demanding jobs at Yahoo and Facebook.
These features make Hadoop popular in both academia and industry.
Figure 1.1 illustrates how one interacts with a Hadoop cluster.
As you can see, a Hadoop cluster is a set of commodity machines networked together in one location.2
Data storage and processing all occur within this “cloud” of machines.
Different users can submit computing “jobs” to Hadoop from individual clients, which can be their own desktop machines in remote locations from the Hadoop cluster.
Not all distributed systems are set up as shown in figure 1.1
A brief introduction to other distributed systems will better showcase the design philosophy behind Hadoop.
While not strictly necessary, machines in a Hadoop cluster are usually relatively homogeneous x86 Linux boxes.
And they’re almost always located in the same data center, often in the same set of racks.
Moore’s law   suited us well for the past decades, but building bigger and bigger servers is no longer necessarily the best solution to large-scale problems.
An alternative that has gained popularity is to tie together many low-end/commodity machines together as a single functional distributed system.
To understand the popularity of distributed systems (scale-out)  vis-à-vis huge monolithic servers (scale-up), consider the price performance of current I/O technology.
With a modest degree of replication, the cluster machines can read the data set in parallel and provide a much higher throughput.
And such a cluster of commodity machines turns out to be cheaper than one high-end server!
The preceding explanation showcases the efficacy of Hadoop relative to monolithic systems.
Now let’s compare Hadoop to other architectures for distributed systems.
SETI@home , where screensavers around the globe assist in the search for extraterrestrial life, represents one well-known approach.
In SETI@home, a central server stores radio signals from space and serves them out over the internet to client desktop machines to look for anomalous signs.
This approach moves the data to where computation will take place (the desktop screensavers)
After the computation, the resulting data is moved back for storage.
Hadoop differs from schemes such as SETI@home in its philosophy toward data.
SETI@home requires repeat transmissions of data between clients and servers.
This works fine for computationally intensive work, but for data-intensive processing, the size of data becomes too large to be moved around easily.
Hadoop focuses on moving code to data instead of vice versa.
Referring to figure 1.1 again, we see both the data and the computation exist within the Hadoop cluster.
The clients send only the MapReduce programs to be executed, and these programs are usually small (often in kilobytes)
More importantly, the move-code-to-data   philosophy applies within the Hadoop cluster itself.
Data is broken up and distributed across the cluster, and as much as possible, computation on a piece of data takes place on the same machine where that piece of data resides.
This move-code-to-data philosophy makes sense for the type of data-intensive processing Hadoop is designed for.
The programs to run (“code”) are orders of magnitude smaller than the data and are easier to move around.
Also, it takes more time to move data across a network than to apply the computation to it.
Let the data remain where it is and move the executable code to its hosting machine.
Now that you know how Hadoop fits into the design of distributed systems, let’s see how it compares to data processing systems, which usually means SQL databases.
Given that Hadoop is a framework for processing data, what makes it better than standard relational databases, the workhorse of data processing in most of today’s applications? One reason is that SQL   (structured query language) is by design targeted at structured data.
Many of Hadoop’s initial applications deal with unstructured data such as text.
From this perspective Hadoop provides a more general paradigm than SQL.
For working only with structured data, the comparison is more nuanced.
In principle, SQL and Hadoop can be complementary, as SQL is a query language which can be implemented on top of Hadoop as the execution engine.
But in practice, SQL databases tend to refer to a whole set of legacy technologies, with several dominant vendors, optimized for a historical set of applications.
Many of these existing commercial databases are a mismatch to the requirements that Hadoop targets.
With that in mind, let’s make a more detailed comparison of Hadoop with typical SQL databases on specific dimensions.
To run a bigger database you need to buy a bigger machine.
More importantly, the high-end machines are not cost effective for many applications.
For example, a machine with four times the power of a standard PC costs a lot more than putting four such PCs in a cluster.
Hadoop is designed to be a scale-out architecture operating on a cluster of commodity PC machines.
Adding more resources means adding more machines to the Hadoop cluster.
Hadoop clusters with ten to hundreds of machines is standard.
In fact, other than for development purposes, there’s no reason to run Hadoop on a single server.
A fundamental tenet of relational databases is that data resides in tables having relational structure defined by a schema.
Although the relational model has great formal properties, many modern applications deal with data types that don’t fit well into this model.
Hadoop uses key/value pairs   as its basic data unit, which is flexible enough to work with the less-structured data types.
In Hadoop, data can originate in any form, but it eventually transforms into (key/value) pairs for the processing functions to work on.
You query data by stating the result you want and let the database engine figure out how to derive it.
This is in fact a hot area within the Hadoop community, and we’ll cover some of the leading projects in chapter 11
Under SQL you have query statements; under MapReduce you have scripts and codes.
MapReduce allows you to process data in a more general fashion than SQL queries.
For example, you can build complex statistical models   from your data or reformat your image data.
On the other hand, when working with data that do fit well into relational structures, some people may find MapReduce less natural to use.
Those who are accustomed to the SQL paradigm may find it challenging to think in the MapReduce way.
I hope the exercises and the examples in this book will help make MapReduce programming more intuitive.
But note that many extensions are available to allow one to take advantage of the scalability of Hadoop while programming in more familiar paradigms.
In fact, some enable you to write queries in a SQL-like language, and your query is automatically compiled into MapReduce code for execution.
Hadoop is designed for offline processing   and analysis of large-scale data.
It doesn’t work for random reading and writing of a few records, which is the type of load for online transaction processing.
In fact, as of this writing (and in the foreseeable future), Hadoop is best used as a write-once , read-many-times   type of data store.
In this aspect it’s similar to data warehouses in the SQL world.
You have seen how Hadoop relates to distributed systems and SQL databases at a high level.
You’re probably aware of data processing models such as pipelines   and message queues.
These models provide specific capabilities in developing different aspects of data processing applications.
Pipelines can help the reuse of processing primitives; simple chaining of existing modules creates new ones.
The programmer writes her data processing task as processing primitives in the form of either a producer or a consumer.
The timing of their execution is managed by the system.
Its greatest advantage is the easy scaling of data processing over multiple computing nodes.
Under the MapReduce model, the data processing primitives are called mappers  and reducers.
Decomposing a data processing application into mappers and reducers is sometimes nontrivial.
But, once you write an application in the MapReduce form, scaling the application to run over hundreds, thousands, or even tens of thousands of machines in a cluster is merely a configuration change.
This simple scalability is what has attracted many programmers to the MapReduce model.
Before going through a formal treatment of MapReduce, let’s go through an exercise of scaling a simple program to process a large data set.
You’ll see the challenges of scaling a data processing program and will better appreciate the benefits of using a framework such as MapReduce to handle the tedious chores for you.
Our exercise is to count the number of times each word occurs in a set of documents.
In this example, we have a set of documents having only one document with only one sentence:
For each document, the words are extracted one by one using a tokenization process.
For each word, its corresponding function prints out all the entries in wordCount.
Many ways to say MapReduce Even though much has been written about MapReduce, one does not find the name itself written the same everywhere.
The original Google paper and the Wikipedia entry use the CamelCase version MapReduce.
However, Google itself has used Map Reduce in some pages on its website (for example, http://research.google.com/ roundtable/MR.html)
At the official Hadoop documentation site, one can find links pointing to a Map-Reduce Tutorial.
Clicking on the link brings one to a Hadoop Map/Reduce Tutorial (http://hadoop.apache.org/core/docs/current/mapred_ tutorial.html) explaining the Map/Reduce framework.
Writing variations also exist for the different Hadoop components such as NameNode (name node, namenode, and namenode), DataNode, JobTracker, and TaskTracker.
For the sake of consistency, we’ll go with CamelCase for all those terms in this book.
The word count we’re trying to generate is a canonical example of a multiset.
This program works fine until the set of documents you want to process becomes large.
For example, you want to build a spam filter   to know the words frequently used in the millions of spam emails you’ve received.
Looping through all the documents using a single computer will be extremely time consuming.
You speed it up by rewriting the program so that it distributes the work over several machines.
Each machine will process a distinct fraction of the documents.
When all the machines have completed this, a second phase of processing will combine the result of all the machines.
The pseudocode for the first phase, to be distributed over many machines, is.
That wasn’t too hard, right? But a few details may prevent it from working as expected.
First of all, we ignore the performance requirement of reading in the documents.
If the documents are all stored in one central storage server, then the bottleneck   is in the bandwidth of that server.
Having more machines for processing only helps up to a certain point—until the storage server can’t keep up.
You’ll also need to split up the documents among the set of processing machines such that each machine will process only those documents that are stored in it.
This will remove the bottleneck of a central storage server.
This reiterates the point made earlier about storage and processing having to be tightly coupled in data-intensive distributed applications.
Another flaw with the program is that wordCount (and totalWordCount) are stored in memory.
When processing large document sets, the number of unique words can exceed the RAM storage   of a machine.
The English language has about one million words, a size that fits comfortably into an iPod, but our word counting program will deal with many unique words not found in any standard English dictionary.
For example, we must deal with unique names such as Hadoop.
We have to count misspellings even if they are not real words (for example, exampel), and we count all different forms of a word separately (for example, eat, ate, eaten, and eating  )
Even if the number of unique words in the document set is manageable in memory, a slight change in the problem definition can explode the space complexity.
In the case of the latter, we’ll work with a multiset with billions of entries, which exceeds the RAM storage of most commodity computers.
Both bigrams and trigrams are important  in natural language processing.
This means we’ll implement a disk-based hash table, which involves a substantial amount of coding.
Furthermore, remember that phase two has only one machine, which will process wordCount sent from all the machines in phase one.
After we have added enough machines to phase one processing, the single machine in phase two will become the bottleneck.
The obvious question is, can we rewrite phase two in a distributed fashion so that it can scale by adding more machines?
To make phase two work in a distributed fashion, you must somehow divide its work among multiple machines   such that they can run independently.
You need to partition wordCount after phase one such that each machine in phase two only has to handle one partition.
In one example, let’s say we have 26 machines for phase two.
We assign each machine to only handle wordCount for words beginning with a particular letter in the alphabet.
For example, machine A in phase two will only handle word counting for words beginning with the letter a.
To enable this partitioning in phase two, we need a slight modification in phase one.
Instead of a single disk-based hash table for wordCount, we will need 26 of them: wordCount-a, wordCount-b, and so on.
After phase one, wordCount-a from each of the phase one machines will be sent to machine A of phase two, all the wordCount-b’s will be sent to machine B, and so on.
Each machine in phase one will shuffle its results among the machines in phase two.
To make it work across a cluster of distributed machines, we find that we need to add a number of functionalities:
Write a disk-based hash table permitting processing without being limited by.
Shuffle the partitions to the appropriate machines in phase two.
Partition the intermediate data   (that is, wordCount) from phase one.
This is a lot of work for something as simple as word counting, and we haven’t even touched upon issues like fault tolerance.
What if a machine fails in the middle of its task?) This is the reason why you would want a framework like Hadoop.
MapReduce programs are executed in two main phases, called mapping  and reducing.
Each phase is defined by a data processing function, and these functions are called mapper and reducer, respectively.
In the mapping phase, MapReduce takes the input data and feeds each data element to the mapper.
In the reducing phase, the reducer processes all the outputs from the mapper and arrives at a final result.
In simple terms, the mapper is meant to filter and transform the input into something that the reducer can aggregate over.
You may see a striking similarity here with the two phases we had to develop in scaling up word counting.
The MapReduce framework was designed after a lot of experience in writing scalable, distributed programs.
This two-phase design pattern was seen in scaling many programs, and became the basis of the framework.
In scaling our distributed word counting program in the last section, we also had to write the partitioning and shuffling functions.
Partitioning and shuffling are common design patterns that go along with mapping and reducing.
Unlike mapping and reducing, though, partitioning and shuffling are generic functionalities that are not too dependent on the particular data processing application.
The MapReduce framework provides a default implementation that works in most situations.
In order for mapping, reducing, partitioning, and shuffling (and a few others we haven’t mentioned) to seamlessly work together, we need to agree on a common structure for the data being processed.
It should be flexible and powerful enough to handle most of the targeted data processing applications.
MapReduce uses lists   and (key/value) pairs as its main data primitives.
The keys and values are often integers or strings but can also be dummy values to be ignored or complex object types.
The map and reduce functions must obey the following constraint on the types of keys and values.
In the MapReduce framework you write applications by specifying the mapper and reducer.
This input format may seem open-ended but is often quite simple in practice.
The list of (key/value) pairs is broken up and each individual (key/value) pair, the key k1 is often ignored by the mapper.
The details of this transformation largely determine what the MapReduce program does.
Note that the (key/value) pairs are processed in arbitrary order.
The transformation must be self-contained in that its output is dependent only on one single (key/value) pair.
As we’ll see, the latter approach is much easier to program.
The former approach may have some performance benefits, but let’s leave such optimization alone until we have fully grasped the MapReduce framework.
The framework asks the reducer to process each one of these aggregated (key/value) pairs individually.
This will not always be the case for other data processing applications.
Let’s rewrite the word counting program in MapReduce to see how all this fits together Listing 1.1 shows the pseudo-code.
Listing 1.1 Pseudo-code for map and reduce functions for word counting.
We’ve said before that the output of both map and reduce function are lists.
As you can see from the pseudo-code, in practice we use a special function in the framework further relieves the programmer from managing a large list.
The code looks similar to what we have in section 1.5.1, except this time it will actually work at scale.
Hadoop makes building scalable distributed programs easy, doesn’t it? Now let’s turn this pseudo-code into a Hadoop program.
Now that you know what the Hadoop and MapReduce framework is about, let’s get it running.
In this chapter, we’ll run Hadoop only on a single machine, which can be your desktop or laptop computer.
The next chapter will show you how to run Hadoop over a cluster of machines, which is what you’d want for practical deployment.
Running Hadoop on a single machine is mainly useful for development work.
Linux   is the official development and production platform for Hadoop, although Windows   is a supported development platform as well.
For a Windows box, you’ll need to install cygwin   (http://www-cygwin.com/) to enable shell and Unix scripts.
In fact, MacBook Pro seems to be the laptop of choice among Hadoop developers, as they’re ubiquitous in Hadoop conferences and user group meetings.
You can download the latest JDK for other operating systems from Sun at http://java.sun.com/javase/downloads/index.jsp.
Install it and remember the root of the Java installation, which we’ll need later.
To install Hadoop, first get the latest stable release at http://hadoop.apache.org/ core/releases.html.
After you unpack the distribution, edit the script conf/hadoopenv.sh to set JAVA_HOME to the root of the Java installation you have remembered from earlier.
For example, in Mac OS X, you’ll replace this line.
Let’s run it without any arguments to see its usage documentation:
We’ll cover the various Hadoop commands in the course of this book.
For our current purpose, we only need to know that the command to run a (     Java) Hadoop program is bin/hadoop jar <jar>
As the command implies, Hadoop programs written in Java are packaged in jar files   for execution.
Fortunately for us, we don’t need to write a Hadoop program first; the default installation already has several sample programs we can use.
The following command shows what is available in the examples jar file:
You’ll see about a dozen example programs prepackaged with Hadoop, and one of them is a word counting program called...
We’ll see how this Java program implements the word counting map and reduce functions we had in pseudo-code in listing 1.1
We’ll modify this program to understand how to vary its behavior.
For now we’ll assume it works as expected and only follow the mechanics of executing a Hadoop program.
Without specifying any arguments, executing wordcount will show its usage information:
To execute wordcount, we need to first create an input directory:
For illustration, let’s put the text version of the 2002 State of the Union address, obtained from http://www.gpoaccess.gov/sou/
We now analyze its word counts and see the results:
You’ll see a word count of every word used in the document, listed in alphabetical order.
This is not bad considering you have not written a single line of code yet! But, also note a number of shortcomings in the included wordcount program.
Tokenization is based purely on whitespace characters   and not punctuation marks, making States, States., and States: separate words.
The same is true for capitalization, where States and states appear as separate words.
Furthermore, we would like to leave out words that show up in the document only once or twice.
Fortunately, the source code for wordcount is available and included in the installation at src/examples/org/apache/hadoop/examples/WordCount.java.
Let’s first set up a directory structure for our playground and make a copy of the program.
Before we make changes to the program, let’s go through compiling and executing this new copy in the Hadoop framework.
You’ll have to remove the output directory each time you run this Hadoop command, because it is created automatically.
As we haven’t changed any program code, the result should be the same as before.
We’ve only compiled our own copy rather than running the precompiled version.
Now we are ready to modify WordCount to add some extra features.
Listing 1.2 is a partial view of the WordCount.java program.
This distinction may not even be apparent from looking at WordCount.java as it’s Hadoop’s default configuration.
The map and reduce functions are inside inner classes of WordCount.
You may notice we use special classes such as LongWritable , IntWritable , and Text instead of the more familiar Long, Integer, and String classes of Java.
The new classes have additional serialization capabilities needed by Hadoop’s internal.
The changes we want to make to the program are easy to spot.
We see q that WordCount uses Java’s StringTokenizer in its default setting, which tokenizes based only on whitespaces.
To ignore standard punctuation marks, we add them to the StringTokenizer’s list of delimiter characters:
When looping through the set of tokens, each token is extracted and cast into a Text object w.
Again, in Hadoop, the special class Text is used in place of String.) We want the word count to ignore capitalization, so we lowercase all the words before turning them into Text objects.
Finally, we want only words that appear more than four times.
After making changes to those three lines, you can recompile the program and execute it again.
Many of these words appear frequently in almost any English text.
Hadoop started out as a subproject of Nutch , which in turn was a subproject of Apache Lucene.
Doug Cutting   founded all three projects, and each project was a logical progression of the previous one.
Given a text collection, a developer can easily add search capability to the documents using the Lucene engine.
Desktop search, enterprise search, and many domain-specific search engines have been built using Lucene.
It tries to build a complete web search engine using Lucene as its core component.
Nutch has parsers for HTML, a web crawler, a link-graph database, and other extra components necessary for a web search engine.
Doug Cutting envisions Nutch to be an open democratic alternative to the proprietary technologies in commercial offerings such as Google.
Besides having added components like a crawler and a parser, a web search engine differs from a basic document search engine in terms of scale.
Whereas Lucene is targeted at indexing millions of documents, Nutch should be able to handle billions of web pages without becoming exorbitantly expensive to operate.
Nutch will have to run on a distributed cluster of commodity hardware.
The challenge for the Nutch team is to address scalability issues in software.
Nutch needs a layer to handle distributed processing, redundancy, automatic failover, and load balancing.
Around 2004, Google published two papers describing the Google File System (GFS) and the MapReduce framework.
Google claimed to use these two technologies for scaling its own search system.
Doug Cutting immediately saw the applicability of these technologies to Nutch, and his team implemented the new framework and ported Nutch to it.
It started to handle several hundred million web pages and could run on clusters of dozens of nodes.
Doug realized that a dedicated project to flesh out the two technologies was needed to get to web scale, and Hadoop was born.
What’s up with the names? When naming software projects, Doug Cutting seems to have been inspired by his family.
Lucene is his wife’s middle name, and her maternal grandmother’s first name.
His son, as a toddler, used Nutch as the all-purpose word for meal and later named a yellow stuffed elephant Hadoop.
Doug said he “was looking for a name that wasn’t already a web domain and wasn’t trademarked, so I tried various words that were in my life but not used by anybody else.
Hadoop is a versatile tool that allows new users to access the power of distributed computing.
By using distributed storage and transferring code instead of data, Hadoop avoids the costly transmission step when working with large data sets.
Moreover, the redundancy of data allows Hadoop to recover should a single node fail.
You have seen the ease of creating programs with Hadoop using the MapReduce framework.
What is equally important is what you didn’t have to do—worry about partitioning the data, determining which nodes will perform which tasks, or handling communication between nodes.
Hadoop handles this for you, leaving you free to focus on what’s most.
In the next chapter we’ll go into further details about the internals of Hadoop and setting up a working Hadoop cluster.
The original papers on the Google File System and MapReduce are well worth reading.
This chapter will serve as a roadmap to guide you through setting up Hadoop.
If you work in an environment where someone else sets up the Hadoop cluster for you, you may want to skim through this chapter.
You’ll want to understand enough to set up your personal development machine, but you can skip through the details of configuring the communication and coordination of various nodes.
Section 2.3 will focus on the three operational modes of Hadoop and how to set them up.
You’ll read about web-based tools that assist monitoring your cluster in section 2.4
We’ve discussed the concepts of distributed storage and distributed computation in the previous chapter.
These daemons have specific roles; some exist only on one server, some exist across multiple servers.
Let’s begin with arguably the most vital of the Hadoop daemons—the NameNode.
Hadoop employs a master/slave architecture   for both distributed storage and distributed computation.
The distributed storage system  is called the Hadoop File System , or HDFS.
The NameNode is the master of HDFS that directs the slave DataNode daemons to perform the low-level I/O tasks.
The NameNode is the bookkeeper of HDFS; it keeps track of how your files are broken down into file blocks, which nodes store those blocks, and the overall health of the distributed filesystem.
The function of the NameNode is memory and I/O intensive.
As such, the server hosting the NameNode typically doesn’t store any user data or perform any computations for a MapReduce program to lower the workload on the machine.
This means that the NameNode server doesn’t double as a DataNode or a TaskTracker.
There is unfortunately a negative aspect to the importance of the NameNode—it’s a single point of failure of your Hadoop cluster.
For any of the other daemons, if their host nodes fail for software or hardware reasons, the Hadoop cluster will likely continue to function smoothly or you can quickly restart it.
Each slave machine in your cluster will host a DataNode   daemon to perform the grunt work of the distributed filesystem—reading and writing HDFS blocks to actual files on the local filesystem.
When you want to read or write a HDFS file, the file is broken into blocks and the NameNode will tell your client which DataNode each block resides in.
Your client communicates directly with the DataNode daemons to process the local files corresponding to the blocks.
Furthermore, a DataNode may communicate with other DataNodes to replicate its data blocks for redundancy.
Figure 2.1 illustrates the roles of the NameNode and DataNodes.
The content of the files are distributed among the DataNodes.
The NameNode keeps track of the file metadata—which files are in the system and how each file is broken down into blocks.
The DataNodes provide backup store of the blocks and constantly report to the NameNode to keep the metadata current.
This ensures that if any one DataNode crashes or becomes inaccessible over the network, you’ll still be able to read the files.
Upon initialization, each of the DataNodes informs the NameNode of the blocks it’s currently storing.
After this mapping is complete, the DataNodes continually poll the NameNode to provide information regarding local changes as well as receive instructions to create, move, or delete blocks from the local disk.
The Secondary NameNode (SNN)   is an assistant daemon for monitoring the state of the cluster HDFS.
Like the NameNode, each cluster has one SNN, and it typically resides on its own machine as well.
No other DataNode or TaskTracker daemons run on the same server.
The SNN differs from the NameNode in that this process doesn’t receive or record any real-time changes to HDFS.
Instead, it communicates with the NameNode to take snapshots of the HDFS   metadata at intervals defined by the cluster configuration.
As mentioned earlier, the NameNode is a single point of failure for a Hadoop cluster, and the SNN snapshots   help minimize the downtime and loss of data.
Nevertheless, a NameNode failure requires human intervention to reconfigure the cluster to use the SNN as the primary NameNode.
We’ll discuss the recovery process in chapter 8 when we cover best practices for managing your cluster.
The JobTracker   daemon is the liaison between your application and Hadoop.
Once you submit your code to your cluster, the JobTracker determines the execution plan by determining which files to process, assigns nodes to different tasks, and monitors all tasks as they’re running.
Should a task fail, the JobTracker will automatically relaunch the task, possibly on a different node, up to a predefined limit of retries.
It’s typically run on a server as a master node of the cluster.
As with the storage daemons, the computing daemons also follow a master/slave architecture: the JobTracker is the master overseeing the overall execution of a MapReduce job and the TaskTrackers   manage the execution of individual tasks on each slave node.
Each TaskTracker is responsible for executing the individual tasks that the JobTracker assigns.
Although there is a single TaskTracker per slave node, each TaskTracker can spawn multiple JVMs   to handle many map or reduce tasks in parallel.
One responsibility of the TaskTracker is to constantly communicate with the JobTracker.
If the JobTracker fails to receive a heartbeat from a TaskTracker within a specified amount of time, it will assume the TaskTracker has crashed and will resubmit the corresponding tasks to other nodes in the cluster.
After a client calls the JobTracker to begin a data processing job, the JobTracker partitions the work and assigns different map and reduce tasks to each TaskTracker in the cluster.
It’s a master/slave architecture in which the NameNode and JobTracker are masters and the DataNodes and TaskTrackers are slaves.
Having covered each of the Hadoop daemons, we depict the topology of one typical Hadoop cluster in figure 2.3
This topology features a master node running the NameNode and JobTracker daemons and a standalone node   with the SNN in case the master node fails.
For small clusters, the SNN can reside on one of the slave nodes.
On the other hand, for large clusters, separate the NameNode and JobTracker on two machines.
The slave machines each host a DataNode and TaskTracker, for running tasks on the same node where their data is stored.
We’ll work toward setting up a complete Hadoop cluster of this form by first establishing the master node and the control channels between nodes.
If a Hadoop cluster is already available to you, you can skip the next section on how to set up Secure Shell (SSH) channels between nodes.
You also have a couple of options to run Hadoop using only a single machine, in what are known as standalone and pseudo-distributed modes.
Configuring Hadoop to run in these two modes or the standard cluster setup (fully distributed mode) is covered in section 2.3
When setting up a Hadoop cluster , you’ll need to designate one specific node as the master node.
As shown in figure 2.3, this server will typically host the NameNode and.
It’ll also serve as the base station contacting and activating the DataNode and TaskTracker daemons on all of the slave nodes.
As such, we need to define a means for the master node to remotely access every node in your cluster.
The public key   is stored locally on every node in the cluster, and the master node sends the private key   when attempting to access a remote machine.
With both pieces of information, the target machine can validate the login attempt.
We’ve been speaking in general terms of one node accessing another; more precisely this access is from a user account on one node to another user account on the target machine.
For Hadoop, the accounts should have the same username on all of the nodes (we use hadoop-user in this book), and for security purpose we recommend it being a user-level account.
Once the cluster daemons are up and running, you’ll be able to run your actual MapReduce jobs from other accounts.
The first step is to check whether SSH is installed on your nodes.
We can easily do this by use of the "which" UNIX command:
If you instead receive an error message such as this,
Better yet, have your system administrator do it for you.)
Having verified that SSH is correctly installed on all nodes of the cluster, we use sshkeygen on the master node to generate an RSA key pair.
Be certain to avoid entering a passphrase, or you’ll have to manually enter that phrase every time the master node attempts to access another node.
Enter file in which to save the key (/home/hadoop-user/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again:
After creating your key pair, your public key will be of the form.
Albeit a bit tedious, you’ll next need to copy the  public key to every slave node as well as the master node:
Manually log in to the target node and set the master key as an authorized key (or append to the list of authorized keys if you have others defined)
After generating the key, you can verify it’s correctly defined by attempting to log in to the target node from the master:
Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'target' (RSA) to the list of known hosts.
After confirming the authenticity of a target node to the master node, you won’t be prompted upon subsequent login attempts.
We’ve now set the groundwork for running Hadoop on your own cluster.
Let’s discuss the different Hadoop modes you might want to use for your projects.
We need to configure a few things before running Hadoop.
Let’s take a closer look at the Hadoop configuration directory :
The first thing you need to do is to specify the location of Java on all the nodes including the master.
In hadoop-env.sh   define the JAVA_HOME   environment variable to point to the Java installation directory.
If you followed the examples in chapter 1, you’ve already completed this step.) The hadoop-env.sh file contains other variables for defining your Hadoop.
The default settings on the other variables will probably work fine.
As you become more familiar with Hadoop you can later modify this file to suit your individual needs (logging directory location, Java class path, and so on)
The majority of Hadoop settings are contained in XML configuration files.
Before version 0.20, these XML files are hadoop-default.xml   and hadoop-site.xml.
As the names imply, hadoop-default.xml contains the default Hadoop   settings to be used unless they are explicitly overridden in hadoop-site.xml.
In version 0.20 this file has been separated out into three XML files: core-site.xml , hdfs-site.xml , and mapred-site.xml.
This refactoring better aligns the configuration settings to the subsystem of Hadoop that they control.
In the rest of this chapter we’ll generally point out which of the three files used to adjust a configuration setting.
If you use an earlier version of Hadoop, keep in mind that all such configuration settings are modified in hadoop-site.xml.
In the following subsections we’ll provide further details about the different operational modes of Hadoop and example configuration files for each.
When you first uncompress the Hadoop source package, it’s ignorant of your hardware setup.
Hadoop chooses to be conservative and assumes a minimal configuration.
All three XML files (or hadoopsite.xml before version 0.20) are empty under this default mode:
With empty configuration files, Hadoop will run completely on the local machine.
Because there’s no need to communicate with other nodes, the standalone mode doesn’t use HDFS, nor will it launch any of the Hadoop daemons.
Its primary use is for developing and debugging the application logic of a MapReduce program without the additional complexity of interacting with the daemons.
When you ran the example MapReduce program in chapter 1, you were running it in standalone mode.
This mode complements the standalone mode for debugging your code, allowing you to examine memory usage, HDFS input/output issues, and other daemon interactions.
Listing 2.1 provides simple XML files to configure a single server in this mode.
Listing 2.1 Example of the three configuration files for pseudo-distributed mode.
In core-site.xml and mapred-site.xml we specify the hostname and port of the NameNode and the JobTracker, respectively.
In hdfs-site.xml we specify the default replication factor for HDFS, which should only be one because we’re running on only one node.
We must also specify the location of the Secondary NameNode   in the masters file and the slave nodes   in the slaves file:
While all the daemons are running on the same machine, they still communicate with each other using the same SSH protocol as if they were distributed over a cluster.
Section 2.2 has a more detailed discussion of setting up the SSH   channels, but for single-node operation simply check to see if your machine already allows you to ssh back to itself.
But first you’ll need to format your HDFS by using the command.
We can now launch the daemons by use of the start-all.sh script.
The Java jps command will list all daemons to verify the setup was successful.
When you’ve finished with Hadoop you can shut  down the Hadoop daemons by the command.
Both standalone and pseudo-distributed modes are for  development and debugging purposes.
An actual Hadoop cluster runs in the third mode, the fully distributed mode.
After continually emphasizing the benefits of distributed storage and distributed computation, it’s time for us to set up a full cluster.
In the discussion below we’ll use the following server names:
We explicitly stated the hostname for location of the NameNode q and JobTracker w daemons.
We increased the HDFS replication factor   to take advantage of distributed storage e.
Recall that data is replicated across HDFS to increase availability and reliability.
We also need to update the  masters and slaves files   to reflect the locations of the other daemons.
Once you have copied these files across all the nodes in your cluster, be sure to format HDFS to prepare it for storage:
A practice that I found useful when starting with Hadoop was to use symbolic links to switch between Hadoop modes instead of constantly editing the XML files.
To do so, create a separate configuration folder for each of the modes and place the appropriate version of the XML files in the corresponding folder.
You can then switch between configurations by using the Linux ln command (e.g., ln -s conf.cluster conf)
This practice is also useful to temporarily pull a node out of the cluster to debug a MapReduce program in pseudo-distributed mode, but be sure that the modes have different file locations for HDFS and stop all daemons on the node before changing configurations.
Now that we’ve gone through all the settings to successfully get a Hadoop cluster up and running, we’ll introduce the Web UI for basic monitoring of the cluster’s state.
Having covered the operational modes of Hadoop, we can now introduce the web interfaces   that Hadoop provides to monitor the health of your cluster.
The browser interface allows you to access information you desire much faster than digging through logs and directories.
It gives you an overview of the state of your cluster’s HDFS.
From this interface, you can browse through the filesystem, check the status of each DataNode in your cluster, and peruse the Hadoop daemon logs   to verify your cluster is functioning correctly.
Hadoop provides a similar status overview of ongoing MapReduce jobs.
Again, a wealth of information is available through this reporting interface.
You can access the status of ongoing MapReduce tasks as well as detailed reports about completed jobs.
The latter is of particular importance—these logs describe which nodes performed which tasks and the time/resources required to complete each task.
Finally, the Hadoop configuration for each job is also available, as shown in figure 2.6
With all of this information you can streamline your MapReduce programs to better utilize the resources of your cluster.
From this interface you can browse through the HDFS filesystem, determine the storage available on each individual node, and monitor the overall health of your cluster.
This tool allows you to monitor active MapReduce jobs and access the logs of each map and reduce task.
The logs of previously submitted jobs are also available and are useful for debugging your programs.
This information is potentially useful when tuning parameters to optimize the performance of your programs.
Though the usefulness of these tools may not be immediately apparent at this stage, they’ll come in handy as you begin to perform more sophisticated tasks on your cluster.
You’ll realize their importance as we study Hadoop more in depth.
In this chapter we’ve discussed the key nodes and the roles they play within the Hadoop architecture.
You’ve learned how to configure your cluster, as well as manage some basic tools to monitor your cluster’s overall health.
Once you’ve formatted the NameNode for your cluster, you’ll (hopefully) never need to do so again.
Likewise, you shouldn’t keep altering the hadoop-site.xml configuration file for your cluster or assigning daemons to nodes.
In the next chapter, you’ll learn about the aspects of Hadoop you’ll be interacting with on a daily basis, such as managing files in HDFS.
With this knowledge you’ll be able to begin writing your own MapReduce applications and realize the true potential that Hadoop has to offer.
In the last chapter we looked at setting up and installing Hadoop.
We covered what the different nodes do and how to configure them to work with each other.
Now that you have Hadoop running, let’s look at the Hadoop framework from a programmer’s perspective.
If the previous chapter is like teaching you how to connect your turntable, your mixer, your amplifier, and your speakers together, then this chapter is about the techniques of mixing music.
We first cover HDFS, where you’ll store data that your Hadoop applications will process.
In chapter 1 we’ve already seen a MapReduce program, but we discussed the logic only at the conceptual level.
In this chapter we get to know the Java classes and methods, as well as the underlying processing steps.
We also learn how to read and write using different data formats.
You can store a big data set of (say) 100 TB as a single file in HDFS , something that would overwhelm most other filesystems.
We discussed in chapter 2 how to replicate the data for availability and distribute it over multiple machines to enable parallel processing.
As HDFS isn’t a native Unix filesystem, standard Unix file tools, such as ls and cp that work similarly to the Linux file commands.
In the next section we’ll discuss those Hadoop file shell commands, which are your primary interface with the HDFS system.
Section 3.1.2 covers Hadoop Java libraries for handling HDFS files programmatically.
Your MapReduce programs then process this data, but they usually don’t read any HDFS files directly.
Instead they rely on the MapReduce framework to read and parse the HDFS files into individual records (key/ value pairs ), which are the unit of data MapReduce programs do work on.
You rarely will have to programmatically read or write HDFS files except for custom import and export of data.
The command cmd is usually named after the corresponding Unix equivalent.
Let’s look at the most common file management tasks   in Hadoop, which include.
There are several ongoing projects that try to make HDFS mountable as a Unix filesystem.
As of this writing these projects aren’t officially part of Hadoop and they may not have the reliability needed for some production systems.
Some older documentation shows file utilities in the form of hadoop dfs -cmd <args>
Both dfs and fs are equivalent, although fs is the preferred form now.
Before you can run Hadoop programs on data stored in HDFS, you’ll need to put the data into HDFS first.
Let’s assume you’ve already formatted and started a HDFS filesystem.
For learning purposes, we recommend a pseudo-distributed configuration   as a playground.) Let’s create a directory and put a file in it.
This directory isn’t automatically created for you, though, so let’s create it with the mkdir command.
You should substitute your user name in the example commands.
It can be hdfs or file, to specify the HDFS filesystem or the local filesystem, respectively.
For HDFS, authority   is the NameNode host and path is the path of the file or directory of interest.
You can use the Hadoop cat command to show the content of that file:
As we’ll see shortly, most setups don’t need to specify the scheme://authority part of the URI.
When dealing with the local filesystem, you’ll probably prefer your standard Unix commands rather than the Hadoop file commands.
For copying files between the local filesystem and HDFS, Hadoop commands, such as put and get use the local filesystem as source and destination, respectively, without you specifying the file:// scheme.
For other commands, if you leave out the scheme://authority part of the URI, the default from the Hadoop configuration is used.
For example, if you have changed the conf/core-site.xml file to the pseudo-distributed configuration, your fs.default.name property in the file should be.
If you’re logged in as chuck, then shorten the URI hdfs://localhost:9000/user/chuck/example.txt to example.txt.
The Hadoop cat command   to show the content of the file is.
Hadoop’s mkdir command   automatically creates parent directories if they don’t already exist, similar to the Unix mkdir command with the -p option.
So the preceding command will create the /user directory too.
You’ll see this response showing the /user directory at the root / directory.
If you want to see all the subdirectories, in a way similar to Unix’s ls with the -r option, you can use Hadoop’s lsr command.
Now that we have a working directory, we can put a file into it.
Create some text file on your local filesystem called example.txt.
The Hadoop   command put is used to copy files from the local system into HDFS.
Note the period (.) as the last argument in the command above.
It means that we’re putting the file into the default working directory.
We can re-execute the recursive file listing command to see that the new file is added to HDFS.
In practice we don’t need to check on all files recursively, and we may restrict ourselves to what’s in our own working directory.
We would use the Hadoop ls command in its simplest form:
The output displays properties, such as permission, owner, group, file size, and last modification date, all of which are familiar Unix concepts.
The column stating “1” reports the replication factor   of the file.
For production clusters, the replication factor is typically 3 but can be any positive integer.
Replication factor is not applicable to directories, so they will only show a dash (-) for that column.
After you’ve put data into HDFS, you can run Hadoop programs to process it.
The output of the processing will be a new set of files in HDFS, and you’ll want to read or retrieve the results.
The Hadoop command get does the exact reverse of put.
Let’s say we no longer have the example.txt file locally and we want to retrieve it from HDFS; we can run the command.
Another way to access the data is to display it.
We can use the Hadoop file command with Unix pipes   to send its output for further processing by other Unix commands.
For example, if the file is huge (as typical Hadoop files are) and you’re interested in a quick check of its content, you can pipe the output of Hadoop’s cat into a Unix head.
After you finish working with files in HDFS, you may want to delete them to free up space.
You shouldn’t be too surprised by now that the Hadoop command for removing files is rm.
The rm command   can also be used to delete empty directories.
A list of Hadoop file commands, together with the usage and description of each command, is given in the appendix.
For the most part, the commands are modeled after their Unix equivalent.
You can execute hadoop fs (with no parameters) to get a complete list of all commands available on your version of Hadoop.
You can also use help to display the usage and a short description of each command.
If path is not specified, the contents of /user/<currentUser> will be listed.
Although the command line utilities are sufficient for most of your interaction with the HDFS filesystem, they’re not exhaustive and there’ll be situations where you may want deeper access into the HDFS API.
Let’s see how to do so in the next section.
To motivate an examination of the HDFS Java API, we’ll develop a PutMerge program for merging files   while putting them into HDFS.
The command line utilities don’t support this operation; we’ll use the API.
The motivation for this example came when we wanted to analyze Apache log files   coming from many web servers.
We can copy each log file into HDFS, but in general, Hadoop works more effectively with a single large file rather than a number of smaller ones.
Smaller” is relative here as it can still be tens or hundreds of gigabytes.) Besides, for analytics purposes we think of the log data as one big file.
That it’s spread over multiple files   is an incidental result of the physical web server architecture.
One solution is to merge all the files first and then copy the combined file into HDFS.
Unfortunately, the file merging will require a lot of disk space in the local machine.
It would be much easier if we could merge all the files on the fly as we copy them into HDFS.
Hadoop’s command line utilities include a getmerge command   for merging a number of HDFS files before copying them onto the local machine.
The main classes for file manipulation in Hadoop are in the package org.apache.
Basic Hadoop file operations include the familiar open, read, write, and close.
In fact, the Hadoop file API is generic and can be used for working with filesystems other than HDFS.
For our PutMerge program, we’ll use the Hadoop file API to both read the local filesystem and write to HDFS.
The starting point for the Hadoop   file API is the FileSystem class.
This is an abstract class for interfacing with the filesystem, and there are different concrete subclasses for handling HDFS and the local filesystem.
You get the desired FileSystem instance by calling the factory method FileSystem.get(Configuration conf)
The Configuration class is a special class for holding key/value configuration parameters.
Its default instantiation is based on the resource configuration for your HDFS system.
We can get the FileSystem object to interface with HDFS by.
To get a FileSystem object specifically for the local filesystem, there’s the FileSystem.
Hadoop file API uses Path objects   to encode file and directory names and FileStatus objects   to store metadata for files and directories.
Our PutMerge program will merge a list of files in a directory.
The length of the inputFiles array is the number of files in the specified directory.
Each FileStatus object in inputFiles has metadata information such as file length, permissions, modification time, and others.
Of interest to our PutMerge program is request an FSDataInputStream object   for reading in the file.
FSDataInputStream is a subclass of Java’s standard java.io.DataInputStream with additional support for random access.
For writing to a HDFS file, there’s the analogous FSDataOutputStream object.
To complete the PutMerge program, we create a loop that goes through all the files in inputFiles as we read each one in and write it out to the destination HDFS file.
The general flow of the program involves first setting the local directory and the HDFS destination file based on user-specified arguments q.
In w we extract information about each file in the local input directory.
We create an output stream to write to the HDFS file in e.
We loop through each file in the local directory, and r opens an input stream to read that file.
The rest of the code is standard Java file copy.
We have covered how to work with files in HDFS.
You now know a few ways to put data into and out of HDFS.
You want to process it, analyze it, and do other things.
Let’s conclude our discussion of HDFS and move on to the other major component of Hadoop, the MapReduce framework, and how to program under it.
As we have mentioned before, a MapReduce program processes data by manipulating (key/value) pairs in the general form.
Not surprisingly, this is an overly generic representation of the data flow.
In this section we learn more details about each stage in a typical MapReduce program.
Figure 3.1 displays a high-level diagram of the entire process, and we further dissect each component as we step through the flow.
Intermediate data of the same key goes to the same reducer.
Note that after distributing input data to different nodes, the only time nodes communicate with each other is at the “shuffle” step.
Before we analyze how data gets passed onto each individual stage, we should first familiarize ourselves with the data types that Hadoop supports.
Despite our many discussions regarding keys and values, we have yet to mention their types.
The MapReduce framework won’t allow them to be any arbitrary class.
For example, although we can and often do talk about certain keys and values as integers, strings, and so on, they aren’t exactly standard Java classes, such as Integer, String, and so forth.
This is because the MapReduce framework has a certain defined way of serializing the key/value pairs to move them across the cluster’s network, and only classes that support this kind of serialization can function as keys or values in the framework.
More specifically, classes that implement the Writable interface can be values, and classes that implement the WritableComparable<T> interface   can be either keys or values.
We need the comparability requirement for keys because they will be sorted at the reduce stage, whereas values are simply passed through.
Hadoop comes with a number of predefined classes that implement WritableComparable, including wrapper classes for all the basic data types, as seen in table 3.1
Table 3.1 List of frequently used types for the key/value pairs.
NullWritable Placeholder when the key or value is not needed.
Keys and values can take on types beyond the basic ones which Hadoop natively supports.
You can create your own custom type as long as it implements the Writable (or WritableComparable<T>) interface.
For example, listing 3.2 shows a class that can represent edges in a network.
Listing 3.2 An example class that implements the WritableComparable interface.
They work with the Java DataInput and DataOutput classes to interface.
With the data type interfaces now defined, we can proceed to the first stage of the data flow process as described in figure 3.1: the mapper.
To serve as the mapper , a class implements from the Mapper interface   and inherits the MapReduceBase class.
The MapReduceBase class, not surprisingly, serves as the base class for both mappers and reducers.
It includes two methods that effectively act as the constructor and destructor for the class:
The Mapper interface is responsible for the data processing step.
It utilizes Java generics of the form Mapper<K1,V1,K2,V2> where the key classes and value classes implement the WritableComparable and Writable interfaces, respectively.
Its single method is to process an individual (key/value) pair:
The OutputCollector receives the output of the mapping process, and the Reporter   provides the option to record extra information about the mapper as the task progresses.
You can see some of them in the table 3.2
As the MapReduce name implies, the major data flow operation after map is the reduce phase, shown in the bottom part of figure 3.1
As with any mapper implementation, a reducer   must first extend the MapReduce base class to allow for configuration and cleanup.
In addition, it must also implement the Reducer interface which has the following single method:
The OutputCollector receives the output of the reduce process and writes it to an output file.
The Reporter provides the option to record extra information about the reducer as the task progresses.
Table 3.3 lists a couple of basic reducer implementations provided by Hadoop.
Although we have referred to Hadoop programs as MapReduce applications, there is a vital step between the two stages: directing the result of the mappers to the different reducers.
A common misconception for first-time MapReduce programmers is to use only a single reducer.
With one reducer, our compute cloud has been demoted to a compute raindrop.
With multiple  reducers, we need some way to determine the appropriate one to send a (key/value) pair outputted by a mapper.
The default behavior is to hash the key to determine the reducer.
Hadoop enforces this strategy by use of the HashPartitioner class.
Let’s return to the Edge class introduced in section 3.2.1
Suppose you used the Edge class to analyze flight information data to determine the number of passengers departing from each airport.
If you used HashPartitioner, the two rows could be sent to different reducers.
The number of departures would be processed twice and both times erroneously.
How do we customize the partitioner for your applications? In this situation, we want all edges with a common departure point to be sent to the same reducer.
This is done easily enough by hashing the departureNode member   of the Edge :
The exact mechanics of the partitioner may be difficult to follow.
Between the map and reduce stages, a MapReduce application must take the output from the mapper tasks and distribute the results among the reducer tasks.
This process is typically called shuffling , because the output of a mapper on a single node   may be sent to reducers across multiple nodes   in the cluster.
Figure 3.2 The MapReduce   data flow, with an emphasis on  partitioning and shuffling.
The shapes represents keys, whereas the inner patterns represent values.
After shuffling, all icons of the same shape (key) are in the same reducer.
Different keys can go to the same reducer, as seen in the rightmost reducer.
Note that the leftmost reducer has more load due to more data under the “ellipse” key.
We have concluded our preliminary coverage of all the basic components of MapReduce.
Now that you’ve seen more classes provided by Hadoop, it’ll be fun to revisit the WordCount example (see listing 3.3), using some of the classes we’ve learned.
We have to write only the driver for this MapReduce program because we have used Hadoop’s predefined TokenCountMapper class   q and LongSumReducer class   w.
Let’s see how MapReduce reads input data and writes output data and focus on the file formats it uses.
To enable easy distributed processing, MapReduce makes certain assumptions about the data it’s processing.
It also provides flexibility in dealing with a variety of data formats.
Input data usually resides in large files, typically tens or hundreds of gigabytes or even more.
One of the fundamental principles of MapReduce’s processing power is the splitting of the input data into chunks.
You can process these chunks in parallel using multiple machines.
The size of each split should be small enough for a more granular parallelization.
If all the input data is in one split, then there is no parallelization.) On the other hand, each split shouldn’t be so small that the overhead of starting and stopping the processing of a split becomes a large fraction of execution time.
The principle of dividing input data (which often can be one single massive file) into splits for parallel processing explains some of the design decisions behind Hadoop’s generic FileSystem as well as HDFS in particular.
For example, Hadoop’s FileSystem provides the class FSDataInputStream for file reading rather than using Java’s java.
FSDataInputStream extends DataInputStream with random read access, a feature that MapReduce requires because a machine may be assigned to process a split that sits right in the middle of an input file.
Without random access, it would be extremely inefficient to have to read the file from the beginning until you reach the location of the split.
You can also see how HDFS is designed for storing data that MapReduce will split and process in parallel.
As different machines will likely have different blocks, parallelization is automatic if each split/ block is processed by the machine that it’s residing at.
Furthermore, as HDFS replicates blocks in multiple nodes for reliability, MapReduce can choose any of the nodes that have a copy of a split/block.
Input splits and record boundaries Note that input splits   are a logical division of your records whereas HDFS   blocks   are a physical division of the input data.
It’s extremely efficient when they’re the same but in practice it’s never perfectly aligned.
A machine processing a particular split may fetch a fragment of a record from a block other than its “main” block and which may reside remotely.
The communication cost for fetching a record fragment is inconsequential because it happens relatively rarely.
So far we’ve seen that Hadoop by default considers each line in the input file to be a record and the key/value pair is the byte offset (key) and content of the line (value), respectively.
You may not have recorded all your data that way.
Hadoop supports a few other data formats and allows you to define your own.
The way an input file is split up and read by Hadoop is defined by one of the implementations of the InputFormat interface.
TextInputFormat is the default InputFormat implementation, and it’s the data format we’ve been implicitly using up to now.
It’s often useful for input data that has no definite key value, when you want to.
The key returned by TextInputFormat is the byte offset of each line, and we have yet to see any program that uses that key for its data processing.
Table 3.4 lists other popular implementations of InputFormat along with a description of the key/value pair each one passes to the mapper.
The object type for key and value are also described.
TextInputFormat Each line in the text files is a record.
Key is the byte offset of the line, and value is the content of the line.
KeyValueTextInputFormat Each line in the text files is a record.
Everything before the separator is the key, and everything after is the value.
It’s optimized for passing data between the output of one MapReduce job to the input of some other MapReduce job.
NLineInputFormat Same as TextInputFormat, but each split is guaranteed to have exactly N lines.
KeyValueTextInputFormat is used in the more structured input files where a predefined character, usually a tab (\t), separates the key and value of each line (record)
For example, you may have a tab-separated data file of timestamps and URLs:
You can set your JobConf object   to use the KeyValueTextInputFormat class to read this file.
Recall that our previous mappers had used LongWritable and Text as the key and value types, respectively.
LongWritable is a reasonable type for the key under TextInputFormat because the key is a numerical offset.
When using KeyValueTextInputFormat, both the key and the value will be of type Text , and new key type.
The input data to your MapReduce job does not necessarily have to be some external data.
In fact it’s often the case that the input to one MapReduce job is the output of some other MapReduce job.
As we’ll see, you can customize your output format too.
The default output format writes the output in the same format that KeyValueTextInputFormat can read back in (i.e., each line is a record with key and value separated by a tab character)
Hadoop provides a much more efficient binary compressed file format called sequence file.
This sequence file is optimized for Hadoop processing and should be the preferred format when chaining multiple MapReduce jobs.
The object type for key and value in a sequence file are definable by the user.
The method have to take in the right input type.
Sometimes you may want to read input data in a way different from the standard InputFormat classes.
In that case you’ll have to write your own custom InputFormat class.
The two methods sum up the functions that InputFormat has to perform:
Identify all the files used as input data and divide them into input splits.
Provide an object (RecordReader) to iterate through records in a given split, and to parse each record into key and value of predefined types.
Who wants to worry about how files are divided into splits ? In creating your own InputFormat class you should subclass the FileInputFormat class, which takes care of file splitting.
In fact, all the InputFormat classes in table 3.4 subclass of splits specified in numSplits , subject to the constraints that each split must have more than mapred.min.split.size number of bytes but also be smaller than the block size of the filesystem.
In practice, a split usually ends up being the size of a block, which defaults to 64 MB in HDFS.
FileInputFormat has a number of protected methods a subclass can overwrite to change its behavior, one of which is the isSplitable(FileSystem fs, Path filename) method.
The default implementation always returns true, so all files larger than a block will be split.
You can’t start reading from the middle of those files.) Some data processing operations, such as file conversion, will need to treat each file as an atomic record and one should also not be able to split it.
In using FileInputFormat you focus on customizing RecordReader, which is responsible for parsing an input split into records and then parsing each record into a key/value pair.
Instead of writing our own RecordReader , we’ll again leverage existing classes provided by Hadoop.
For example, LineRecordReader implements RecordReader with byte offset as key and line content as value.
For the most part, your custom RecordReader will be a wrapper around an existing implementation, and most of the action will be in the.
One use case for writing your own custom InputFormat class is to read records in a specific type rather than the generic Text type.
For example, we had previously used KeyValueTextInputFormat to read a tab-separated data file of timestamps.
The class ends up treating both the timestamp and the URL as Text type.
For our illustration, let’s create a TimeUrlTextInputFormat that works exactly the same but treats the URL as a URLWritable type 3
As mentioned earlier, we create our InputFormat class by extending FileInputFormat and implementing the factory method to return our RecordReader.
Our TimeUrlLineRecordReader will implement the six methods in the RecordReader interface, in addition to the class constructor.
It’s mostly a wrapper around KeyValueTextInputFormat, but converts the record value from Text to type URLWritable.
We leave that as an exercise for the reader as we focus on a simpler illustration.
MapReduce outputs data into files using the OutputFormat class , which is analogous to the InputFormat class.
The output has no splits, as each reducer writes its output only to its own file.
The output   files reside in a common directory and are typically named part-nnnnn, where nnnnn is the partition ID of the reducer.
RecordWriter objects format the output and RecordReaders parse the format of the input.
Hadoop provides several standard implementations of OutputFormat, as shown in table 3.5
Not surprisingly, almost all the ones we deal with inherit  from the File OutputFormat abstract class; InputFormat classes inherit from FileInputFormat.
Are there OutputFormat (InputFormat) classes that don’t work with files? Well, the NullOutputFormat implements OutputFormat in a trivial way and doesn’t need to subclass FileOutputFormat.
More importantly, there are OutputFormat (InputFormat) classes that work with databases rather than files, and these classes are in a separate branch in the class hierarchy from FileOutputFormat (FileInputFormat)
These classes have specialized applications, and the interested reader can dig further in the online Java documentation for DBInputFormat and DBOutputFormat.
Keys and values are written as strings and separated by a tab (\t) character, which can be changed in the mapred.
SequenceFileOutputFormat<K,V> Writes the key/value pairs in Hadoop’s proprietary sequence file format.
The default OutputFormat is TextOutputFormat, which writes each record as a line a tab (\t) character   separates them.
The separator character can be changed in the mapred.textoutputformat.separator property.
It can also output in a format readable by TextInputFormat if you make the key type a NullWritable.
In that case the key in the key/value pair is not written out, and neither is the separator character.
If you want to suppress the output completely, then you should use the NullOutputFormat.
Suppressing the Hadoop output is useful if your reducer writes its output in its own way and doesn’t need Hadoop to write any additional files.
Finally, SequenceFileOutputFormat writes the output in a sequence file format that can be read back in using SequenceFileInputFormat.
It’s useful for writing intermediate data results when chaining MapReduce jobs.
Hadoop is a software framework that demands a different perspective on data processing.
It has its own filesystem, HDFS, that stores data in a way optimized for data-intensive processing.
You need specialized Hadoop tools to work with HDFS, but fortunately most of those tools follow familiar Unix or Java syntax.
The data processing part of the Hadoop framework is better known as MapReduce.
Although the highlight of a MapReduce program is, not surprisingly, the Map and the Reduce operations, other operations done by the framework, such as data splitting and shuffling, are crucial to how the framework works.
You can customize the other operations, such as Partitioning and Combining.
Hadoop provides options for reading data and also to output data of different formats.
Now that we have a better understanding of how Hadoop works, let’s go on to part 2 of this book and look at various techniques for writing practical programs using Hadoop.
Part 2 teaches the practical skills required to write and run data processing programs in Hadoop.
We explore various examples of using Hadoop to analyze a patent data set, including advanced algorithms such as the Bloom filter.
We also cover programming and administration techniques that are uniquely useful to working with Hadoop in production.
The MapReduce programming model is unlike most programming models you may have learned.
To help develop your proficiency, we go through many example programs in the next couple chapters.
In one of the advanced applications we introduce the Bloom filter, a data structure not normally taught in the standard computer science curriculum.
You’ll see that processing large data sets, whether you’re using Hadoop or not, often requires a rethinking of the underlying algorithms.
We assume you already have a basic grasp of Hadoop.
You can set up Hadoop, and you have compiled and run an example program, such as word counting from chapter 1
Many of our examples will use patent data sets, both of which are available from the National Bureau of Economic Research   (NBER ) at http://www.nber.org/patents/
You can practice writing MapReduce programs using them even when you don’t have access to a live cluster.
A popular development tactic is to create a smaller, sampled   subset of your large production data and call it the development data set.
This development data set may only have several hundred megabytes.
You develop your program in standalone or pseudo-distributed mode with the development data set.
This gives your development process a fast turnaround time, the convenience of running on your own machine, and an isolated environment for debugging.
We have chosen these two data sets for our example programs because they’re similar to most data types you’ll encounter.
First of all, the citation data encodes a graph   in the same vein that web links and social networks   are also graphs.
Patents are published in chronological order; some of their properties resemble time series.
Each patent is linked with a person (inventor) and a location (country of inventor)
Finally, you can look at the data as generic database relations   with well-defined schemas, in a simple commaseparated format.
It has more than 16 million rows and the first few lines resemble the following:
Other missing types include XML, image, and geolocation (the lat-long variety)
Math matrix is not represented in general, although the citation graph can be interpreted as a sparse 0/1 matrix.
The data set is in the standard comma-separated values (CSV) format, with the first line a description of the columns.
We can see that patent 3858241 cites five patents in total.
Analyzing the data more quantitatively will give us deeper insights into it.
If you’re only reading the data file, the citation data appears to be a bunch of numbers.
You can “think” of this data in more interesting terms.
In figure 4.1 we’ve shown a portion of this citation graph.
We can see that some patents are cited often whereas others aren’t cited at all.
We use Hadoop to derive descriptive statistics about this patent data, as well as look for interesting patterns that aren’t immediately obvious.
The other data set we use is the patent description data.
It has the patent number, the patent application year, the patent grant year, the number of claims, and other metadata about patents.
Look at the first few lines of this data set.
It’s similar to a table in a relational   database, but in CSV format.
As in many real-world data sets, it has many missing values.
As with any data analysis, we must be careful when interpreting with limited data.
When a patent doesn’t seem to cite any other patents, it may be an older patent for which we have no citation information.
On the other hand, more recent patents are cited less often because only newer patents can be aware of their existence.
The first row contains the name of a couple dozen attributes, which are meaningful only to patent specialists.
Even without understanding all the attributes, it’s still useful to have some idea of a few of them.
Figure 4.1 A partial view of the patent citation data set as a graph.
Each patent is shown as a vertex (node), and each citation is a directed edge (arrow)
Now that we have two patent data sets, let’s write Hadoop programs to process the data.
We write most MapReduce programs in brief and as variations on a template.
When writing a new MapReduce program, you generally take an existing MapReduce program and modify it until it does what you want.
In this section, we write our first MapReduce program and explain its different parts.
This program can serve as a template for future MapReduce programs.
Our first program will take the patent citation data and invert it.
For each patent, we want to find and group the patents that cite it.
For this section we won’t focus too much on the MapReduce data flow, which we’ve already covered in chapter 3
Instead we focus on the structure of a MapReduce program.
We need only one file for the entire program as you can see in listing 4.1
Our convention is that a single class, called MyJob in this case, completely defines each MapReduce job.
Hadoop requires the Mapper and the Reducer to be their own static classes.
These classes are quite small, and our template includes them as inner classes to the MyJob class.
The advantage is that everything fits in one file, simplifying code management.
But keep in mind that these inner classes are independent and don’t interact much with the MyJob class.
Various nodes with different JVMs clone and run the Mapper and the Reducer during job execution , whereas the rest of the job class is executed only at the client machine.
We investigate the Mapper and the Reducer classes in a while.
Without those classes, the skeleton of the MyJob class is.
The driver needs to specify in job the input paths, the output paths, the Mapper class, and the Reducer class—the method on the JobConf object to set up any configuration parameter.
It becomes the blueprint for how the job will be run.
The JobConf object has many parameters, but we don’t want to program the driver to set up all of them.
The configuration files of the Hadoop installation are a good starting point.
When starting a job from the command line, the user may also want to pass extra arguments to alter the job configuration.
The driver can define its own set of commands and process the user arguments itself to enable the user to modify some of the configuration parameters.
As this task is needed often, the Hadoop framework provides ToolRunner , Tool , and Configured to simplify it.
When used together in the MyJob skeleton above, these classes enable our job to understand user-supplied options that are supported by GenericOptionsParser.
For example, we have previously executed the MyJob class using this command line:
Had we wanted to run the job only to see the mapper’s output (which you may want to do for debugging   purposes), we could set the number of reducers to zero with the.
It works even though our program doesn’t explicitly understand the -D option.
By using ToolRunner , MyJob will automatically support the options in table 4.2
These files are automatically distributed to all task nodes to be locally available.
The convention for our template is to call the Mapper class MapClass and the Reducer class Reduce.
The naming would seem more symmetrical if we call the Mapper class Map, but Java already has a class (interface) named Map.
Both the Mapper and the Reducer extend MapReduceBase , which is a small class providing no-op implementations won’t need to override them except for more advanced jobs.
The signatures for the Mapper class and the Reducer class are.
Finally, all the key and value types   must be subtypes of Writable , which ensures a serialization interface for Hadoop to send the data around in a distributed cluster.
In fact, the key types implement WritableComparable   , a subinterface of Writable.
Much of what the layperson thinks of as statistics is counting, and many basic Hadoop jobs involve counting.
We’ve already seen the word count   example in chapter 1
For the patent citation data, we may want the number of citations a patent has received.
In each record, a patent number is associated with the number of citations it has received.
Like we said earlier, you hardly ever write a MapReduce program from scratch.
You copy that and modify it until it fits what you want.
We already have a program for getting the inverted citation index.
We can modify that program to output the count instead of the list of citing patents.
If we choose to output the count as an IntWritable, we need to specify IntWritable in three places in the Reducer code.
By changing a few lines and matching class types, we have a new MapReduce program.
Let’s go through another example that requires more changes, but you’ll see that the basic MapReduce program structure remains.
After running the previous example, we now have a data set that counts the number of citations for each patent.
We expect a large number of patents to have been only cited once, and a small number may have been cited hundreds of times.
It would be interesting to see the distribution of the citation counts.
Citations from patents outside of that period aren’t counted.) We also don’t deal with patents that supposedly have been cited zero times.
The first step to writing a MapReduce program is to figure out the data flow.
The reducer will sum up the number of 1s for each citation count and output the total.
Let’s use the KeyValueTextInputFormat , which automatically breaks each input record into key/value pairs based on a separator character.
Based on the data flow and the data types, you’ll be able to see the final program shown in listing 4.2 and understand what it’s doing.
You can see that it’s structurally similar to the other MapReduce programs we’ve seen so far.
We go into details about the program after the listing.
Listing 4.2 CitationHistogram.java: count patents cited once, twice, and so on.
The input format and output format are still KeyValueTextInputFormat and TextOutputFormat , respectively.
It sets the separator character used by KeyValueTextInputFormat to break each input line into a key/value pair.
Previously it was a comma for processing the original patent citation data.
By not setting this property it defaults to the tab character, which is appropriate for the citation count data.
The data flow for this mapper is similar to that of the previous mappers, only here we’ve chosen to define and use a couple class variables—citationCount and uno.
The reason for defining citationCount and uno in the class rather than many times as there are records (in a split, for each JVM)
It seems inefficient because we know all values are 1s (uno, to be exact)
Why do we need to sum the count? We’ve chosen this route because it will be easier for us later if we choose to add a combiner  to enhance a new IntWritable rather than reuse an existing one.
We can improve performance by using an IntWritable class variable.
But the number more than a thousand times (across all reducers)
We don’t have much need to optimize this particular code.
Running the MapReduce job on the citation count data will show the following result.
As we suspect, a large number (900K+) of patents have only one citation, whereas some have hundreds of citations.
As this histogram output is only several hundred lines long, we can put it into a spreadsheet and plot it.
Figure 4.2 shows the number of patents at various citation frequencies.
When a distribution shows as a line in a log-log plot, it’s considered to be a power law distribution.
The citation count histogram seems to fit the description, although its approximately parabolic curvature also suggests a lognormal distribution.
As you’ve seen in our examples so far, a MapReduce program is often not very big, and you can keep a certain structure across them to simplify development.
Most of the work is in thinking through the data flow.
We see in section 5.1.3 that this reliance will forbid the ChainMapper from using pass-by-reference.
One of the main design goals driving toward Hadoop’s major 1.0 release is a stable and extensible MapReduce API.
As of this writing, version 0.20 is the latest release and is considered a bridge between the older API (that we use throughout this book) and this upcoming stable API.
The 0.20 release supports the future API while maintaining backward-compatibility with the old one by marking it as deprecated.
Future releases after 0.20 will stop supporting the older API.
As of this writing, we don’t recommend jumping into the new API yet for a couple reasons:
You won’t be able to use those classes if your MapReduce code uses the new API in 0.20
Many still consider the most production-ready and stable version of Hadoop as of this writing to be 0.18.3
By the time you read this the situation may be different.
In this section we cover the changes the new API presents.
Fortunately, almost all the changes affect only the basic MapReduce template.
We rewrite the template under the new API to enable you to use it in the future.
The general consensus is that its initial release was problematic and full of bugs.
Some minor releases tried fixing the problems, but the community seems to want to skip straight to 0.20 instead.
Figure 4.2 Plotting the number of patents at different citation frequencies.
Many patents have one citation (or not at all, which is not shown on this graph)
On a log-log graph, this looks close enough to a straight line to be considered a power-law distribution.
The first thing you’ll notice in the new API is that many classes in org.apache.
After you’ve moved to the new API, you shouldn’t have any import statements (or full references) to any classes under org.apache.
The most meaningful change in the new API is the introduction of context objects.
Its most immediate impact is to replace the OutputCollector and Reporter objects consequences are to unify communication between your code and the MapReduce framework, and to stabilize the Mapper and Reducer API such that the basic method signatures will not change when new functionalities are added.
New functionalities will only be additional methods on the context objects.
Programs written before the introduction of those functionalities will be unaware of the new methods, and they will continue to compile and run against the newer releases.
They replace the Mapper and Reducer interfaces in the original API (org.apache.hadoop.mapred.Mapper and org.apache.hadoop.mapred.
The new abstract classes also replace the MapReduceBase class, which has been deprecated.
In addition, an Iterable, which is easier to iterate through using Java’s foreach syntax.
We can summarize the changes we’ve discussed so far in the method signatures for MapClass and Reduce.
You also need to change a few things in the driver to support the new API.
Their functionalities have been pushed to the Configuration class (which was originally the parent class of JobConf) and a new class Job.
The Configuration class purely configures a job, whereas the Job class and submission for execution are now under Job.
It incorporates all the changes we’ve mentioned in this section.
To keep presentation of examples in the rest of this book unified, we continue to use the API before 0.20
We have been using Java to write all our Hadoop programs.
Hadoop supports other languages via a generic API called Streaming.
In practice, Streaming is most useful for writing simple, short MapReduce programs that are more rapidly developed in a scripting language that can take advantage of non-Java libraries.
Hadoop Streaming interacts with programs using the Unix streaming paradigm.
Inputs come in through STDIN and outputs go to STDOUT.
Data has to be text based and each line is considered a record.
Unix commands work, and Hadoop Streaming enables those commands to be used as mappers and reducers.
If you’re familiar with using Unix commands, such as wc, cut, or uniq for data processing, you can apply them to large data sets using Hadoop Streaming.
The overall data flow in Hadoop Streaming is like a pipe where data streams through the mapper, the output of which is sorted and streamed through the reducer.
In the first example, let’s get a list of cited patents in cite75_99.txt.
The Streaming API is in a contrib package at contrib/streaming/hadoop-*streaming.jar.
The first part and the -input and the -output arguments specify that we’re running a Streaming program with the corresponding input and output file/directory.
The mapper and reducer are specified as arguments in quotes.
We see that for the mapper we use the Unix cut command to extract the second column, where columns are separated by commas.
In the citation data set this column is the patent number of a cited patent.
These patent numbers are then sorted and passed to the reducer.
The uniq command at the reducer will remove all duplicates in the sorted data.
The first row has the column descriptor “CITED” from the original file.
Note that the rows are sorted lexicographically because Streaming processes everything as text and doesn’t know other data types.
After getting the list of cited patents, we may want to know how many are there.
Again we can use Streaming to quickly get a count, using the Unix command wc –l.
Here we use wc –l as the mapper to count the number of records in each split.
We want the mapper to directly output the record count without any reducer, so we set mapred.reduce.tasks to 0 and don’t specify the -reducer option at all.
More than 3 million patents have been cited according to our data.
We can use  any executable script that processes a line-oriented data stream from STDIN and outputs to STDOUT with Hadoop Streaming.
For example, the Python script in listing 4.4 randomly samples data from STDIN.
For those who don’t know Python, the program has a for loop that reads STDIN one line at a time.
The comparison determines whether to pass that line on to the output or ignore it.
You can use the script in Unix to uniformly sample a line-oriented data file, for example:
Listing 4.4 RandomSample.py: a Python script printing random lines from STDIN.
We can apply the same script in Hadoop to get a smaller sample of a data set.
A sampled data set is often useful for development purposes, as you can run your Hadoop program on the sampled data in standalone or pseudo-distributed mode to quickly debug and iterate.
You may notice that this approach counts the number of records in each split, not the entire file.
With a bigger file, or multiple files, the user will have to sum up the counts herself to get the overall total.
To fully automate a complete counting, the user will have to write a script at the reducer to sum up all the partial counts.
Finding data clusters is one example of such descriptive information.
Optimized implementations of a variety of clustering algorithms are readily available in R, MATLAB, and other packages.
It makes a lot more sense to sample down the data and apply some standard software.
It depends on what you’re trying to compute and the distribution of your data set.
For example, it’s usually fine to compute an average from a sampled data set, but if the data set is highly skewed   and the average is dominated by a few values, sampling can be problematic.
Similarly, clustering on a sampled data set is fine if it’s used only to get a general understanding of the data.
If you were looking for small, anomalous clusters, sampling may get rid of them.
For functions such as maximum and minimum, it’s not a good idea to apply them to sampled data.
Running RandomSample.py using Streaming is like running Unix commands using Streaming, the difference being that Unix commands are already available on all nodes in the cluster, whereas RandomSample.py is not.
Hadoop Streaming supports a -file option to package your executable file as part of the job submission.
Note that we’ve set the number of reducers (mapred.reduce.tasks) to 1
As we haven’t specified any particular reducer, it will use the default IdentityReducer.
As its name implies, IdentityReducer passes its input straight to output.
In this case we can set the number of reducers to any non-zero value to get an exact number of output files.
Alternatively, we can set the number of reducers to 0, and let the number of output files be the number of mappers.
This is probably not ideal for the sampling task as each mapper’s output is only a small fraction of the input, and we may end up with a number of small files.
We can easily correct that later using the HDFS shell command getmerge or other file manipulations to arrive at the right number of output files.
The approach to use is more or less a personal preference.
It’s also implicitly assumed that you have installed the Python language on all the nodes in your cluster.
The random sampling script was implemented in Python, although any scripting language that works with STDIN and STDOUT would work.
Listing 4.5 RandomSample.php.: a PHP script printing random lines from STDIN.
The random sampling scripts don’t require any custom reducer, but you can’t always write a Streaming program like that.
As you’ll use Streaming quite often in practice, let’s see another exercise.
Suppose we’re interested in finding the most number of claims in a single patent.
In the patent description data set, the number of claims for a given patent is in the ninth column.
Our task is to find the maximum   value in the ninth column of the patent description data.
Under Streaming, each mapper sees the entire stream of data, and it’s the mapper that takes on the responsibility of breaking the stream into (line-oriented) records.
In the standard Java model, the framework itself breaks input data into records, and easy to keep state information across records in a split, which we take advantage of in computing the maximum.
The standard Java model, too, can keep track of state across records in a split, but it’s more involved.
In creating a Hadoop program for computing maximum, we take advantage of the distributive property   of maximum.
Recall that PHP was originally designed to work within static HTML content.
When using PHP as a pure scripting language, you need to be careful that you leave no whitespaces outside the brackets.
Otherwise they will be outputted and may cause unintended behavior that is hard to debug.
It would appear whitespaces were introduced in the output data out of nowhere.)
It’s easy to ensure that there’s no whitespaces before the opening bracket <?php by putting the bracket at the beginning of the script file.
But, it’s easy to accidentally leave whitespaces after the closing bracket ?>, as ending whitespaces don’t grab attention.
When using a file as a PHP script, it’s safer to omit the closing bracket ?>
The PHP interpreter will quietly read everything till the end-of-file as PHP commands rather than static content.
That sounded like a mouthful, but a simple example will make it clear.
Our strategy is to have mapper calculate the maximum over its individual split.
Each mapper will output a single value at the end.
We have a single reducer that looks at all those values and outputs the global maximum.
Listing 4.6 depicts the Python script for a mapper to compute the maximum over a split.
Listing 4.6 AttributeMax.py: Python script to find maximum value of an attribute.
It has a for loop to read one record at a time.
It tokenizes the record into fields and updates the maximum if the user-specified field is bigger.
Note that the mapper doesn’t output any value until the end, when it sends out the maximum value of the entire split.
This is different from what we’ve seen before, where each record sends out one or more intermediate records to be processed by the reducers.
Given the parsimonious output of the mapper, we can use the default IdentityReducer to record the (sorted) output of the mappers.
It outputs the maximum of the ninth column in a split.
Given seven mappers, the final output of the above command is this:
We see that one split has zero claims in all its records.
This sounds suspicious until we recall that the claim count attribute is not available for patents before 1975
We see that our mapper is doing the right thing.
We can use a reducer that outputs the maximum over the values outputted by the mappers.
We have an interesting situation here, due to the distributive property of maximum, where we can also use AttributeMax.py as the reducer.
Only now the reducer is trying to find the maximum in the “first” column.
The output of the above command should be a one-line file, and you’ll find the maximum number of claims in a patent to be 868
Classes of aggregation functions We use aggregation functions to compute descriptive statistics.
They’re generally grouped into three classes: distributive , algebraic , and holistic.
The maximum function is an example of a distributive function.
Other distributive functions include minimum , sum , and count.
Similar to the maximum function, you can globally compute these functions by iteratively applying them to smaller chunks of data.
They don’t follow the distributive property, and their derivation will require some “algebraic” computation over simpler functions.
We get into examples of this in the next section.
Finally, functions such as median   and K smallest/largest value belong to the holistic class of aggregation functions.
Readers interested in a challenge should try to implement the median function in an efficient manner using Hadoop.
At this point you may wonder what happened to the key/value pair way of encoding records.
Our discussion on Streaming so far talks about each record as an atomic unit.
The truth is that Streaming works on key/value pairs just like the standard Java MapReduce model.
By default, Streaming uses the tab character to separate the key from the value in a record.
When there’s no tab character, the entire record is considered the key and the value is empty text.
For our data sets, which have no tab character, this provides the illusion that we’re processing each individual record as a whole unit.
Furthermore, even if the records do have tab characters in them, the Streaming API will only shuffle and sort the records in a different order.
As long as our mapper and reducer work in a record-oriented way, we can maintain the record-oriented illusion.
Working with key/value pairs allows us to take advantage of the key-based shuffling and sorting to create interesting data analyses.
To illustrate key/value pair processing using Streaming, we can write a program to find the maximum number of claims in a patent for each country.
This would differ from AttributeMax.py in that this is trying to find the maximum for each key, rather than a maximum across all records.
Let’s make this exercise more interesting by computing the average   rather than finding the maximum.
As we see, Hadoop already includes a package called Aggregate that contains classes that help find the maximum for each key.)
First, let’s examine how key/value pairs work in the Streaming API for each step of the MapReduce data flow.
As we’ve seen, the mapper under Streaming reads a split through STDIN and extracts each line as a record.
Your mapper can choose to interpret each input record as a key/value pair or a line of text.
The Streaming API will interpret each line of your mapper’s output as a key/ value pair separated by tab.
Similar to the standard MapReduce model, we apply the partitioner   to the key to find the right reducer to shuffle   the record to.
All key/value pairs with the same key will end up at the same reducer.
Recall that in the Java model, all key/value pairs of the same key are grouped together into one key and a list of values.
This is not too bad as the key/value pairs are already sorted by key.
All records of the same key are in one contiguous chunk.
Your reducer will read one line at a time from STDIN and will keep track of the new keys.
For all practical purposes, the output (STDOUT) of your reducer is written to a file directly.
Technically a no-op step is taken before the file write.
In this step the Streaming API breaks each line of the reducer’s output by the tab character and feeds the key/value pair to the default TextOutputFormat , which by default re-inserts the tab character before writing the result to a file.
Without tab characters in the reducer’s output it will show the same.
You can reconfigure the default behavior to do something different, but it makes sense to leave it as a no-op and push the processing into your reducer.
To understand the data flow better, we write a Streaming program to compute the average number of claims for each country.
The mapper will extract the country and the claims count for each patent and package them as a key/value pair.
In accord with the default Streaming convention, the mapper outputs this key/value pair with a tab character to separate them.
The Streaming API will pick up the key and the shuffling will guarantee that all claim counts of a country will end up at the same reducer.
An extra concern with our data set is that missing values   do exist.
We’ve added a conditional statement to skip over records with missing claim counts.
Listing 4.7 AverageByAttributeMapper.py: output country and claim count of patents.
Before writing the reducer, let’s run the mapper in two situations: without any reducer, and with the default IdentityReducer.
It’s a useful approach now for learning as we can see exactly what’s being outputted by the mapper (by using no reducer) and what’s being inputted into the reducer (by using IdentityReducer)
You’ll find this handy later when debugging your MapReduce program.
You can at least check if the mapper is outputting the proper data and if the proper data is being sent to the reducer.
The output should consist of lines where a country code is followed by a tab followed by a numeric count.
The order of the output records is not sorted by (the new) key.
In fact, it’s in the same order as the order of the input records, although that’s not obvious from looking at the output.
The more interesting case is to use the IdentityReducer with a non-zero number of reducers.
We see how the shuffled and sorted records are presented to the reducer.
Under the Streaming API, the reducer will see these text data in STDIN.
We have to code our reducer to recover the key/value pairs by breaking each line at the tab character.
As you read each line from STDIN, you’ll be responsible for keeping track of the boundary between records of different keys.
Note that although the keys are sorted, the values don’t follow any particular order.
Finally, the reducer must perform its stated computation, which in this case is calculating the average value across a key.
The program keeps a running sum and count for each key.
When it detects a new key in the input stream or the end of the file, it computes the average for the previous key and sends it to STDOUT.
After running the entire MapReduce job, we can easily check the correctness of the first few results.
Looking at the output of our job and the country codes, we see that Andorra (AD) patents have an average 14 claims.
Antigua and Barbuda (AG) patents average 13.25 claims, and so forth.
Hadoop includes a library package called Aggregate that simplifies obtaining aggregate statistics of a data set.
This package can simplify the writing of Java statistics collectors, especially when used with Streaming, which is the focus of this section.9
The Aggregate package under Streaming functions as a reducer that computes aggregate statistics.
You only have to provide a mapper that processes records and sends out a specially formatted output.
The output string starts with the name of a value aggregator function (from the set of predefined functions available in the Aggregate package)
The Aggregate reducer applies the function to the set of values for each key.
For example, if the function is LongValueSum , then the output is the sum of values for each key.
As the function name implies, each value is treated as a Java long type.) If the function is LongValueMax, then the output is the maximum   value for each key.
You can see the list of aggregator functions supported in the Aggregate.
Table 4.3 List of value aggregator functions supported by the Aggregate package.
LongValueMax Finds the maximum of a sequence of long values.
LongValueMin Finds the minimum of a sequence of long values.
StringValueMax Finds the lexicographical maximum of a sequence of string values.
StringValueMin Finds the lexicographical minimum of a sequence of string values.
UniqValueCount Finds the number of unique values (for each key)
ValueHistogram Finds the count, minimum, median, maximum, average, and standard deviation of each value.
Using the Aggregate package in Java is explained in http://hadoop.apache.org/core/docs/current/api/ org/apache/hadoop/mapred/lib/aggregate/package-summary.html.
Let’s go through an exercise using the Aggregate package to see how easy it is.
We want to count the number of patents granted each year.
We can approach this problem in a way similar to the word counting example we saw in chapter 1
For each record, our mapper will output the grant year as the key and a “1” as the value.
The reducer will sum up all the values (“1”s) to arrive at a count.
Our result will be the simple mapper shown in listing 4.9
The user only has to specify the column index to count the number of records for each attribute in that column.
The print statement has the main “action” of this short program.
It tells the Aggregate package to sum up all the values (of 1) for each key, defined as the userspecified column (index)
To count the number of patents granted each year, we run this Streaming program with the Aggregate package, telling the mapper to use the.
You’ll find most of the options of running the Streaming program familiar.
The main thing to point out is that we’ve specified the reducer to be 'aggregate'
This is the signal to the Streaming API that we’re using the Aggregate package.
The first row is anomalous because the first row of the input data is a column description.
Otherwise the MapReduce job neatly outputs the patent count for each year.
You’ll see that it has a mostly steady upward trend.
Looking at the list of functions in the Aggregate package in table 4.3, you’ll find that most of them are combinations of maximum, minimum, and sum for atomic data type.
They would be trivial modifications of LongValueMax and LongValueMin and an added advantage.) UniqValueCount and ValueHistogram are slightly different and we look at some examples of how to use them.
UniqValueCount gives the number of unique values   for each key.
For example, we may want to know whether more countries are participating in the U.S.
We can examine this by looking at the number of countries with patents granted each year.
In the output we get one record for each year.
Figure 4.3 Using Hadoop to count patents published each year and Excel to plot the result.
This analysis using Hadoop quickly shows the annual patent output to have almost quadrupled in 40 years.
The aggregate function ValueHistogram   is the most ambitious function in the Aggregate package.
In its most general form, it expects the output of the mapper to have the form.
We specify the function ValueHistogram followed by a colon, followed by a tabseparated key, value, and count triplet.
The Aggregate reducer outputs the six statistics above for each key.
Note that for everything except the first statistics (number of unique values) the counts are summed over each key/value pair.
We performed the computation with a MapReduce job and graphed the result with Excel.
A useful variation is for the mapper to only output the key and value, without the count and the tab character that goes with it.
ValueHistogram automatically assumes a count of 1 in this case.
We run this program to find the distribution of countries with patents granted for each year.
The output is a tab-separated value (TSV) file with seven columns.
The first column, the year of patent granted, is the key.
The other six columns are the six statistics the ValueHistogram is set to compute.
A partial view of the output is here (we skip the first two rows for formatting reasons):
The first column after the year is the number of unique values.
This is exactly the same as the output of UniqValueCount.
The second, third, and fourth columns are the minimum , median , and maximum , respectively.
We’ve seen how using the Aggregate package under Streaming is a simple way to get some popular metrics.
It’s a great demonstration of Hadoop’s power in simplifying the analysis of large data sets.
The mapper reads each record and outputs a key/value pair for the record’s attribute and count.
It shuffles the key/value pairs across the network, and the reducer computes the average for each key.
In our example of computing the average number of claims for each country’s patents, we see at least two efficiency bottlenecks:
If we were computing a function such as maximum, it’s obvious that the mapper only has to output the maximum   for each key it has seen.
For a function such as average , it’s a bit more complicated, but we can still redefine the algorithm such that for each mapper only one record is shuffled for each key.
Using country from the patent data set as key illustrates data skew.
The data is far from uniformly distributed, as a significant majority of the records would have U.S.
Not only does every key/value pair in the input map to a key/value pair in the intermediate data, most of the intermediate key/value pairs will end up at a single reducer, overwhelming it.
Hadoop solves these bottlenecks by extending the MapReduce framework with a combiner step in between the mapper and reducer.
You can think of the combiner as a helper for the reducer.
It’s supposed to whittle down the output of the mapper to lessen the load on the network and on the reducer.
If we specify a combiner, the MapReduce framework may apply it zero, one, or more times to the intermediate data.
In order for a combiner to work, it must be an equivalent transformation of the data with respect to the reducer.
If we take out the combiner, the reducer’s output will remain the same.
Furthermore, the equivalent transformation property must hold when the combiner is applied to arbitrary subsets of the intermediate data.
If the reducer only performs a distributive function,   such as maximum, minimum, and summation (counting), then we can use the reducer itself as the combiner.
We can rewrite some of them, such as averaging to take advantage of a combiner.
The averaging approach taken by AverageByAttributeMapper.py is to output only each key/value pair.
AverageByAttributeReducer.py will count the number of key/value pairs it receives and sum up their values, in order for a single final division to compute the average.
The main obstacle to using a combiner is the counting operation, as the reducer assumes the number of key/value pairs it receives is the.
We can refactor the MapReduce program to track the count explicitly.
The combiner becomes a simple summation function with the distributive property.
Let’s first refactor the mapper and reducer before writing the combiner, as the operation of the MapReduce job must be correct even without a combiner.
We write the new averaging program in Java as the combiner must be a Java class.
For versions up to at least 0.20, the combiner must still be a Java class.
It’s best to write your mapper and reducer in a Java language.
Fortunately, the Hadoop roadmap supports native Streaming scripts as combiners.
In addition, if you’re using the Aggregate package, each value aggregator already has a built-in (Java) combiner.
Let’s write a Java mapper (listing 4.12) that’s analogous to AverageByAttributeMapper.
The crucial difference in this new Java mapper is that the output is now appended with a count of 1 q.
We could’ve defined a new Writable data type that holds both the value and count, but things are simple enough that we’re just keeping a commaseparated string in Text.
At the reducer, the list of values for each key are parsed.
The total sum and count are then computed by summation and divided at the end to get the average.
The logic of the refactored MapReduce job was not too hard to follow, was it? We added an explicit count for each key/value pair.
This refactoring allows the intermediate data to be combined at each mapper before it’s sent across the network.
The a bad naming scheme, but recall that for the important class of distributive functions , the combiner and the reducer perform the same operations.
Therefore, the combiner has adopted the reducer’s signature to simplify its reuse.
You don’t have to rename your Reduce class to use it as a combiner class.
In addition, because the combiner is performing an equivalent transformation, the type for the key/value pair in its output must match that of its input.
In the end, we’ve created a Combine class that looks similar to the Reduce class, except it only outputs the (partial) sum and count at the end, whereas the reducer computes the final average.
To enable the combiner, the driver must specify the combiner’s class to the JobConf the mapper, combiner, and the reducer:
Figure 4.5 Monitoring the effectiveness of the combiner in the AveragingWithCombiner job.
You should monitor the job’s behavior to see if the number of records outputted by the combiner is meaningfully less than the number of records going in.
The reduction must justify the extra execution time of running a combiner.
You can easily check this through the JobTracker’s Web UI , which we’ll see in chapter 6
Clearly the combiner has reduced the amount of intermediate data.
Note that the reduce side executes the combiner, though the benefit of this is negligible in this case.
You can try the following exercises to hone your ability to think in the MapReduce paradigm.
Top K records—Change AttributeMax.py (or AttributeMax.php) to output the entire record rather than only the maximum   value.
Rewrite it such that the MapReduce job outputs the records with the top K values rather than only the maximum.
Web traffic measurement—Take a web server log   file and write a Streaming program with the Aggregate package   to find the hourly traffic to that site.
Inner product of two sparse vectors—A vector is a list of values.
All elements not explicitly specified are considered to have a value of zero.
Note that the keys don’t need to be in a sorted order.
For natural language processing , the keys can be words in a document, and the inner product is a measure of document similarity .) Write a Streaming job to compute the inner product of two sparse vectors.
You can add a postprocessing step after the MapReduce job to complete the computation.
Time series processing—Consider time-series   data, where each record has a timestamp as key and a measurement (on that time period) as value.
We want an output that is a linear function of the time series in a form: where t stands for time and a0,…,aN are known constants.
In signal processing , this is known as an FIR filter.
A particularly popular instance the average of the previous N points in x.
If you order the time series data chronologically (as they usually are) and N is relatively small, what’s the reduction in network traffic for shuffling when a combiner is used? For extra credit, write your own partitioner so the output stays ordered chronologically.
For the more advanced practitioners, this example illustrates the difference between scalability and performance.
Implementing an FIR filter in Hadoop makes it scalable to process terabytes or more of data.
Students of signal processing will recognize that a high performance implementation of an FIR filter often calls for a technique known as Fast Fourier Transform   (FFT )
A solution that is scalable and high performing would call for a MapReduce implementation of FFT, which is beyond the scope of this book.
Commutative property—Recall from basic math that the commutative property means the order of operation is irrelevant.
Multiplication (product)—Many machine-learning   and statistical-classification algorithms involve the multiplication of a large number of probability values.
Usually we compare the product of one set of probabilities to the product of a different set, and choose a classification corresponding to the bigger product.
Is the product also distributive ? Write a MapReduce program that multiplies all values in a data set.
For full credit, apply the program to a reasonably large data set.
Writing your own floating-point library is a popular answer, but not a good one.)
Write such translations and use Hadoop to apply it to a large corpus such as Wikipedia.
Often the whole program is defined within a single Java class.
Within the class, a driver sets up a MapReduce job’s configuration object, which is used as the blueprint for how the job is set up and run.
You’ll find the map and reduce functions in subclasses of Mapper and Reducer, respectively.
Those classes are often no more than a couple dozen lines long, so they’re usually written as inner classes for convenience.
Hadoop provides a Streaming API for writing MapReduce programs in a language other than Java.
Many MapReduce programs are much easier to develop in a scripting language using the Streaming API, especially for ad hoc data analysis.
MapReduce programs are largely about the map and the reduce functions, but Hadoop allows for a combiner function to improve performance by “pre-reducing” the intermediate data at the mapper before the reduce phase.
In standard programming (outside of the MapReduce paradigm), counting, summing, averaging, and so on are usually done through a simple, single pass of the data.
Refactoring those programs to run in MapReduce, as we’ve done in this chapter, is relatively straightforward conceptually.
More complex data analysis algorithms call for deeper reworking of the algorithms, which we cover in the next chapter.
Although we’ve focused on the patent data sets   in this chapter, there are other large few examples.
As part of a competition, it released a data set of user ratings to challenge people to develop better recommendation algorithms.
For example, one of the biological data sets is an annotated human genome   data of roughly 550 GB.
Under economics you can find data sets, such as the 2000 U.S.
It’s a crawl of a billion web pages in 10 languages.
Given the size of the data set, the most efficient way to get it is in compressed form (which takes up 5 TB) shipped in hard disk drives.
As your data processing becomes more complex you’ll want to exploit different Hadoop features.
This chapter will focus on some of these more advanced techniques.
When handling advanced data processing, you’ll often find that you can’t program the process into a single MapReduce job.
Hadoop supports chaining MapReduce programs together to form a bigger job.
You’ll also find that advanced data processing often involves more than one data set.
We’ll explore various joining techniques in Hadoop for simultaneously processing multiple data sets.
You can code certain data processing tasks more efficiently when processing a group of records at a time.
We’ve seen how Streaming   natively supports the ability to process a whole split at a time, and the Streaming implementation of the maximum function takes advantage of this ability.
We’ll see that the same is true for Java programs.
We’ll discover the Bloom filter and implement it with a mapper   that keeps state information across records.
You’ve been doing data processing tasks which a single MapReduce job can accomplish.
As you get more comfortable writing MapReduce programs and take on more ambitious data processing tasks, you’ll find that many complex tasks need to be broken down into simpler subtasks, each accomplished by an individual MapReduce job.
For example, from the citation data set you may be interested in finding the ten mostcited patents.
Though you can execute the two jobs manually one after the other, it’s more convenient to automate the execution sequence.
You can chain MapReduce jobs to run sequentially, with the output of one MapReduce job being the input to the next.
Recall that a driver sets up a JobConf object with the configuration parameters for a MapReduce job calling the driver of one MapReduce job after another.
The driver at each job will have to create a new JobConf object and set its input path to be the output path of the previous job.
You can delete the intermediate data generated at each step of the chain at the end.
Sometimes the subtasks of a complex data processing task don’t run sequentially, and their MapReduce jobs are therefore not chained in a linear fashion.
The third job, mapreduce3, performs an inner join of the first two jobs’ output.
Hadoop has a mechanism to simplify the management of such (nonlinear) job dependencies via the Job and JobControl classes.
A Job object is a representation of a MapReduce job.
You instantiate a Job object by passing a JobConf object to its constructor.
In addition to holding job configuration information, Job also holds Job objects x and y,
Whereas Job objects store the configuration and dependency information, JobControl objects do the managing and monitoring of.
A lot of data processing tasks involve record-oriented preprocessing and postprocessing.
For example, in processing documents for information retrieval , you may have one step to remove stop words  (words like a, the, and is that occur frequently but aren’t too meaningful), and another step for stemming  (converting different forms of a word into the same form, such as finishing and finished into finish.) You can write a separate MapReduce job for each of these pre- and postprocessing steps and chain them together, using IdentityReducer   (or no reducer at all) for these steps.
This approach is inefficient as each step in the chain takes up I/O and storage to process the intermediate results.
Another approach is for you to write your mapper such that it calls all the preprocessing steps beforehand and the reducer to call all the postprocessing steps afterward.
This forces you to architect the pre- and postprocessing steps in a modular and composable manner.
Hadoop introduced the ChainMapper and the ChainReducer classes in version 0.19.0 to simplify the composition of pre- and postprocessing.
You can think of chaining MapReduce jobs, as explained in section 5.1.1, symbolically using the pseudo-regular expression:
The analogous expression for a job using ChainMapper and ChainReducer would be.
The beauty of this mechanism is that you write the pre- and postprocessing steps as standard mappers.
You can run each one of them individually if you want.
This is in ChainMapper and ChainReducer to compose the pre- and postprocessing steps, respectively.
Running all the pre- and postprocessing steps in a single job leaves no intermediate file and there’s a dramatic reduction in I/O.
In this setup, you should think of Map2 and Reduce as the core of the MapReduce job, with the standard partitioning and shuffling applied between the mapper and reducer.
You can specify the composition of this sequence of mappers and reducer with the driver.
You need to make sure the key and value outputs of one task have matching types (classes) with the inputs of the next task.
Listing 5.1 Driver for chaining   mappers within a MapReduce job.
The driver first sets up the “global” JobConf object with the job’s name, input path, output path, and so forth.
It adds the five steps of the chained job one at a time, in the sequence of the steps’ execution.
It adds all the steps before Reduce using method, it adds the last steps.
The global JobConf object (job) is passed through all five add* methods.
The recommended local JobConf object   is a new JobConf object initiated without defaults — new JobConf(false)
The first and last are the global and local JobConf objects, respectively.
The second argument (klass ) is the Mapper class that will do the data processing.
The four arguments inputValueClass, inputKeyClass, outputKeyClass, and outputValueClass are the input/output class types of the Mapper class.
In the standard Mapper model , the output key/value pairs are serialized and written to disk,1 prepared to be shuffled.
The key and value’s ability to be cloned and serialized is provided by them being implemented as Writables.
Formally this is considered to be passed by value , as a copy of the key/value pair is sent over.
In the current case where we can chain one Mapper to another, we can execute the two in the same JVM thread.
Therefore, it’s possible for the key/value pairs to be passed by reference , where the output of the initial Mapper stays in place in memory and the following Mapper refers to it directly in the same memory location.
When Map1 calls OutputCollector.collect performance   by not having to clone a potentially large volume of data between the mappers.
But doing this can violate one of the more subtle “contracts” in Hadoop’s MapReduce API.
The call to OutputCollector.collect(K k, V v) is guaranteed to not alter the content of k and v.
Map1 can call OutputCollector.collect(K k, V v) and then use the objects k and v afterward, fully expecting their values to stay the same.
If you’re not sure of the Mapper’s internal code, it’s best to play safe and let byValue be true, maintaining the pass-by-value model, and be certain that the Mappers will work as expected.
It’s inevitable that you’ll come across data analyses where you need to pull in data from different sources.
For example, given our patent data sets, you may want to find out if certain countries cite patents from another country.
In the database world it would just be a matter of joining two tables, and most databases automagically take care of the join processing   for you.
Unfortunately, joining data in Hadoop is more involved, and there are several possible approaches with different trade-offs.
We use a couple toy data sets to better illustrate joining in Hadoop.
Let’s take a comma-separated Customers file where each record has three fields: Customer ID, Name, and Phone Number.
We store Customer orders in a separate file, called Orders.
If we want an inner join   of the two data sets above, the desired output would look a listing 5.2
Listing 5.2 Desired output of an inner join between Customers and Orders data.
Hadoop can also perform outer joins, although to simplify explanation we focus on inner joins.
Hadoop has a contrib package called datajoin that works as a generic framework for data joining in Hadoop.
To distinguish it from other joining techniques, it’s called the reduce-side join, as we do most of the processing on the reduce side.
It’s also known as the repartitioned join   (or the repartitioned sort-merge join), as it’s the same as the database technique of the same name.
Although it’s not the most efficient joining technique, it’s the most general and forms the basis of some more advanced techniques (such as the semijoin )
Reduce-side join introduces some new terminologies and concepts, namely, data source , tag , and group key.
A data source is analogous to a table in relational   databases.
We have two data sources in our toy example: Customers and Orders.
A data source can be a single file or multiple files.
The important point is that all the records in a data source have the same structure, analogous to a schema.
The MapReduce paradigm calls for processing each record one at a time in a stateless manner.
If we want some state information to persist, we have to tag the record with such state.
For example, given our two files, a record may look to a mapper like this:
Tagging the record will ensure that specific metadata will always go along with the record.
For the purpose of data joining, we want to tag each record with its data source.
The group key  functions like a join key   in a relational database.
For our example, the group key is the Customer ID.
As the datajoin package allows the group key to be any user-defined function, group key is more general than a join key in a relational database.
Before explaining how to use the contrib package, let’s go through all the major steps in a repartitioned sort-merge join of our toy datasets.
After seeing how those steps fit together, we’ll see which steps are done by the datajoin package, and which ones we program.
We’ll have code to see the hooks for integrating our code with the datajoin package.
Figure 5.1 illustrates the data flow of a repartitioned join on the toy data sets Customers and Orders, up to the reduce stage.
We’ll go into more details later to see what happens in the reduce stage.
First we see that mappers receive data from two files, Customers and Orders.
The value in this key/value package will be the original record, tagged with the data source (i.e., filename)
Customer ID that will be used to join with records from the Customers file.
One value is tagged with "Customers" and the other value is tagged value, which is tagged with "Customers"
This is expected as there is no record in for the (group) key "3"
This is due to one record from Customers and two more from Orders.
Figure 5.1 In repartitioned join , the mapper first wraps each record with a group key and a tag.
The group key is the joining attribute, and the tag is the data source (table in SQL parlance) of the record.
The partition and shuffle step will group all the records with the same group key together.
The reducer is called on the set of records with the same group key.
We have three values, one tagged with Customers and two tagged with Orders.
Each combination consists of the Customers value and one of the Orders value.
For a given join key, the reduce task performs a full cross-product of values from different sources.
For more complicated settings, the number of combinations generated by the cross-product is the product of the number of records under each tag.
Don’t confuse with combiners as explained in section 4.5.) Due to the nature of the function that determines whether the whole operation is an inner join , outer join , or the records from different sources into a single output record.
Now you see why we call this joining process the repartitioned sort-merge join.
The records in the original input sources can be in random order.
They are repartitioned onto the reducers in the right grouping.
The reducer can then merge records of the same join key together to create the desired join output.
The sort happens but it’s not critical to understanding the operation.)
Hadoop’s datajoin package implements the dataflow of a join as described previously.
Hadoop ’s datajoin package has three abstract classes that we inherit and make concrete: DataJoinMapperBase , DataJoinReducerBase , and TaggedMapOutput.
As the names suggest, our MapClass will extend DataJoinMapperBase, and our Reduce class will extend DataJoinReducerBase.
The datajoin package has already perform the join dataflow describe in the last section.
Our subclass will only have to implement a few new methods to configure the details.
Before explaining how to use DataJoinMapperBase and DataJoinReducerBase, you need to understand a new abstract data type TaggedMapOutput that is used throughout the code.
Recall from the dataflow description that the mapper outputs a specifies the (group) key to be of type Text and the value (i.e., the tagged record) to be of type TaggedMapOutput.
TaggedMapOutput is a data type for wrapping our records to handle the type of the record.
In addition, as the output of a mapper, TaggedMapOutput needs to be methods.
We created TaggedWritable , a simple subclass for handling any Writable record type.
Recall from the join dataflow that the mapper’s main function is to package a record such that it goes to the same reducer as other records with the same join key.
DataJoinMapperBase performs all the packaging, but the class specifies three abstract methods for our subclass to fill in:
Note working on the Customers file will receive the string "Customers" as the argument to.
For example, the tag (data source) can be the filename before the dash (-) sign.
We can also store the filename in DataJoinMapperBase’s inputFile variable if we want to refer to it again.
It calls the two abstract methods that we have yet to implement.
In the standard case, we want the tag stored in this.inputTag.
For our current purpose, we unwrap the tagged record and take the first field in the CSV-formatted value as the join key.
In a more general implementation, the user will be able to specify which field should be the joining key and if the record separator may be some character other than a comma.
DataJoinMapperBase is a simple class, and much of the mapper code is in our subclass.
DataJoinReducerBase, on the other hand, is the workhorse of the datajoin combinations to get the desired join operation (inner join, left outer join, etc.)
Each combination will have either two records (meaning there’s at least one record in each data source with the join key) or one (meaning only one data source has that join key)
An array of tags and an array of values represent the combination.
The size of those two arrays is guaranteed to be the same and equal to the number of tagged records in the combination.
As tags correspond to the data sources, in the canonical case of joining two data sources, being called twice.
For the left side, the tags and values arrays are like this:   2
It’s a sensible choice to loop through the values[] array to get the default alphabetical ordering based on data source names.
For each legal Note that the join key is still present in each element of the values[] array.
Otherwise the join key will be shown multiple times in one output record.
It’s unclear why as DataJoinReducerBase ignores the tag in the TaggedMapOutput object.
Listing 5.3 shows the complete code, including our reduce subclass.
Listing 5.3 Inner join of data from two files using reduce-side join.
We ignore those details to focus on the their contents.
Next we’ll look at another way of doing joins that is more efficient in some common applications.
The reduce-side join technique discussed in the last section is flexible, but it can also be quite inefficient.
We shuffle all data across the network first, and in many situations we drop the majority of this data during the joining process.
It would be more efficient if we eliminate the unnecessary data right in the map phase.
Even better would be to perform the entire joining operation in the map phase.
The main obstacle to performing joins in the map phase is that a record being processed by a mapper may be joined with a record not easily accessible (or even located) by that mapper.
If we can guarantee the accessibility of all the necessary data when joining a record, joining on the map side can work.
For example, if we know that the two sources of data are partitioned into the same number of partitions and the partitions are all sorted on the key and the key is the desired join key, then each mapper (with the proper InputFormat and RecordReader ) can deterministically locate and retrieve all the data necessary to perform joining.
Unfortunately, situations where we can naturally apply this are limited, and running extra MapReduce jobs to repartition the data sources to be usable by this package seems to defeat the efficiency gain.
There’s another data pattern that occurs quite frequently that we can take advantage of.
When joining big data, often only one of the sources is big; the second source may be orders of magnitude smaller.
When the smaller source can fit in memory of a mapper, we can achieve a tremendous gain in efficiency by copying the smaller source to all mappers and performing joining in the map phase.
This is called replicated join  in the database literature as one of the data tables is replicated across all nodes in the cluster.
The next section will cover the case when the smaller source doesn’t fit in memory.)
Hadoop has a mechanism called distributed cache that’s designed to distribute files to all nodes in a cluster.
It’s normally used for distributing files containing “background” data needed by all mappers.
For example, if you’re using Hadoop to classify documents , you may have a list of keywords for each class.
For executing replicated joins, we consider the smaller data source as background data.
Distributed cache is handled by the appropriately named class DistributedCache.
First, when configuring a job, you call the static to all nodes.
These files are specified as URI objects, and they default to HDFS unless a different filesystem is specified.
The JobTracker will take this list of URIs and create a local copy of the files in all the TaskTrackers when it starts the job.
In the second step, your mappers on each individual TaskTracker will call the static method the local copy is located.
At this point the mapper can use standard Java file I/O techniques to read the local copy.
We plan on performing the joining in the map phase and will configure this job to have no reducers.
The crucial addition here is where we take the file specified by the first argument and add it to DistributedCache.
When we run the job, each node will create a local copy of that file from HDFS.
The second and third arguments denote the input and output paths of the standard Hadoop job.
Note that we’ve limited the number of data sources to two.
This is not an inherent limitation of the technique, but a simplification that makes our code easier to follow.
Mapper interface (and also the Reducer interface) has two more abstract methods, The MapReduceBase class provides default no-op implementations for these methods.
This way we can have the data available each time we call.
As our driver method has only pushed one file (given by our first argument) into DistributedCache, this should be an array of size one.
For our purpose, the program assumes each line is a record, the key/value pair is comma separated, and the key is unique and will be used for joining.
The program reads this source file into a Java Hashtable called joinData that’s available throughout the mapper’s lifespan.
If we don’t find the join key in joinData, we drop the record.
Otherwise, we match the (join) key to the value in joinData and concatenate the values.
The result is outputted directly into HDFS as we don’t have any reducer for further processing.
A not-infrequent situation in using DistributedCache is that the background data (the smaller data source in our data join case) is in the local filesystem of the client rather than stored in HDFS.
One way to handle this is to add code to upload the local file on the client to HDFS before calling DistributedCache.
With these minor changes our DistributedCache join program will take a local file on the client machine as one of the input sources.
One of the limitations in using replicated join is that one of the join tables has to be small enough to fit in memory.
Even with the usual asymmetry of size in the input sources, the smaller one may still not be small enough.
You can solve this problem by rearranging the processing steps to make them more efficient.
Both the Orders and Customers tables may be too big for replicated join and you’ll have to resort to the inefficient reduce-side join.
A better approach is to first filter out customers living in the 415 area code.
There is some overhead in creating and distributing the Customers415 file, but it’s often compensated by the overall gain in efficiency.
Sometimes you may have a lot of data to analyze.
You can’t use replicated join no matter how you rearrange your processing steps.
We still have ways to make reduce-side joining more efficient.
Recall that the main problem with reduceside joining is that the mapper only tags the data, all of which is shuffled across the network but most of which is ignored in the reducer.
The inefficiency is ameliorated if the mapper has an extra prefiltering function to eliminate most or even all the unnecessary data before it is shuffled across the network.
When processing records from Customers and Orders, the mapper will drop any record.
This is sometimes called a semijoin , taking the terminology from the database world.
This situation calls for a data structure called a Bloom filter.
A Bloom filter is a compact representation of a set that supports only the contain query.
Does this set contain this element?”) Furthermore, the query answer is not completely accurate, but it’s guaranteed to have no false negatives and a small probability of false positives.
The slight inaccuracy is the trade-off for the data structure’s compactness.
It still guarantees the correctness of the data join algorithm.
The Bloom filter will also pass a small portion of customers not in the 415 area code to the reduce phase.
This is fine because those will be ignored in the reduce phase.
We’ll still have improved performance by reducing dramatically the amount of traffic shuffled across the network.
The use of Bloom filters is in fact a standard technique for joining in distributed databases, and it’s used in commercial products such as Oracle 11g.
We’ll describe Bloom filter and its other applications in more details in the next section.
If you use Hadoop for batch processing of large data sets, your data-intensive computing needs probably include transaction-style processing as well.
We won’t cover all the techniques for running real-time distributed data processing (caching, sharding, etc.)
They aren’t necessarily Hadoop-related and are well beyond the scope of this book.
One lesser-known tool for real-time data processing is the Bloom filter, which is a summary of a data set whose usage makes other data processing techniques more efficient.
When that data set is big, Hadoop is often called in to generate the Bloom filter representation.
As we mentioned earlier, a Bloom filter is also sometimes used for data joining within Hadoop itself.
As a data processing expert, you’ll be well rewarded to have the Bloom filter in your bag of tricks.
In this section we’ll explain this data structure in more detail and we’ll go through an online ad network example that will build a Bloom filter using Hadoop.
The probability of false positives depends on the number of elements in the set and some configuration parameters of the Bloom filter itself.
The major benefit of a Bloom filter is that its size, in number of bits, is constant and is set upon initialization.
Adding more elements to a Bloom filter doesn’t increase its size.
A Bloom filter also has another configuration parameter to denote the number of hash functions it uses.
We’ll discuss the reason for this parameter and how the hash functions are used later when we discuss the Bloom filter’s implementation.
For now, its main implication is that it affects the false positive rate.
In practice, m and n are determined by the requirement of the system, and therefore, k is chosen to minimize the false positive rate given m and n, which (after a little calculus) is.
The false positive rate with the given k is 0.6185m/n, and k has to be an integer.
From a design point of view, one should think in terms of (m/n), number of bits per element, rather than m alone.
In summary, the signature of our Bloom filter class will look like the following:
More applications of the Bloom filter The Bloom filter found its first successful applications back when memory was scarce.
Not being able to store a whole dictionary in memory, spellcheckers used a Bloom filter representation (of the dictionary) to catch most misspellings.
As memory size grew and became cheaper, such space consideration waned.
Bloom filter usage is finding a resurgence in large-scale dataintensive operations as data is fast outgrowing memory and bandwidth.
We’ve already seen commercial products, such as Oracle 11g, using  Bloom filters to join data across distributed databases.
Squid caches frequently accessed web content to save bandwidth and give users a faster web experience.
In a cluster of Squid servers, each one can cache a different set of content.
An incoming request should be routed to the Squid server holding a copy of the requested content, or in case of a cache miss, the request is passed on to the originating server.
The routing mechanism needs to know what each of the Squid servers contains.
As sending a list of URLs for each Squid server and storing it in memory is expensive, Bloom filters are used.
A false positive means a request is forwarded to the wrong Squid server, but that server would recognize it as a cache miss and pass it on to the originating server, ensuring the correctness of the overall operation.
The small performance hit from a false positive is far outweighed by the overall improvement.
In a nutshell, database sharding is the partitioning of a database across multiple machines such that each machine only has to deal with a subset of records.
Each record has some ID that determines which machine it’s assigned to.
In more basic designs, the ID is hashed statically to one of a fixed number of database machines.
This approach is inflexible to adding more shards or rebalancing existing ones.
To add flexibility, it uses a dynamic look-up for each record ID, but unfortunately that adds processing delay if the look-up is done through a database (i.e., using disk)
Like Squid, more advanced shard systems use in-memory Bloom filters as a fast look-up.
It needs some mechanism to handle false positives, but the occurrence is small enough to not impact the overall performance improvement.
For online display ad networks, it’s important to be able to target an ad from the right category to a visitor.
Given the volume of traffic a typical ad network receives and the latency requirements, one can end up spending a lot of money on hardware to have the capability of retrieving the category in real time.
A design based on Bloom filters can dramatically decrease that cost.
Use an offline process to tag web pages (or visitors) on a limited number of categories (sports, family-oriented, music, etc.)
Build a Bloom filter for each category and store it in memory at the ad servers.
When an ad request arrives, the ad servers can quickly and cheaply determine which category of ads to show.
Conceptually the implementation of a Bloom filter is quite straightforward.
We describe its implementation in a single system first before implementing it using Hadoop in a distributed way.
The internal representation of a Bloom filter is a bit array of size m.
We use the integer output as an index into the bit array.
When we “add” an element to the Bloom filter, we use the hash functions to generate k indexes into the bit array.
Figure 5.3 shows what happens when we add several objects (x, y, and z) over time, in a Bloom filter that uses three hash functions.
Note that a bit will be set to 1 regardless of its previous state.
The number of 1s in the bit array can only grow.
When an object comes in and we want to check whether it has been previously added to the Bloom filter, we use the same k hash functions to generate the bit array indexes as we would do in adding the object.
Now we check whether all those k bits in the bit array are 1s.
If all k bits are 1, we return true and claim that the Bloom filter contains the object.
We see that if the object has in fact been added before, then the Bloom filter will necessarily return true.
There are no false negatives (returning false when the object is truly in the set)
The k bits corresponding to the queried object can all be set to 1 even though the object has never been added to the set.
It may happen that adding other objects set those bits leading to false positives.3
Our implementation of a Bloom filter in Java would use the Java BitSet class as its internal representation.
We have a function getHashIndexes(obj) that takes an object and returns an integer array of size k, containing indexes into the BitSet.
Figure 5.3 A Bloom filter is a bit array that represents a set with some probability of false positives.
Objects (such as x, y, and z) are deterministically hashed into positions in the array, and those bits are set to 1
You can check whether an object is in the set or not by hashing and checking the values of those bit positions.
An ingenious way of creating a Bloom filter for the union of two sets is by OR’ing the (bit array of the) Bloom filters of each individual set.
As adding an object is setting certain bits in a bit array to 1, it’s easy to see why this union rule is true:
We’ll be exploiting this union trick to build Bloom filters in a distributed fashion.
Each mapper will build a Bloom filter based on its own data split.
We’ll send the Bloom filters to a single reducer, which will take a union of them and record the final output.
As the Bloom filter will be shuffled around as the mappers’ output, the BloomFilter class will have to implement the Writable interface, which consists of methods internal BitSet representation and a byte array such that the data can be serialized to DataInput/DataOutput.
Next we’ll create the MapReduce program to make a Bloom filter   using Hadoop.
As we said earlier, each mapper will instantiate a BloomFilter object and add the key of each record in its split into its BloomFilter instance.
We’re using the key of the record to follow our data joining example.) We’ll create a union of the BloomFilters by collecting them into a single reducer.
Our mappers will output a key/value pair where the value is a BloomFilter instance.
The output key will not matter in terms of partitioning because we only have a single reducer.
We want our reducer to output the final BloomFilter as a binary file.
Hadoop’s OutputFormats outputs either text files or assumes a key/value pair.
Our reducer, therefore, won’t use Hadoop’s MapReduce output mechanism and instead we’ll write the result out to a file ourselves.
Your tasks are no longer guaranteed to be idempotent and you’ll need to understand how various failure scenarios can affect your tasks.
For example, your files may only be partially written when some tasks are restarted.
Our example here is safe(r) because all the file operations take A more careful/paranoid implementation would check each individual file operation more closely.
Recall that our strategy for the mapper is to build a single Bloom filter   on the entire split.
The BloomFilters generated by all the mappers are sent to a single reducer.
As we mentioned earlier, we want the final BloomFilter to be written out in a file of our own format rather than one of Hadoop’s OutputFormats.
We had already set the reducer’s OutputFormat to NullOutputFormat in the driver to turn off that output will have to know the output path as specified by the user, which can be found in the job configuration object.
We handle this oversight the same way we handled OutputCollector in the mapper.
We keep a reference to the JobConf object in the Reduce Hadoop’s file I/O   to write out our BloomFilter in binary to a file in HDFS.
Listing 5.5 A MapReduce program to create a Bloom filter.
Hadoop version 0.20 has a Bloom filter class in it.
It plays a support role to some of the new classes introduced in version 0.20, and it will likely stay around for future versions as well.
It functions much like our BloomFilter class in listing 5.4, although it’s much more rigorous in its implementation of the hashing functions.
As a built-in class, it can be a good choice for semijoin   within Hadoop.
But it’s not easy to separate this class from the Hadoop framework to use it as a standalone class.
If you’re building a Bloom filter for non-Hadoop applications, Hadoop’s built-in BloomFilter may not be appropriate.
You can test your understanding of more advanced MapReduce techniques through these exercises:
Write a MapReduce program to aggregate the number of visits for each IP address.
Write another MapReduce program to find the top K IP addresses in terms of visits.
These frequent visitors may be legitimate ISP proxies (shared among many users) or they may be scrapers and fraudsters (if the server log is from an ad network)
Chain these two MapReduce jobs   together such that they can be easily run on a daily basis.
So far we’ve had to explicitly or implicitly filter out that row in our mappers, or interpret our results knowing that the metadata record has some deterministic influence.
A more permanent solution is to remove the metadata row from the input data and keep track of it elsewhere.
Another solution is to write a mapper as a preprocessor that filters all records that look like metadata.
For example, records that don’t start with a numeric patent number.) Write such a mapper and use ChainMapper /ChainReducer to incorporate it into your MapReduce programs.
A business may choose to re-engage these customers with discounts or other incentives.
Calculating ratios—Ratios are often a better unit of analysis than raw numbers.
For example, say you have a data set of today’s stock prices and another data set for stock prices from yesterday.
You may be more interested in each stock’s growth rate than its absolute price.
Use the datajoin framework to write a program that takes two data sources and output the ratio.
Product of a vector with a matrix—Look up your favorite linear algebra text on the definition of matrix multiplication.
Implement a MapReduce job to take the product of a vector and a matrix.
You should use DistributedCache to hold the value of the vector.
You have one file with the location of foos, and another file with the location of bars.
Each record in those files is a comma-separated (x,y) coordinate.
Although both foos and bars are relatively sparse in this 2D space, their respective files are too big to be stored in memory.
Hint: The datajoin package as it’s currently implemented doesn’t work that well for this problem either, but you can solve it with your own mapper and reducer that have a similar data flow as the datajoin package.
Hint #2: In all the MapReduce programs we’ve discussed up till now, the keys are only extracted and passed around, whereas the values go through various computations.
You should consider computing the key for the mapper’s output.
Spatial join, enhanced with Bloom filter—After you’ve answered the last question, figure out how you can use a Bloom filter to speed up the join operation.
Assume bars are much fewer in number than foos, but still too many to fit all their locations in memory.
We can often write the basic MapReduce programs as one job operating on one data set.
We may need to write the more advanced programs as multiple jobs or we may operate them on multiple data sets.
Hadoop has several different ways of coordinating multiple jobs together, including sequential chaining and executing them according to predefined dependencies.
For the frequent case of chaining map-only jobs around a full MapReduce job, Hadoop has special classes to do it efficiently.
Joining is the canonical example for processing data from multiple data sources.
Though Hadoop has a powerful datajoin package for doing arbitrary joins, its generality comes at the expense of efficiency.
A couple other joining methods can provide faster joins by exploiting the relative asymmetry in data source sizes typical of most data joins.
One of these methods leverages the Bloom filter, a data structure that’s useful in many data processing tasks.
At this point, your knowledge of MapReduce programming should enable you to start writing your own programs.
As all programmers know, programming is more than writing code.
You have various techniques and processes—from development to deployment and testing and debugging.
The nature of MapReduce programming and distributed computing adds complexity and nuance to these processes, which we’ll cover in the next chapter.
Many of the tools to enhance Hadoop (such as Pig, Hive, and CloudBase) offer data joins as a first-class operation.
The limitation of this technique is that the side data is replicated to every TaskTracker, and the side data must fit into memory.
Jimmy Lin and colleagues explore the use of memcached , a distributed in-memory object caching system, to provide global access to side data.
Now that you’ve gone through various programming techniques in MapReduce, this chapter will step back and cover programming practices.
Programming on Hadoop differs from traditional programming mainly in two ways.
Second, Hadoop programs are run over a distributed set of computers.
Performance tuning techniques tend to be specific to the programming platform, and Hadoop is no different.
We cover tools and approaches to optimizing Hadoop programs in section 6.3
Presumably you’re already familiar with standard Java software engineering techniques.
We focus on practices unique to data-centric programming within Hadoop.
Chapter 2 discussed the three modes   of Hadoop: local   (standalone ), pseudo-distributed , and fully distributed.
Your development process will go through each of the three modes.
You’ll have to be able to switch between configurations easily.
In practice you may even have more than one fully distributed cluster.
Larger shops may, for example, have a “development” cluster to further harden MapReduce programs before running them on the real production cluster.
For example, there can be an in-house cluster for running many small- to medium-sized jobs and a cluster in the cloud that’s more cost effective for running large, infrequent jobs.
Section 2.3 discussed how you can have different versions of the hadoop-site.
You can also specify the exact configuration file you want at each Hadoop command with the -conf option.
Before you run and test your Hadoop program, you’ll need to make data available for the configuration you’re running.
Section 3.1 describes various ways to get data into and out of HDFS.
For local and pseudo-distributed modes, you’ll only want a subset of your full data.
As it’s a Python script, you can also use it to sample down a local file with a Unix pipe:
Now that you have all the different configurations set up and know how to put data into each configuration, let’s look at how to develop and debug in local and pseudodistributed modes.
The techniques build on top of each other as you get closer to the production environment.
We defer the discussion of debugging on the fully distributed cluster ‘till the next section.
Hadoop in local mode runs everything within one single Java Virtual Machine (JVM) and uses the local filesystem (i.e., no HDFS)
Using files from the local filesystem means you can quickly apply Unix commands or simple scripts on the input and output data.
Examining files in HDFS, on the other hand, is limited to commands provided by the Hadoop command line.
For example, to count how many records are in an output file, you can use wc -l if the file is in the local filesystem.
If the file is in HDFS, then you’ll either have to write a MapReduce program or download the file to local storage before applying the Unix commands.
As you’ll see, being able to access input and output files easily will be important to our development practices under local mode.
For example, it doesn’t support distributed cache , and it only allows a maximum of one reducer.
A program running in local mode will output all log and error messages to the console.
It will also summarize the amount of data processed at the end.
For example, running our skeleton MapReduce job (MyJob.java) to invert the patent citation data , the output is quite verbose, and figure 6.1 is a snapshot in the middle of the job.
At the end of the job, Hadoop will print out the values of various internal counters.
They’re the number of records and bytes going through the different stages of MapReduce:
Figure 6.1 Running a Hadoop program in local mode outputs all the log messages to the console.
The input and output of the MapReduce job are both in the local filesystem.
We can examine them using standard Unix commands such as wc   -l or head.
As we are deliberately using smaller data sets during development, we can even load them into a text editor or a spreadsheet.
We can use the many features of those applications to sanity check the correctness of our program.
Most MapReduce programs involve at least some counting or arithmetic, and bugs (especially typos) in mathematical programming don’t call attention to themselves in the form of thrown exceptions or threatening error messages.
Your math can be wrong even though your program is technically “correct,” and everything will run smoothly, but the end result will be useless.
There’s no simple way to uncover  arithmetic mistakes, but some sanity checking will go a long way.
At a high level you can look at the overall count, maximum, average, and so on, of various metrics and see if they match expectation.
At a low level you can pick a particular output record and verify that it was produced correctly.
For example, when we created the inverted citation graph, the first few lines were.
We can verify this claim by grepping   over the sampled input data to look for records where patent number 1 is cited.
You can verify a few more records to gain confidence in the correctness of your program’s math and logic.1
An eyesore about the output of this inverted citation graph is that the first line is not real data.
It’s an artifact from the first line of the input data being used as data definition.
Let’s add some code to our mapper to filter out non-numeric keys and values, and in the process demonstrate regression testing.
For our particular change, we should only be taking out one line from the job’s output.
To verify that this indeed is the case, let’s first save the output of our current job.
In local mode, we have a maximum of only one reducer, so the job’s output is only one file, which we call job_1_output.
For regression testing, it’s also useful to save the output of the map phase.
This will help us isolate bugs to either the map phase or the reduce phase.
We can save the output of the map phase by running the MapReduce job with zero reducers.
We can there will be multiple files as each map task will write its output to its own file.
Let’s copy all of them into a directory called job_1_intermediate.
Compile and execute the new code against the same input data.
Let’s run it as a maponly job first and compare the intermediate data.
As we’ve only changed the mapper, any bug should first manifest in differences in the intermediate data.
In this case, you may suspect whether patent number 1 is really cited by those two patents.
The number 1 feels wrong, an outlier in the range of patent numbers being cited.
We have to track down the patents themselves if we want to verify this.
In any case, ensuring data quality is an important topic but is beyond our discussion of Hadoop.
This is an internal file for HDFS to keep checksums   for the file part-00000
A difference in checksum means that part-00000 has changed, and diff prints out the exact differences later.
The new intermediate file, under output/test, is missing the quoted field descriptors.
More reducer, we expect the final output to differ by one line too.
If you run the whole job with one reducer and compare the final output with job_1_output from the original run, you’ll find many differences.
What do you think happened? Let’s look at the first few lines of the diff to find out.
As to the rest of the differences, there’s a definite pattern.
Hadoop doesn’t provide any guarantee as to the order of those values.
We see that taking out one line in the intermediate data impacts the order of the values for many keys at the reducer.
As we know that this job’s correctness is invariant to the ordering, we can ignore the differences.
Regression testing is inherently conservative and tends to set off false alarms.
We have advocated the use of a sampled data set for development, because it is more representative of the structure and properties of the data set we use in production.
We have used the same sampled data set for regression testing, but you can also manually construct a separate input data set with edge cases that are atypical of the production data.
For example, you may put in empty values or extra tab characters or other unusual.
This test data set   is for ensuring that your program continues to handle the edge cases even as it evolves.
This test data set doesn’t need to have more than several dozen records.
You can visually inspect the entire output to see if your program still functions as expected.
Most Java programmers instinctively default to the int type   (or Integer or IntWritable ) to represent integer values.
When you’re processing Hadoop-scale data, it’s not unusual for some counter variables to need a bigger range.
You won’t see this requirement under your development data set , which by design is small.
It may not even matter to your current production data, but as your business operation grows, your data set will get bigger.
It may get to a point where some variables will outgrow the int range and cause arithmetic errors.
But as you grow to process tens of millions or hundreds of millions of documents, counting frequent words like the can cross the limit of an int type.
Rather than wait for this kind of bug to creep up on you in production, which is much harder to debug and costlier to fix, now is the time you should go through your code and carefully consider whether your numeric variables should be long or LongWritable   to handle future scale.3
Local mode has none of the distributed characteristics of a production Hadoop cluster.
We may not see many bugs when running in local mode.
All the software components are distributed, and pseudo-distributed mode differs from a production cluster at only the system-and-hardware level.
We should make sure our jobs can run in pseudo-distributed mode before deploying them to a full production cluster.
Chapter 2 describes the configuration and commands to start pseudo-distributed mode.
You’ll start all the daemons on your computer to make it function like a cluster.
You interface with it as if it is a distinct Hadoop cluster.
You submit your jobs to it for running rather than run them in the same user space.
Most importantly, you now monitor it “remotely” through log files and the web interface.
You’ll use the same tools later to monitor a production cluster.
This is not absolutely true and will depend on your documents’ size and content.
More recently, almost all web operations that have experienced explosive growth (such as Facebook, Twitter, and RockYou) have had to retool their systems to handle a bigger range of user IDs or document IDs than they originally expected.
Let’s run in the pseudo-distributed cluster the same job we had in local mode.
You put the input file into HDFS using the hadoop fs command.
Submit the job for running using the same hadoop jar command as in local mode.
The first thing you’ll notice is that you no longer have the torrent of messages on your console.
You only get a measure of progress in the map phase and reduce phase, and the same summary of counters   at the end as in local mode.
Figure 6.2 In pseudo-distributed mode, the console only outputs a job’s progress and its counters at the end.
You can find the log files under the /logs directory.
Under the default setting, Hadoop doesn’t delete old log files automatically.
You should proactively archive and delete them to make sure they’re not taking up too much space.
Log files for the NameNode , SecondaryNameNode , DataNode , and JobTracker are used for debugging the respective services.
In production clusters, you as a system administrator can look at them to debug problems in those corresponding nodes.
As a programmer, you are always interested in the TaskTracker log though, as it records exceptions thrown.
Your MapReduce program can output to STDOUT and STDERR (System.out and System.err in Java) its own logging   messages.
Hadoop records those under files named stdout and stderr, respectively.
There will be a distinct file for each task attempt.
A task can have more than one attempt if the first one fails.) These user log files are under the /logs/userlogs subdirectory.
Besides logging to STDOUT and STDERR, your program can also send out live status updated by sending a string of the form reporter:status:message to STDERR.) This is useful for long-running jobs where you can monitor them as they run.
The status message is shown on the JobTracker Web UI, to be described next.
By definition events occur in many different places in a distributed program.
The system becomes more like a black box, and we need specialized monitoring tools to peek into the various states within it.
The JobTracker provides a web interface for tracking the progress and various states of your jobs.
Under the default configuration, you can set your browser to.
A job ID is a string prefixed with job_, followed by the cluster ID   (which is a timestamp of when the cluster was started), followed by an auto-incremented job number.
The web UI lists each job with the user name and job name.
In pseudo-distributed mode, it’s relatively easy to identify the job.
In fully distributed mode, replace “localhost” with the domain of the JobTracker master.
When you get to a multiuser production environment, you’ll have to narrow down your jobs by looking for your Hadoop user name and the name of your current job.
The name of your job is set is set through a configuration property shown in table 6.1
In the administration page, you can see each job with the completion percentage of its map phase.
It shows the number of map tasks for the job and the number of completed ones.
You can see the same metrics for the reduce side.
This gives you a rough summary of your job’s progress.
To drill down more on a particular job, you can click on the job ID, which is a link that’ll take you to the job’s administration page.
The job page shows the volume of various input/output due to the running of the job.
The page refreshes itself periodically but you can also refresh the page manually to get the updated numbers.
You can start exploring the various aspects of your job from the many links on this page.
For example, clicking on the map link will take you to a list of all map tasks for the job.
Figure 6.4 The JobTracker ’s administration page for a single job.
This figure shows all the map tasks for a single job.
You further append it with an auto-incremented number within each group.
In the TaskTracker Web UI, you’ll see each task with its status , which.
Clicking on a task ID will bring you to a page that further describes different attempts of a task.
Hadoop makes several retry attempts at a failed task before failing the entire job.
The JobTracker and TaskTracker UIs provide many other links and metrics.
Unfortunately, sometimes a job goes awry after you’ve started it but it doesn’t actually fail.
It may take a long time to run or may even be stuck in an infinite loop.
In (pseudo-) distributed mode you can manually kill a job using the command.
After successfully running your job in a pseudo-distributed cluster, you’re ready to run it on a production cluster using real data.
We can apply all the techniques we’ve used for development and debugging on the production cluster, although the exact usage may be slightly different.
Your cluster should still have a JobTracker Web UI, but the domain is no longer localhost.
The port number will still be 50030 unless it’s been configured differently.
In pseudo-distributed mode, when there’s only one node, all the log files are in a single /logs directory that you can access locally.
In a fully distributed cluster, each node has its own /logs directory to keep its log files.
You can diagnose problems on a node through the log files of that particular node.
In addition to the development and testing techniques we’ve mentioned so far, you also have monitoring and debugging techniques that are more useful in a production cluster on real data, which we explore in this section.
You can instrument your Hadoop job with counters to profile its overall operation.
Your program defines various counters and increments their counts in response to specific events.
Hadoop automatically sums the same counter from all tasks (of the same job) so that it reflects the profile of the overall job.
It displays the value of your counters in the JobTracker’s Web UI along with Hadoop’s internal counters.
The canonical application of counters   is for tracking different input record types, particularly for tracking “bad” records.
Recall from section 4.4 our example for finding the average number of claims for patents from each country.
We know the number of claims is not available for many records.
Our program skips over those records, and it’s useful to know the number of records we’re skipping.
You use uniquely named counters counter is initialized and takes on the increment value.
The first form is more general in that it allows you to specify the counter name with dynamic strings at run time.
The combination of two strings, group and counter, uniquely defines a counter.
When counters are reported (in the Web UI or as text at the end of a job run), counters of the same group are reported together.
The second form uses a Java enum to specify counter   names, which forces you to have them defined at compile time, but it also allows for type checking.
The enum’s name is used as the group string, whereas the enum’s field is used as the counter string.
Only the first row of column description should be a “quoted” value.) An enum called ClaimsCounters is defined with values MISSING and QUOTED.
Logic in the code increments the counters to reflect the record it’s processing.
Listing 6.1 A MapClass with Counters to count the number of  missing values.
After running the program, we can see the defined counters along with Hadoop’s internal counters in the JobTracker’s Web UI.
Figure 6.6 JobTracker ’s Web UI collects and shows the counter information.
We see that the enum’s fully qualified Java name (with $ to separate out the inner class hierarchy) is used as the group name.
The fields MISSING and QUOTED are used to define separate counters.
As expected, it increments the QUOTED counter only once and the MISSING counter 939,867 times.
The numbers seem consistent and we can feel more confident about the correctness of the processing.
It needs to send a specially formatted line to STDERR in the form of.
Hadoop Streaming will not properly interpret the string without that.
When dealing with large data sets, it is inevitable that some records will have errors.
It’s not unusual to focus several iterations of your development cycle on making the program robust to unexpected data.5 Your program may never be completely foolproof, though.
Your program will process new data, and new data will think of new ways to misbehave.
You may even be using a parser that depends on third-party libraries you have no control over.
While you should make your program as robust as possible to malformed records, you should also have a recovery mechanism to handle the cases you couldn’t plan for.
You don’t want your whole job to fail only because it fails to handle one bad record.
Hadoop’s mechanism for recovering from hardware failures doesn’t work for recovering from deterministic software failures caused by bad records.
Instead it provides a feature for skipping over records that it believes to be crashing a task.
If this skipping feature is on, a task will enter into skipping mode after the task has been retried several times.
Once in skipping mode, the TaskTracker   will track and determine which record range is causing failure.
The TaskTracker will then restart the task but skip over the bad record range.
Someone once told me he had a program that was crashing in processing users’ geographical information.
Further digging revealed that one user was from a real city named Null.
The skipping feature is available starting with version 0.19, but it’s disabled by default.
In Java, the feature is controlled through the class SkipBadRecords, which consists entirely of static methods.
The job driver needs to call one or both methods:
The driver calls the methods with the configuration object and the maximum number of records in a skip range.
If the maximum skip range size is set to 0 (default), then record skipping is disabled.
It executes the task with the skip range halved each time, and determines the half with the bad record(s)
The process iterates until the skip range is within the acceptable size.
This is a rather expensive operation, particularly if the maximum skip range size is small.
You may need to increase the maximum number of task attempts in Hadoop’s normal task recovery mechanism to accommodate the extra attempts.
You can do this using the or set the equivalent properties mapred.map.max.attempts and mapred.reduce.
If skipping   is enabled, Hadoop enters skipping mode after the task has failed twice.
You can set the number of task failures needed to trigger skipping mode in int attemptsToStartSkipping)
Hadoop will log skipped records to HDFS for later analysis.
We cover sequence files in more detail in section 6.3.3
For now you can think of it as a Hadoop-specific compressed format.
You can change the log directory for skipped records from _log/skip using the method SkipBadRecords.setSkipOutputPath(JobConf conf, Path path)
If path is set to null or to a Path with a string value of "none", Hadoop will not record the skipped records.
Although you can set the record-skipping feature in Java by calling methods in SkipBadRecords in your driver, sometimes you may want to set this feature using the generic options available in GenericOptionsParser instead.
This is because the person running the program can have a better idea about the range of bad records to expect and set the parameters more appropriately than the original developer.
Furthermore, Streaming programs can’t access SkipBadRecords; the record skipping features must.
Table 6.2 Equivalent JobConf  properties to method calls in SkipBadRecords.
Table 6.2 shows the JobConf properties being set by the SkipBadRecords method calls.
Their default values are fine for most Java programs but we need to change them for Streaming ones.
In determining the record range to skip, Hadoop needs an accurate count of the number of records a task has processed.
Hadoop uses an internal counter and by default it’s incremented after each call to the map (reduce) function.
For Java programs this is a good approach to track the number of records processed.
It can break down in some cases, such as programs that process records asynchronously (say, by spawning threads) or buffer them to process in chunks, but it usually works.
In Streaming   programs, this default behavior wouldn’t work at all because there’s no equivalent of the map (reduce) function that gets called to process each record.
In those situations you have to disable the default behavior by setting the Boolean properties to false, and your task has to update the record counters itself.
In Python, the map task can update the counter with.
Java programs that cannot depend on the default record counting should use.
Debugging through log files is about reconstructing events using generic historical records.
Sometimes there’s not enough information in the logs to trace back the cause of failure.
Hadoop has an IsolationRunner  utility that functions like a time machine for debugging.
This utility can isolate and rerun the failed task with the exact same input on the same node.
You can attach a debugger to monitor the task as it runs and focus on gathering evidence specific to the failure.
To use the IsolationRunner feature, you must run your job with the configuration property keep.failed.tasks.files set to true.
This tells every TaskTracker to keep all the data necessary to rerun the failed tasks.
When a job fails, you use the JobTracker Web UI to locate the node, the job ID, and the task attempt ID of the failed task.
You log into the node where the task failed and go to the work directory under the directory for the task attempt.
Note that Hadoop allows a node to use multiple local directories (by setting mapred.local.dir to a comma-separated list of directories) to spread out disk I/O among multiple drives.
If the node is configured that way, you’ll have to look in all the local directories to find the one with the right attempt_id subdirectory.
Within the work directory you can execute IsolationRunner to rerun the failed task with the same input that it had before.
In the rerun, we want the JVM to be enabled for remote debugging.
As we’re not running the JVM directly but through the bin/ hadoop script, we specify the JVM debugging options through HADOOP_OPTS:
The job.xml file contains all the configuration information IsolationRunner needs.
Given our specification, the JVM will wait for a debugger’s attachment before executing the task.
For example, if you’re using jdb , you can attach it to the JVM via.
Options to configure the Sun JVM for debugging are further explained in Sun’s documentation: http:// java.sun.com/javase/6/docs/technotes/guides/jpda/conninv.html#Invocation.
I hope you’re using something better than jdb!) Consult your IDE’s documentation for how to connect its debugger to a JVM.
After you have developed your MapReduce   program and fully debugged it, you may want to start tuning it for performance.
Before doing any optimization, note that one of the main attractions of Hadoop   is its linear scalability.
You can speed up many jobs by adding more machines.
This makes economic sense when you have a small cluster.
Consider the value of time it takes to optimize your program to gain a 10 percent improvement.
The cost of your development time may well be higher than the cost of the additional computer.
At that scale the brute force approach of adding hardware to boost performance may be less cost effective.
Hadoop has a number of specific levers and knobs for tuning performance, some of which boost the effectiveness of the cluster as a whole.
We cover those in the next chapter when we discuss system administration issues.
In this section we examine techniques that can be applied on a per-job basis.
Combiner can reduce the amount of data shuffled between the map and reduce phases, and lower network traffic improves execution time.
The details and the benefits of using combiner   are thoroughly described in section 4.6
We mention it here again for the sake of completeness.
When processing large data sets , sometimes a nontrivial portion of the processing time is spent scanning data from disk.
Reducing the number of bytes to read can enhance overall throughput.
The simplest way to reduce the amount of bytes processed is to reduce the amount of data processed.
We can choose to process only a sampled subset of the data.
Often your MapReduce jobs don’t use all the information in the input data set.
It has almost a couple dozen fields, yet most of our jobs access only a few common ones.
It’s inefficient for every job on that data set to read the unused fields every time.
One can “refactor” the input data into several smaller data sets.
Each has only the fields necessary for a particular type of data processing.
This technique is similar in spirit to vertical partitioning and column-oriented databases in the relational database management system (RDBMS) world.
Finally, you can reduce the amount of disk and network I/O by compressing your data.
You can apply this technique to the intermediate as well as output data sets.
Hadoop has many options for data compression, and we devote the next subsection to this topic.
Even with the use of a combiner, the output of the map phase can be large.
This intermediate data has to be stored on disk and shuffled across the network.
Compressing this intermediate data will improve performance for most MapReduce jobs, and it’s easy too.
Enabling compression on the mapper’s output involves setting two configuration properties, as you can see in table 6.3
Table 6.3 Configuration properties to control the compression of mapper ’s output.
Class property denoting which CompressionCodec to use for compressing mapper’s output.
To enable compression on the mapper’s output, you set mapred.compress.map.
In addition, you should set mapred.map.output.compression.codec to the appropriate codec class.
Hadoop supports a number of compression codecs (see table 6.4)
For example, to use GZIP compression, you can set the configuration object:
Table 6.4 List of codecs   available under the org.apache.hadoop.io.compress package.
By Hadoop convention filenames for these files end in .deflate.
This compression format is unique in that it’s splittable for Hadoop, even when used outside the sequence file format.
Data output from the map phase of a job is used only internally to the job, so enabling compression for this intermediate data is transparent to the developer and is a no-brainer.
As many MapReduce applications involve multiple jobs, it makes sense for jobs to be able to output and input in compressed form.
It’s highly recommended that data that are passed between Hadoop jobs use the Hadoop-specific sequence file format.
Sequence file   is a compressable binary file format for storing key/value pairs.
Recall that one of the parallelisms of Hadoop is its ability to split an input file   for reading and processing by multiple map tasks.
If the input file is in a compressed format, Hadoop will have to be able to split the file such that each split can be decompressed by the map tasks independently.
Otherwise parallelism is destroyed if Hadoop has to decompress the file as a whole first.
Not all compressed file formats are designed for splitting and decompressing in chunks.
The file format provides sync markers  to Hadoop to denote splittable boundaries.7
In addition to its compressability and splittability, sequence files   support binary keys and values.
Therefore, a sequence file is often used for processing binary documents, such as images, and it works great for text documents and other large key/value objects as well.
Each document is considered a record within the sequence file.
You can make a MapReduce job output a sequence file by setting its output format to SequenceFileOutputFormat.
You’ll want to change its compression type from the default RECORD to BLOCK.
With block compression, a block of records is compressed together and achieves a higher compression ratio.
Finally, you have to call the static methods (or SequenceFileOutputFormat , which inherits those methods) to enable output compression using a specific codec.
The supported codecs are the same as those given in table 6.4
Table 6.5 lists the equivalent properties for configuring for sequence file output.
A Streaming program can output sequence files when given the following options:
All the input files we’ve seen so far are uncompressed text files where each record is a line.
The newline character (\n) can trivially be thought of as the sync marker pointing out to both splittable boundaries and record boundaries.
Class property that is used to specify which compression codec to use for compressing the job’s output.
To read a sequence file as input, set the input format to SequenceFileInputFormat.
There’s no need to configure the compression type or codec class, as the SequenceFile.Reader class (used by SequenceFileRecordReader) will automatically determine those settings from the file header.
By default, the TaskTracker runs each Mapper and Reducer task in a separate JVM as a child process.
This necessarily incurs the JVM start-up cost for each task.
If the mapper does its own initialization, such as reading into memory a large data structure (see the example of joining using distributed cache in section 5.2.2), that initialization   is part of the start-up cost as well.
If each task runs only briefly, or if the mapper initialization takes a long time, then the start-up cost can be a significant portion of a task’s total run time.
Starting with version 0.19.0, Hadoop allows the reuse of a JVM across multiple tasks of the same job.
The start-up cost can, therefore, be amortized across many tasks.
A new property, mapred.job.reuse.jvm.num.tasks , specifies the maximum number of tasks (of the same job) a JVM can run.
You can enable JVM reuse by setting the property to a higher number.
The JobConf object has a convenience method, setNumTasksToExecutePerJvm(int) , to set the property for a job.
One of the original design assumptions of MapReduce (as stated in the Google MapReduce paper) is that nodes are unreliable and the framework must handle the situation where some nodes fail in the middle of a job.
Under this assumption, the original MapReduce   framework specifies the map tasks and the reduce tasks to be idempotent.
This means that when a task fails, Hadoop can restart that task and the overall job will end up with the same result.
Hadoop can monitor the health of running nodes and restart tasks   on failed nodes automatically.
Often nodes don’t suddenly fail but experience slowdown as I/O devices go bad.
In such situations everything works but the tasks run slower.
This doesn’t affect the correctness of the running job but certainly affects its performance.
Even one slow-running task will delay the completion of a MapReduce job.
Until all mappers have finished, none of the reducers will start running.
Similarly, a job is not considered finished until all the reducers have finished.
Hadoop uses the idempotency property again to mitigate the slow-task problem.
Instead of restarting a task only after it has failed, Hadoop will notice a slow-running task and schedule the same task   to be run in another node in parallel.
Idempotency guarantees the parallel task will generate the same output.
As soon as one finishes successfully, Hadoop will use its output and kill the other parallel tasks.
Note that speculative execution of map tasks will take place only after all map tasks have been scheduled to run, and only for map tasks that are making much less progress than is average on the other map tasks.
It’s the same case for speculative execution of reduce tasks.
Speculative execution does not “race” multiple copies of a task to get the best completion time.
It only prevents the slow tasks from dragging down the job’s completion time.
One can turn it off for map tasks and reduce tasks separately.
To do this, set one or both of the properties in table 6.7 to false.
They’re applied on a per-job basis, but you can also change the cluster-wide default by setting them in the cluster configuration file.
Table 6.7 Configuration properties for enabling and disabling speculative execution.
Boolean property denoting whether speculative execution is enabled for map tasks.
Boolean property denoting whether speculative execution is enabled for reduce tasks.
The primary reason to turn it off is if your map tasks or reduce tasks   have side effects and are therefore not idempotent.
For example, if a task writes to external files, speculative execution can cause multiple copies of a task to collide in attempting to create the same external files.
You can turn off speculative execution to ensure that only one copy of a task is being run at a time.
For example, if a task writes to an external file, it’s possible that the task dies right after writing to the external file.
In that case, Hadoop will restart the task, which will try to write to that external file again.
You need to make sure your tasks’ operation remains correct in such situations.
If you’re willing to rewrite your MapReduce programs to optimize performance, some straightforward techniques and some nontrivial, application-dependent rewritings can speed things up.
One straightforward technique for a Streaming   program is to rewrite it for Hadoop Java.
Streaming is great for quickly creating a MapReduce job for ad hoc data analysis, but it doesn’t run as fast as Java under Hadoop.
Streaming jobs that start out as one-off queries but end up being run frequently can gain from a Java re-implementation.
If you have several jobs that run on the same input data, there are probably opportunities to rewrite them into fewer jobs.
For example, if you’re computing the maximum as well as the minimum of a data set , you can write a single MapReduce job that computes both rather than compute them separately using two different jobs.
This may sound obvious, but in practice many jobs are originally written to do one function well.
A job’s conciseness makes it widely applicable to different data sets for different purposes.
Only after some usage should you start looking for job groupings that you can rewrite to be faster.
One of the most important things you can do to speed up a MapReduce   program is to think hard about the underlying algorithm and see if a more efficient algorithm can compute the same results faster.
This is true for any programming, but it is more significant for MapReduce programs.
Hadoop programs, on the other hand, tend to touch on “exotic” areas, such as distributed computing, functional programming, statistics, and data-intensive processing, where best practices are less known to most programmers and there is still exciting research today to explore new approaches.
One example we’ve already seen that leverages a new data structure to speed up MapReduce programs is the use of Bloom filters in semijoins (section 5.3)
The Bloom filter   is well-known in the distributed computing community but relatively unknown outside of it.
Another classic example of using a new algorithm to speed up a MapReduce program comes from statistics in the calculation of variance.
The variable Xavg is the average of the data set.
If we don’t know that average ahead of time, then a non-statistician may decide to run one MapReduce job to find the average first, and a second MapReduce job to compute the variance.
Someone more familiar with computing statistics will use an equivalent definition:
From this definition one needs the sum of X as well as the sum of X2, but you can compute both sums together in one scan of the data, using only a single MapReduce job.
This is analogous to the example of calculating maximum and minimum in a single job.) A little statistical background has halved the processing time in computing variance.8
You should also pay attention to the computational complexity of your algorithms.
Hadoop provides “only” linear scalability , and you can still bring it to its knees with large data sets running under computationally intensive algorithms that are quadratic or worse.
You certainly should look for more efficient algorithms in those cases, and sometimes you may have to settle for faster algorithms that only give approximate results.
Development methodologies for Hadoop build on top of best practices for Java programming, such as unit testing and test-driven development.
Hadoop’s central role of processing data calls for more data-centric testing processes.
Math and logic errors are more prevalent in data-intensive programs and they’re often inconspicuous.
There’s a lot of nuisance in numerical computation over large data.
In this variance calculation example we note our refactored MapReduce job has lower numerical precision and is more likely to run into overflow problems.
To lessen the burden, you should test in stages, from a nondistributed (i.e., local) mode to a single-node pseudo-distributed mode, and finally to a fully distributed mode.
Beyond thinking through general algorithmic and computational issues, performance enhancement is platform-specific, and Hadoop has a number of specific techniques to make jobs run more efficiently.
This book so far has covered the core techniques for making a MapReduce program.
Hadoop is a big framework that supports many more functionalities than those core techniques.
In this age of Bing and Google, you can look up specialized MapReduce techniques rather easily, and we don’t try to be an encyclopedic reference.
In our own usage and from our discussion with other Hadoop users, we’ve found a number of techniques generally useful, techniques such as being able to take a standard relational database as input or output to a MapReduce job.
We’ve collected some of our favorite “recipes” in this cookbook chapter.
In writing your Mapper and Reducer , you often want to make certain aspects configurable.
For example, our joining program in chapter 5 is hardcoded to take the.
The program can be more generally applicable if the column for the join key can be specified by the user at run time.
Hadoop itself uses a configuration object to store all the configuration properties for a job.
You can use the same object to pass parameters to your Mapper and Reducer.
We’ve seen how the MapReduce driver configures the JobConf object with properties, such as input format, output format, mapper class, and so forth.
To introduce your own property, you give your property a unique name and set it with a value in the same configuration object.
This configuration object is passed to all TaskTrackers , so the properties in the configuration object are available to all tasks in that job.
Your Mapper and Reducer can read the configuration object and retrieve the property value.
The Configuration class (parent of JobConf) has a number of generic setter methods.
Properties are key/value pairs, where key has to be a String, but value can be one of a number of common types.
All the other setter methods are convenience methods for set(String, String)
For example, the setStrings(String, String...) method takes a String array , turns it into a With that in mind, don’t keep any commas in the strings in the original array.
If you want commas, you should use your own string-encoding function.
Your driver   will first set the properties in the configuration object to make them available to all tasks.
Your Mapper and Reducer have access to the configuration methods will access your copy of those properties later.
In the following example we call our new property myjob.myproperty, and it takes an integer value specified by the user.
If you want to use the property in the Reducer, it will also have to retrieve the property.
The Configuration class   has a larger list of getter methods than setter methods, although they are largely self-explanatory.
Almost all the getter methods require a default value as argument.
The exception is get(String) , which returns null if the property with the specified name is not set.
Given that our job class implements the Tool interface and uses ToolRunner , we can also let the user set custom properties directly using the generic options syntax, in the same way the user would set Hadoop configuration properties.
We can remove the line in the driver that requires the user to always specify the value of this property as an argument.
This is more convenient for the user when the default value would work most of the time.
When you allow the user to specify custom properties, it’s good practice for the driver to validate any user input.
The example above ensures that the user will not be allowed to specify a negative value for myjob.myproperty.
In addition to retrieving custom properties and global configuration, we can also use the getter methods   on the configuration object to obtain certain state information about the current task and job.
For example, in the Mapper you can grab the map.
This is exactly what infer a tag for the data source.
Table 7.1 lists some of the other task-specific state information.
Table 7.1 Task-specific state information one can get in the configuration object.
Configuration properties are also available to Streaming   programs through environment variables.
Before executing a script, the Streaming API will have added all configuration properties to the running environment.
The property names are reformatted such that non-alphanumeric characters are replaced with an underscore (_)
For example, a Streaming script should look at the environment variable map_input_file for the full file path that the current mapper is reading from.
The preceding code shows how one would access configuration properties in Python.
Up ‘till now all the MapReduce jobs we’ve seen output a single set of files.
However, there are often cases where it’s more convenient to output multiple sets of files, or split a data set into multiple data sets.
A popular example is the partitioning of a large log file into distinct sets of log files for each day.
MultipleOutputFormat provides a simple way of grouping similar records into separate data sets.
Before writing each output record, this OutputFormat class   calls an internal method to determine the filename to write to.
More specifically, you will extend a particular subclass of MultipleOutputFormat and implement the the output format.
For example, MultipleTextOutputFormat will output text files whereas MultipleSequenceFileOutputFormat will output sequence files.
In either case, you’ll override the following method to return the filename for each output key/ value pair:
The default implementation returns the argument name, which is the leaf filename.
You can make the method return a filename that’s dependent on the content of the record.
For our example here, we take the patent metadata and partition it by country.
The skeleton of this example program is a map-only job that takes its input and immediately outputs it.
The main change we’ve made is to create our own subclass of MultipleTextOutputFormat called PartitionbyCountryMTOF.
Note that MTOF is an acronym for MultipleTextOutputFormat .) Our subclass will store each record to a location based on the inventing country listed in that record.
Listing 7.1 Partition patent  metadata into multiple directories based on country.
After executing the preceding program, we can see that the output directory now has a separate directory for each country.
And within the directory for each country are files with only records (patents) created by those countries.
We’ve written this simple partitioning exercise as a map-only program.
You can apply the same technique to the output of reducers as well.
Be careful not to confuse this with the partitioner   in the MapReduce framework.
That partitioner looks at the keys of intermediate records   and decides which reducer will process them.
The partitioning we’re doing here looks at the key/value pair of the output and decides which file to store to.
For example, we were able to split the input data by row, but what if we want to split by column? Let’s say we want to create two data sets from the patent metadata: one containing time-related information (e.g., (e.g., country of invention)
These two data sets may be of different output formats and different data types for the keys and values.
We can look to MultipleOutputs, introduced in version 0.19 of Hadoop, for more powerful capabilities.
Rather than asking for the filename to output each record, MultipleOutputs creates multiple OutputCollectors.
Each OutputCollector can have its own OutputFormat and types for the key/value pair.
Your MapReduce program will decide what to output to each OutputCollector.
Listing 7.2 shows a program that takes our patent metadata.
The other data set has geographical information associated with each patent.
This, too, is a map-only program, but you can apply the multiple output collectors to reducers in a straightforward way.
Listing 7.2 Program to project different  columns of input data to different files.
To use MultipleOutputs, the driver of the MapReduce program must set up the output collectors it expects to use.
Creating the collectors involves a call to MultipleOutputs’ and another one called geo.
We’ve created them both to use TextOutputFormat and have the same key/value types, but we can choose to use different output formats or data types.
After setting up the output collectors in the driver, we need to get the MultipleOutputs object that tracks them when the mapper is initialized in the MultipleOutputs object to get back the chrono and the geo OutputCollectors.
We will write out different data that’s appropriate for each output collector.
We have given a name to each output collector in MultipleOutputs , and MultipleOutputs will automatically generate the output filenames.
We can look at the files outputted by our script to see how MultipleOutputs generates the output names:
We have a set of files prefixed with chrono and another set of files prefixed with geo.
Note that the program created the default output files part-* even though it wrote nothing explicitly.
It’s entirely possible to write to these files using the original map-only program, records written to the original OutputCollector, and only those records, would be passed to the reducers for processing.
One of the trade-offs with MultipleOutputs is that it has a rigid naming structure compared to MultipleOutputFormat.
Your output collector’s name cannot be part, because that’s already in use for the default.
The output filename is also strictly defined as the output collector’s name followed by m or r depending on whether the output was collected at the mapper or the reducer.
Looking at the output files, we see that we’ve successfully projected out the columns on the patent data set into distinct files.
Although Hadoop is useful for processing large data, relational databases remain the workhorse of many data processing applications.
Although it’s possible to set up a MapReduce program to take its input by directly querying a database rather than reading a file in HDFS , the performance is less than ideal.
More often you would copy a data set from a database to HDFS.
You can easily do it with a standard database dump utility to get a flat file.
You then upload to HDFS using its file put shell command.
But sometimes it is sensible having a MapReduce program write directly to a database.
Many MapReduce programs take large data sets and process them into a manageable size for databases to handle.
For example, we often use MapReduce   in the ETL-like process of taking humongous log files and computing a much smaller and more manageable set of statistics for analysts to look at.
In your driver you set the output format to this class.
You’ll need to specify the configuration for method in DBConfiguration :
After that, you’ll specify what table you’re writing to and what fields are there.
Your driver should have a few lines that look like this:
Using DBOutputFormat forces your output key to implement the DBWritable interface.
The signatures for Writable and DBWritable are similar; only the DBWritable takes a ResultSet.
Unless you plan on fetching input data straight be called.
We want to emphasize again that reading and writing to databases from within Hadoop is only appropriate for data sets that are relatively small by Hadoop standards.
Unless your database setup is as parallel as Hadoop (which can be the case if your Hadoop cluster is relatively small while you have many shards in your database system), your DB will be the performance bottleneck, and you may not gain any scalability advantage from your Hadoop cluster.
Oftentimes, it’s better to bulk load data into a database   rather than make direct writes from Hadoop.
The MapReduce framework guarantees the input to each reducer to be in sorted order based on key.
In many cases, the reducer only does a simple computation on the value part of a key/value pair.
Keep in mind that the MapReduce framework does not guarantee the sorted order of the reducer output.
Rather, it’s a byproduct of the sorted input and the typical type of operations reducers perform.
For some applications, the sorted order   is unnecessary, and sometimes questions are raised about turning off the sorting operation to eliminate an unnecessary step in the reducer.
The truth is that the sorting operation is not so much about enforcing the sorted order of the reducer’s input.
Rather, sorting is an efficient way to group all records of the same key together.
If the grouping function is unnecessary, then we can directly generate an output record from a single input record.
In that case, you’ll be able to improve performance by eliminating the entire reduce phase.
You can do this by setting the number of reducers to 0, making the application a maponly job.
LinkedIn has an interesting blog post on challenges faced in moving massive amounts of data resulting from offline processes (i.e., Hadoop) into live systems: http://project-voldemort.com/blog/2009/06/building-a1-tb-data-cycle-at-linkedin-with-hadoop-and-project-voldemort/
On the other hand, for some applications it’s desirable that all output   is sorted in total.
The key to doing this is the partitioner   operation in the framework.
The job of the partitioner is to deterministically assign a reducer   to each key.
All records of the same key are grouped and processed together in the reduce stage.
An no one reducer is given many more keys than other reducers.
Without any prior information about the distribution of keys, the default partitioner uses a hashing function to uniformly assign keys to reducers.
This often works well in distributing work evenly across reducers, but the assignment is intentionally arbitrary and not in any order.
If we have prior knowledge that the keys are approximately uniformly distributed, we can use a partitioner that assigns key ranges to each reducer and still be certain that the reducers’ loads are fairly balanced.
For example, in highly skewed data sets, a significant number of records may have the same key.
If possible, you should use a combiner to lessen the load at the reduce phase by doing as much preprocessing as possible at the map phase.
In addition, you can also choose to write a special partitioner to distribute keys unevenly in such a way that it balances out the inherent skew of the data and its processing.
The TotalOrderPartitioner is a partitioner that ensures sortedness between output partitions, not only within.
Sorting of large-scale data (i.e., the TeraSort benchmark) originally used a similar version of this class.
This class takes a sequence file with a sorted partition keyset and proceeds to partition keys in different ranges to the reducers.
This chapter discussed many tools and techniques to make your Hadoop job more user-friendly or make it interface better with other components of your data processing infrastructure.
The full extent of the capabilities available in a Hadoop job is documented in the Hadoop API: http://hadoop.apache.org/common/docs/current/ api/index.html.
You may also want to check out additional abstractions such as Pig and Hive to simplify your programming.
If your role involves administrating a Hadoop cluster, you will find the tips on managing a Hadoop cluster in the next chapter useful.
The installation instructions in chapter 2 produced a running Hadoop cluster fairly quickly.
The configuration was relatively simple, but unfortunately it’s not good for a production cluster, which will be under heavy sustained use.
There are various configuration parameters that you would want to tune for a production cluster, and section 8.1 will cover those parameters.
In addition, like any system, a Hadoop   cluster will change over time and you (or some administrator) will have to know how to maintain it to keep it running in good shape.
These include adding/removing nodes (capacity) and recovery from NameNode failure.
We end the chapter with a section on setting up a scheduler to manage multiple running jobs.
Their default values tend to target running in standalone mode.
The default values are more likely to work on more systems without causing any errors.
However, oftentimes they’re far from optimal in a production cluster.
Table 8.1 shows some of the system properties that you’ll want to change for a production cluster.
Table 8.1 Hadoop   properties that you can tune for a production cluster.
Maximum number of map and reduce tasks that can run simultaneously in a TaskTracker.
The default values for dfs.name.dir and dfs.data.dir point to directories under /tmp, which is intended only for temporary storage in almost all Unix systems.
You will definitely want to change those properties for a production cluster.
In addition, these properties can take comma-separated lists of directories.
If a DataNode   has multiple drives, you should have a data directory in each one and list them all in dfs.data.dir.
The rationale for using /tmp illustrates how default values are idiotproof.
The DataNode will use them all in parallel to speed up I/O.2 You should also specify directories in multiple drives for mapred.local.dir to speed up processing of temporary data.
The default configuration for Hadoop’s temporary directories, hadoop.tmp.dir, is dependent on the user name.
You should avoid having any Hadoop property that depends on a user name, as there can be mismatches between the user name used to submit a job and the user name used to start a Hadoop node.
You should set it to something like /home/hadoop/tmp to be independent of any user name.
Another problem with the default value of hadoop.tmp.dir is that it points to the /tmp directory.
Although that’s an appropriate place for temporary storage, most default Linux configurations have a quota on /tmp that is too small for Hadoop.
Rather than increase the quota for /tmp, it’s better to point hadoop.tmp.dir to a directory that’s known to have a lot of space.
By default, HDFS   doesn’t require DataNodes to have any reserved free space.
In practice, most systems have questionable stability when the amount of free space gets too low.
You should set dfs.datanode.du.reserved to reserve 1 GB of free space in a DataNode.
A DataNode will stop accepting block writes when its amount of free space falls below the reserved amount.
Each TaskTracker   is allowed to run a configurable maximum number of map and reduce tasks.
Hadoop’s default is four tasks (two map tasks and two reduce tasks)
The right number depends on many factors, although most setups call for one to two tasks per core.
You can set a quad core machine to have a maximum of six map and reduce tasks (three each), because there will already be one task each allocated for TaskTracker and DataNode, to make a total of eight.
Similarly, you can set up a dual quad core machine to have a maximum of fourteen map and reduce tasks.
You should reduce the maximum number of tasks allowed if you expect more CPU-intensive loads.
In considering the number of tasks allowed, you should also consider the amount of heap memory   allocated to each task.
Hadoop’s default of 200 MB per task is quite underwhelming.
Each job can request more (or less) heap space per task.
Be sure that you have sufficient usable memory in your machines for your configuration parameters.
Keep in mind that DataNode   and TaskTracker   each already uses 1 GB of RAM.
Although you can set the number of reduce tasks per each individual MapReduce job, it’s desirable to have a default that works well most of the time.
There’s been some discussion in the Hadoop forums about whether one should configure multiple hard drives in a DataNode as RAID or JBOD.
Hadoop doesn’t need RAID’s data redundancy because HDFS already replicates data across machines.
Furthermore, Yahoo has stated that they were able to get noticeable performance improvement using JBOD.
The stated reason is that hard drives, even of the same model, have high variance in their speed.
A RAID configuration would slow down the I/O to the slowest drive.
On the other hand, letting each drive function independently will allow each one to operate at its top speed, making the overall throughput of the system higher.
A factor of 0.95 will have all the reduce tasks launched immediately and start copying map tasks’ output as they finish.
At a factor of 1.75, some reduce tasks will be able to launch immediately whereas others will wait.
The faster nodes will finish the first round of reduce tasks earlier and start on the second round.
The slowest nodes won’t need to process any reduce tasks from the second round.
You call it with a file path and it’ll recursively check the health of all the files under that path.
Call it with the argument / and it’ll check the entire filesystem.
By default fsck will ignore files still open for writing by a client.
You can get a list of such files by running fsck with the -openforwrite argument.
As fsck checks the filesystem, it will print out a dot for each file it found healthy (not shown in the above output)
It’ll print out a message for each file that is less than healthy, including ones that have over-replicated blocks , under-replicated blocks , misreplicated blocks , corrupt blocks , and missing replicas.
Over-replicated blocks, underreplicated blocks, and mis-replicated blocks are not too alarming as HDFS is self-healing.
But, corrupt blocks and missing replicas mean that data has been permanently lost.
By default fsck doesn’t act on those corrupt files, but you can run fsck with the -delete option to remove them.
Better yet is to run fsck with the -move option, which moves corrupted files into the /lost+found directory for salvaging.
You can tell fsck to print out more information by adding -files, -blocks, -locations, and -racks options to fsck.
The -locations option requires both -files and -blocks options be used, and so forth.
The -files option tells fsck to print out, for each file it checks, a line of information containing the file’s path, the file’s size in bytes and blocks, and the file’s status.
The -blocks option tells fsck to go further and print out a line of information for each block in the file.
This line will include the block’s name, its length, and its number of replicas.
The -locations option will include in each line the location of the block’s replicas.
The -racks option will add the rack name to the location information.
For example, a short one-block file will have its report as.
While fsck reports on each file in HDFS, there is a dfsadmin command for reporting on each DataNode.
You can get it through the -report option on the dfsadmin command:
To look at the NameNode ’s current activity, you can use the -metasave option in dfsadmin:
This will save some of NameNode’s metadata into its log directory under filename.
In this metadata, you’ll find lists of blocks waiting for replication, blocks being replicated, and blocks awaiting deletion.
For replication each block will also have a list of DataNodes being replicated to.
Finally, the metasave file will also have summary statistics on each DataNode.
Each file has nine permission settings: the read (r), write (w), and execute (x) permissions for each of the file’s associated owner, group, and other users.
Under HDFS, we can’t execute files; so we can’t set the x permission.
Permission settings for directories also closely follow the POSIX   model.
The w permission allows creation or deletion of files or directories.
The x permission allows one to access children of the directory.
Current HDFS releases don’t provide much in terms of security.
You should use the HDFS   permission system only to prevent accidental misuse and overwriting of data among trusted users sharing a Hadoop cluster.
Your Hadoop username is your login name, which is equivalent to what’s shown by whoami.
An exception is the username that started the name node.
This superuser   can perform any file operation regardless of permission settings.
In addition, the administrator can specify members in a supergroup   through the configuration parameter dfs.permissions.
You can change permission settings and ownership using bin/hadoop fs -chmod, -chown, and -chgrp.
They behave similarly to Unix commands of the same name.
By default HDFS doesn’t have any quota   to limit how much you can put in a directory.
You can enable and specify name quotas  on specific directories, which place a hard limit on the number of file and directory names under that directory.
The main use case for name quotas is to prevent users from generating too many small files and overwork the NameNode.
The following commands are for setting and clearing name quotas:
Starting with version 0.19, HDFS also supports space quotas on a per directory basis.
This helps manage the amount of storage a user or application can take up.
The setSpaceQuota command   takes an argument for the number of bytes as each directory’s quota.
To get the quotas associated with a directory as well as a count of the number of names and bytes it uses, use the HDFS shell command count with the -q option.
In addition to file permissions, an additional safeguard against accidental deletion of files in HDFS is the trash feature.
When this feature is enabled, the command line utilities for deleting files don’t delete files immediately.
Instead, they move the files temporarily to a .Trash/ folder   under the user’s working directory.
The files are not permanently removed until after a user-configurable time delay.
As long as a file is still in the .Trash/ folder, you can restore it by moving it back to its original location.
To enable the trash feature and set the time delay for the trash removal, set the fs.trash.interval property in core-site.xml to the delay (in minutes)
Setting the value to 0 will disable the trash feature.
You may want to remove DataNodes from your HDFS cluster at some point.
For example, you want to take a machine offline for upgrade or maintenance.
Although it’s not recommended, you can kill the nodes or disconnect them from the cluster.
Taking one or two DataNodes offline   will not affect ongoing operation.
The NameNode will detect their death and will initiate replication of blocks that have fallen below the desired replication factor.
For a smoother and safer operation, particularly when retiring large number of DataNodes, you should use Hadoop’s decommissioning   feature.
Decommissioning ensures that all blocks will have the desired replication factor among the remaining active nodes.
In order to use this feature, you must create an (initially empty) exclude file in the NameNode’s local file- system, and the configuration parameter dfs.hosts.exclude must point to this file during NameNode’s startup.
When you want to retire DataNodes, list them in the exclude file, one node per line.
You have to specify the nodes using the full hostname, IP, or IP:port format.
If you have started HDFS without setting dfs.hosts.exclude to point to an exclude file, the proper way to decommission DataNodes is this: Shut down the NameNode.
The NameNode thinks that it is being contacted by a DataNode outside the system rather than a node to be decommissioned.
If the decommissioned machines may rejoin the cluster at some later point, you should remove them from the exclude file and rerun bin/hadoop dfsadmin -refreshNodes now to update the NameNode.
When the machines are ready to rejoin the cluster, you can add them using the procedure described in the next section.
DataNodes8.7 Adding Besides bringing back a machine from offline maintenance, you may want to add DataNodes to your Hadoop cluster as you use it for more processing jobs with more data.
On the new node, install Hadoop and set up the configuration files as you would any DataNode in the cluster.
It will automatically contact the NameNode and join the cluster.
You should also add the new node to the conf/slaves file in the master server.
When you add a new DataNode, it will initially be empty, whereas existing DataNodes will already be filled to some capacity.
New files will likely go to the new node, but their replicated blocks will still go to the old nodes.
One should proactively start the HDFS   balancer to balance the cluster for optimal performance.
The script will run in the background until the cluster is balanced.
A cluster is considered balanced when the utilization rates of all the DataNodes are within the range of the average utilization rate plus or minus a threshold.
You can specify a different threshold when you start the balancer script.
For example, to set the threshold to 5 percent for a more evenly distributed cluster, start the balancer with.
As balancing can be network intensive, we recommend doing it overnight or over a weekend when your cluster may be less busy.
NameNode   is one of the most important components in the HDFS architecture.
It holds the filesystem’s metadata and caches the cluster’s blockmap in RAM for reasonable performance.
When you have anything other than a tiny cluster, you should dedicate a machine to run as NameNode and don’t put any DataNode, JobTracker, or TaskTracker service on it.
This NameNode machine should be the most powerful machine in the cluster.
Although DataNodes may have higher performance with JBOD   disk drives, you should definitely use RAID   drives in your NameNode   for higher reliability against any single drive failure.
One approach to reducing the burden on the NameNode is to reduce the amount of filesystem   metadata   by increasing the block size.
Doubling the block size will almost half the amount of metadata.
Unfortunately, this also decreases parallelism for files that are not large.
The ideal block size will depend on your specific deployment.
The block size is set in the configuration parameter dfs.block.size.
By default, the Secondary NameNode3 and the NameNode run on the same machine.
For moderate size clusters   (10 or more nodes), you should separate the.
As of this writing, the Secondary NameNode is slated to be deprecated by version 0.21 of Hadoop, which should be released as this book goes to press.
The Secondary NameNode will be replaced by a more robust design for warm standby.
You should check the online documentation of the version of Hadoop you’re using to confirm whether it’s still using Secondary NameNode or not.
Secondary NameNode into its own machine, the spec of which should be comparable to the NameNode.
But, before going into how to set up a separate server as a Secondary NameNode, I should explain what the Secondary NameNode does and doesn’t do, and in turn some of NameNode’s underlying mechanics.
Due to its unfortunate naming, the Secondary NameNode   (SNN) is sometimes confused with a failover backup for NameNode.
The SNN only serves to periodically clean up and tighten the filesystem’s state information in NameNode, helping NameNode become more efficient.
NameNode manages the filesystem’s state information using two files, FsImage and EditLog.
The file FsImage is a snapshot of the filesystem at some checkpoint, and EditLog records each incremental change (delta) to the filesystem after that checkpoint.
These two files can completely determine the current state of the filesystem.
When you initialize NameNode, it merges these two files to create a new snapshot.
At the end of NameNode’s initialization, FsImage will contain the new snapshot and EditLog will be empty.
Afterward any operation that changes the state of HDFS is appended to EditLog, whereas FsImage will remain unchanged.
When you shut down NameNode and restart it, the consolidation will take place again and make a new snapshot.
Note that the two files are only for retaining the filesystem’s state information while NameNode is not running (either intentionally shut down or due to system malfunction )
NameNode keeps in memory a constantly maintained copy of the filesystem’s state information  to quickly answer queries about the filesystem.
For a busy cluster , the EditLog file will grow quite large, and the next restart of NameNode will take a long time to merge EditLog into FsImage.
For busy clusters, it can also be a long time in between NameNode restarts, and you may want more frequent  snapshots for archival purposes.
It consolidates FsImage and EditLog into a new snapshot and leaves the NameNode alone to serve live traffic.
Therefore, it’s more appropriate to think of the SNN as a checkpointing server.
Merging FsImage and EditLog is memory intensive, requiring an amount of memory on the same order as normal NameNode operation.
It’s best for the SNN to be on a separate server that is as powerful as the primary NameNode.
To configure HDFS to use a separate server as the SNN, first list that server’s host name or IP address in the conf/masters file.
The masters in Hadoop (NameNode and JobTracker) are whichever machine you run bin/start-dfs.sh and bin/start-mapred.sh on.
What’s listed in conf/masters is the SNN, not any of the masters.
You should also modify the conf/hdfs-site.xml file on the SNN such that the dfs.
You should set this property because the SNN retrieves FsImage and EditLog from the NameNode by sending HTTP Get requests to the URLs:
The SNN also updates the NameNode with the merged metadata using the same address and port.
Failures happen, and Hadoop has been designed to be quite  resilient.
A common design for setting up a backup NameNode server is by reusing the SNN.4
After all, the SNN has similar hardware specs as the NameNode, and Hadoop should’ve already been installed with the same directory configurations.
If we do some additional work of maintaining the SNN to be a functional mirror image of the NameNode, we can quickly start this backup machine as a NameNode instance in the case of a NameNode failure.
Some manual intervention and time are necessary to start the backup node as the new NameNode, but at least we wouldn’t lose any data.
NameNode keeps all the filesystem’s metadata, including the FsImage and EditLog files, under the dfs.name.dir directory.
Note that the SNN server   doesn’t use that directory at all.
It downloads the system’s metadata into the fs.checkpoint.dir directory and proceeds to merge FsImage and EditLog there.
As the dfs.name.dir directory on the SNN is unused, we can expose it to the NameNode via the Network File System (NFS)
We instruct the NameNode to always write to this mounted directory in addition to writing to the NameNode’s local metadata directory.
You have to specify dfs.name.dir on the NameNode with a comma separated list, like.
This works, assuming the local dfs.name.dir directory at both the NameNode and the Secondary NameNode are at /home/hadoop/dfs/name, and that the directory on the SNN is mounted to the NameNode at /mnt/hadoop-backup.
When HDFS sees a commaseparated list in dfs.name.dir, it writes its metadata to every directory on the list.
Given this setup, when the NameNode dies, the local dfs.name.dir directory at both the NameNode and the backup node (SNN) should have the same content.
To have the backup node serve as the replacement NameNode, you’ll have to switch its IP address to the original NameNode’s IP address.
Unfortunately, changing only the hostname is not sufficient as the DataNodes cache the DNS entry.) You’ll also have to run the backup node as a NameNode by executing bin/start-dfs.sh on it.
Unfortunately, this common design also contributes to the misperception of the Secondary NameNode as a backup node.
You can set up the backup node in a totally different machine from the NameNode and SNN, but that machine would be idle almost all the time.
To be safer, this new NameNode   should also have a backup node set up before you start it.
Otherwise you’ll be in trouble if this new NameNode fails too.
If you don’t have a machine readily available as a backup, you should at least set up an NFS-mounted directory.
This way the filesystem’s state information is in more than one location.
As HDFS writes its metadata to all directories listed in dfs.name.dir, if your NameNode   has multiple hard drives, you can specify directories from different drives to hold replicas of the metadata.
This way if one drive fails, it’s easier to restart the NameNode without the bad drive than to switch over to the backup node, which involves moving the IP address, setting up a new backup node, and so on.
Recall that the SNN creates a snapshot of the filesystem’s metadata in the fs.checkpoint.dir directory.
As it checkpoints only periodically (once an hour under the default setup), the metadata is too stale to rely on for failover.
But it’s still a good idea to archive this directory periodically over to remote storage.
In catastrophic situations, recovering from stale data is better than no data at all.
This can be true if both the NameNode and the backup fail simultaneously (say, a power surge affecting both machines)
Another unfortunate scenario is if the filesystem’s metadata has been corrupted (say, by human error or a software bug) and has poisoned all the replicas.
Recovering from a checkpoint image is explained in http://issues.apache.org/jira/ browse/HADOOP-2585
HDFS’s backup and recovery mechanism is undergoing active improvements as of this writing.
You should check with HDFS’s online documentation for the latest news.
There have also been applications of specialized Linux software such as DRBD 5
When your Hadoop cluster gets big, the nodes will be spread out in more than one rack and the cluster’s network topology starts to affect reliability and performance.
You may want the cluster to survive the failure of an entire rack.
You should place your backup server for NameNode, as described in the previous section, in a separate rack from the NameNode itself.
This way the failure of any one rack will not destroy all copies of the filesystem’s metadata.
With more than one rack, the placement of both block replicas and tasks becomes more complex.
Replicas of a block should be placed in separate racks to reduce the potential of data loss.
For the standard replication value of 3, the default placement policy for writing a block is this: If the client performing the write operation is part of the Hadoop cluster, place the first replica   on the DataNode where the client resides.
Place the second replica on a random rack different from the rack where the first replica resides.
Write the third replica to a different node on the same rack as the second replica.
For replication values higher than 3, place the subsequent replicas on random nodes.
A task is usually placed on a node that has a copy of the block the task is assigned to process.
When no such node is available to take on the new task, the task is randomly assigned to a node on a rack where a copy of the block is available somewhere on that rack.
That is, when data locality can’t be enforced at a node level, Hadoop tries to enforce it at the rack level.
Failing that, a task would be randomly assigned to one of the remaining nodes.
At this point you may wonder how Hadoop knows which rack a node is at.
It assumes a hierarchical network topology   for your Hadoop cluster, structurally similar to figure 8.1
Each node has a rack name similar to a file path.
In most cases you’ll be dealing with multiple racks co-located together.
To help Hadoop know the location of each node, you have to provide an executable script that can map IP addresses into rack names.
This network topology script must reside on the master node and its location is specified in the topology.script.file.name property in core-site.xml.
Hadoop will call this script with a set of IP addresses as separate arguments.
The script should print out (through STDOUT) the rack name corresponding to each IP address in the same order, separated by whitespace.
The topology.script.number.args property controls the maximum number of IP addresses Hadoop will ask for at any one time.
It’s convenient to simplify your script by setting that value to 1
Each datacenter has multiple racks (R), and each rack has multiple machines.
This bash script takes an IPv4 address and looks at the last of the four octets (assuming dot-decimal notation)
A table lookup may make more sense for more complex cluster topologies.
On the other hand, if there is no network topology script, Hadoop assumes a flat topology where all nodes are assigned to /default-rack.
As you have more and more jobs coming from multiple users for your Hadoop cluster, you’ll need some control to prevent contention.
Under Hadoop’s default FIFO scheduler , as soon as a job is sent to Hadoop for execution, the JobTracker will assign as many TaskTrackers as necessary to process that job.
This works fairly well when things are not busy and you have a good amount of processing capacity to spare.
But some big Hadoop jobs can easily tie up the cluster for a long time and force the smaller jobs to wait.
Wouldn’t it be great if something akin to an express checkout existed for smaller jobs in a Hadoop cluster?
Back in the days before Hadoop version 0.19, you had to physically set up multiple MapReduce clusters to provide rudimentary CPU allocation among jobs.
To keep storage utilization reasonably efficient though, there would still be one single HDFS cluster.
Let’s say, you have Z slave nodes   available for your Hadoop cluster.
You’ll have a single NameNode that takes all Z nodes as DataNodes.
Until now, all these TaskTrackers would point to the same/only JobTracker.
The trick in making a multicluster  setup is having multiple JobTrackers, and each JobTracker controls a (mutually exclusive) subset of TaskTrackers.
For example, to create two MapReduce clusters, you have X TaskTrackers point to one JobTracker (via the mapred.job.tracker property) and Y  TaskTrackers configured to use the second JobTracker.
The slave nodes between the two MapReduce clusters are distinct to give jobs go to the other JobTracker.
This limits the number of TaskTrackers available to each type of job.
The job type need not necessarily determine the assignment of jobs to the MapReduce pool.
More typical is to allocate each MapReduce pool to one user group.
This enforces a limit to how much resource one group can take up.
Physically setting up multiple MapReduce clusters this way has many drawbacks.
It’s not very user-friendly as one has to remember which pool to use.
It may be the case that all replicas are in DataNodes outside of one’s pool.) The setup is not flexible to changing resource requirements.
Fortunately, starting with version 0.19, Hadoop has a pluggable architecture for the scheduler, and two new schedulers have become available for resolving job contention.
One is the Fair Scheduler developed at Facebook, and another one is the Capacity Scheduler developed at Yahoo.
Jobs   are tagged to belong to specific pools, and each pool is configured to have a guaranteed capacity of a certain number of map slots and a certain number of reduce slots.
When task slots are freed up, the Fair Scheduler will allocate them to meet these minimum guarantees first.
You can set priority on jobs to give more capacity to higher priority jobs.
The Fair Scheduler is available as the jar file hadoop-*-fairscheduler.jar under the contrib/fairscheduler directory of the Hadoop installation.
To install it, you can move the jar file directly into Hadoop’s lib/ directory.
Alternatively, you can modify HADOOP_CLASSPATH in the script conf/hadoop-env.sh to include this jar.
You’ll need to set a few properties in hadoop-site.xml to fully enable and configure the Fair Scheduler.
You first instruct Hadoop to use the Fair Scheduler instead of the default one by setting mapred.jobtracker.taskScheduler to org.apache.hadoop.
The most important is mapred.fairscheduler.allocation.file, which points to the file that defines the different pools.
This file is typically named pools.xml and specifies each pool’s name and capacity.
The mapred.fairscheduler.poolnameproperty defines the jobconf property the scheduler will use to determine which pool to use for a job.
A useful configuration pattern is to set this to a new property, say pool.name, and assign pool.name to have a default value of ${user.name}
The Fair Scheduler automatically gives each user her own individual pool.
This particular pool.name will by default assign each job to its owner’s pool.
You can change the pool.name property in a job’s jobconf to assign the job to a different pool.7 Finally, the mapred.
To summarize, your mapred-site.xml will have the following properties set:
Yes, you can run your job in another user’s pool, but that’s not very polite.
The main usage is to assign special jobs to specific pools.
For example, you may want all cron jobs to go to a single pool rather than have them run under each individual user’s pool.
The allocation file pools.xml defines the pools for the scheduler.
The constraints can include the minimum number of map slots or reduce slots.
They can also include the maximum number of running jobs.
In addition, you can set the maximum number of running jobs   per user, and also override this maximum for specific users.
Each is guaranteed to have at least two map slots and two reduce slots.
The “hive” pool is limited to running at most two jobs at once.
This pools.xml also caps the number of simultaneous running jobs a user can have to three, but the user “chuck” is given a higher cap of six.
Note that the pools.xml file is reread every 15 seconds.
You can modify this file and dynamically reallocate capacity at run time.
Any pool not defined in this file has no guaranteed capacity and no limit on number of jobs running at once.
The top table shows all the available pools and each pool’s usage.
When you have your Hadoop cluster running with the Fair Scheduler , there’s a Web UI   available to administer the scheduler.
Besides letting you know how the jobs are scheduled, it also allows you to change the pool a job belongs to and the job’s priority.
The interested reader can learn more about the Capacity Scheduler from the online documentation at http://hadoop.
Managing distributed clusters is complicated and Hadoop is no different.
If you have a complex setup and have more sophisticated questions, a useful resource is the Hadoop mailing lists.8
Many Hadoop administrators with deep expertise are active on those mailing lists, and chances are that one of them will have encountered your situation.
On the other hand, if you mostly want a basic Hadoop cluster without all the hassle of administration, you may want to consider using the Cloudera distribution9 or checking out one of the Hadoop cloud services, which we cover in the next chapter.
Cloud services provide an alternative to buying and hosting your own hardware to create a Hadoop cluster.
We show several case studies where Hadoop solves real business problems in practice.
Transferring data into and out of an AWS Hadoop cloud.
Depending on your data processing needs, your Hadoop workload can vary widely over time.
You may have a few large data processing jobs that occasionally take advantage of hundreds of nodes, but those same nodes will sit idle the rest of the time.
You may be new to Hadoop and want to get familiar with it first before investing in a dedicated cluster.
You may own a startup that needs to conserve cash and wants to avoid the capital expense of a Hadoop cluster.
In these and other situations, it makes more sense to rent a cluster of machines rather than buy it.
The general paradigm of provisioning compute resources as a remote service in a flexible, cost-effective manner is called cloud computing.
The best-known infrastructure service platform for cloud computing is Amazon Web Services (AWS)
You can rent computing and storage services from AWS on demand as your requirement scales.
As of this writing, renting a compute unit with the equivalent power of a 1.0 GHz.
Thanks to AWS and other such services, large-scale compute power is available to many people today.
Because of its flexibility and cost effectiveness, running Hadoop on the AWS cloud is a popular setup, and we learn how to install and configure this configuration in this chapter.
Learning all the capabilities of Amazon Web Services is worthy of a book itself.
We recommend you to explore the AWS website (http://aws.amazon.com) to get more details and the latest offerings.
We only cover enough basics here to get a Hadoop cluster running.
The EC2 service provides compute capacity for running Hadoop nodes.
You can think of EC2 as a large farm of virtual machines.
An EC2 instance is the AWS terminology for a virtual compute unit.
You rent an EC2 instance for only as long as you need and pay on an hourly basis.
A car rental company throws out whatever you leave in the trunk when you return it.
Similarly, all your data on an EC2 instance is deleted when you terminate the instance.
If you want the data to be around for future use, you have to ensure that it’s in some persistent storage.
The Amazon S3 service is a cloud storage service that you may use for such purposes.
Each EC2 instance functions like a commodity Intel machine that you can access and control over the internet.
You boot up an instance using an Amazon Machine Image , also known as an AMI or an image.
More demanding users can create their own images, but most users are well served by one of the many preconfigured ones.
Supported operating systems on EC2   include more than six variants of Linux, plus Windows Server and OpenSolaris.
Other images include one of the operating systems plus pre-installed software, such as database server, Apache HTTP server, Java application server, and others.
This section is a quick introduction to setting up AWS.
We only cover the essentials to get a Hadoop cluster running.
If you are already familiar with launching and using EC2 instances, you should skip directly to the next section on setting up Hadoop on AWS.
To start using AWS, you first have to sign up for an account.
It’s no more complicated than buying a book from Amazon.
The sign-up process sets up your Amazon account (which you may already have if you have bought stuff from Amazon before) and activates it to pay for your usage of AWS.
The most important simplification is that you no longer have to set up and launch your own cluster of EC2 instances.
The trade-off is that you lose some control over how the cluster works and you have to pay extra for this EMR service.
But we highly encourage you to keep on reading and understand how to set up your own EC2 cluster running Hadoop, even if you don’t go through the process of setting it up.
At the very least, knowing more details about how Hadoop runs on an EC2 cluster will clarify what EMR is doing underneath the hood.
After you have activated your Amazon account for AWS, there are three more steps before you can start creating machine instances and using them:
Obtain your  account IDs and your authentication keys and certificates.
You’ll set these up on your local machine to secure your communication with AWS.
These security mechanisms ensure that only you can rent compute resources with your account.
Download and set up command line tools to manage your EC2   instances.
These include special programs to start and stop EC2 instances in your virtual cluster.
After you have started an EC2   instance, you’ll log into it using SSH (either directly or indirectly through the use of special tools)
The default SSH mechanism uses the SSH key pair to authenticate you to your EC2 instance in lieu of using a password.
To run Hadoop on AWS you’ll need both mechanisms, and they can be set up from the Access Identifiers page where you manage your AWS account (http://aws.amazon.com/account/)
Figure 9.1 shows a section of the Access Identifiers page.
The web page requires an extra click on Show to display it (in case anyone is looking over your shoulder)
You should generate a new Secret Access Key if the one you have has been compromised.
You’ll need to specify your Access Key ID and Secret Access Key later when you set up the Hadoop cluster.
Unfortunately, AWS allows slashes (/) in its Secret Access Keys, which will cause confusion inside a URI.
Although there are ways you can tell Hadoop your AWS Access Key ID without using a URI, it may be more convenient to regenerate your Secret Access Key until you get one without a slash (/) in it.
Setting up the X.509 Certificate involves a bit more work.
You click on Create New to generate a new X.509 certificate.
A certificate has two keys: a public key and a keys in your X.509 certificate are hundreds of characters long, and they have to be stored and managed as files.
After creating a new X.509 certificate, you’ll arrive at a page to download both keys/files.
You can upload your own, or ask AWS to create one.
Your private key, as the name implies, should not be shared with anyone.
The filenames for the certificate and the private key are prefixed with cert- and pk-, respectively, and they have the .pem file extension.
You should create a directory called .ec2 under your home directory on your local machine and save those two files in the new directory.
On Linux you’ll have saved the following files to your local machine:
You may think these are a lot of values to generate and write down.
To summarize, here are the five values you should have at this point:
You’ll use these values later to authenticate yourself to AWS and control your Hadoop cluster.
After getting all the security credentials, you should download and configure the AWS command line tools to instantiate and manage your EC2 instances.
These tools are written in Java, which presumably is already installed on your local machine.
In the unzipped files you’ll see the Java tools plus shell scripts for Windows, Linux, and Mac OS X.
You don’t have to configure the command line tools, but you do have to set a few environment variables before using them.
The environment variable EC2_HOME should be a path pointing to the directory where the command line tools were unzipped.
Unless you have renamed that directory, its name is ec2-api-tools-A.B-nnnn, where A, B, and n are version/release numbers.
I’ve found it useful to have a script for setting up all the necessary environment variables to use the AWS command line tools.
You run this script before using any AWS related tools by executing.
You can run it in a command prompt by executing ec2-init.bat.
If you’ll be working with AWS often, you may choose to not use a separate script and instead integrate it directly into your operating system’s start-up script (for example, .profile and autoexec.bat)
The pathnames in the script will be different for your particular installation.
The environment variable JAVA_HOME needs to be set for the AWS command line tools to work.
We set it here although most likely it has already been set elsewhere.
The script adds the command line tools’ bin directory to your system PATH.
This makes executing the tools much easier, as you don’t need to specify the full path every time.
The Hadoop EC2 tools’ directory is also added to PATH, although we won’t cover them until the next section.
As of this writing, AWS supports two regions, the U.S.
As an optional step, you can choose which region to run your EC2 instances to reduce latency.
After you have run the preceding script to set up the environment variables, let’s run our first AWS command line tool to ask Amazon what regions are currently available:
Set the environment variable EC2_URL to the service endpoint of a different region if you choose to.
You can do this within the preceding AWS initialization shell script.
Two of the more popular ones are both Firefox extensions:  Elasticfox and S3Fox.
Elasticfox (http://developer.amazonwebservices.com/connect/ as launching new EC2 instances and listing currently running ones.
It functions much like a GUI-based FTP client in managing remote storage.
After starting an EC2 instance, you’ll want to log into it to run programs and services.
The default login mechanism (of public images) uses SSH with a key pair.
Half of this key pair (public key ) is embedded in the EC2 instance, and the other half (private key ) is in your local machine.
Together, the key pair secures the communication between your local machine and the EC2 instance.
Using SSH with a key pair is an alternative mechanism.
Instead of a password, you authenticate yourself with a private key that’s stored as a file on your local machine.
Like your password, your private key file should not be accessible by unauthorized people.
Each SSH key pair   has a key name to identify it.
When requesting Amazon EC2 to create an instance, you have to specify the public key to be embedded in that instance by its corresponding key name.
The SSH public key has to exist and be registered with Amazon before creating any EC2 instances.
The following command generates an SSH   public/private key pai r and registers the.
Interestingly, the command doesn’t save the private key to a local file.
Instead, it generates a standard output (stdout) similar to figure 9.4, part of which is the private key.
You’ll have to manually save it to a file using a text editor.
Specifically, copy and paste the output between the following two lines, inclusive, to a new file named id_rsa-gsg-keypair.
You’ll also need to lock down the file permission such that it’s only readable by you.
On Linux and other Unix-based systems, use the following command:
All EC2 instances in a single Hadoop cluster will have the same public key.
A single You can also choose to use more than one SSH key pair when working with multiple Hadoop   clusters, or when you use extra EC2 instances outside of your Hadoop cluster.
At this point you have finished the one-time setup of credentials and certificates to start a compute cluster in the Amazon cloud.
You can manually use the AWS tools to launch EC2 instances and log into them to launch your Hadoop cluster.
Fortunately, Hadoop has included tools to work with AWS, which we discuss in the next section.
The first line is a key signature and the rest is the.
Understanding them will come in handy when you start to tune your AWS Hadoop cluster.
Your local Hadoop installation contains scripts to help you launch and log into an EC2 Hadoop cluster.
These scripts are in the directory src/contrib/ec2/bin under your Hadoop installation.
Inside that script, set the following three variables to values you obtained in section 9.2.1:
You’ll need to specify the configuration of your Hadoop cluster in hadoop-ec2-env.
Before telling you how to set these parameters, let’s go over a little background.
Before the creation of an instance, Amazon EC2 must know the instance type and the image used to boot up the instance.
Instance type is the physical configuration of the virtual machine (CPU, RAM, disk space, etc.)
As of this writing, five instance types are available, grouped into two families: standard and high-CPU.
Rarely are they used for Hadoop applications, which tend to be data-intensive.
The standard family has three instance types, and table 9.1 lists their attributes.
The more powerful instance types cost more, and you should look up the AWS website to find the latest pricing.
Many existing images are available for all kinds of setups.
You can use one of the public images, or pay for special custom images, or even create your own.
The Hadoop team puts new EC2 images in the hadoopimages bucket when significant versions of Hadoop are released.
At any point in time, execute the following EC2 command to see the available Hadoop images:
A bucket is owned by exactly one AWS account and must have a globally unique name.
Figure 9.5 Some of the available Hadoop images in AWS.
Figure 9.5 shows an example output from the previous command.
Each image lists eleven properties, most of which are useful only for advanced AWS users.
For our purpose, all the information we need can be read from the third column, also known as the manifest location of the image.
These are expressed in a two-level hierarchy, in which the top level is the S3 bucket where the image resides.
As mentioned earlier, the hadoop-images bucket is the one we focus on.
An example image that’s available as of this writing has a manifest location of hadoop-images/hadoop-0.19.0i386.manifest.xml.
Unless you’re using a custom image, you should set S3_BUCKET to hadoop-images.
Finally, HADOOP_VERSION should be set to the Hadoop version you want to use.
Recall that our ec2-init.sh script has already added that directory to your system PATH.
Within that directory is hadoop-ec2, which is a meta-command for executing other commands.
After this it boots the requested number of slave nodes to point to the master node.
When this command returns, it will print out the public DNS name to the master node, which we denote <master-host>
At this point, not all slave nodes necessarily have been fully booted.
You can view the master node’s JobTracker web interface at http://<master-host>:50030/ to monitor the cluster and the operational status of the slave nodes.
After launching a Hadoop cluster, you’re ready to log into the master node and use the cluster as if you’ve set it up on your own machines.
We run a quick test to show that Hadoop is running in this cluster:
The commands above run an example Hadoop program to estimate the value of pi.
All Hadoop applications consist of two components: code and data.
In the next subsection we discuss making our data accessible (which may or may not involve moving data to the cluster)
You’ll copy your application code to the master node in your Hadoop EC2 cluster using scp.
As the Hadoop EC2 cluster is being rented, data stored in the cluster (including in HDFS) is not persistent.
Your input data has to persist somewhere else and be brought into the EC2 cluster for processing.
Many options exist for where to put your data and bring it into the Hadoop cluster, and each option has its trade-offs.
When the input data is small (<100 GB) and is processed only once, the simplest approach is to copy the data into the master node and then copy it from there to the cluster’s HDFS.
Copying data into the master node is no different than copying application code into the master node (see section 9.4.1)
Once the data is in the master node, you log into the master node and copy the data into HDFS using the standard Hadoop command:
One is that AWS charges for bandwidth between AWS and the outside world (in addition to hourly charges of each EC2 instance), but bandwidth within AWS is free.
In this case there’s a monetary cost to copying data into the master node but not the copying of the data from the master node into HDFS.
There’s also no cost to any of the communication within MapReduce processing and between MapReduce and HDFS.) Whichever way you get data into the Hadoop cluster, you’ll incur this bandwidth cost at least once.
The time it takes to move data into the master node will also be relatively long, as the connection between your machine and AWS is much slower than the connections within AWS.
Again, this sunk time will be incurred at least once no matter how you architect the dataflow.
The problem with the current dataflow architecture is that you’ll incur the time and monetary costs each time you bring up the Hadoop cluster.
If the input data will be processed in different ways over multiple sessions, this dataflow is not recommended.
Another shortcoming to the existing flowpath is the size limitation on the input data.
All the data must be able to reside at the master node first, and a small EC instance only has 150 GB of disk partition.
You can overcome this limitation if you can divide your input data over several chunks and move one chunk at a time.
You may also choose to use bigger instances, which have multiple 420 GB disk partitions.
But before trying these more complicated schemes, you should consider using S3 in your datapath.
You’ve already seen it in action as storage for EC2 images.
Storing data   in S3 is charged by bandwidth for data I/O with nonAWS machines, plus a monthly storage charge based on the size of the data.
More particularly, it’s well suited for use with Hadoop EC2 clusters.
There’s now an additional monthly storage cost for hosting your input data in S3, but it’s usually minimal.
If you need to have a scalable archival storage for your data, S3 can function in that role under this dataflow architecture, further justifying its cost model.
The default Hadoop installation has built-in support for using S3
After the data is in HDFS, you can run your Hadoop program in the cluster in the usual fashion.
Up ’till now we’ve always copied data into the cluster’s HDFS before running our Hadoop applications.
This preserves data locality between storage and the MapReduce program.
For very small jobs, you may choose to bypass HDFS and forego data locality, in return for skipping the copying of data from S3 to HDFS.
To work in this architecture, specify S3 as the input filepath when executing your Hadoop application:
The preceding command will store the output file in HDFS, but you can change that to be S3 as well.
There are a couple of variations on how you can use S3 that may be useful in some situations.
The main disadvantage with the native system is a limitation on file size of 5 GB.
If the files for your input data are smaller than that limit, the native system can be an excellent option.
If you’re using S3 often, you’ll find it cumbersome to type out the long URI for each file you want to access.
One way to shorten it is to add the following to your conf/core-site.xml file:
After adding the two properties to core-site.xml, the URI for S3 files can be shortened to.
The only way to use that AWS/S3 account is by embedding the secret key in coresite.xml as described before.
Some documentation has suggested escaping the secret key by replacing the slash (/) with the string %2F inside a URI, although that doesn’t seem to work in practice.)
For an additional shortcut, it may be appropriate to make S3 your default filesystem , in place of HDFS.
To do this, change the fs.default.name property in conf/coresite.xml after adding the two properties above:
Hadoop stores the output data of your Hadoop job in the cluster’s HDFS by default, and you should save it to somewhere more persistent.
The options for retrieving output data are the same as the options for copying input data into the Hadoop EC2 cluster, only running in reverse.
The main difference is that the output data is usually orders of magnitude smaller than the input data.
Given generally small output data, copying through the master node may turn out to be your best option.
As you’re renting your EC2 instances from AWS on an hourly basis, it’s important that you shut down the instances when you’re done and tell AWS to stop charging you.
It’s easy to log out of a cluster and forget that the instances are still running and you’re being charged! To properly terminate a cluster, use the following command:
All the EC2 instances in the cluster will shut down and data in them lost.
Amazon Web Services is constantly adding new capabilities, many of which will make life easier for Hadoop developers.
Two of the newest services that they’ve announced during the writing of this book include Amazon Elastic MapReduce (EMR) and AWS Import/Export.
For a small extra fee, the EMR service will launch a preconfigured Hadoop cluster   for you to run your MapReduce programs.
The major simplification this service provides is that you don’t need to worry about setting up EC2 instances, and therefore you don’t need to deal with all the certificates and command line tools and so forth.
You interact with EMR purely through a web-based console at https://console.aws.amazon.
You submit a MapReduce job, either as a (Streaming, Pig, or Hive) script or a JAR file, and EMR will set up a cluster to run the job.
By default the cluster will shut down at the end of the job.
The input (output) of the job is read (written) directly to S3
A heavy user of Hadoop usually has many jobs running against the same data, making this setup relatively inefficient, as explained in section 9.4.2
But a light user will find EMR dramatically simplifies running MapReduce in the cloud.
In addition, it’s not difficult to imagine that the sophistication of EMR will only grow in the future and eventually become the primary way to run Hadoop on AWS.
Figure 9.9 The introductory screen of the web console to Amazon Elastic MapReduce.
You can follow the steps onscreen to create a job flow.
You can find more information about Amazon Elastic MapReduce at these sites: http://aws.amazon.com/elasticmapreduce/ http://docs.amazonwebservices.com/ElasticMapReduce/latest/GettingStartedGuide/
One of the main obstacles to large-scale data processing in the cloud is the difficulty of moving large data sets into the cloud.
On the other hand, if you have to move data to the Amazon cloud for the sole purpose of analyzing it, then the data transfer itself can be a significant hurdle.
Amazon introduced the AWS Import/Export service by which you can physically send a hard drive to them and they upload the data to S3 using their high-speed internal network.
The point at which this service makes sense depends on your available internet connection speed.
Table 9.2 Size of data set at which AWS Import/Export   is more practical than internet upload.
You can find more details about AWS Import/Export at http://aws.amazon.com/
Cloud infrastructure is a great place for running Hadoop, as it allows you to easily scale to hundreds of nodes and gives you the financial flexibility to avoid upfront investments.
This chapter started with the basics of setting up an account and renting compute services from AWS.
Once you’re ready to rent computing nodes from AWS, you’ll find Hadoop tools for automating the setting up and running of a Hadoop cluster.
Finally, it’s important to remember to shut down your Hadoop cluster when you’re finished.
You’re renting the cloud infrastructure by the hour, and fees will continue to accrue unless you explicitly shut down the machines.
Computing similar documents efficiently, using a simple Pig Latin script.
One frequent complaint about MapReduce is that it’s difficult to program.
When you first think through a data processing task, you may think about it in terms of data flow operations, such as loops and filters.
However, as you implement the program in MapReduce, you’ll have to think at the level of mapper and reducer functions and job chaining.
Certain functions that are treated as first-class operations in higherlevel languages become nontrivial to implement in MapReduce, as we’ve seen for joins in chapter 5
Pig is a Hadoop extension that simplifies Hadoop programming by giving you a high-level data processing language while keeping Hadoop’s simple scalability and reliability.
Yahoo , one of the heaviest user of Hadoop (and a backer of both the Hadoop Core   and Pig), runs 40 percent of all its Hadoop jobs with Pig.
Pig simplifies programming because of the ease of expressing your code in Pig Latin.
The compiler helps to automatically exploit optimization opportunities in your script.
This frees you from having to tune your program manually.
As the Pig compiler improves, your Pig Latin program will also get an automatic speed-up.
We expect ease of use, high performance, and massive scalability from any Hadoop subproject.
More unique and crucial to understanding Pig are the design choices of its programming language (a data flow language called Pig Latin), the data types it supports, and its treatment of user-defined functions (UDFs ) as first-class citizens.
You write Pig Latin   programs in a sequence of steps where each step is a single highlevel data transformation.
The transformations support relational-style operations, such as filter, union, group, and join.
An example Pig Latin program that processes a search query log may look like.
Even though the operations are relational in style, Pig Latin remains a data flow language.
A data flow language   is friendlier to programmers who think in terms of algorithms, which are more naturally expressed by the data and control flows.
On the other hand, a declarative language such as SQL   is sometimes easier for analysts who prefer to just state the results one expects from a program.
Hive   is a different Hadoop subproject that targets users who prefer the SQL model.
Popular formats, such as tab-delimited text files, are natively supported.
Users can add functions to support other data file formats as well.
Pig doesn’t require metadata or schema on data, but it can take advantage of them if they’re provided.
Pig can operate on data that is relational, nested, semistructured, or unstructured.
To support this diversity of data, Pig supports complex data types, such as bags   and tuples   that can be nested to form fairly sophisticated data structures.
Pig   was designed with many applications in mind—processing log data, natural language processing, analyzing network graphs, and so forth.
It’s expected that many of the computations will require custom processing.
Pig is architected from the ground up with support for user-defined functions.
Knowing how to write UDFs is a big part of learning to use Pig.
You can download the latest release of Pig from http://hadoop.apache.org/pig/ releases.html.
As usual, make sure to set JAVA_HOME to the root of your Java installation, and Windows users should install Cygwin.
Ideally it’s a real cluster in fully distributed mode, although a pseudo-distributed setup is fine for practice.
You install Pig on your local machine by unpacking the downloaded distribution.
There’s nothing you have to modify on your Hadoop cluster.
Think of the Pig distribution as a compiler and some development and deployment tools.
It enhances your MapReduce programming but is otherwise only loosely coupled with the production Hadoop cluster.
Under the directory where you unpacked Pig, you should create the subdirectories logs and conf (unless they’re already there)
If you are creating the conf directory just now, there’s obviously no configuration file, and you’ll need to put in conf a new file named pig-env.sh.
This script is executed when you run Pig, and it can be used to set up environment variables for configuring Pig.
You set these variables to instruct Pig about your Hadoop cluster.
For example, the following statements in pig-env.sh will tell Pig the version of Hadoop used by the cluster is Pig’s classpath:
We assume HADOOP_HOME is set to Hadoop’s installation directory on your local machine.
By adding Hadoop’s conf directory to Pig’s classpath, Pig can automatically pick up the location of your Hadoop cluster’s NameNode   and JobTracker.
Instead of using Pig’s classpath, you can also specify the location of your Hadoop cluster by creating a pig.properties file.
This properties file will be under the conf directory you created earlier.
It should define fs.default.name and mapred.job.tracker, the filesystem (i.e., HDFS’s NameNode ) and the location of the JobTracker.
For the sake of convenience, let’s add the Pig installation’s bin directory to your path.
With Pig’s bin directory set as part of your command line path, you can start Pig with the command pig.
Let’s start Pig’s interactive shell   to see that it’s reading the configurations properly.
The filesystem and the JobTracker Pig reports should be consistent with your configuration setup.
You’re now inside Pig’s interactive shell, also known as Grunt.
Each way can work in one of two modes—local mode   and Hadoop mode.
Hadoop mode is sometimes called Mapreduce mode in the Pig documentation.) At the end of the previous section we’ve entered the Grunt shell running in Hadoop mode.
The Grunt   shell allows you to enter Pig commands manually.
This is typically used for ad hoc data analysis or during the interactive cycles of program development.
Large Pig programs or ones that will be run repeatedly are run in script files.
To run a Pig script, execute the same pig command with the file name as the argument, such as pig myscript.pig.
The convention is to use the .pig extension for Pig scripts.
You can think of Pig programs as similar to SQL queries , and Pig provides a PigServer class that allows any Java program to execute Pig queries.
Conceptually this is analogous to using JDBC to execute SQL queries.
Embedded Pig programs is a fairly advanced topic and you can find more details at http://wiki.apache.org/pig/ EmbeddedPig.
When you run Pig in local mode, you don’t use Hadoop at all.2 Pig commands are compiled to run locally in their own JVM , accessing local files.
This is typically used for development purposes, where you can get fast feedback by running locally against.
There are plans to change Pig such that it uses Hadoop even in local mode, which helps to make some programming more consistent.
The discussion for this topic is taking place at https://issues.apache.org/ jira/browse/PIG-1053
Running Pig in Hadoop mode means the compile Pig program will physically execute in a Hadoop installation.
The pseudo-distributed Hadoop setup we used in section 10.2 was purely for demonstration.
It’s rarely used except to debug configurations.) The execution mode is specified to the pig command via the -x or -exectype option.
You can enter the Grunt shell in local mode through:
In addition to running Pig Latin statements (which we’ll look at in a later section), the Grunt shell supports some basic utility commands.3 Typing help will print out a help screen of such utility commands.
You can stop a Hadoop   job   with the kill command followed by the Hadoop job ID.
The debug parameter states whether debug-level logging is turned on or off.
It’s useful to set a meaningful name to easily identify your Pig job in Hadoop’s Web UI.
The Grunt shell also supports file utility commands, such as ls and cp.
You can see the full list of utility commands and file commands in table 10.1
The file commands are mostly a subset of the HDFS filesystem   shell commands, and their usage should be self-explanatory.
Table 10.1 Utility and file commands in the Grunt shell.
Utility commands help quit kill jobid set debug [on|off] set job.name 'jobname'
File commands cat, cd, copyFromLocal, copyToLocal, cp, ls, mkdir, mv, pwd, rm, rmf, exec, run.
They run Pig scripts while inside the Grunt shell and can be useful in debugging Pig scripts.
The exec command executes a Pig script in a separate space from the Grunt shell.
Aliases defined in the script aren’t visible to the shell and vice versa.
The command run executes a Pig script in the same space as Grunt (also known as interactive mode )
It has the same effect as manually typing in each line of the script into the Grunt shell.
Before formally describing Pig’s data types and data processing operators, let’s run a few commands in the Grunt   shell to get a feel for how to process data in Pig.
For the purpose of learning, it’s more convenient to run Grunt in local mode:
You may want to first try some of the file commands, such as pwd and ls, to orient yourself around the filesystem.
We’ll later reuse the patent data we introduced in chapter 4, but for now let’s dig into an interesting data set of query logs from the Excite   search engine.
This data set already comes with the Pig installation, and it’s in the file tutorial/data/excite-small.log under the Pig installation directory.
The second column is a Unix timestamp, and the third is the search query.
A decidedly non-random sample from the 4,500 records of this file looks like.
From within Grunt, enter the following statement to load this data into an “alias” (i.e., variable) called log.
Note that nothing seems to have happened after you entered the statement.
In the Grunt shell, Pig parses your statements but doesn’t physically execute them until you use a DUMP or STORE command to ask for the results.
The DUMP command   prints out the content of an alias whereas the STORE command   stores the content to a file.
The fact that Pig doesn’t physically execute any command until you explicitly request some end result will make sense once you remember that we’re processing large data sets.
There’s no memory space to “load” the data, and in any case we want to verify the logic of the execution plan before spending the time and resources to physically execute it.
Like Hadoop, Pig will automatically partition the data into files named part-nnnnn.) When you DUMP an alias, you should be sure that.
The common way to do that is to create another alias through the LIMIT command   and DUMP the new, smaller alias.
The LIMIT command allows you to specify how many tuples (rows) to return back.
Table 10.2 summarizes the read and write operators in Pig Latin.
The data can be given a schema using the AS option.
The LIMIT operator defies categorization because it’s certainly not a read/write operator but it’s not a true relational operator either.
We include it here for the practical reason that a reader looking up the DUMP operator, explained later, will remember to use the LIMIT operator right before it.
You can apply the LIMIT operation on an alias to make sure it’s small enough for display.
Pig will create the directory and store the relation in files named part-nnnnn in it.
Uses the PigStorage store function as default unless specified otherwise with the USING option.
You may find loading and storing data not terribly exciting.
Let’s execute a few data processing statements and see how we can explore Pig Latin through Grunt.
The preceding statements count the number of queries each user has searched for.
The content of the output files (you’ll have to look at the file from outside Grunt) look like this:
Conceptually we’ve performed an aggregating operation   similar to the SQL query :
Two main differences between the Pig Latin and SQL versions are worth pointing out.
As we’ve mentioned earlier, Pig Latin is a data processing language.
You’re specifying a series of data processing steps instead of a complex SQL query with clauses.
The other difference is more subtle—relations in SQL always have fixed schemas.
In SQL, we define a relation’s schema before it’s populated with data.
In fact, you don’t need to use schemas if you don’t want to, which may be the case when handling semistructured or unstructured data.
Here we do specify a schema   for the relation log, but it’s only in the load statement and it’s not enforced until we’re loading in the data.
Any field that doesn’t obey the schema in the load operation is casted to a null.
In this way the relation log is guaranteed to obey our stated schema for subsequent operations.
As much as possible, Pig tries to figure out the schema for a relation based on the operation used to create it.
You can expose Pig’s schema for any relation with the DESCRIBE command.
This can be useful in understanding what a Pig statement is doing.
For example, we’ll look at the schemas for grpd and cntd.
Before doing this, let’s first see how the DESCRIBE command describes log.
As expected, the load command gives log the exact schema we’ve specified.
The relation log consists of three fields named user, time, and query.
The fields user and query are both strings (chararray in Pig) whereas time is a long integer.
A GROUP BY operation on the relation log generates the relation grpd.
Based on the operation and the schema for log, Pig infers a schema for grpd:
The field log is a bag with subfields user, time, and query.
As we haven’t covered Pig’s type system and the GROUP BY operation, we don’t expect you to understand this schema yet.
The point is that relations in Pig can have fairly complex schemas, and DESCRIBE is your friend in understanding the relations you’re working with:
Finally, the FOREACH command operates on the relation grpd to give us cntd.
Having looked at the output of cntd, we know it has two fields—the user ID and a count of the number of queries.
Pig’s schema for cntd, as given by DESCRIBE, also has two fields.
The second field has no name, but it has a type of long.
This field is generated by the COUNT function , and the function doesn’t automatically provide a name, although it does tell Pig that the type has to be a long.
Whereas DESCRIBE can tell you the schema of a relation, ILLUSTRATE does a sample run to show a step-by-step process on how Pig would compute the relation.
Pig tries to simulate the execution of the statements to compute a relation, but it uses only a small sample of data to make the execution fast.
The best way to understand ILLUSTRATE is by applying it to a relation.
The output is reformatted to fit the width of a printed page.)
The header row of each table describes the schema of the output relation after transformation, and the rest of the table shows example data.
The data hasn’t changed from one to the next, but the schema has changed from a generic bytearray (Pig’s type for binary objects) to the specified.
The GROUP operation on log is executed on the three sample log tuples to arrive at the data for grpd.
Based on this we can infer the GROUP operation to have taken the user field and made it the group field.
In addition, it groups all tuples in log with the same user value into a bag in grpd.
Seeing sample data in a simulated run by ILLUSTRATE can greatly aid the understanding of different operations.
Finally, we see the FOREACH operation applied to grpd to arrive at cntd.
Although DESCRIBE and ILLUSTRATE are your workhorses in understanding Pig Latin statements, Pig also has an EXPLAIN command to show the logical and physical execution plan in detail.
When used with a script name, for example, EXPLAIN myscript.pig, it will show the execution plan of the script.
To keep the display and processing manageable, only a (not completely random) sample of the input data is used to simulate the execution.
In the unfortunate case where none of Pig’s initial sample will survive the script to generate meaningful data, Pig will “fake” some similar initial data that will survive to generate data for alias.
This way B won’t be an empty relation and users can see how the script works.
In order for ILLUSTRATE to work, the load command in the first step must include a schema.
The subsequent transformations must not include the LIMIT or SPLIT operators, or the nested FOREACH operator, or the use of the map data type (to be explained in section 10.5.1)
You now know how to use Grunt to run Pig Latin statements and investigate their execution and results.
We can come back and give a more formal treatment of the language.
You should feel free to use Grunt to explore these language concepts as we present them.
Let’s first look at Pig data types from a bottom-up view.
The atomic types include numeric scalars as well as string and binary objects.
Type casting is supported and done in the usual manner.
A field in a tuple or a value in a map can be null or any atomic or complex type.
Whereas data structures can be arbitrarily complex, some are definitely more useful and occur more often than others, and nesting usually doesn’t go deeper than two levels.
In the Excite log example earlier, the GROUP BY operator generated a relation grpd where each tuple has a field that is a bag.
The schema for the relation seems more natural once you think of grpd as the query history of each user.
Each tuple represents one user and has a field that is a bag of the user’s queries.
We can also look at Pig’s data model from the top down.
At the top, Pig Latin statements work with relations, which is a bag of tuples.
It’s most often used as a row in a relation.
It’s represented by fields separated by commas, all enclosed by parentheses.
An inner bag is a bag that is a field within some complex type.
A bag is represented by tuples separated by commas, all enclosed by curly brackets.
Tuples in a bag aren’t required to have the same schema or even have the same number of fields.
It’s a good idea to do this though, unless you’re handling semistructured or unstructured data.
Map [key#value] A map is a set of key/value pairs.
But, Pig’s data model has more power and flexibility by allowing nested data types.
Maps are helpful in processing semistructured data   such as JSON, XML, and sparse relational data.
In addition, it isn’t necessary that tuples in a bag have the same number of fields.
Besides declaring types for fields, schemas can also assign names to fields to make them easier to reference.
Users can define schemas for relations using the AS keyword with the LOAD, STREAM, and FOREACH operators.
For example, in the LOAD statement for getting the Excite query log, we defined the data types for the fields in log, as well as named the fields user, time, and query.
In defining a schema, if you leave out the type, Pig will default to bytearray as the most generic type.
You can also leave out the name, in which case a field would be unnamed and you can only reference it by position.
You can apply expressions and functions to data fields to compute various values.
You can reference the named fields’ value directly by the name.
You can reference an unnamed field by $n, where n is its position inside the tuple.
Position is numbered starting at 0.) For example, this LOAD command provides named fields to log through the schema.
Let’s say we want to extract the time field into its own relation; we can use this statement:
Most of the time you should give names to fields.
One use of referring to fields by position is when you’re working with unstructured data.
When using complex types, you use the dot notation   to reference fields nested inside tuples or bags.
For example, recall earlier that we’d grouped the Excite log by user ID and arrived at relation grpd with a nested schema.
Each bag has tuples with three named fields: user, time, and query.
You reference fields inside maps through the pound operator   instead of the dot operator.
For a map named m, the value associated with key k is referenced through m#k.
Being able to refer to values is only a first step.
Pig supports the standard arithmetic, comparison, conditional, type casting , and Boolean expressions   that are common in most popular programming languages.
Numeric values with a decimal point are treated as double unless f or F is appended to the number to make it a float.
Sign +x, -x Negation (-) changes the sign of a number.
Cast (t)x Convert the value of x into type t.
Modulo x % y The remainder of x divided by y.
Conditional (x ? y : z) Returns y if x is true, z otherwise.
Equals to, not equals to, greater than, less than, etc.
Table 10.7 shows Pig’s built-in functions, most of which are self-explanatory.
If the two fields are bags, it will return tuples that are in one bag but not the other.
If the two fields are values, it will emit tuples where the values don’t match.
The column must be a numeric type or a chararray.
The column must be a numeric type or a chararray.
Word separators are space, double quote ("), comma, parentheses, and asterisk (*)
You must use them within relational operators to transform data.
The most salient characteristic about Pig Latin as a language is its relational operators.
These operators define Pig Latin as a data processing language.
We’ll quickly go over the more straightforward operators first, to acclimate ourselves to their style and syntax.
Afterward we’ll go into more details on the more complex operators such as COGROUP and FOREACH.
You can use the DISTINCT operator to remove duplicates from a relation.
It’s possible to write conditions such that some rows will go to both d and e or to neither.
The FILTER operator alone trims a relation down to only tuples that pass a certain test:
We’ve seen LIMIT being used to take a specified number of tuples from a relation.
The operations ‘till now are relatively simple in the sense that they operate on each tuple as an atomic unit.
More complex data processing, on the other hand, will require working on groups of tuples together.
Unlike previous operators, these grouping operators   will create new schemas in their output that rely heavily on bags   and nested data types.
The generated schema may take a little time to get used to at first.
Keep in mind that these grouping operators are almost always for generating intermediate data.
Their complexity is only temporary on your way to computing the final results.
Continuing with the same set of relations we used earlier,
The first field is group key, which is a3 in this case.
Looking at g’s  dump, we see that it has three tuples, corresponding to the three unique values in c’s third column.
The bag in the first tuple represents all tuples in c with the third column equal to 2
The bag in the second tuple represents all tuples in c with the third column equal to 4
After you understand how g’s data came about, you’ll feel more comfortable looking at its schema.
The first field of GROUP’s output relation is always named group, for the group key.
In this case it may seem more natural to call the first field a3, but currently Pig doesn’t allow you to assign a name to replace group.
You’ll have to adapt yourself to refer to it as group.
The second field of GROUP’s output relation is always named after the relation it’s operating on, which is c in this case, and as we said earlier it’s always a bag.
Before moving on, we want to note that one can GROUP by any function or expression.
For example, if time is a timestamp and there exists a function DayOfWeek , one can conceivably do this grouping that would create a relation with seven tuples.
Finally, one can put all tuples in a relation into one big bag.
This is useful for   aggregate analysis on relations, as functions work on bags but not relations.
This is one way to count the number of tuples in c.
The first field in GROUP ALL’s output is always the string all.
Now that you’re comfortable with GROUP, we can look at COGROUP , which groups together tuples from multiple relations.
For example, let’s cogroup a and b on the third column.
Whereas GROUP always generates two fields in its output, COGROUP always generates three (more if cogrouping more than two relations)
The first field is the group key, whereas the second and third fields are bags.
These bags hold tuples from the cogrouping relations that match the grouping key.
If a grouping key matches only tuples from one relation but not the other, then the field corresponding to the  nonmatching relation will have an empty bag.
To ignore group keys that don’t exist for a relation, one can add the INNER keyword   to the operation, like.
Conceptually, you can think of the default behavior of COGROUP as an outer join, and the INNER keyword can modify it to be left outer join, right outer join, or inner join.
Another way to do inner join in Pig is to use the JOIN operator.
The main difference between JOIN and an inner COGROUP is that JOIN creates a flat set of output records, as indicated by looking at the schema:
It goes through all tuples in a relation and generates new tuples in the output.
Behind this seeming simplicity lies tremendous power though, particularly when it’s applied to complex data types outputted by the grouping operators.
There’s even a nested form of FOREACH designed for handling complex types.
First let’s familiarize ourselves with different FOREACH operations on simple relations.
At its simplest, we use FOREACH to project specific columns of a relation into the output.
We can also apply arbitrary expressions, such as multiplication in the preceding example.
For relations with nested bags (e.g., ones generated by the grouping operations), FOREACH has special projection syntax, and a richer set of functions.
For example, applying nested projection to have each bag retain only the first field:
Most built-in Pig functions are geared toward working on bags.
Recall that g is based on grouping c on the third column.
This FOREACH statement therefore generates a frequency count of the values in c’s third column.
As we said earlier, grouping operators are mainly for generating intermediate data that will be simplified by other operators such as FOREACH.
As we’ll see, you can create many other functions via UDFs.
The FLATTEN function   is designed to flatten nested data types.
Syntactically it looks like a function, such as COUNT and AVG, but it’s a special operator as it can change the structure of the output created by FOREACH...GENERATE.
Its flattening   behavior is also different depending on how it’s applied and what it’s applied to.
For example, consider a relation with tuples of the form (a, (b, c))
When applied to bags, FLATTEN modifies the FOREACH...GENERATE statement to generate new tuples.
It removes one layer of nesting and behaves almost the opposite of grouping operations.
If a bag contains N tuples, flattening it will remove the bag and create N tuples in its place.
Another way to understand FLATTEN is to see that it produces a cross-product.
This view is helpful when we use FLATTEN multiple times within a single FOREACH statement.
For example, let’s say we’ve somehow created a relation l.
The following statement that flattens two bags outputs all combinations of those two bags for each tuple:
The corresponding output in m therefore has four rows representing the full cross-product.
Finally, there’s a nested form of FOREACH to allow for more complex processing of bags.
Let’s assume you have a relation (say l) and one of its fields (say a) is a bag, a FOREACH with nested block has this form:
The GENERATE statement must always be present at the end of the nested block.
It will create some output for each tuple in l.
The operations in the nested block can create new relations based on the bag a.
For example, we can trim down the a bag in each element of l’s tuple.
As of this writing, only five operators are allowed in the nested block: DISTINCT, FILTER, LIMIT, ORDER, and SAMPLE.
It’s expected that more will be supported in the future.
In those cases, users can specify the output schema using.
Doesn’t require the relations to have the same schema or even the same.
This syntax differs from the LOAD command where the schema is specified as a list after the AS option, but in both cases we use AS to specify a schema.
On many operators you’ll see an option for PARALLEL n.
The number n is the degree of parallelism you want for executing that operator.
In practice n is the number of reduce tasks in Hadoop that Pig will use.
If you don’t set n it’ll default to the default setting of your Hadoop cluster.
Pig documentation recommends setting the value of n according to the following guideline:
Splits a relation into two or more relations, based on the given Boolean expressions.
Note that a tuple can be assigned to more than one relation, or to none at all.
As with any relation, there’s no guarantee to the order of tuples.
Usually applied to transform columns of data, such as adding or deleting fields.
One can optionally specify a schema for the output relation; for example, naming new fields.
Loop through each tuple in nested_alias and generate new tuple(s)
At least one of the fields of nested_alias should be a bag.
DISTINCT, FILTER, LIMIT, ORDER, and SAMPLE are allowed operations in nested_op to operate on the inner bag(s)
Compute inner join of two or more relations based on common field values.
When using the replicated option, Pig stores all relations after the first one in memory for faster processing.
You have to ensure that all those smaller relations together are indeed small enough to fit in memory.
Under JOIN, when the input relations are flat, the output relation is also flat.
In addition, the number of fields in the output relation is the sum of the number of fields in the input relations, and the output relation’s schema is a concatenation of the input relations’ schemas.
Within a single relation, group together tuples with the same group key.
Usually the group key is one or more fields, but it can also be the entire tuple (*) or an expression.
One can also use GROUP alias ALL to group all tuples into one group.
The first field is always named “group” and it has the same type as the group key.
The second field takes the name of the input relation and is a bag type.
The schema for the bag is the same as the schema for the input relation.
Group tuples from two or more relations, based on common group values.
The output relation will have a tuple for each unique group value.
The second field is a bag containing tuples from the first input relation with matching group value.
In the default OUTER join semantic, all group values appearing in any input relation are represented in the output relation.
If an input relation doesn’t have any tuple with a particular group value, it will have an empty bag in the corresponding output tuple.
If the INNER option is set for a relation, then only group values that exist in that input relation are allowed in the output relation.
There can’t be an empty bag for that relation in the output.
For this, you have to specify the fields in a comma-separated list enclosed by parentheses for field_alias.
If you retrieve the relation right after the ORDER operation (by DUMP or STORE), it’s guaranteed to be in the desired sorted order.
At this point you’ve learned various aspects of the Pig Latin language—data types, expressions, functions, and relational operators.
But before discussing that we’ll end this section with a note on Pig Latin compilation and optimization.
As with many modern compilers, the Pig compiler   can reorder the execution sequence to optimize performance, as long as the execution plan remains logically equivalent to the original program.
For example, imagine a program that applies an expensive function (say, encryption) to a certain field (say, social security number) of every record, followed by a filtering function to select records based on a different field (say, limit only to people within a certain geography)
The compiler can reverse the execution order of those two operations without affecting the final result, yet performance is much improved.
Having the filtering step first can dramatically reduce the amount of data and work the encryption step will have to do.
As Pig matures, more optimization will be added to the compiler.
Therefore it’s important to try to always use the latest version.
But there’s always a limit to a compiler’s ability to optimize arbitrary code.
You can read Pig’s web documentation for techniques to improve performance.
Fundamental to Pig Latin’s design philosophy is its extensibility through user-defined functions (UDFs ), and there’s a well-defined set of APIs   for writing UDFs.
This doesn’t mean that you’ll have to write all the functions you need yourself.
Only if you don’t find an appropriate function should you consider writing your own.
You should also consider contributing your UDF back to PiggyBank to benefit others in the Pig community.
As of this writing UDFs are always written in Java and packaged in jar files.
To use a particular UDF you’ll need the jar file   containing the UDF’s class file(s)
For example, when using functions from PiggyBank you’ll most likely obtain a piggybank.jar   file.
To use a UDF, you must first register the jar file with Pig using the REGISTER statement.
Afterward, you invoke the UDF by its fully qualified Java class name.
For example, there’s an UPPER function in PiggyBank that transforms a string to uppercase:
If you need to use a function multiple times, it’ll get annoying to write out the fully qualified class name every time.
Pig offers the DEFINE statement to assign a name to a UDF.
Currently UDFs are only written in Java, and alias is the.
All UDFs must be registered before they can be used.
If you’re only using UDFs written by other people, this is all you need to know.
But if you can’t find the UDF you need, you’ll have to write your own.
Pig supports two main categories of UDFs: eval 6 and load/store.
We use the load/ store functions only in LOAD and STORE statements to help Pig read and write special formats.
Most UDFs are eval functions that take one field value and return another field value.
Some eval functions are quite common and have special considerations.
These include filter functions (eval functions that return a Boolean) and aggregate functions (eval functions that take a bag and return a scalar value)
As of this writing, you can only write a UDF using Pig’s Java API.
To create an eval UDF you make a Java class that extends the abstract EvalFunc<T> class.
It has only one abstract method which you need to implement:
This method is called on each tuple in a relation, where each tuple is represented by some of which are native Java classes and some of which are Pig extensions.
Table 10.10 Pig Latin   types and their equivalent classes in Java.
The best way to learn about writing UDFs is to dissect one of the existing UDFs in PiggyBank.
Even when writing your own, it’s often useful to start with a working UDF that’s functionally similar to what you want and only modify the processing logic.
The object input belongs to the Tuple class, which has two methods for retrieving its content.
In UPPER the retrieved field is casted to a Java String, which usually works but may cause a cast exception if we were casting   between incompatible data types.
We’ll see later how to use Pig to ensure that our casting works.
In any case, the try/catch block would’ve caught and handled any exception.
If everything works, most UDFs should implement the default behavior that the output is null when the input tuple is null.
The instantiation of FuncSpec is quite convoluted, which is due to Pig’s ability to handle complex nested types.
Fortunately, unless you work with unusually complicated types, you’ll probably find a FuncSpec instantiation for the type you want already in one of PiggyBank ’s UDFs.
Besides telling Pig the input schema, you can also tell Pig the schema of your output.
You may not need to do this if the output of your UDF is a simple scalar , as Pig will use Java’s Reflection mechanism to infer the schema automatically.
But if your UDF returns a tuple or a bag, the Reflection mechanism   will fail to figure out the schema completely.
In that case you should specify it so that Pig can propagate the schema correctly.
In UPPER’s case it only outputs a simple String, so it’s not necessary to specify the it’s returning a string (DataType.CHARARRAY)
Again, the Schema object construction looks convoluted because of Pig’s ability to have complex nested types.
One special case is if the schema of your UDF’s output is the same as the input.
As with the construction of FuncSpec, you’ll probably find some preexisting UDFs in PiggyBank with your desired output schema.
Filter functions   are eval functions that return a Boolean, and we use them in Pig Latin’s FILTER and SPLIT statements.
Aggregate functions are eval functions that take in a bag and return a scalar.
They’re usually used for computing aggregate metrics, such as COUNT, and we can sometimes optimize them in Hadoop by using a combiner.
We haven’t covered the load/save UDFs for reading and writing data sets.
These more advanced topics are covered in Pig’s documentation on UDFs: http://hadoop.apache.org/pig/docs/r0.3.0/udf.html.
Writing Pig Latin scripts is largely about packaging together the Pig Latin statements that you’ve successfully tested in Grunt.
As you’ll reuse your Pig Latin script, it’s obviously a good idea to leave comments   for other people (or yourself) to understand it in the future.
Pig Latin   supports two forms of comments, single-line and multiline.
You start the single-line comment   by a double hyphen and the comment ends at the end of the line.
For example, a Pig Latin script with comments can look like.
When you write a reusable script, it’s generally parameterized such that you can vary its operation for each run.
For example, the script may take the file paths of its input and output from the user each time.
Pig supports parameter substitution   to allow the user to specify such information at runtime.
It denotes such parameters by the $ prefix within the script.
For example, the following script displays a user-specified number of tuples from a user-specified log file:
Note that you don’t need the $ prefix in the arguments.
You can enclose a parameter value in single or double quotes, if it has multiple words.
A useful technique is to use Unix commands   to generate the parameter values, particularly for dates.
This is accomplished through Unix’s command substitution, which executes commands enclosed in back ticks (`)
By doing this, the input file for Myscript.pig will be based on the date the script is run.
If you have to specify many parameters, it may be more convenient to put them in a file and tell Pig to execute the script using parameter substitution based on that file.
For example, we can create a file Myparams.txt with the following content:
The parameter file is passed to the pig command with the -param_file filename argument.
You can specify multiple parameter files   as well as mix parameter   files with direct specification of parameters at the command line using -param.
If you define a parameter multiple times, the last definition takes precedence.
When in doubt about what parameter values a script ends up using, you can run the pig command with the -debug option.
This tells Pig to run the script and also output a file named original_script_name.
Executing pig with the -dryrun option outputs the same file but doesn’t execute the script.
The exec and run commands allow you to run Pig Latin scripts from within the Grunt shell, and they support parameter substitution using the same -param and -param_file arguments; for example:
However, parameter substitution in exec and run doesn’t support Unix commands, and there’s no debug or dryrun option.
In the Grunt shell, a DUMP or STORE operation processes all previous statements needed for the result.
On the other hand, Pig optimizes and processes an entire Pig script as a whole.
This difference would have no effect at all if your script has only one DUMP or STORE command at the end.
If your script has multiple DUMP/STORE, Pig   script’s multiquery   execution improves efficiency by avoiding redundant evaluations.
For example, let’s say you have a script that stores intermediate data:
If you enter the statements in Grunt, where there’s no multiquery execution, it will generate a chain of jobs on the STORE b command to compute b.
This works on the assumption that the stored value of b has not been modified, because Grunt, by itself, has no way of knowing.
On the other hand, if you run all the statements as a script, multiquery execution can optimize the execution by intelligently handling intermediate data.
Pig compiles all the statements together and can locate the dependency and redundancy.
Multiquery execution is enabled by default and usually has no effect on the computed results.
But multiquery execution can fail if there are data dependencies that Pig is not aware of.
This is quite rare but can happen with, for example, UDFs.
If the custom function MYUDF is such that it accesses a through the file out1, the Pig compiler would have no way of knowing that.
Not seeing the dependency, the Pig compiler may erroneously think it OK to evaluate b and c before evaluating a.
To disable multiquery execution, run the pig command with -M or -no_multiquery option.
Given the extra power that Pig provides, we can take on more challenging data processing applications.
One interesting application from the patent data set is finding similar patents based on citation data.
Patents that are often cited together must be similar (or at least related) in some way.
For our purpose here, let’s suppose we want to look into patents that are cited together more than N times, where N is a fixed number we specify.8
For applications that involve pair-wise computations   (e.g., computing number of cocitations for each pair of patents), it’s often easy to imagine an implementation involving a pair of nested loops enumerating all pair combinations and performing the computation on each pair.
Even though Hadoop makes it easy to scale by adding more hardware, we should continue to remember fundamental concepts in computational complexity.
Quadratic complexity   will still bring linear scalability to its knees.
The main insight to leverage is that the resulting data is sparse.
Most pairs will have zero similarity as most pairs of patents are never cited together.
Our similarity computation will become much more manageable if we redesign it to only work on patent pairs that are known to have been cited together.
This advantage would be even more apparent if the patent data set is larger.
Even though we’ve figured out the algorithm for this application, implementing it in MapReduce can still be tedious.
It’ll require chaining multiple jobs together, and each job will require its own class.
Pig Latin, on the other hand, takes only a dozen lines to implement the three-step program (listing 10.1), and further optimization can eliminate more lines and increase efficiency still.
Listing 10.1 Pig Latin script to find patents   that are often cited together.
Variations of this may involve more advanced scoring functions, such as normalizing for frequent items, or computing a similarity ranking rather than a simple cutoff.
The simple cutoff criterion we chose here is easier to implement and illustrates the essence of computing similarity.
Pig Latin, and probably complex data processing in general, can be hard to read.
Fortunately, we can use Grunt’s ILLUSTRATE command on cocite_bag to get a simulated sample run of the statements and see what each operation is generating.
We’ve reformatted the output to fit the width of the printed page.)
The relation cite_grpd contains a bag for each patent, and in this bag are the cited patents.
Grouping cocited patents was done by the GROUP operation in creating cite_grpd.
This duplication will allow the cross-product operation to generate all pair-wise combinations.
We know that cocite   is a big relation, even under our scheme which is more efficient than brute force.
The first potential reduction is to notice that each cited patent is considered to have been cocited with itself.
As we know that it’s quite pointless for our application to figure out that a patent is similar to itself, we can ignore all such pairs.
Note that if we keep these “identity” pairs in the calculation, the cocitation count for them will end up being exactly the citation count.
These numbers can still be useful if we’re looking for the percentage of times patents are cocited.
As we’re not computing percentages, that consideration wouldn’t affect us.
As cocitation is symmetric , pairs always appear twice, in reverse order.
Given our current application need to find patent pairs that are cocited more than N times together, we can put in a simple rule retaining only one of the two redundant pairs and trim cocite’s size by half.
This rule can be thus: retain only pairs where the first field is smaller than the second field.
But keeping the redundant pairs can be useful for lookup later in some applications.
For example, we can find all patents cocited with X by searching for X in the first field.
In the more condensed version we’d have to look for X in both fields.
Finally, we can use heuristics   to remove cocitation pairs that we don’t think are.
Note that cocite can be computed from cite_grpd directly by using a more complicated FOREACH statement, and you may choose to do it when you feel more comfortable reading Pig Latin.
In our case, a patent that cites many patents together will generate a quadratic number of rows in cocite.
If we believe that such “verbose” patents don’t help us understand similar patent pairs, removing them can significantly reduce the size of data to process with little impact on final results.
The benefit of this heuristic is much greater if we’re looking at reverse patent citation or text documents, where frequency of items are extremely skewed and quadratic expansion on a few popular items can dominate the amount of data processed.
In fact, in such situations approximate heuristics are almost necessary.
An important process check is to note that we’ve focused on a higher level of data processing issues.
We grouped the patent   pair citations together, counted them, and flattened out the relation.
Unfortunately, ILLUSTRATE generates sample data that only has cocitation counts of 1
However, we see that the operations are doing basically what we wanted.
If we stick to the original application requirement of only looking for patent pairs that have been cocited more than N times, we would apply a filter on cocite_flat and be finished.
But we want to show how we can further group the tuples, which would be needed for other types of filtering.
For example, you may want to find the K most cocited patents for each patent.
Let’s see an example of a nested FOREACH statement to filter out tuples inside bags that have counts of 5 or less.
As you can see, Pig has simplified the implementation of this data processing application tremendously.
Using Pig and Hadoop, this turns into only an afternoon’s work.
Furthermore, its improved ease of development enables rapid prototyping of alternative features.
For your own exercise, instead of finding patents that are often cited together, can you find patents that have similar citations?
Pig is a higher-level data processing layer on top of Hadoop.
Its Pig Latin language provides programmers a more intuitive way to specify data flows.
It supports schemas in processing structured data, yet it’s flexible enough to work with unstructured text or semistructured XML data.
It vastly simplifies data joining and job chaining—two aspects of MapReduce programming that many developers found overly complicated.
To demonstrate its usefulness, our example of computing patent cocitation shows a complex MapReduce program written in a dozen lines of Pig Latin.
As powerful as Hadoop is, it doesn’t offer everything for everybody.
Many projects have sprung up to extend Hadoop for specific purposes.
The most prominent and well-supported ones have officially become subprojects under the umbrella of the Apache Hadoop   project.1 These subprojects include.
ZooKeeper—A reliable coordination system for managing shared state between distributed applications.
We covered Pig in detail in chapter 10, and we’ll learn about Hive in this chapter.
Some of these aren’t associated with Apache (e.g., Cascading, CloudBase)
You’ll see some of these tools in action in the case studies of chapter 12
Hive2 is a data warehousing package built on top of Hadoop.
It began its life at Facebook   processing large amount of user and log data.
Its target users remain data analysts who are comfortable with SQL and who need to do ad hoc queries , summarization , and data analysis on Hadoop-scale data.3 You interact with Hive by issuing queries in a SQL-like language called HiveQL.
For example, a query to get all active users from a user table looks like this:
Hive’s design reflects its targeted use as a system for managing and querying structured data.
By focusing on structured data , Hive can add certain optimization and usability features that MapReduce, being more general, doesn’t have.
Hive’s SQL-inspired language separates the user from the complexity of MapReduce   programming.
It reuses familiar concepts from the relational database   world, such as tables, rows, columns, and schema, to ease learning.
In addition, whereas Hadoop naturally works on flat files, Hive can use directory structures to “partition” data to improve performance on certain queries.
To support these additional features, a new and important component of Hive is a metastore   for storing schema information.
You can interact with Hive using several methods, including a Web GUI   and Java Database Connectivity (JDBC) interface.
Most interactions, though, tend to take place over a command line interface (CLI ), which is what we focus on.
You can see a high-level architecture diagram of Hive in figure 11.1
It’s therefore not a direct replacement for traditional SQL data warehouses, such as ones offered by Oracle.
The metastore is an determine how queries will be run.
You can find the latest release of Hive at http://hadoop.apache.org/hive/releases.html.
Download and extract the tarball   into a directory that we call HIVE_HOME.
In addition, you need to set up a couple directories in HDFS for Hive to use.
If you let Hive manage your data completely for you, Hive will store your data under the /user/hive/warehouse directory.
Hive can automatically add compression   and special directory structures (such as partitions ) to those data to improve query performance.
It’s good to let Hive manage your data if you plan on using Hive to query it.
But if you already have your data in some other directories in HDFS and want to keep them there, Hive can work with them too.
In that case, Hive will take your data as is and won’t try to optimize your data storage for query processing.
Some casual users don’t understand this distinction, and believe that Hive requires data to be in some special Hive format.
Out-of-the-box Hive comes with an open source, lightweight, embedded SQL database called Derby, 4 which is installed and run on the client machine along with Hive.
If you are the only Hive user, this default setup should be fine.
But beyond the initial testing and evaluation, you’ll most likely deploy Hive in a multi-user environment, where you wouldn’t want each user to have their own version of the metadata.
Typically, you use a shared SQL database such as MySQL, but any JDBC-compliant database   will do.
You’ll need a database server and you’ll need to create a database dedicated as a Hive metastore.
Once you have created this, configure every Hive installation to point to it as the metastore.
A raw installation doesn’t have a hive-site.xml file, and you’ll have to create it.
Properties in this file override the properties in hive-default.xml, in the same way that hadoopsite.xml overrides hadoop-default.xml.
The file hive-site.xml   should override three properties and look like the following:
Specify the javax.jdo.option.ConnectionURL and javax.jdo.option.ConnectionDriverName properties again in the file jpox.
In addition, the username and password to log into the database are also specified in jpox.properties.
Table 11.1 Configuration for using a MySQL database   as a metadata store in multi-user mode.
Set this to false to use a remote metastore server.
Once you have the database set up, or if you’re only evaluating Hive and can use its default single-user mode, you’re ready to go into its CLI.
You’ll receive the Hive prompt, ready to take your Hive commands.
Before we formally explain HiveQL, it’s useful to run a few commands from the CLI.
You’ll get a feel of how it works and can explore on your own.
Let’s assume you have the patent citation data cite75_99.txt on your local machine.
Recall that this is a comma-separated data set of patent citations.
In Hive, we first define a table that will store this data:
You can have a statement that goes over multiple lines as long as you type in a semicolon only at the end, as we’ve done here.
Most of the action in this four-line command is in the first line.
The first column is called citing and is of type INT , whereas the second column is called cited and is also of type INT.
The complete format for the MySQL JDBC driver is described in http://dev.mysql.com/doc/refman/5.0/ en/connector-j-reference-configuration-properties.html.
We can see what tables are currently in Hive with the SHOW TABLES command:
We can check its schema with the DESCRIBE command :
The table has the two columns from our definition, as expected.
Managing and defining tables in HiveQL are similar to standard relational databases.
This tells Hive to load data from a file called cite75_99.txt in the local filesystem into our cite table.
Underneath the hood, the local machine uploads this data into HDFS, under some directory managed by Hive.
Unless you’ve changed the configuration, this will be some directory under /user/hive/warehouse.)
When loading data, Hive will not let any data into a table that violates its schema.
In place of those data Hive will substitute a null.
We can use a simple SELECT statement to browse data in the cite table:
We see that there’s a row with nulls , indicating that a record violated the schema.
This is due to the first line of cite75_99.txt, which has the column names rather than patent numbers.
Now that we’re pretty confident that Hive has read the data and is managing it, we can run all kinds of queries on it.
Let’s start by counting how many rows are in the table.
In SQL this is accomplished by the familiar SELECT COUNT(*)
Reading the messages, you can see that this query had created a MapReduce job.
The beauty of Hive is that the user doesn’t need to know anything about MapReduce at all.
As far as she’s concerned, she’s only querying a database using a language similar to SQL.
The result of the previous query was printed directly to the screen.
In most cases the query result should be saved to disk, which usually would be some other Hive table.
Our next query finds the citation frequency of each patent.
We can execute a query to find the citation frequency.
The query uses the COUNT and GROUP BY features, again in a way similar to SQL.
There’s an additional INSERT OVERWRITE TABLE clause to tell Hive to write the result to a table:
The query execution helpfully tells us that 3,258,984 rows were loaded into the citation frequency table.
We can execute more HiveQL statements to browse this citation frequency table:
When you’re finished with using a table, you can delete it with the DROP TABLE command :
It doesn’t ask you for confirmation whether you really want to delete the table or not.
It’s difficult to recover a table once you have dropped it.
Finally, you can exit your Hive session with the exit command.
Having seen Hive in action, we’re ready to formally look at different aspects and usage of HiveQL.
We’ve already seen that Hive supports tables as a fundamental data model.
For example, the cite table we created earlier would have its data under the /user/hive/warehouse/cite directory.
In the most basic setup, the directory hierarchy under a table is only one level deep, and the table’s data are spread out over many files under that one directory.
Relational databases use indexes   on columns to speed up queries   on those columns.
Hive, instead, uses a concept of partition columns , which are columns whose values would divide the table into separate partitions.
Hive treats partition columns differently than regular data columns, and executes queries involving partition columns much more efficiently.
This is because Hive physically stores different partitions in different directories.
For example, let’s say you have a table named users that has two partition columns date and state (plus the regular data columns)
Hive   will have a directory structure like this for that table:
Queries over ranges in the partition columns will involve processing multiple directories, but Hive will still avoid a full scan of all data in users.
In some sense partitioning brings similar benefits to Hive as indexing provides to a traditional relational database, although partitioning works at a much less granular level.
You’ll want each partition to still be big enough that a MapReduce job on it can be reasonably efficient.
In practice you’ll also have to handle District of Columbia and various territories.
In addition to partitions, the Hive data model also has a  concept of buckets , which provide efficiency to queries that can work well on a random sample of data.
For example, in computing the average of a column, a random sample of data can provide a good approximation.) Bucketing divides data into a specified number of files based on the hash of the bucket column.
If we specify 32 buckets based on user id in our users table, the full file structure for our table in Hive will look like.
By bucketing on user id, Hive will know that each file in part-00000 ...
The computation of many aggregate statistics remains fairly accurate on a sampled data set.
Hive can still do sampling without buckets (or on columns other than the bucket column), but this involves scanning in all the data and randomly ignoring much of it.
Much of the efficiency advantage of sampling would therefore be lost.
We’ve already seen how to create a simple table for the patent citation data set.
Let’s now break down the different parts of a more complicated table creation statement.
It specifies the name of the table (page_view) and its schema, which includes the name of the columns as well as their type.
Noticeably missing is the Boolean type, which is usually handled as TINYINT.
Hive also has complex types , such as structs , maps , and arrays   that can be nested.
But they’re currently not well supported in the language and are considered advanced topics.
We can attach a descriptive comment to each column, as was done here for the ip column.
In addition, we also add a descriptive comment to the table:
The next part of the CREATE TABLE statement specifies the partition columns:
As we’ve discussed previously, partition columns are optimized for querying.
The value of a partition column for a particular row is not explicitly stored with the row; it’s implied from the directory path.
But there’s no syntactical difference in queries over    partition columns or data columns.
The choice of the number of buckets   will depend on the following:
The first criterion is important because after you divide a partition into the specified number of buckets, you wouldn’t want each bucket file to be so small that it becomes inefficient for Hadoop to handle.
On the other hand, a bucket should be the same size or smaller than your intended sample size.
Specifying bucketing information merely tells Hive that you’ll manually enforce the bucketing (sampling) criteria when data is written to a table and that Hive can take advantage of it in processing queries.
To enforce the bucketing criteria you need to correctly set the number of reducers when populating the table.
The ROW FORMAT clause   tells Hive how the table data is stored per row.
Without this clause, Hive defaults to the newline character as the row delimiter and an ASCII value.
Our clause tells Hive to use the tab character as the field delimiter   instead.
We also tell Hive to use the newline character as the line delimiter, but that’s already the default and we include it here only for illustrative purposes:
Finally, the last clause tells Hive the file format to store the table data:
Sequence file is a compressed format and usually provides higher performance.
We can add an EXTERNAL modifier   to the CREATE TABLE statement such that the table is created to point to an existing data directory.
After you’ve created a table, you can ask Hive the table’s schema with the DESCRIBE command:
You can also change the table structure with the ALTER command.
To delete the whole table, use the DROP TABLE command:
To know what tables are being managed by Hive, you can show them all with.
If there are so many tables in use that it becomes unwieldy to list them all, you can narrow down the result with a Java regular expression :
There are multiple ways to load data into a Hive table.
If we omit the OVERWRITE modifier, the content is added to the table rather than replacing whatever already exists in it.
If we omit the LOCAL modifier, the file is taken from HDFS instead of the local filesystem.
The  LOAD DATA command also allows you to name a specific partition in the table to load the data into:
When working with data from the local filesystem, it’s useful to know that you can execute local Unix   commands from within the Hive CLI.
You prepend the command with the exclamation mark (!) and end it with a semicolon (;)
Note that the spaces around ! and ; aren’t necessary.
For the most part, running HiveQL queries   is surprisingly similar to running SQL queries.
One of the general differences is that the results of HiveQL queries are relatively large.
You should almost always have an INSERT clause   to tell Hive to store your query result somewhere.
Note that the query is over a partition column (country), but the query would look exactly the same if it was a data column instead.
For example, you would use this HiveQL query to find the number of page views from the U.S.:
Like SQL, the GROUP BY clause allows one to do aggregate queries on groups.
This query will list the number of page views from each country:
And this query will list the number of unique users from each country:
These are quite standard in SQL and programming languages and we won’t explain them in detail.
HiveQL provides two commands for regular expression matching—LIKE and REGEXP.
One of the main motivators for users to seek a higher-level language, such as Pig Latin and HiveQL, is the support of joins.
The format of Java regular expression is fully explained in the Javadoc http://java.sun.com/j2se/1.4.2/ docs/api/java/util/regex/Pattern.html.
Syntactically, in the FROM clause you add the JOIN keyword between the tables and then specify the join columns after the ON keyword.
To join more than two tables, we repeat the pattern like this:
We can add sampling to any query by modifying the FROM clause.
This query tries to compute the average view time, except the average is only taken from data in the first bucket out of 32 buckets:
In addition, y needs to be a multiple or factor of the number of buckets specified for the table at table creation time.
For example, if we change y to 16, the query becomes.
On the other hand, if y is specified to be 64, Hive will execute the query on half of the data in one bucket.
The value of x is only used to select which bucket to use.
Programmers can also add UDFs to Hive for custom processing.
A brief introduction on how to create a UDF is given in http:// wiki.apache.org/hadoop/Hive/AdminManual/Plugins.
Hive is a data warehousing layer built on top of Hadoop’s massively scalable architecture.
By focusing on structured data, Hive has added many performance-enhancing techniques (such as partitions) and usability features (such as a SQL-like language)
Hive is introducing Hadoop technology to a wider audience of analysts and other nonprogrammers.
Facebook decided to build out its Hadoop infrastructure is given in http://www.facebook.com/note.
The result goes until the end of str unless an optional length argument is specified.
Returns the smallest integer (BIGINT) that’s equal to or bigger than num.
Returns a random number (that changes from row to row)
The optional seed value can make the random number sequence deterministic.
Returns whether the string s matches the Java regular expression regex.
Returns a string where all parts of s that match the Java regular expression regex are replaced with replacement.
Returns the day part of a date or timestamp string.
Returns the number of members in the group, or the number of distinct values of the column.
Returns the sum of the values of the column, or the sum of the distinct values of the column.
Returns the average value of the column, or the average of the distinct values of the column.
The following are projects or vendors related to Hadoop that we find useful or that have tremendous potential.
All of them except Aster Data and Greenplum are open source in some way.
It’s modeled after Google’s Bigtable9   and targeted to support large tables, on the order of billions of rows and millions of columns.
It uses HDFS   as the underlying filesystem and is designed to be fully distributed and highly available.
You can use it independently from the Hadoop Core framework.
It implements many of the common services used in large distributed applications, such as configuration management, naming, synchronization, and group services.
Historically developers have to reinvent these services for each distributed application, which is time consuming and error prone, as these services are notoriously difficult to implement correctly.
By abstracting away the underlying complexity, ZooKeeper makes it easy to implement consensus, leader election, presence protocols, and other primitives, and frees the developer to focus on the semantics of her application.
ZooKeeper is often a major component in other Hadoop-related projects, such as HBase   and Katta.
It abstracts away the MapReduce model into a data processing model consisting of tuples , pipes , and (source and sink) taps.
Pipes operate on streams of tuples, where operations include Each, Every, GroupBy, and CoGroup.
One difference, though, is that Pig’s Grunt shell makes it easier to execute ad hoc queries.
Another difference is that Pig programs are written in Pig Latin, whereas Cascading works more like a Java framework in which you create a data processing flow through instantiating various Java classes (Each, Every, etc.)
Using Cascading doesn’t require learning a new language, and the data process flow created can be more efficient because you’ve written it directly yourself.
It’s supporting and packaging Hadoop to be easy and friendly to enterprise users.
It provides live training sessions in major cities as well as educational videos on their web site.
You can simplify your deployment of Hadoop by using their free Hadoop distribution, in either RPM   or Ubuntu /Debian   Packages.
Their Hadoop distribution is based on the most recent stable release of Hadoop, plus useful (and tested) patches from future releases, and additional tools such as Pig and Hive.
Cloudera also offers consulting and support services to help enterprises use Hadoop.
Nutch   is a web search engine built on top of Hadoop.10 But as a web search engine, Nutch has many unique requirements.
It is often a mismatched solution for specific search applications.
It may be more accurate to say that Nutch motivated the creation of Hadoop.
In some sense it’s adding some extra capabilities (such as replication, redundancy, fault tolerance, and scalability) to Lucene   while retaining the basic application semantics.
Unlike Hive, CloudBase works directly on flat files without any metadata store.
It makes a stricter goal of ANSI SQL adherence, and interaction is primarily through a JDBC driver, which makes it easier to connect to business intelligence reporting tools.
For the most part, CloudBase is a compiler that takes SQL queries and compiles them into MapReduce programs.
As of this writing, CloudBase has a less active developer community than Pig or Hive, and its GPL license is more restrictive than the Apache license.
Although they support the MapReduce programming model, they were both created independently from Hadoop and had made many different underlying design choices.
Unlike Hadoop, their offerings are architected much more specifically toward enterprise customers looking for higher-performing SQL data warehouses.
As they come at the MapReduce paradigm from a different angle than Hadoop, studying them can help understand some of Hadoop’s architectural trade-offs.
Hama is a matrix computation package for calculating products, inverse, eigenvalues, eigenvectors, and other matrix operations.
Mahout is targeted more specifically at implementing machine learning algorithms on Hadoop (for more information, see Mahout in Action, Manning Publications)
At the time of this writing both projects are relatively new and under the Apache incubator.
As a Hadoop programmer, you’ll often have the need to find some piece of documentation about Hadoop or its subprojects.
Sematext , a company specializing in search and analytics, runs http://search-hadoop.com/, a site that lets you search across all.
Hadoop subprojects and data sources—mailing list archives, Wikis, issue tracking systems, source code, and so on.
Search results allow filtering by project, data source, and author, and can be sorted by date, relevance, or the combination of the two.
This chapter covered many of the additional tools you can use with Hadoop.
We gave special attention to Hive, a data warehousing package that allows you to process data in Hadoop using an SQL-like language.
A rich ecosystem of supporting software has sprung up around Hadoop, and you’ll see some of them in action in the case studies in the next chapter.
We’ve been through many exercises and sample programs by now.
The next step is to integrate what you’ve learned about Hadoop into your own real-world applications.
To help you in that transition, this chapter provides examples of how other enterprises have used Hadoop as part of the solution to their data processing problems.
One is to step back and see the broader systems that utilize Hadoop as a critical part.
You’ll discover complementary tools, such as Cascading, HBase, and Jaql.
The second purpose is to demonstrate the variety of businesses that have used Hadoop to solve their operational challenges.
Our case studies span industries, including media (the New York Times), telecom (China Mobile), internet (StumbleUpon), and enterprise software (IBM)
Because the Times had stored its older articles as scanned TIFF images , they needed image processing to combine different pieces of each article together into a single file in the desired PDF format.
Previously, these articles were behind a paid wall and didn’t receive much traffic.
The Times could use a real-time approach to scale, glue, and convert the TIFF images.
Although that worked well enough for a low volume of requests, it would not scale to handle the significant traffic increase expected from the articles’ free availability.
The Times needed a better architecture to handle the opening of its archive.
The solution was to pregenerate all the articles as PDF files and serve them like any other static content.
The New York Times already had the code to convert the TIFF images to PDF files.
It looked like a simple matter of batch processing all the articles in one setting instead of dealing with each individual article as a request came in.
Derek Gottfrid , a software programmer at the Times, thought this was a perfect opportunity to use the Amazon Web Services (AWS ) and Hadoop.
Storing and serving the final set of PDFs from Amazon’s Simple Storage Service (S3 ) was already deemed a more cost-effective approach than scaling up the storage back-end of the website.
Why not process the PDFs in the AWS cloud as well?
Thanks to Derek’s work, it has become much easier for people to look up the New York Times’ account of historic events.
China Mobile Communication Corporation (CMCC) is the largest mobile phone operator in the world.
Traded on NYSE under the symbol CHL, China Mobile is seventh in BrandZ’s global brand equity ranking for 2009, behind McDonald’s and Apple but ahead of General Electric.
With more than two-thirds of China’s mobile phone market, CMCC serves the communication needs of 500 million subscribers.
Even at its size, China Mobile has experienced rapid growth.
As with any telecom operator, China Mobile generates a lot of data in the normal course of running its communication network.
For example, each call generates a call data record (CDR) , which includes information such as the caller’s phone number, the callee’s phone number, the start time of the call, the call’s duration, information about the call’s routing, and so forth.
In addition to CDR, a phone network also generates signaling data between various switches, nodes, and terminals within the network.
At a minimum, we need this data for completing calls and accurately billing customers.
We also need it to analyze for marketing, network tuning, and other purposes.
At China Mobile, the size of its network naturally leads to large amounts of data created.
A branch company of China Mobile can have more than 20 million subscribers, leading to more than SMS every day.
China Mobile looks to data warehousing   and mining of this data to extract insights for improving marketing operations, network optimization, and service optimization.
China Mobile has experiences with commercial data mining tools from some wellknown vendors.
These tools’ architectural design limits China Mobile’s current data mining system because it requires all data to be processed within a single server.
The current system at one of the branch companies is based on commercial solutions and consists of a Unix server   with eight CPU cores, 32 GB memory, and a storage array.
Even within the limitation on the amount of data that can be processed, the current system takes too much time for many applications.
In addition, the high-end Unix servers and storage arrays are expensive, and the commercial package software don’t support custom algorithms well.
Because of the limitations of the current system, China Mobile initiated an experimental project to develop a parallel data mining tool set on Hadoop and evaluated it against its current system.
They named the project Big Cloud–based Parallel Data Mining (BC-PDM ) and it was architected to achieve four objectives :
BC-PDM implemented many of the standard ETL operations   and data mining algorithms in MapReduce.
The ETL operations include computing aggregate statistics, attribute processing, data sampling, redundancy removal, and others.
The categories include clustering (e.g., K-means ), classification   (e.g., C4.5), and association analysis   (e.g., Apriori )
They were all fairly large, and for certain evaluation tasks smaller, sampled subsets were needed.
You’ll see the original size (Large scale) as well as the size of the sampled subsets (Middle scale and Small scale) in table 12.1
China Mobile evaluated BC-PDM on four dimensions : correctness, performance, cost, and scalability.
Correctness was, of course, necessary for the new system to be useful.
They verified BC-PDM’s parallel ETL operations by ensuring the same results were generated as the existing serial ETL implementation.
The data mining algorithms, on the other hand, were not expected to generate the exact same results as the existing.
This is because minor implementation and execution details, such as initial conditions and ordering of the input data, can affect the exact output.
In addition, the   UCI data sets   were also employed to verify BC-PDM’s parallel data mining algorithms.
The UCI data sets are popular among researchers in the machine-learning community and are well understood.
China Mobile can verify BC-PDM’s output with known expected models.
After establishing the correctness of the MapReduce implementations, the performance of BC-PDM was compared to the current system.
As we’ll see, this small cluster is cheaper than the monolithic big-iron server of the current system.
Figure 12.1 shows the timing between the two setups for ETL operations (left graph)  and data mining tasks (right graph)
Note that BC-PDM was tasked to process 10 times the amount of data as the current system.
For data mining tasks BC-PDM was further stresstested with 100 times the amount of data as the current system.
Even with data size two orders of magnitude larger, BC-PDM   was faster than the current system at the Apriori (association) and C4.5 (classification) algorithms.
The K-means clustering algorithm took slightly longer to complete than the current system at 10 times the data size.
These real-world applications include channel preference modeling, new service association modeling, and subscriber subdivision modeling.
Recall that BC-PDM in this evaluation is based on a relatively small 16-node Hadoop cluster.
As we’ll see later, BC-PDM and Hadoop scale well with additional nodes.
Not only is the 16-node BC-PDM cluster outperforming the current system, it’s also significantly cheaper.
Table 12.2 shows a cost breakdown of the two systems.
As of this writing, one USD converts to a little less than seven RMB.) The 16-node Hadoop/ BC-PDM   cluster  is roughly one fifth the cost of the current commercial solution.
The biggest saving comes from the use of low-cost commodity servers.
Figure 12.1 Performance comparison of the Hadoop   cluster versus existing commercial Unix   server.
The left graph tests ETL operations whereas the right graph is for data mining algorithms.
Till now, we’ve investigated the correctness, performance, and cost of the new BCPDM system.
Let’s examine the scalability of the system as we add more nodes to the cluster.
We measured the speed-up in execution on larger clusters, taking the execution time in the 32-node cluster as baseline.
You’ll see the results in figure 12.2, with the left graph showing the speed-up for ETL operations and the right graph showing data mining operations.
We see that many ETL operations are close to this linear scalability ideal.
Figure 12.2 Scalability of ETL (left) and   data mining (right) algorithms on Hadoop cluster as extra nodes are added.
The horizontal axis represents the number of nodes in the BC-PDM cluster.
The vertical axis represents the speed-up, relative to the execution time on the 32-node cluster.
As of this writing, one USD converts to a little less than seven RMB.)
Yet the 16-node cluster could handle an order of magnitude more data faster than the existing commercial solution.
Together these evaluations demonstrate our BC-PDM cluster’s ability to handle data at the 100-TB level going forward.
After the thorough evaluation of the BC-PDM system, we worked with the Shanghai Branch to apply our system to some of their business needs.
One application was to characterize their user base to enable precision marketing.
More specifically, they wanted to know how their users are segmented, the characteristics and differences of each segment, and to classify each user for targeted marketing.
We used the parallel K-means   algorithm from our data mining toolset to cluster their user base and created the market segmentation graph in figure 12.3
Further analysis helped to characterize each segment according to the average bill and types of service used.
BC-PDM performed this analysis 3 times faster than their existing Unix solution.
In conclusion, China Mobile is a large mobile communication provider, and there’s tremendous and growing need to analyze large data sets.
Current commercial offerings are expensive and inadequate for analyzing our user data.
We built a data mining system called BC-PDM on top of MapReduce and HDFS and found this system to be accurate, fast, cheap, and scalable.
Going forward, we’ll improve BC-PDM’s efficiency as well as expand its scope by implementing more ETL operations and data mining algorithms.
Using a combination of human opinions and machine learning to immediately deliver relevant content, StumbleUpon presents only websites that have been suggested by other like-minded Stumblers.
Each time you click the Stumble button, you are  presented with a high-quality website based on the collective opinions of other like-minded web surfers.
When you “stumble,” you’ll only see pages that friends and like-minded.
The two exceptions were the Join and the Duplicate Removal operations.
They ran in roughly constant time irrespective of the cluster size.
One possible explanation for Hadoop running a job in constant time (independent of cluster size) is that the job is not evenly distributed and one task is the bottleneck to the job’s completion.
This will help you discover great content that is hard to find using a traditional search engine.
To collect and analyze this stumbling data, StumbleUpon requires its highly available back-end platform to collect, analyze, and transform millions of ratings per day.
With nearly 10 million users at present, StumbleUpon fairly quickly surpassed the abilities a traditional LAMP   (Linux, Apache, MySQL, PHP) stack afforded us, and we began to build a distributed platform for the following reasons:
Twenty Hadoop nodes may cost only as much as a single redundant database slave pair.
Freedom of development —Developers have fewer restrictions when compared to designing around a carefully architected, somewhat fragile RDBMS.
Operational concerns —Removing as many single-point-of-failure cases as possible is crucial to smooth operation of a world-class service.
Data processing speed —Many system-wide calculations were simply not possible to perform with a monolithic system.
The result can be used for the company’s marketing campaigns.
HBase   plays a critical part in StumbleUpon ’s distributed platform.
HBase is a distributed, column-oriented database that harnesses the power of the Hadoop and HDFS platform underneath it.
But, as with any complex system, there are trade-offs: HBase shelves traditional relational database concepts, such as joins, foreign key relations, and triggers in the pursuit of a system that hosts immensely large, sparsely populated data on commodity hardware in a scalable manner.
HBase is modeled after Google’s Bigtable,3 a distributed storage system.
Writes are buffered in memory, then flushed into read-only files after a while.
To keep the number of files low, they are merged in a compaction process that.
Special tablets or regions are used to track the locations of the data.
Due to the column-oriented nature of the datastore, sparse tables—those with a.
Table cells are stored with multiple versions instead of overwriting existing data.
Capacity (both storage size and processing speed) can be increased by simply.
Strong open source community Web-based UIs for management of both the master and region servers.
Thrift is a remote procedure call library originally developed at Facebook.
Figure 12.4 describes a simplistic version of the data write path in an HBase   region server.
As the MemStore grows beyond a threshold, it’s flushed to a new file on disk.
Please visit the project site for further information on obtaining, running, and enhancing HBase.5
StumbleUpon carefully selected HBase   from a host of candidate database and database-like storage and retrieval systems.
We value full consistency, where any query subsequent to a write operation is guaranteed to reflect that write.
In addition, StumbleUpon is committed to the open source model where we are free to contribute back to the community, and HBase’s strong development community both reflected that commitment and offered a valuable resource with which to drive improvements to the product.
Our first large test of HBase was in importing existing, legacy data from our MySQL -based systems.
In the past, we undertook this process only when absolutely necessary (such as migrating tables or hosts) and could take days or weeks to complete.
You can see one example of the column -store design pattern in the storage of arbitrary attributes for a user across multiple logical attribute groupings.
Contact data—Email and web addresses, instant messaging names, profile photo URLs.
In the traditional RDBMS   world, we may arbitrarily assign each group to a table.
User attributes may be retrieved and associated simultaneously with joins and foreign keys.
With careful design6 and a relatively moderate amount of data, such a system is flexible and maintainable.
Furthermore, this design suffers its most fatal flaws when the data volume scales past a moderate amount and the schema needs to be refactored.
The idea of doing an ALTER TABLE on a production database table containing millions or billions of rows as well as the headache of vetting systemic schema changes for both correctness and completeness is a daunting prospect.
Even with a perfect, static, concrete table, data analysis becomes bottlenecked by the selection, input, and output of records.
It’s a simple example where our typical user only has an ID and a record per Stumble:
Rarely achieved on the first attempt, since final schemas are rarely known fully a priori!
In this example, we look at a routine StumbleUpon task: counting stumbles per user as well as stumbles per URL.
Although this task is not particularly complex or insightful, we provide it here as a concrete example to the reader of a type of analytic task we perform on a daily basis.
The most interesting bit is that this trivial example completes in about 1 hour (using twenty commodity nodes) when processing a key count in the tens of billions.
The MySQL-based counterpart doesn’t complete in a reasonable amount of time—at least not without special handling and support to dump the data from MySQL , split the lines to a reasonable chunk size, and then combine the results.
You may find this series of operations familiar: mapping, then reducing! By using the generalized facilities of both HBase and Hadoop, we are able to conduct similar statistical surveys as needed, without special preparation and runtime handling.
To apply this straightforward example to the real world, we are now able to complete all analysis tasks in the same day they’re requested.
We can provide the ability to run ad hoc queries at a rate not thought possible before Hadoop and HBase were powering our platform.
As a business thrives and dies on the data it can analyze, this decreased turnaround time makes an incredible impact from the front office number crunching to the research engineers doing instant spam analysis on content submissions.
One can only imagine the difficulty of refactoring the custom-processing pipeline when the data schema is more complex than this trivial example, if we didn’t have our distributed processing platform to power the extraction, transformation, and analysis.
As we’ve outlined, one of the most important scalability features of HBase is the ability to (finally) transcend the write limits of a single machine.
Typically, scaling a database involves adding read slaves   and caching   to the system.
Read slaves can only help if your application is reading more than writing.
Caching only helps if your data set doesn’t change too often.
Even so, these architectural features frequently add vast application-side complexity.
HBase   hosts each region on any one of the machines in the cluster (each is a region server )
Writes touch the region server hosting that region, and the HBase region server writes to three (by default) HDFS data nodes.
With a large table and a similarly large cluster, writes are spread out to many different machines, inherently avoiding the single machine write problem that master-slave data stores have.
This feature can help you scale beyond traditional relational systems at a fraction of the cost.
As larger hardware tends to become expensive faster than the actual performance delivered, this is a fairly profound and important ability.
For the large work loads at StumbleUpon, the savings could literally be millions of dollars.
Some problems simply aren’t approachable on a single machine setup!
For highly dynamic data sets, where we frequently read things that were just written, caching in a system, such as memcached, may not help much.
This action could completely obviate the need for a caching layer.
One example of a highly dynamic data set is event counters.
This is a difficult problem because most high-speed solutions tend to be RAM-only for performance (e.g., memcached), while requiring high durability as well.
Enter HBase and its change, by both logging to disk and buffering up in the write buffer.
Reads can come directly from the write buffer, accelerating both and achieving high performance and durability.
StumbleUpon harnesses the natural ability of HBase to support event counters in just about every niche of the site—clicks, hits, ads served, and so on.
Furthermore, HBase offers a superior choice to typical sharding solutions.
Most traditional sharding approaches require a priori assumptions of the key space.
This can have surprising performance implications when the hashing function isn’t evenly distributed or when the keys are distributed outside the assumptions of your sharding scheme.
HBase   takes a data-sized approach to splitting tables   into regions; as the data in a region grows and reaches a configured size (currently defaulted to 256 MB), a midkey is picked from the middle of the data, splitting the region into two roughly equal chunks.
Each chunk becomes its own region and now has room to grow.
Repeating this procedure thousands of times gives a net result of a table with 2000 roughly equal-sized regions.
The ROOT table contains pointers to other META tables that hold user table.
As StumbleUpon’s data continues to grow in an uneven manner, we don’t end up with unbalanced shards or regions requiring manual intervention later, a problem frequently encountered by most manual sharding solutions involving RDBMS.
For all the talk about HBase’s advantages, initially we found that the performance of the system was not up to online data servicing.
To fix that, Ryan contributed back a large number of internal performance and reliability enhancements to the HBase project.
The previous format had inefficiencies in the index strategy, read paths, and internal APIs.
Index efficiency was sensitive to the size of the data.
The reader and writer are separated in code and there are no mutate or update methods available.
As most HFiles are hosted on HDFS, it would be impossible anyway because HDFS files are immutable as well.
The HFile writer has a straightforward write path, with four elements:
Any attempt to add keys in nonsorted order results in exceptions.
Close the file, finalize the index and write the trailers.
Useful for additional data, or features, such as Bloom filters.
As keys and values are appended to the HFile , the code keeps track of how large the current block is.
Once it exceeds the block size specified, it’s flushed and the compression system is reset for the next block.
As HFile writer appends a block, an in-memory index of the first key of each block is formed, along with its in-file offset.
When the close method is called, the block index is written immediately behind the last block.
Optional metadata blocks are appended next, followed by the metadata block index.
Finally, a trailer with pointers to the indexes is appended and the file is closed.
When a file is opened for reading, the data block index   and the meta block index are loaded.
They stay in resident for the duration, until the reader object is reclaimed.
The index allows for fast seek and reads of blocks of data.
To find a key in the file, first the reader does a binary search of the index.
Finding a block number, it reads in and decompresses the data block and stores it in the block cache.
Code then iterates through the block in memory finding the key, or the closest match.
Pointers are then returned, allowing clients a view into the single copy of data.
HFile gains its strengths from simplicity in both concept and implementation.
The implementation is one file (tests excepted) and is about 1,600 lines for both reader and writer.
HFile   provided a new internal platform to rewrite the rest of the region server.
The internal algorithms for read-merging multiple files into a single-scan result had grown organically over time and needed a fresh look.
Jon Gray   and Erik Holstad   at Streamy.com designed and implemented a brand-new read implementation by adding new delete semantics and restructuring the internal key formats.
By using more efficient algorithms and redoing the implementation on the 0-copy HFile, more speed enhancements were gained from the code.
On the low end, scanning a series of rows got a 30 times speed-up.
On the high end, single row gets can be up to 100 times faster.
HBase demonstrates excellent parallel speed-up on read and write workloads.
As StumbleUpon has stored so much data in MySQL, insert performance is important.
To copy data into HBase, Hadoop jobs with only mappers that read from MySQL and subsequently write into HBase were written.
As impressive as the write performance is, the read performance is exceptional.
At this rate, reading our largest tables takes less than an hour.
The ability to write entire table analytics is a powerful ability that previously didn’t exist.
All the machines involved were dual quad core Intels, with 16 GB of RAM.
Each node had two SATA disks , each 1 TB in size.
These relatively modest and standard nodes provide an excellent level of performance, and the cluster only performs better with more.
Hadoop excels in this traditionally strong area for distributed processing: log-and-click collection combined with analysis.
StumbleUpon harnesses this natural aptitude of Hadoop for a variety of analysis tasks, including Apache   log file collection and user-session analysis.
Now imagine needing to do this across a fleet of web server frontends, millions of users, and billions of clicks.
Scribe, 8 a Facebook   project made public, is a platform for aggregating real-time streamed log data in such a context.
The service is failure tolerant at both the machine and network level and easily integrates into just about any infrastructure.
StumbleUpon uses Scribe to collect data directly into HDFS where it’s reviewed and processed by a number of systems.
A combination of Cascading   and plain MapReducebased analysis jobs extract data from the logs for vanilla statistics (such as click counts), while more sophisticated consumers feed data into real-time feedback systems based around BerkeleyDB   and TokyoCabinet.
A second set of systems use this streamed data for search index updates and thumbnail generation.
Figure 12.6 StumbleUpon   data collection, analysis, and storage using Hadoop.
We wrote a naive single-node Perl   hash-based program as an example of a typical quick solution a sysadmin may create.
The results shown in table 12.3 confirm that our results easily achieve linear (or better) speed-up with the simple addition of more nodes to the cluster.
Times are the average of 10 mixed executions, to allow for variances.
We see that even the single-node Cascading solution achieves double the throughput of the naive Perl application due to the intelligent segmentation and bucketing built in to the MapReduce framework versus the effect of keeping all data mapped to a single Perl hash.
Given familiarity with Cascading, you may also consider the Perl code more complex to optimize (and maintain) to boot!
To wit, StumbleUpon uses the native map and reduce functionality in Hadoop and related products, including Nutch   and custom-written content surveyors, to perform this data retrieval, analysis, and storage.
Keeping the resultant data close to the processing pipeline maximizes our data locality benefits.
Putting it all together, StumbleUpon has taken the maximum advantage of the vast power the MapReduce paradigm unlocks by adopting and extending Hadoop, HDFS, and HBase.
We’re excited to help lead the future of distributed processing.
In contrast with the radical advances in web search over the last several years, search over enterprise intranets has remained a difficult and largely unsolved problem.
Several enterprise-specific factors complicate the task of finding the “correct” answers for these queries:
Absence of an economic incentive for content creators to make their pages easily discoverable (in contrast with the presence of such incentives on the web)
The use of enterprise-specific vocabulary, abbreviations, and acronyms, in the search queries and in the intranet pages.
From earlier efforts at IBM [4], we learned that these problems are difficult to overcome using traditional information retrieval techniques.
Subsequently, in [5], we proposed an approach consisting of detailed offline analyses to pre-identify navigational   pages and the use of a special-purpose navigational   index.
We demonstrated the viability of this approach through experiments over a 5.5-million-page corpus from the IBM intranet.
The system in [5], uses a mix of proprietary platforms and relational databases.
In principle, the Nutch crawler, the Hadoop MapReduce framework, and the Lucene indexing engine provide a full suite of software components for building a complete search engine.
But, to truly address the challenges described earlier, it’s not sufficient to merely stitch these systems together.
We describe how to use sophisticated analysis and mining of the crawled pages, and special-purpose navigational indexes in conjunction with intelligent query processing to ensure effective search quality.
To understand how these elements come together, we now examine some illustrative search queries and their corresponding results on ES2
The IDP is an acronym for Individual Development Plan, a web-based HR application in IBM to assist in tracking employee career development.
The first two results returned by ES2 represent two different URLs that both allow the user to launch the IDP web application.
The first result in figure 12.7 doesn’t have the word idp in the title and indeed not even in the content of the page.
In ES2, we use several hundred such carefully crafted patterns applied to the URL, title, META headers, and various other features of a web page to detect and associate index terms with navigational pages.
Section 12.4.3 describes how we execute such analysis, known as Local Analysis , in parallel on a Hadoop cluster.
To narrow down the result to the two specific URLs shown in figure 12.7, we use a complex set of analysis algorithms as part of a.
To accomplish this match, the following two steps took place during offline analysis: (1) the phrase Individual Development Plan was extracted as part of local analysis using patterns applied to the title and (2) during indexing, the extracted phrase was recognized to be the expansion of the acronym idp and resulted in the term idp being added to the index as well.
In general, we employ a process known as variant generation  whereby multiple variants of the terms extracted through local analysis are generated and added to the index.
In the interest of space we don’t describe these algorithms in detail.
Finally, to enable results to be customized based on the search users’ geography and to support the type of result grouping shown in figure 12.7, we label each page in the intranet with a specific geography (country, region, and/or IBM location)
In ES2, this labeling is accomplished using a rule-driven classifier that uses a number of page features extracted during local analysis.
Each page in the ES2 collection is pushed through multiple logical workflows, each consisting of a local analysis phase, a global analysis phase, and an appropriate variant generation strategy.
The output of a workflow is some subset of the input pages along with a set of index terms.
Depending on the particular extraction patterns and variant generation rules, the output of two different workflows will have correspondingly different “precision” characteristics.
For example, a workflow consisting of careful extraction of a person’s name from the title followed by name-specific variant generation is likely to yield much higher-quality answers than a workflow that only generates all possible n-grams of the title of a page.
The creation of an index structure consisting of the output of multiple workflows with different precision characteristics is only half the story.
To fully leverage such an index, ES2 employs a sophisticated runtime query processing strategy.
A discussion of the runtime component of ES2 is beyond the scope of this case study.
We restrict our attention to the offline analysis workflows and their implementation on Hadoop.
We assume that readers are broadly familiar with Hadoop and Nutch.11 Nutch is an open source crawler implemented on the Hadoop MapReduce platform for web crawling.
Jaql provides a Unix pipes-like syntax to connect multiple stages of processing over semistructured JSON data.
The ES2 workflow involves invoking multiple algorithms for local analysis, global analysis, and variant generation before inserting data into indexes.
Without adequate data management support, this complex multistage workflow quickly becomes overwhelming.
There are six components in ES2: the crawler, local analysis, global analysis, variant generation and indexing, background mining, and search runtime.
In addition, ES2 also gathers information from IBM’s social bookmarking service (called Dogear )
Much like delicious.com, Dogear contains various URLs that have been bookmarked by the IBM community along with a collection of tags associated with each URL.
The tags associated with the URLs contain valuable clues about the page, and ES2 uses this information in building its indexes.
All the stages use a common distributed filesystem, HDFS, for both input and output.
Local analysis processes each page to extract features about the pages and stores the results as JSON objects in HDFS.
ES2 uses Jaql to push each page through the rest of the pipeline, transforming the data as needed at each.
Local Analysis (Low - level analytics on individual page features.
Global Analysis (Analytics on groups of pages produced by Local analysis)
Jaql queries are used to bring together the results of different local analyses and invoke global analysis.
Jaql is also used to invoke the variant generation and indexing workflow using the outputs of local and global analyses.
The indexes are periodically copied to a different set of machines that serve user queries.
Although not part of the main workflow, ES2 periodically executes several mining and classification tasks.
Examples of this include algorithms to automatically produce acronym libraries, regular expression libraries [6], and geo-classification rules.
A primary data structure in Nutch is the CrawlDB : a keyvalue set where the keys are the URLs known to Nutch and the value is the status of the URL.
The status contains metadata about the URL, such as the time of discovery, whether it has been fetched, and so on.
Nutch   is architected as a sequence of three MapReduce jobs:
Generate—In this phase, a fetch list is generated by scanning the input key/value pairs (from CrawlDB) for URLs that have been discovered, but not fetched.
A common choice in generating this list is to select the top k unfetched URLs using an appropriate scoring mechanism (k is a configuration parameter in Nutch)
Fetch—In this phase, the pages associated with the URLs in the input fetch list   are fetched and parsed.
The output consists of the URL and the parsed representation of the page.
Update—The update phase collects all the URLs that have been discovered by parsing the contents of the pages in the fetch phase and merges them with the CrawlDB.
The pages fetched in each cycle of generate-fetch-update are referred to as a segment.
Out of the box, the first problem we encountered was crawl speed.
A deeper problem we encountered after a sample crawl of 80 million pages was that the quality of discovered pages was surprisingly low.
In this section, we identify the underlying reasons for both these problems and describe the enhancements made to Nutch to adapt it to the IBM intranet.
When using it to crawl the IBM intranet, we observed multiple performance bottlenecks.
We discovered that the reason for the bottlenecks was that the enterprise intranet contains far fewer hosts than the web, and some of the design choices made in Nutch assume a large number of distinct hosts.
We describe two ways in which this problem manifests itself, and the approach used in ES2 to adapt Nutch’s design for the enterprise.
A major performance bottleneck in the fetch   phase, called long tail problem , exhibits the following behavior.
The crawl rate in the early part of the fetch phase is relatively high (typically dozens of pages a second)
But this deteriorates relatively quickly to less than a page per second, where it remains until completion of the segment.
You can understand this by observing that the fetch rate in Nutch is controlled by two parameters: the number of distinct hosts in the fetch list that Nutch can concurrently crawl from, and the duration for which Nutch waits before making consecutive requests to the same host.
A straightforward solution to the long tail problem is to restrict the number of URLs for a particular host in the fetch list.
Unfortunately, this is not sufficient because not all host servers are identical, and the time required to fetch the same number of pages from different hosts can be dramatically different.
We added a time-shutoff parameter that terminates the fetcher after a fixed amount of time as an engineering fix to this problem.
While this terminates the fetch phase early (and fewer pages are retrieved in total in the segment), by avoiding the slow tail phase, we sustain a higher average crawl rate.
In practice, we observed that by appropriately setting this shutoff parameter, the average crawl rate could be improved to nearly three times the original crawl rate.
Ideally, the current fetch rate should determine such a shutoff; this unfortunately requires pooling information across map tasks and can’t easily be performed in Hadoop today.
A main-memory data structure in the fetcher causes a different performance bottleneck.
The fetcher   works by first creating a set of queues where each queue stores URLs for a particular host—we call this data structure FetchQueues.
A fixed amount of memory is allocated to FetchQueues to be shared across the individual queues.
The fetcher reads the URLs to be fetched from its input and inserts them into FetchQueues until it exhausts the allocated memory.
Worker threads assigned to each queue in FetchQueues concurrently fetch pages from different hosts as long as their queues are non-empty.
The bottleneck arises because URLs in the input are ordered by host (this is an artifact of the generate phase) and the fetcher exhausts the memory allocated to FetchQueues with URLs from very few hosts.
Such a design is appropriate for crawling a large number of hosts on the web as each host in the fetch list would then have only a few URLs.
In the enterprise, host diversity is limited to a few thousand at best.
As a result, few worker threads are actively fetching from FetchQueues, leading to severe under-utilization of resources.
We address this problem by replacing FetchQueues with a disk-based data structure without any limits on the total size.
This allows the fetcher to populate FetchQueues with all the URLs in the input, thereby keeping the maximum possible number of worker threads active.
Much of the complexity and power in ES2 lies in its analytics.
In this section, we briefly describe the different algorithms, paying special attention to the design choices made in mapping these algorithms onto Hadoop.
In local analysis, each page is individually analyzed to extract clues that help decide whether that page is a candidate navigational page.
These algorithms use rules based on regular expression patterns, dictionaries, and information extraction tools [7] to identify candidate navigational pages.
Chaitin’s Home” indicates that this is the home page of G.
The next section describes the impact of redirections on local analysis and discusses a solution.
Many sites in IBM’s intranet employ redirection for updating, load balancing, upgrading, and handling internal reorganizations.
Unfortunately, redirections can cause complications in the local analysis algorithms.
For instance, URLHomePage uses the text of the URL to detect a candidate navigational page.
After redirection, the target URL may not contain the same features as the original URL.
Local analysis algorithms can correctly identify this URL as the home page for the Employee Referral Bonus Program (ERBP)   using clues from the URL.
The clues in the source URL are no longer available in the target, and the local analysis algorithm can no longer identify this page as navigational.
To prevent this, ES2 resolves all redirections, collects the set of URLs that lead to the target page through redirections, and provides local analysis with the appropriate URLs.
To track redirections, we modified Nutch to tag every page that was a target of redirection with the source URL.
The crawler follows redirections from a page A to page B, and from page B to arrive at page C.
We track these redirections by tagging pages B and C with the source URL, A.
This tag is stored as a metadata field in the segment file.
Listing 12.2 (called ResolveSimple ) outlines the map and reduce functions that are used to resolve redirections on a segment and invoke local analysis.
The map phase outputs the source URL and the page contents.
The reduce phase brings all the pages with the same source URL into a single group.
In the preceding example of figure 12.9, the common source URL for pages A, B, and C is A.
The target page in this group (C) is then passed to local analysis along with the other URLs in the group—A and B.
This requires Hadoop to pass along the contents of each page from the map phase to the reduce phase.
This involves sorting and moving a large amount of data across the network.
To avoid this, we modify ResolveSimple (listing 12.2) and separate the task of redirection resolution and the local analysis so that the algorithms in local analysis are run in the map phase.
This allows the local analysis computation to be colocated with the data, and therefore results in significant performance improvement.
In the map phase of this algorithm, we only pass the metadata along and the page content (which accounts for a majority of the data volume) is projected out.
In the reduce phase of ResolveSimple, we output a table with two columns: the first column is the URL of the target page in the group of pages, and the second column is the set of URLs to be associated with the page when it’s submitted to local analysis.
If a URL results in a redirection, we don’t add an entry for it in this table.
In a subsequent map-only job for local analysis, the map tasks read the redirection table into memory.
This table is fairly small for typical segments and easily fits in memory.
For each URL in the input segment, the mapper looks up the table if it finds a non-empty entry.
By invoking local analysis in the map phase, Resolve2Step avoids the transfer of the page contents over the network to the reducers as in ResolveSimple.
We executed both algorithms for local analysis on a segment of around 400,000 pages on the cluster using eight nodes.
The speedup graph shows that for the early part of the curve, we get linear scaling; the benefits of adding more nodes decreases after this point.
This is because the input consists of only a single segment of 400,000 pages.
Hadoop is unable to efficiently divide this task at a finer granularity.
We’ll see in the next section that with larger input data sets, Hadoop can efficiently divide the task and provide linear scaling.
The local analysis tasks  described in the previous section identify candidate navigational pages by extracting relevant features from each page.
But as described in section 12.4.1, the same navigational feature can be associated with multiple pages.
Consider the case where homepage authors use the same title for many of their web pages.
Chaitin home page” is the title for many pages on G.
Local analysis for personal home pages considers all such pages to be candidates.
ES2 uses global analysis to determine an appropriate subset of pages.
We briefly review these algorithms and describe how Jaql is used to implement these algorithms on large data sets.
Each global analysis task takes as input a set of pages and the associated features discovered during local analysis.
Listing 12.4 Example JSON output from global analysis and Jaql query.
Site root analysis —Both algorithms are used to group candidate pages and identify a set of representative pages.
Given a collection of candidate pages, it’s first partitioned by the feature of interest, for example, PersonalHomePage.
For each group, a forest of pages is constructed where each URL is a node in the forest, relating the two URLS A and B as parent and child if A is the longest prefix of B.
Shorter prefixes are higher ancestors.) The forest is pruned using some complex logic that may involve inputs from other local analysis algorithms, the details of which are beyond the scope of this case study.
Anchor text analysis —This algorithm collects all the anchor text for each page by examining all the pages that point to it.
The aggregated anchor text is processed to pick a set of representative terms for that URL.
In global analysis, first, a merge step joins together the results of local analysis on the main crawl and the tags for the URLs collected from Dogear.
This is followed by a deduplication step where duplicate pages are eliminated.
Each global analysis task then involves some standard data manipulation (e.g., partitioning, filtering, joining) in conjunction with some task-specific user-defined function, such as URL forest generation and pruning.
Jaql is used to specify these tasks at a high level, and execute them in parallel using Hadoop.
Consider the Jaql query in listing 12.4 used for the global analysis on PersonalHomePage data.
The first two lines specify the input and output files.
The third line is the start of a Jaql pipe: pages flow from the input file, referred to by $allDocs , to subsequent operators.
Following the input, the “filter” operator produces a value when its predicate evaluates to true.
In the example, only pages that have a local analysis (LA) field, a PersonalHomePage field, and a non-null name are output to the next operator.
The $ is a variable that refers to the current value in the pipe.
Finally, the annotated pages are written to $results  output file.
Jaql evaluates the query shown in the preceding listing 12.4 by translating to a MapReduce job and submitting the job to Hadoop for evaluation.
In this example, the map stage filters pages and extracts the partitioning key.
The reduce stage evaluates the SiteRootAnalysis function per partition and writes the output to a file.
Jaql automatically translates a collection of pipe definitions into a directed acyclic graph of MapReduce jobs.
Figure 12.11 shows detailed elapsed times for each stage involved after local analysis through the end of global analysis.
Figure 12.12 shows that as servers were added to the cluster, the total time to evaluate merge, dedup, and global analysis improved proportionally.
ES2 builds acronym libraries , regular expression patterns , and geo-classification rules   automatically using the crawled data in background mining tasks.
Periodically, the local analysis is rerun on all the pages after updating these resources.
As an example, we provide a brief description of the acronym mining algorithm and the geo-classification algorithm used in ES2 below.
Acronym mining is a computationally intensive task that benefits from a parallel implementation on Hadoop.
The reduce function gathers all the possible longForms together for a given shortForm and ranks them by frequency before producing the output.
Reduce (Key: shortForm, Values: longForms) Canonicalize longForms that differ slightly Compute frequency of each longForm Output longForms in sorted order End.
As can be seen, the overall task completes in less than 25 minutes even with two nodes.
But we see that this task doesn’t scale linearly with the size of the cluster.
We suspect that this is because the input data is fragmented over several segments, and Hadoop chooses to split this job into a large number of tasks in the map phase which imposes a large, fixed overhead independent of the cluster size.
The goal of the geo-classification   task is to label each page on the intranet with the country, region, and/or IBM location for which the page is most relevant.
For instance, many new business processes and web applications within IBM are first deployed in the U.S.
Site administrators responsible for developing the content for the subsequent rounds of deployment often use the U.S.
But when performing this customization, quite often the administrators don’t edit the corresponding HTML meta headers that.
In ES2, we’re currently employing a complex rule-driven classifier consisting of a robust set of manually created rules over a small set of page features (e.g., presence of a country name in the title, a country code in the URL, etc.)
Improvement in recall requires the use of significantly more features from a page than is used by our current classifier.
But manually developing accurate rule sets over these larger feature sets is extremely laborious.
We’re now in the process of developing a scalable mining algorithm   to automatically “induce” additional classification rules over these new features, given the high-quality rule set already available today.
The use of a platform like Hadoop is critical to scale our mining algorithms to millions of pages, each with several hundred features.
We described the architecture of ES2—a scalable enterprise search system developed at IBM using open source components, such as Nutch, Hadoop, Lucene, and Jaql.
We also outlined the changes we needed to make to Nutch for the purposes of crawling the enterprise.
We mapped the local and global analysis algorithms from [5] on to Hadoop.
In implementing a complex workflow involving crawling, local analysis, global analysis, and indexing, we found JSON to be a convenient data format and Jaql to be an extremely powerful tool.
In summary, we believe that Hadoop, Nutch, Lucene, and Jaql constitute a powerful set of tools with which sophisticated, scalable systems like ES2 can be built.
Figure 12.13 Acronym mining has a speed-up linear in cluster size.
Note that the meta headers are intended for consumption by browsers and crawlers and not visible when the page is rendered.
You can see the command usage in the following convention.
The user must be the files’ owner or a superuser.
See section 8.3 for more background information on the HDFS file permission system.
The user must be the files’ owner or a superuser.
See section 8.3 for more background information on the HDFS file permission system.
See section 8.3 for more background information on the HDFS file permission system.
If PATH is a directory, the size of each file in the directory.
Note that although du stands for disk usage, it should not be taken literally, as disk usage depends on block size and replica factors.
If the trash feature is enabled, when a file is deleted, it.
The file will be permanently deleted from the .Trash/ folder only after a user-configurable delay.
The expunge command forcefully deletes all files from the .Trash/ folder.
Note that as long as a file is in the .Trash/ folder, it can be restored by moving it back to its original location.
If multiple source files are specified, local destination has to be a directory.
If LOCALDST is -, the files are copied to stdout.
The checksums for a file are stored separately in a hidden file.
When a file is read from HDFS, the checksums in that hidden file are used to verify the file’s integrity.
For the get command, the -crc option will copy that hidden checksum file.
The -ignorecrc option will skip the checksum checking when copying.
The option addnl will add a newline character to the end of each file.
The replication factor will take some time to get to the target.
The -w option will wait for the replication factor to match the target.
Files in known compressed format (gzip and Hadoop’s binary sequence file format) are uncompressed first.
Apache Hadoop is a NoSQL applications framework that runs on distributed clusters.
If you need analytic information from your data, Hadoop’s the way to go.
Hadoop in Action introduces the subject and teaches you how to write programs in the MapReduce style.
It starts with a few easy examples and then moves quickly to show Hadoop use in more complex data analysis tasks.
Included are best practices and design patterns of MapReduce programming.
Knowing basic statistical concepts can help with the more advanced examples.
For online access to the author and a free ebook for owners of this book, go to manning.com/HadoopinAction.
Front Cover brief contents contents preface acknowledgments about this book Roadmap Code conventions and downloads Author Online About the author About the cover illustration.
