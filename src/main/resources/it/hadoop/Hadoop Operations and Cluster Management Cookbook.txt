Over 60 recipes showing you how to design, configure, manage, monitor, and tune a Hadoop cluster.
No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
However, the information contained in this book is sold without warranty, either express or implied.
Neither the author, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
He is enthusiastic about open source technologies and has been working as a System Administrator, Programmer, and Researcher at State Street Corp.
I would like to sincerely thank my wife, Min Han, for her support both technically and mentally.
This book would not have been possible without encouragement from her.
Hector Cuesta-Arvizu provides consulting services for software engineering and data analysis with over eight years of experience in a variety of industries, including financial services, social networking, e-learning, and Human Resources.
He has published Robotics and Raspberry Pi in his spare time.
He has been designing software for many years and Hadoop-based systems since 2008
He is the President of SHMsoft, a provider of Hadoop applications for various verticals, and a co-author of the book/project Hadoop Illuminated.
I would like to acknowledge the help of my colleagues, in particular Sujee Maniyam, and last but not least, my multitalented family.
Harvinder Singh Saluja has over 20 years of software architecture and development experience, and is the co-founder of MindTelligent, Inc.
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book library.
Here, you can access, read and search across Packt's entire library of books.
Why Subscribe? f Fully searchable across every book published by Packt f Copy and paste, print and bookmark content f On demand and accessible via web browser.
Preface Today, many organizations are facing the Big Data problem.
Managing and processing Big Data can incur a lot of challenges for traditional data processing platforms such as relational database systems.
Hadoop was designed to be a distributed and scalable system for dealing with Big Data problems.
A Hadoop-based Big Data platform uses Hadoop as the data storage and processing engine.
It deals with the problem by transforming the Big Data input into expected output.
Hadoop Operations and Cluster Management Cookbook provides examples and step-by-step recipes for you to administrate a Hadoop cluster.
It covers a wide range of topics for designing, configuring, managing, and monitoring a Hadoop cluster.
The goal of this book is to help you manage a Hadoop cluster more efficiently and in a more systematic way.
In the first three chapters, you will learn practical recipes to configure a fully distributed Hadoop cluster.
The subsequent management, hardening, and performance tuning chapters will cover the core topics of this book.
In these chapters, you will learn practical commands and best practices to manage a Hadoop cluster.
The last important topic of the book is the monitoring of a Hadoop cluster.
And, we will end this book by introducing steps to build a Hadoop cluster using the AWS cloud.
What this book covers Chapter 1, Big Data and Hadoop, introduces steps to define a Big Data problem and outlines steps to build a Hadoop-based Big Data platform.
Chapter 2, Preparing for Hadoop Installation, describes the preparation of a Hadoop cluster configuration.
Topics include choosing the proper cluster hardware, configuring the network, and installing the Linux operating system.
Chapter 3, Configuring a Hadoop Cluster, introduces recipes to configure a Hadoop cluster in pseudo-distributed mode as well as in fully distributed mode.
We will also describe steps to verify and troubleshoot a Hadoop cluster configuration.
Chapter 4, Managing a Hadoop Cluster, shows you how to manage a Hadoop cluster.
We will learn cluster maintenance tasks and practical steps to do the management.
For example, we will introduce the management of an HDFS filesystem, management of MapReduce jobs, queues and quota, and so on.
Chapter 5, Hardening a Hadoop Cluster, introduces recipes to secure a Hadoop cluster.
We will show you how to configure ACL for authorization and Kerberos for authentication, configure NameNode HA, recover from a failed NameNode, and so on.
Chapter 6, Monitoring a Hadoop Cluster, explains how to monitor a Hadoop cluster with various tools, such as Ganglia and Nagios.
Chapter 7, Tuning a Hadoop Cluster for Best Performance, introduces best practices to tune the performance of a Hadoop cluster.
We will tune the memory profile, the MapReduce scheduling strategy, and so on to achieve best performance for a Hadoop cluster.
We will explain steps to register, connect, and start VM instances on EC2
We will also show you how to configure a customized AMI for a Hadoop cluster on EC2
What you need for this book This book is written to be as self-contained as possible.
Each chapter and recipe has its specific prerequisites introduced before the topic.
In general, in this book, we will use the following software packages:
Who this book is for This book is for Hadoop administrators and Big Data architects.
You are not required to have solid knowledge about Hadoop to read this book, but you are required to know basic Linux commands and have a general understanding of distributed computing concepts.
Conventions In this book, you will find a number of styles of text that distinguish between different kinds of information.
Here are some examples of these styles, and an explanation of their meaning.
When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:
Words that you see on the screen, in menus or dialog boxes for example, appear in the text like this: "By clicking on the link Analyze This Job, we will go to a web page."
Warnings or important notes appear in a box like this.
Let us know what you think about this book—what you liked or may have disliked.
Reader feedback is important for us to develop titles that you really get the most out of.
Customer support Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.
Errata Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
By doing so, you can save other readers from frustration and help us improve subsequent versions of this book.
If you find any errata, please report them by visiting http://www.packtpub.com/submit-errata, selecting your book, clicking on the errata submission form link, and entering the details of your errata.
Once your errata are verified, your submission will be accepted and the errata will be uploaded on our website, or added to any list of existing errata, under the Errata section of that title.
Any existing errata can be viewed by selecting your title from http://www.packtpub.com/support.
Piracy Piracy of copyright material on the Internet is an ongoing problem across all media.
At Packt, we take the protection of our copyright and licenses very seriously.
If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Introduction Today, many organizations are facing the Big Data problem.
Managing and processing Big Data can incur a lot of challenges for traditional data processing platforms such as relational database systems.
Hadoop was designed to be a distributed and scalable system for dealing with Big Data problems.
The design, implementation, and deployment of a Big Data platform require a clear definition of the Big Data problem by system architects and administrators.
A Hadoop-based Big Data platform uses Hadoop as the data storage and processing engine.
It deals with the problem by transforming the Big Data input into the expected output.
On one hand, the Big Data problem determines how the Big Data platform should be designed, for example, which modules or subsystems should be integrated into the platform and so on.
On the other hand, the architectural design of the platform can determine complexity and efficiency of the platform.
A Hadoop-based Big Data platform is capable of dealing with most of the Big Data problems, but might not be good fit for others.
Because of these and many other reasons, we need to choose from Hadoop alternatives.
Defining a Big Data problem Generally, the definition of Big Data is data in large sizes that go beyond the ability of commonly used software tools to collect, manage, and process within a tolerable elapsed time.
More formally, the definition of Big Data should go beyond the size of the data to include other properties.
In this recipe, we will outline the properties that define Big Data in a formal way.
Getting ready Ideally, data has the following three important properties: volume, velocity, and variety.
In this book, we treat the value property of Big Data as the fourth important property.
And, the value property also explains the reason why the Big Data problem exists.
How to do it… Defining a Big Data problem involves the following steps:
The volume should not only include the current data volume, for example in gigabytes or terabytes, but also should include the expected volume in the future.
There are two types of data in the real world: static and nonstatic data.
The volume of static data, for example national census data and human genomic data, will not change over time.
While for nonstatic data, such as streaming log data and social network streaming data, the volume increases over time.
The velocity estimate should include how much data can be generated within a certain amount of time, for example during a day.
The velocity property of Big Data defines the speed that data can be generated.
This property will not only affect the volume of data, but also determines how fast a data processing system should handle the data.
In other words, the data variety means the different sources of data, such as web click data, social network data, data in relational databases, and so on.
The difference requires specifically designed modules for each data variety to be integrated into the Big Data platform.
For example, a web crawler is needed for getting data from the Web, and a data translation module is needed to transfer data from relational databases to a nonrelational Big Data platform.
The value property of Big Data defines what we can potentially derive from and how we can use Big Data.
For example, frequent item sets can be mined from online clickthrough data for better marketing and more efficient deployment of advertisements.
For a Big Data problem, the volume, velocity, and variety properties together define the input of the system, and the value property defines the output.
In the past few years, Hadoop has become a widely used platform and runtime environment for the deployment of Big Data applications.
In this recipe, we will outline steps to build a Hadoop-based Big Data platform.
Getting ready Hadoop was designed to be parallel and resilient.
It redefines the way that data is managed and processed by leveraging the power of computing resources composed of commodity hardware.
How to do it… Use the following steps to build a Hadoop-based Big Data platform:
The subsystems should transfer data from different data sources to Hadoop-compatible data storage systems such as HDFS and HBase.
The subsystems need to be designed based on the input properties of a Big Data problem, including volume, velocity, and variety.
The platform should consume the Big Data located on HDFS or HBase and produce the expected and valuable output.
The delivery subsystems should transform the analytical results from a Hadoop-compatible format to a proper format for end users.
For example, we can design web applications to visualize the analytical results using charts, graphs, or other types of dynamic web applications.
How it works… The architecture of a Hadoop-based Big Data system can be described with the following chart:
Although Hadoop borrows its idea from Google's MapReduce, it is more than MapReduce.
A typical Hadoop-based Big Data platform includes the Hadoop Distributed File System (HDFS), the parallel computing framework (MapReduce), common utilities, a column-oriented data storage table (HBase), high-level data management systems (Pig and Hive), a Big Data analytics library (Mahout), a distributed coordination system (ZooKeeper), a workflow management module (Oozie), data transfer modules such as Sqoop, data aggregation modules such as Flume, and data serialization modules such as Avro.
It was designed as a distributed filesystem that provides high-throughput access to application data.
The data blocks are replicated on several computing nodes and their checksums are computed.
In case of a checksum error or system failure, erroneous or lost data blocks can be recovered from backup blocks located on other nodes.
MapReduce provides a programming model that transforms complex computations into computations over a set of key-value pairs.
It coordinates the processing of tasks on a cluster of nodes by scheduling jobs, monitoring activity, and re-executing failed tasks.
In a typical MapReduce job, multiple map tasks on slave nodes are executed in parallel, generating results buffered on local machines.
Once some or all of the map tasks have finished, the shuffle process begins, which aggregates the map task outputs by sorting and combining key-value pairs based on keys.
Then, the shuffled data partitions are copied to reducer machine(s), most commonly, over the network.
Then, reduce tasks will run on the shuffled data and generate final (or intermediate, if multiple consecutive MapReduce jobs are pipelined) results.
When a job finishes, final results will reside in multiple files, depending on the number of reducers used in the job.
The anatomy of the job flow can be described in the following chart:
A NameNode keeps track of the filesystem metadata such as the locations of data blocks.
For efficiency reasons, the metadata is kept in the main memory of a master machine.
A DataNode holds physical data blocks and communicates with clients for data reading and writing.
In addition, it periodically reports a list of its hosting blocks to the NameNode in the cluster for verification and validation purposes.
The MapReduce framework has two types of nodes, master node and slave node.
JobTracker is the daemon on a master node, and TaskTracker is the daemon on a slave node.
The master node is the manager node of MapReduce jobs.
It splits a job into smaller tasks, which will be assigned by the JobTracker to TaskTrackers on slave nodes to run.
When a slave node receives a task, its TaskTracker will fork a Java process to run the task.
Meanwhile, the TaskTracker is also responsible for tracking and reporting the progress of individual tasks.
Hadoop common Hadoop common is a collection of components and interfaces for the foundation of Hadoop-based Big Data platforms.
Apache HBase Apache HBase is an open source, distributed, versioned, and column-oriented data store.
It can scale to host very large tables, containing billions of rows and millions of columns.
Apache Mahout Apache Mahout is an open source scalable machine learning library based on Hadoop.
It has a very active community and is still under development.
Currently, the library supports four use cases: recommendation mining, clustering, classification, and frequent item set mining.
It supports Big Data by compiling the Pig statements into a sequence of MapReduce jobs.
Pig uses Pig Latin as the programming language, which is extensible and easy to use.
Apache Hive Apache Hive is a high-level system for the management and analysis of Big Data stored in Hadoop-based systems.
Similar to Apache Pig, the Hive runtime engine translates HiveQL statements into a sequence of MapReduce jobs for execution.
Apache ZooKeeper Apache ZooKeeper is a centralized coordination service for large scale distributed systems.
It maintains the configuration and naming information and provides distributed synchronization and group services for applications in distributed systems.
Apache Oozie Apache Oozie is a scalable workflow management and coordination service for Hadoop jobs.
It is data aware and coordinates jobs based on their dependencies.
In addition, Oozie has been integrated with Hadoop and can support all types of Hadoop jobs.
Apache Sqoop Apache Sqoop is a tool for moving data between Apache Hadoop and structured data stores such as relational databases.
It provides command-line suites to transfer data from relational database to HDFS and vice versa.
More information about Apache Sqoop can be found at http://sqoop.apache.org.
Apache Flume Apache Flume is a tool for collecting log data in distributed systems.
It has a flexible yet robust and fault tolerant architecture that streams data from log servers to Hadoop.
Apache Avro Apache Avro is a fast, feature rich data serialization system for Hadoop.
The serialized data is coupled with the data schema, which facilitates its processing with different programming languages.
More information about Apache Avro can be found at http://avro.apache.org.
Choosing from Hadoop alternatives Although Hadoop has been very successful for most of the Big Data problems, it is not an optimal choice in many situations.
In this recipe, we will introduce a few Hadoop alternatives.
Getting ready Hadoop has the following drawbacks as a Big Data platform:
Because of the preceding drawbacks as well as other reasons, such as special data processing requirements, we need to make an alternative choice.
Hadoop is not a good choice for data that is not categorized as Big Data; for example, data that has the following properties: small datasets and datasets with processing that requires transaction and synchronization.
How to do it… We can choose Hadoop alternatives using the following guidelines:
Choose Enterprise Hadoop if there is no qualified Hadoop administrator and there is sufficient budget for deploying a Big Data platform.
Choose Spark or Storm if an application requires real-time data processing.
Choose GraphLab if an application requires handling of large graph datasets.
How it works… Enterprise Hadoop refers to Hadoop distributions by some Hadoop-oriented companies.
Compared with the community Hadoop releases, Enterprise Hadoop distributions are enterprise ready, easy to configure, and sometimes new features are added.
In addition, the training and support services provided by these companies make it much easier for organizations to adopt the Hadoop Big Data platform.
It is also one of the biggest contributors of the Hadoop codebase.
Hadapt unifies SQL and Hadoop and makes it easy to handle different varieties of data.
It can be up to 40 times faster than Hadoop.
So it is ideal for iterative and responsive Big Data applications.
Besides, Spark can be integrated with Hadoop, and the Hadoop-compatible storage APIs enable it to access any Hadoop-supported systems.
The MapReduce framework parallels computation by splitting data into a number of distributed nodes.
Some large natural graph data, such as social network data, has the problem of being hard to partition and thus, hard to split for Hadoop parallel processing.
The performance can be severely panelized if Hadoop is used.
Phoenix and Haloop do not have an active community and they are not recommended for production deployment.
As the Big Data problem floods the whole world, many systems have been designed to deal with the problem.
Different from Hadoop, MPI was designed for high performance on both massively parallel machines and on workstation clusters.
In addition, MPI lacks fault tolerance and performance will be bounded when data becomes large.
The system includes configurations for both parallel batch processing and high performance online query applications using indexed data files.
The HPCC platform contains two cluster processing subsystems: Data Refinery subsystem and Data Delivery subsystem.
The Data Refinery subsystem is responsible for the general processing of massive raw data, and the Data Delivery subsystem is responsible for the delivery of clean data for online queries and analytics.
Introduction The configuration of a Hadoop cluster is a systematic project, especially, due to its large scale and distributed property.
Efforts are needed in choosing the proper storage and computing hardware, designing the interconnected network, installing and configuring the operating system, and so on.
In a Hadoop cluster, different types of nodes may require different hardware configurations.
For example, the JobTracker on a master node schedules jobs and assigns tasks to proper slave nodes for execution, and the NameNode on the master node manages the metadata for files and data blocks.
In addition, the master node is a critical failure point in a default cluster configuration, which configures only one master node.
A critical requirement for the master node is to be responsive and reliable.
On the other hand, a slave node is responsible for hosting data blocks and running tasks upon the data blocks.
But a slave node should have enough storage space and computing power to satisfy the storage and computing requirements.
Similarly, different Hadoop cluster sizes may have different configuration requirements.
For example, for a small to medium-sized cluster with up to a hundred slave nodes, the NameNode, JobTracker, and SecondaryNameNode daemons can be put on the same master machine.
When the cluster size grows up to hundreds or even thousands of slave nodes, it becomes advisable to put these daemons on different machines.
In this book, we assume to build a cluster with five slave nodes, which makes it reasonable to put the NameNode, JobTracker, and SecondaryNameNode daemons on the same physical machine.
Nodes in a Hadoop cluster are interconnected through network devices such as switches and routers.
Data will be transferred from one node to another over the network during different phases of a MapReduce job.
There are many factors that can affect the performance of a Hadoop cluster, some of which have greater influence than others.
For example, network segmentation caused by device failures can greatly degrade the cluster performance, while network speed and latency have much smaller influence comparatively.
So, a highly available and resilient network architecture is crucial for a Hadoop cluster.
Hadoop runs on Linux (although Windows operating systems are supported, it is still not stable at the time of writing this book)
We need to install and configure Linux on all cluster nodes before the Hadoop installation process.
If you have experience working with Linux, you may know that installing Linux on a single machine is straightforward by following the installation instructions.
For example, we can burn the downloaded operating system ISO image onto a DVD optical disk and then boot and install the operating system using this DVD.
However, the simple and straightforward installation method is too inefficient to be practical for a Hadoop cluster with a large number of nodes.
We are going to explore more practical and efficient installation methods in this chapter.
Some operating system configuration is needed after installing the Linux operating system.
For example, we need to configure users, groups, and system security, such as firewalls and SELinux.
We also need to install the required Hadoop dependency software, Java, and some optional tools that can improve cluster management efficiency.
Choosing hardware for cluster nodes A Hadoop cluster contains two types of nodes: a master node and a slave node.
By default, the NameNode, SecondaryNameNode, and JobTracker daemons reside on a master node, and DataNode and TaskTracker daemons reside on slave nodes.
Properly selecting hardware for these computing and storage nodes can maximize the efficiency of a Hadoop cluster.
In this recipe, we will list suggestions on hardware selection for a computing node.
Although special requirements exist for a master node and a slave node, there is no gold standard for choosing optimal hardware for both types of nodes.
It is reasonable to say that the hardware configuration is closely related to the properties of Big Data to be processed.
In addition, the choice of hardware is an empirical and adaptive process with the changing requirements on a Hadoop cluster.
For example, if the requirements for the throughput of a Hadoop cluster are high, we might need to choose high-end CPUs and hard drives.
If we have a large number of potential Hadoop users, we may need to upgrade the hardware configuration for both the master node and the slave nodes.
Empirically, we recommend the following configurations for a small to medium-sized Hadoop cluster:
On a Hadoop master node, the NameNode keeps the metadata, such as permissions of each file, in main memory.
The amount of memory needed by a master node depends on the number of file system objects (for example, numbers of files and block replicas) to be created and tracked.
The memory requirement will be high when the cluster is large.
The SecondaryNameNode keeps a backup for the latest filesystem checkpoint mirrored from the NameNode, so its memory requirement is similar to the NameNode.
In default configuration, the master node is a single failure point.
In Hadoop, each slave node simultaneously executes a number of map or reduce tasks.
The maximum number of parallel map/reduce tasks are known as map/reduce slots, which are configurable by a Hadoop administrator.
Each slot is a computing unit consisting of CPU, memory and disk I/O resources.
When a slave node was assigned a task by the JobTracker, its TaskTracker will fork a JVM for that task, allocating a preconfigured amount of computing resources.
In addition, each forked JVM also will incur a certain amount of memory requirements.
Higher data throughput requirement can incur higher I/O operations for the majority of Hadoop jobs.
That's why higher end and parallel hard drives can help boost the cluster performance.
To maximize parallelism, it is advisable to assign two slots for each CPU core.
See also f The Designing the cluster network recipe in Chapter 2, Preparing for.
Designing the cluster network The network is the backbone of a Hadoop cluster.
Its stability is critical for the performance of the cluster.
In this recipe, we will outline a few general rules for designing a Hadoop cluster network.
The network architecture for a small to medium-sized cluster can be as simple as connecting the cluster nodes with one or more switches.
Warning! Computing nodes in a Hadoop cluster should be configured within the same network segment (Local Area Network (LAN))
Advanced features such as VLANs that can cause overhead are not recommended.
The network architecture for the Hadoop clusters with hundreds or thousands of nodes is much more complex.
In a large cluster, the physical nodes are usually so small, for example, a blade server, that they can be mounted on racks.
Each rack has a local switch that interconnects nodes on the same rack.
Nodes on the same rack can be interconnected with a 1 GBps (Gigabyte per second) Ethernet switch.
Cluster level switches then connect the rack switches with faster links, such as 10 GBps optical fiber links, and other networks such as InfiniBand.
The cluster-level switches may also interconnect with other cluster-level switches or even uplink to another higher level of switching infrastructure.
With the increasing size of a cluster, the network, at the same time, will become larger and more complex.
Connection redundancies for network high availability can also increase its complexity.
In this book, we assume to discuss the basic network architecture design method.
If you want to learn more advanced network design techniques, please refer to related books and online materials.
In general, the network architecture of a medium-sized cluster can be described with the following diagram:
In this diagram, we assume there is a Hadoop cluster administrator machine and the clients connect to the cluster through a gateway, through which Hadoop jobs can be submitted.
The increasing bandwidth of network devices makes it possible for Hadoop to load and replicate large datasets across interconnected nodes.
Resilient and scalable network architecture can secure the high data throughput and performance requirements for a Hadoop cluster.
As we have mentioned previously, the most efficient way to install Linux on a large number of machines is to install over the network.
In this book, we assume to use the administrator machine as the installation server.
We will learn steps to configure this server, including the configuration of the following two services: DHCP and FTP.
Getting ready Before getting started, we assume that the cluster administrator machine has a 64 bit Red Hat compatible Linux operating system installed.
This user should have sudo privileges to install software packages, configure system services, and so on.
We also assume administrative tools such as a command-line text editor has been installed on this machine.
We will use these tools and commands directly in the upcoming recipes.
We will follow the Red Hat syntax for all the administrative commands.
If you are using a Linux distribution other than CentOS, such as Debian, please refer to corresponding documentation.
Log in to the administrator machine as hdadmin and change the hostname of the machine with the following command: sudo sed -i 's/^HOSTNAME.
We will use directory ~/mnt as the mount point for ISO images.
Install the DHCP and FTP servers on the machine with the following commands: sudo yum –y install dhcp.
We will use the DHCP server to assign IP addresses and bootstrap the operating system in the installation process, and use the FTP server to host the installation packages.
After selecting the nearest mirror, we can use either HTTP or FTP to download the image.
Let's choose FTP as the download method by clicking on the link in the corresponding line of the selected mirror.
In this directory, as shown in the following screenshot, we choose to download two ISO image files.
If you are not sure about the architecture of the cluster machines, please refer to the product hardware menu.
We can also use the following wget command to download the image file:
We can see from the directory trees that the minimal installation image file contains packages and boot images for system installation.
The netinstall package only contains files for booting, including network booting files in the images/pxeboot directory.
The following recipe will explain how to configure the DHCP server:
Start the DHCP server with the following command: sudo service dhcpd start.
Start the FTP server with the following command: $ sudo service vsftpd start.
See also f The Creating the kickstart file and boot media recipe in Chapter 2, Preparing.
Creating the kickstart file and boot media Installing Linux on a large number of nodes with a kickstart file has a few advantages.
For example, the installation process can be automated by specifying a list of to-be installed packages and configuring system settings for the post-installation process.
In this section, we will cover steps of creating a kickstart file and a USB boot media with the operating system image.
Getting ready A kickstart file is a plain text file used for the automatic installation of Linux.
Prepare a USB flash drive with storage capacity larger than 512MB.
We can use the following command to check the filesystem type:
If the TYPE attribute is other than vfat, use the following command to clear the first few blocks of the drive:
Log in to the administrative machine using the following command:
We will use the following steps to create a kickstart file:
Create swap partition, 16GB, double size of the main memory.
Tell the nodes hostname and ip address of the admin machine.
Put the kickstart file into the root directory of the FTP server with the command:
This will make the kickstart file available during the installation process.
We will now create a USB boot media using the following recipe:
Plug in a USB flash drive on the administrator machine and write the bootable ISO image into the USB flash drive with the following command (assuming the USB drive corresponds to the /dev/sdb device file):
Warning! Make sure you have a backup of the data on the USB flash drive, all the information will be wiped out when we write the ISO image file into the drive.
A kickstart file specifies a number of installation options, such as installation media, networking configuration, firewall configuration, and so on.
The file contains a %packages section, which specifies the packages to be installed.
In this section, both specific packages and package groups can be specified to install.
For example, in our kickstart file, we configure to install the Linux base package with @Base.
In addition, if a package is not intended to be installed, we can add a dash symbol before the package.
For example, if we don't want to install OpenJDK, we can specify this with -java.
For a Hadoop cluster, basic packages are enough, so we have ignored the unnecessary packages in the kickstart file.
The %post section allows us to specify configurations and commands after installation.
This is very helpful when we need to do some administrative configurations after installing the operating system.
For example, we might want to create a regular user for Hadoop with privileges to run Hadoop commands and to configure system services such as SSHD and FTP.
The USB boot media was used to boot a system and start the installation process automatically.
We can specify the following kernel start up option in the grub.conf file:
Once the kickstart file is located and transferred to the local machine, automatic installation will start.
There are other installation methods other than FTP, for example, we can also use NFS and HTTP.
The difference of these methods from FTP lies only in the configuration of the corresponding repository URL.
For example, if we want to use an HTTP server, we can make the following two changes in our configuration:
See also f The Installing the Linux operating system recipe in Chapter 2, Preparing for.
Installing the Linux operating system Although there are many ways to install Linux on a machine, installing over the network with the help of a kickstart file is the most efficient option.
The installation process can be automated requiring minimal human intervention.
A kickstart file can be kept on a server and read by individual machines during the installation process.
In the recipe, we will outline steps to install Linux on a number of Hadoop nodes over the network.
Getting ready Before getting started, we need to verify that the DHCP server and FTP server are running correctly on the administrative machine.
Use the following command on the administrator machine to check if the DHCP server is working properly:
If this command gives non-empty output, then it is working correctly, otherwise, we need to start the service with the following command:
Similarly, the following command can be used to check the FTP server on the administrator machine:
We should be able to log in anonymously and list the kickstart file and installation packages in the root directory.
In addition, we assume that the cluster nodes have been physically configured.
For example, racks and networking devices are all working without any issues.
Use the following recipe to install Linux on a machine:
Plug in the USB flash drive boot media and power on the computer.
If F9 does not work, please refer to the related product manual.
From the list of boot devices, choose USB or Removable Devices.
When the installation starts, you can remove the boot media and start the installation on the next machine.
After we power on the machine and select as the boot media, the boot loader, grub in our case, will start to work.
Then, stage 2 will load the kernel, which resides on boot media.
When installing the operating system, the kernel has very limited functionality to start the installation process, for example, finding the location of software packages.
In our case, the kernel option kickstart file contains all the specification for the installation process.
Thus, everything will be automated after booting from the USB boot media.
One advantage of separating the boot media from the installation package repository is that the installation on multiple machines can be paralleled to reduce the total installation time.
With the help of a kickstart file, we can automate the installation of Linux on a number of machines.
One disadvantage of this method is that we need to manually boot each machine.
This is tedious and requires a lot of repetitive work.
Even worse, in reality, we may find that a lot of servers don't even have a monitor or video card installed.
In this part, we will introduce the steps to automate the installation process with the help of DHCP and TFTP servers.
A DHCP server is configured as a booting server, which serves similarly as a USB drive boot media, and the TFTP is configured to host the actual operating system packages.
Configuring DHCP for network booting We have mentioned the basic configuration of a DHCP server in the previous section.
To enable network booting for DHCP, we will use the Preboot Execution Environment (PXE) method of TFTP.
Create the configuration file /etc/dhcpd.conf for DHCP with the following content:
Install the TFTP server with the command: sudo yum install tftpd.
In this file, we enabled the TFTP service by setting the disable primitive to be no.
Start the TFTP server with the following command: sudo service tftpd start.
Test the TFTP configuration with the following command: tftp hadoop.admin.
If we can log in and list files, the TFTP has been configured correctly.
Start the installation process by powering on the cluster machines.
Installing Java and other tools Hadoop was built using Java, so Java is required before installing Hadoop.
But if we use OpenJDK for Hadoop, it will cause low level and hard to tackle problems.
If no output is given, it means OpenJDK has not been installed.
If Java has been installed in the system, we can check its version with: java -version.
After confirming whether we are using OpenJDK, we need to remove the package and reinstall the version downloaded from Oracle's official website.
Warning! This command can be destructive, especially, when some dependent software packages have been installed.
In such a case, it will prompt you to confirm the removal of OpenJDK together with the depending software packages.
If you don't want all the packages to be removed, answer NO to the question.
This command will only remove the OpenJDK package, regardless of the dependent software packages.
Note that this command can break software package dependencies, causing dependent software not working properly.
As another alternative method, we can tweak the PATH environment variable to let both Java versions co-exist on the system while let the system to prefer the Java from Oracle.
Suppose we have both OpenJDK and Oracle Java installed in /usr/openjdk and / usr/jdk respectively.
Or, if we would like to only use the Oracle Java, we can set PATH to be:
Use the following recipe to install Java and other tools:
If Java is correctly installed, the output should be similar to the following:
If these packages have been specified in the installation kickstart file, this step will be optional.
It is not interactive and can be used from command line and scripts for file download.
It is widely used for file copying and synchronization under Linux.
Network Mapper (nmap) is a famous tool for network exploration and security auditing.
We can use nmap to scan large networks and identify security problems.
For example, to scan the service on a local machine, we can use the following command:
The output tells us that the local machine has the following services running: ftp, ssh, smtp, rpcbind (service for remote procedure calls), and jpp (service for Java packaging)
Similarly, we can use the following command to scan the IP segment 10.0.1.*:
Under Linux, we can use the man command to get the usage of a command.
For example, to get usage of wget, we can use man wget.
If more detailed information about a command is desired, we can use the info command.
For example, the command info wget gives more details about the wget command.
Configuring SSH SSH is the de facto standard protocol for secure data connection and remote command execution.
In this section, we are going to learn how to configure SSH on the cluster nodes.
Specifically, we are discussing how to configure SSH for a passwordless login to a remote machine.
Getting ready Start up the SSHD service on all the cluster nodes (both the slave nodes and the master node) with the following command:
Verify whether sshd works properly with the command from the master node:
If it is the first time of logging into to the host, we will get a message similar to the following:
We need to type in yes and then provide the password for the user hduser to log in to the host.
Log in to the master node from the cluster administrator machine with the following command: ssh hduser@master.
Restart the SSHD server with the following command: sudo service sshd restart.
Copy the public key file to the remote machine with the following command: ssh-copy-id slave1
If we can log in without entering the password, then the configuration is successful!
Configuration of a passwordless login failure can be caused by many reasons, for example, the configuration of the firewall (or iptables, to be more specific), SELinux, and even the SSHD server itself.
We will discuss methods to deal with these potential problems.
Erroneous SSH settings If the /etc/ssh_config file contains the following lines:
It means that the public key authorization has been disabled.
We need to change these two lines to the following:
Make sure that the SSHD service has been successfully restarted on the remote machine with the following command:
Log out of the remote machine and log in again, if the problem persists, go to the next section.
Erroneous iptables configuration Check the status of iptables with the following command:
If no rules are printed, go to the next step, otherwise, disable iptables by flushing all the existing rules with the following command:
Erroneous SELinux configuration Security Enhanced Linux (SELinux) is a Linux feature that provides the mechanism for supporting access control security policies.
SELinux that is in enforcing mode can block the passwordless login operation.
We can check the current SELinux status with the following command:
The output means SELinux is currently in enforcing mode, we need to put it in permissive mode with command:
Note that system reboot is required for the changes to take effect in this method.
See also f The Creating the kickstart file and boot media recipe.
Introduction After finishing all the preparing tasks, we are ready to configure a Hadoop cluster in this chapter.
First, we will give you some tips on choosing a proper Hadoop release version.
Then we will show you how to configure a Hadoop cluster in pseudo-distributed and fully-distributed mode.
Pseudo-distributed mode is a very good starting point if you have no experience configuring a Hadoop cluster.
In this mode, we will configure all the Hadoop daemons to run on a single machine, which can give us the first feeling of a working Hadoop cluster while minimizing configuration difficulties.
Next, we will show you how to validate a Hadoop cluster.
The importance of validating a Hadoop cluster configuration will never be overemphasized.
We typically use this step to confirm that the Hadoop cluster is running as expected.
The last few recipes will show you how to install a few components in the cluster.
Choosing a Hadoop version As an open source project, Hadoop has been under active development over the past few years.
These new releases either fix bugs contributed by the community, leading to a more stable Hadoop software stack, or add new features for the purpose of more full-fledged and enterprise-level distribution.
In this section, we are going to review the history of releases of Hadoop, pointing out features of these releases.
More importantly, we will give tips on choosing a proper Hadoop distribution.
Getting ready In general, the release version number of a Hadoop distribution consists of three parts: the version number, the major revision number, and the minor revision number.
Sometimes the revision number can have a fourth part, for example, 0.20.203.0, but this is relatively rare.
A Hadoop release name can be described with the following figure:
The table tells us that Hadoop is evolving rapidly, with new features such as security, HDFS federation, and NameNode HA being added over time.
Another lesson we can learn from the table is that the most recent stable release, Version 1.1.x, does not contain all the features.
And although release Version 2.0.x is the most feature-rich Hadoop release, it is still in alpha state requiring further improvements.
So, which version should you choose for your deployment? Generally, we need to consider two properties: stability and features.
For a production deployment, we definitely want to deploy a stable release and we want to use the release that contains all the required features.
See also f More information about Hadoop releases can be found at http://hadoop.
Pseudo-distributed mode refers to a Hadoop cluster configuration that contains only one node.
This mode can be helpful for debugging and validation purposes.
In this recipe, we will outline steps to configure Hadoop in pseudo-distributed mode.
Getting ready Before configuring Hadoop in pseudo-distributed mode, we assume that we have a machine, for example, the master node of the Hadoop cluster, with Linux installed.
We also assume that all the necessary tools have been installed and properly configured.
To check that Java has been properly installed, we can use the following command: $ java -version.
If you have installed OpenJDK, please refer to the Installing Java and other tools recipe from Chapter 2, Preparing for Hadoop installation.
In this book, we assume that we're using Hadoop release 1.1.2
To download a Hadoop release, please visit the following URL:
Choose the proper mirror site (or use the suggested link on top of the mirror)
We suggest downloading a .gzip archived file with the filename ending with tar.gz.
Perform the following steps to configure Hadoop in pseudo-distributed mode:
The uncompressed archive file will contain the following files and folders:
The folder contains several .jar files and folders such as bin, sbin, and conf.
The conf folder contains cluster configuration files, the bin folder contains commands and scripts to start and stop a cluster, and the sbin folder contains scripts to perform specific tasks.
Make a soft link for Hadoop root directory: sudo ln -s hadoop-1.1.2 hadoop.
We are assuming Oracle Java has been installed under the /usr/java/latest directory.
Configure localhost as the single slave node with the following command:
Use the following steps to start and stop a Hadoop cluster:
Format the HDFS filesystem from NameNode with the following command: hadoop namenode -format.
The output shows that the following HDFS daemons have been started: NameNode, DataNode, and SecondaryNameNode.
The output shows that the JobTracker and TaskTracker MapReduce daemons have been started.
With the jps command, we can get a list of all running daemons as follows:
Under Unix-like operating systems, system runtime configurations and environment variables are specified via plain text files.
These files are called run configuration files, meaning that they provide configurations when the program runs.
For example, the .bashrc file under a user's home directory is the run configuration file for bash shell.
It will be sourced (loaded) automatically every time when a bash terminal is opened.
So, in this file, we can specify commands and environment variables for a running bash environment.
Under Linux, the bash shell has two run configuration files for a user, .bashrc and .bash_profile.
The difference between the two files is that .bash_profile is executed for login shells, while .bashrc is for interactive, non-login shells.
More specifically, when we log in to the system by entering username and password either locally or from a remote machine, .bash_profile will be executed, and a bash shell process is initialized.
On the other hand, if we open a new bash terminal after logging into a machine or type the bash command from command line,the .bashrc file will be used for initialization before we see the command prompt on the terminal window.
In this recipe, we used the .bashrc file, so that new configurations will be available after opening a new bash process.
Alternatively, we can manually source a configuration file after it is created or changed with the source command.
The following table shows configuration files for configuring a Hadoop cluster in pseudodistributed mode:
File Description hadoop-env.sh Configures the environment variable used by Hadoop core-site.xml Configures parameters for the whole Hadoop cluster hdfs-site.xml Configures parameters for HDFS and its clients mapred-site.
For example, the home directory of Java installation JAVA_HOME, those related to Hadoop runtime options and cluster logging, and so on.
For example, the value 2 specifies that each data block will be replicated twice on the filesystem.
The dfs.data.dir property specifies the location of the data directory on the host Linux filesystem.
For example, we can configure the total number of the jvm tasks, the number of the map slots, and reduce slots on a slave node, reduce the amount of memory for each task, and so on.
In our single node configuration, we put localhost in this file.
A SecondaryNameNode daemon will be started on localhost, which has been verified with the jps command.
In our pseudo-distributed mode configuration, localhost is the only slave node in the cluster.
Hadoop provides a number of bash scripts for convenience of starting and stopping a cluster.
Script Description start-dfs.sh This is the script to start HDFS daemons, including NameNode,
This command will try to find the PID files of the HDFS daemons, and kill the processes with the PID files.
So, if the PID file is missing, this script will not work.
This is the script to start MapReduce daemons, including the JobTracker and TaskTracker daemons.
Similar to start-hdfs.sh script, PID files will be created for each daemon process.
Similar to stop-dfs.sh script, the script will try to find the PID files and then kill those processes.
So, we can use the following command to install Hadoop:
The locations of installed files will be different from the tarball method, and we can check the file layout with the following command:
Then we can use the following command to configure a Hadoop cluster in single node:
See also f The Configuring Hadoop in fully distributed mode recipe in Chapter 3, Configuring a.
Configuring Hadoop in fully-distributed mode To configure a Hadoop cluster in fully-distributed mode, we need to configure all the master and slave machines.
Although different from the pseudo-distributed mode, the configuration experience will be similar.
In this recipe, we will outline steps to configure Hadoop in fully-distributed mode.
Getting ready In this book, we propose to configure a Hadoop cluster with one master node and five slave nodes.
Before getting started, we assume that Linux has been installed on all the cluster nodes and we should validate password-less login with the following commands on the master node:
Unlike the pseudo-distributed mode, configuring a Hadoop cluster in fully-distributed mode requires the successful configuration of all the nodes in the cluster.
We should be cautious about the interconnection of the cluster nodes.
Connection problems might be caused by configurations of firewalls, network, and so on.
Use the following recipe to configure Hadoop in fully-distributed mode:
Log in to the master node from the administrator machine with the following command: ssh hduser@master.
Make a proper soft link for Hadoop root directory: sudo ln -s hadoop-1.1.2 hadoop.
We can get the following output information: Configuring hadoop on slave node slave1
Making symbolic link for Hadoop home directory on host host slave1
Making symbolic link for Hadoop home directory on host host slave2
Making symbolic link for Hadoop home directory on host host slave3
Making symbolic link for Hadoop home directory on host host slave4
The for-loop command copies the bash run configuration file from the master node to all the slave nodes in the cluster.
We can get the following output message: Copying local bash run configuration file to host slave1
Format the HDFS filesystem on the master node with the following command: hadoop namenode -format.
If this is the first time to format the HDFS, the command should finish automatically.
If you are reformatting an existing filesystem, it will ask you for permission to format the filesystem.
In such a case, we need to press Y to confirm the reformatting of the filesystem.
Be cautious that all the data will be wiped out after you hit the Enter key.
Check the directory structure of the formatted NameNode with the following command: tree /hadoop/dfs/
The output message shows that NameNode and SecondaryNameNode daemons are started on the master node, and a DataNode daemon is started on each slave node.
Start the MapReduce cluster daemons with the following command: start-mapred.sh.
The output message shows that a JobTracker daemon is started on the master node and a TaskTracker daemon is started on each slave node.
On the master node, check the status of the Hadoop daemons with the following command: jps.
On a slave node, we can check the status of the daemon processes with the same command, and the output will be similar to the following:
The highlighted daemons in the previous two steps must be present.
You can review the recipe Validating Hadoop installation for troubleshooting and debugging suggestions.
Check the status of each node in the HDFS cluster with the following command:
The output shows that there are 5 DataNodes in the cluster, and the status of each DataNode such as capacity and percentage of usage is reported.
Use the following two steps to stop a running Hadoop cluster:
Stop the MapReduce daemons with the following command on the master node: stop-mapred.sh.
We will get an output message similar to the following:
The output shows that the JobTracker daemon on the master node and TaskTracker daemons on the slave nodes are being stopped.
Stop the HDFS daemons with the following command on the master node:
The output shows that the NameNode and SecondaryNameNode daemons on the master node and the DataNode daemons on the slave nodes are being stopped.
The following table shows the properties used in this recipe:
Max number of parallel reduce tasks that a TaskTracker daemon can run.
Alternatively, we can use the following steps to configure a fully-distributed Hadoop cluster:
Log in to the master node with the following command: ssh hduser@master.
Configure the Hadoop cluster by modifying the configuration files located in the /etc/hadoop folder.
Validating Hadoop installation The configuration of a Hadoop cluster is not done before the validation step.
Validation plays an important role in the configuration of a Hadoop cluster; for example, it can help us figure out configuration problems.
The most straightforward way to validate a Hadoop cluster configuration is to run a MapReduce job from the master node.
Alternatively, there are two methods to validate the cluster configuration.
One is from web interface and the other is from the command line.
In this recipe, we will list steps to validate the configuration of a Hadoop cluster.
Getting ready To validate the configuration from the web interface, a web browser such as Firefox or Google Chrome is needed.
Sometimes if a GUI web browser is not available, we can use a command line based web browser such as elinks and lynx.
In this book, we assume to use elinks for illustration purpose.
We assume that elinks has been installed with the following command:
Log in to the master node with the following command: ssh hduser@master.
If this job finishes without any problem, we can say that the Hadoop cluster is working.
But this is not enough, because we also need to make sure all the slave nodes are available for running tasks.
Use the following steps to validate Hadoop cluster configuration through a web user interface:
The web page will be similar to the following screenshot:
Check the status of each slave node by clicking on the link, which leads us to a web page similar to the following screenshot:
From this screenshot, we can easily check the status of the active TaskTrackers on the slave nodes.
For example, we can see the count of failed tasks, the number of MapReduce slots, the heart beat seconds, and so on.
Check the status of slave DataNodes by opening the master:50070 URL.
The web page will be similar to the following screenshot:
By clicking on the Live Nodes link we can see the details of each node as shown in the following screenshot:
The job status web page will be similar to the following screenshot:
After the teragen job finishes, we can check the node storage space usage by opening the URL http://master:50070/dfsnodelist.jsp?whatNodes=LIVE.
The web page will be similar to the following screenshot:
Sometimes, a command-line based web browser can be handier than a GUI browser.
Use the following steps to validate the configuration of a Hadoop cluster from command line:
The output confirms that all the configured TaskTrackers are active in the Hadoop cluster.
Check the status of the HDFS cluster with the following command:
The output gives us the same information as from the web interface, and the last line tells us that the root filesystem is HEALTHY.
Hadoop provides commands and web interfaces for system administrators to check the status of the cluster.
When we start Hadoop daemons, a built-in web server will be started and a number of prewritten .jsp script files are used to respond to the user's requests from a web browser.
If you have programming experience, you can take advantage of the .jsp files to develop personalized Hadoop cluster management tools.
In this part, we list a few typical Hadoop configuration problems and give suggestions on dealing with these problems.
Can't start HDFS daemons There are many possible reasons that can cause this problem.
For example, the NameNode on the master node has not been formatted, in which case, we can format the HDFS before starting the cluster with the following command:
Warning! Be cautious when formatting the filesystem with this command.
More generically, to troubleshoot this problem, we need to check that HDFS has been properly configured and daemons are running.
If the output of this command does not contain the NameNode and SecondaryNameNode daemons, we need to check the configuration of HDFS.
To troubleshoot the HDFS startup problem, we can open a new terminal and monitor the NameNode logfile on the master node with the following command:
This command will show the content of the logfile in a dynamic way when a new log is appended to the file.
If an error happens, we can get error messages similar to the following:
The previous message shows that the hostname of the NameNode is wrong.
Cluster is missing slave nodes Most likely, this problem is caused by hostname resolution.
To confirm, we can check the content of the /etc/hosts file with the following command:
If the IP address and hostname mapping does not exist or has been erroneously specified in this file, correcting the error can solve this problem.
MapReduce daemons can't be started The following two reasons can cause this problem:
To troubleshoot this problem, we can refer to tips from the Can't start HDFS daemons section.
Before starting a cluster, we need to make sure that the total amount of configured memory should be smaller than the total amount of system memory.
To clear this problem, we can decrease the number of the map slots and reduce slots from six to three.
Configuring ZooKeeper ZooKeeper provides highly reliable centralized service for maintaining configuration information, naming, and providing distributed synchronization and group services.
In this recipe, we will outline steps to install ZooKeeper.
Please refer to the previous recipes in this chapter about installation of Hadoop on a cluster.
Log in to the master node from the Hadoop administrator machine as hduser with the following command:
Log in to the master node with the following command: ssh hduser@master.
The highlighted section makes every node know the other nodes in the ZooKeeper ensemble.
Start ZooKeeper on master node with the following command: zkServer.sh start.
Verify ZooKeeper configuration with the following command: zkCli.sh -server master:2181
In this section, we are going to list steps about installing HBase in our Hadoop cluster.
Getting ready To install HBase, we assume that Hadoop has been configured without any issues.
Similar to downloading Hadoop, HBase is hosted on mirrors all over the world.
Visit the link http://www.apache.org/dyn/closer.cgi/ hbase/, and select the nearest mirror (the suggested mirror on the top is the optimal choice)
After selecting the mirror, follow the link to select the HBase version; we suggest the stable version.
For example, follow the link http://mirror.quintex.com/apache/hbase/ stable/ and you can see the downloadable files as shown in the following screenshot:
Then, copy the file to the FTP repository with the following command:
Alternatively, we can download the file with the following command:
Log in to the master node from the administrator machine with the following command: ssh hduser@master.
Decompress the HBase archive with the following commands: cd /usr/local.
Create a symbolic link with the following command: ln -s hbase-0.94.5 hbase.
Use your favorite text editor to open the ~/.bashrc file and append the following lines into the file:
Connect to the running HBase with the following command: hbase shell.
Verify the HBase installation with the following HBase shell commands: hbase(main):001:0> create 'test', 'c'
Installing Hive As a top-level abstraction language, Hive provides a handy tool for manipulating data storage on HDFS with SQL-like language.
In this section, we will talk about installing Hive on our Hadoop cluster.
Getting ready Before we install Hive, we need to make sure Hadoop has been properly installed.
Please refer to the previous sections about the configuration of a Hadoop cluster.
Download Hive from a mirror site with a command similar to the following on the administrator machine:
Log in to the master node from the Hadoop administrator machine as hduser with the following command: ssh hduser@master.
Decompress the Hive archive with the following command: cd /usr/local.
Use your favorite text editor to open the ~/.bashrc file and add the following lines to this file:
In this recipe, we are going to discuss the installation of Apache Pig.
Getting ready Before we install Pig, we need to make sure Hadoop has been properly installed.
Please refer to the previous sections about the configuration of a Hadoop cluster.
Download the Pig archive file from a mirror site with the following command on the administrator machine:
Log in to the master node from the Hadoop administrator machine as hduser with the following command: ssh hduser@master.
Decompress the Pig archive file with the following command: cd /usr/local.
Open the ~/.bashrc file with your favorite text editor and add the following lines into the file:
Run Pig in local mode with the following command: pig -x local.
Run Pig in MapReduce mode with the following command: pig.
Pig that runs in MapReduce mode will utilizes the power of distributed computing provided by Hadoop.
Installing Mahout Apache Mahout is a machine learning library that scales machine learning algorithms on Big Data.
It is implemented on top of the Hadoop Big Data stack.
It already implements a wide range of machine learning algorithms.
In this recipe, we will outline steps to configure Apache Mahout.
Getting ready Before we install Mahout, we need to make sure Hadoop has been properly installed.
Download Mahout from the mirror site with the following command on the master node:
Log in to the master node from the Hadoop administrator machine as hduser with the following command: ssh hduser@master.
Decompress the Mahout archive with the following commands: cd /usr/local.
Open the ~/.bashrc file with your favorite text editor and add the following lines to the file:
Install Maven with the following command: sudo yum install maven.
Compile and install Mahout core with the following commands: cd $MAHOUT_HOME.
The install command will run all the tests by default; we can ignore the tests to speed up the installation process with command sudo mvn -DskipTests install.
Put the downloaded data into HDFS with the following commands: hadoop fs -mkdir testdata.
See also f More documentation about Mahout can be obtained from.
Introduction From the perspective of functionality, a Hadoop cluster is composed of an HDFS cluster and a MapReduce cluster.
The HDFS cluster consists of the default filesystem for Hadoop.
It has one or more NameNodes to keep track of the filesystem metadata, while actual data blocks are stored on distributed slave nodes managed by DataNode.
Similarly, a MapReduce cluster has one JobTracker daemon on the master node and a number of TaskTrackers on the slave nodes.
It splits jobs into smaller tasks and schedules the tasks to run by the TaskTrackers.
A TaskTracker executes tasks assigned by the JobTracker in parallel by forking one or a number of JVM processes.
As a Hadoop cluster administrator, you will be responsible for managing both the HDFS cluster and the MapReduce cluster.
In general, system administrators should maintain the health and availability of the cluster.
More specifically, for an HDFS cluster, it means the management of the NameNodes and DataNodes and the management of the JobTrackers and TaskTrackers for MapReduce.
Other administrative tasks include the management of Hadoop jobs, for example configuring job scheduling policy with schedulers.
At the end of this chapter, we will cover topics for configuring Hadoop logging and doing a system upgrade.
Logging provides insights for diagnosing cluster failure or performance problems, and system upgrade plays an important role in keeping the software up to date.
Even worse, it can make the cluster not function properly.
For example, DataNode's unavailability caused by network segmentation can lead to some under-replicated data blocks.
When this happens, HDFS will automatically replicate those data blocks, which will bring a lot of overhead to the cluster and cause the cluster to be too unstable to be available for use.
In this recipe, we will show commands to manage an HDFS cluster.
Getting ready Before getting started, we assume that our Hadoop cluster has been properly configured and all the daemons are running without any problems.
Log in to the master node from the administrator machine with the following command:
Use the following steps to check the status of an HDFS cluster with hadoop fsck:
Check the status of the root filesystem with the following command: hadoop fsck /
The output shows that some percentage of data blocks is under-replicated.
But because HDFS can automatically make duplication for those data blocks, the HDFS filesystem and the '/' directory are both HEALTHY.
Check the status of all the files on HDFS with the following command: hadoop fsck / -files.
Hadoop will scan and list all the files in the cluster.
This command scans all files on HDFS and prints the size and status.
Check the locations of file blocks with the following command: hadoop fsck / -files -locations.
The output of this command will contain the following information:
The following lines list the location of each block on the DataNode.
The number 50010 is the port number of the DataNode.
Check the locations of file blocks containing rack information with the following command: hadoop fsck / -files -blocks -racks.
Delete corrupted files with the following command: hadoop fsck -delete.
Use the following steps to check the status of an HDFS cluster with hadoop dfsadmin:
Report the status of each slave node with the following command: hadoop dfsadmin -report.
The first section of the output shows the summary of the HDFS cluster, including the configured capacity, present capacity, remaining capacity, used space, number of under-replicated data blocks, number of data blocks with corrupted replicas, and number of missing blocks.
The following sections of the output information show the status of each HDFS slave node, including the name (ip:port) of the DataNode machine, commission status, configured capacity, HDFS and non-HDFS used space amount, HDFS remaining space, and the time that the slave node contacted the master.
Refresh all the DataNodes using the following command: hadoop dfsadmin -refreshNodes.
Check the status of the safe mode using the following command: hadoop dfsadmin -safemode get.
We will be able to get the following output: Safe mode is OFF.
The output tells us that the NameNode is not in safe mode.
In this case, the filesystem is both readable and writable.
If the NameNode is in safe mode, the filesystem will be read-only (write protected)
Manually put the NameNode into safe mode using the following command: hadoop dfsadmin -safemode enter.
Make the NameNode to leave safe mode using the following command: hadoop dfsadmin -safemode leave.
If the NameNode has been in safe mode for a long time or it has been put into safe mode manually, we need to use this command to let the NameNode leave this mode.
Wait until NameNode leaves safe mode using the following command: hadoop dfsadmin -safemode wait.
This command is useful when we want to wait until HDFS finishes data block replication or wait until a newly commissioned DataNode to be ready for service.
Save the metadata of the HDFS filesystem with the following command:
The meta.log file will be created under the directory $HADOOP_HOME/logs.
The HDFS filesystem will be write protected when NameNode enters safe mode.
When an HDFS cluster is started, it will enter safe mode first.
The NameNode will check the replication factor for each data block.
If the replica count of a data block is smaller than the configured value, which is 3 by default, the data block will be marked as under-replicated.
Finally, an under-replication factor, which is the percentage of under-replicated data blocks, will be calculated.
If the percentage number is larger than the threshold value, the NameNode will stay in safe mode until enough new replicas are created for the under-replicated data blocks so as to make the under-replication factor lower than the threshold.
We can get the usage of the fsck command using:
By default fsck ignores files opened for write, use -openforwrite to report such files.
They are usually tagged CORRUPT or HEALTHY depending on their block allocation status.
We can get the usage of the dfsadmin command using:
There's more… Besides using command line, we can use the web UI to check the status of an HDFS cluster.
For example, we can get the status information of HDFS by opening the link http://master:50070/dfshealth.jsp.
We will get a web page that shows the summary of the HDFS cluster such as the configured capacity and remaining space.
For example, the web page will be similar to the following screenshot:
By clicking on the Live Nodes link, we can check the status of each DataNode.
We will get a web page similar to the following screenshot:
By clicking on the link of each node, we can browse the directory of the HDFS filesystem.
The web page will be similar to the following screenshot:
We can browse the content of each partition by clicking on the part-0000x link.
Configuring SecondaryNameNode Hadoop NameNode is a single point of failure.
By configuring SecondaryNameNode, the filesystem image and edit log files can be backed up periodically.
And in case of NameNode failure, the backup files can be used to recover the NameNode.
In this recipe, we will outline steps to configure SecondaryNameNode.
Getting ready We assume that Hadoop has been configured correctly.
Log in to the master node from the cluster administration machine using the following command:
And the tree structure of the SecondaryNameNode data directory will be similar to the following:
To increase redundancy, we can configure NameNode to write filesystem metadata on multiple locations.
Managing the MapReduce cluster A typical MapReduce cluster is composed of one master node that runs the JobTracker and a number of slave nodes that run TaskTrackers.
The task of managing a MapReduce cluster includes maintaining the health as well as the membership between TaskTrackers and the JobTracker.
In this recipe, we will outline commands to manage a MapReduce cluster.
Getting ready We assume that the Hadoop cluster has been properly configured and running.
Log in to the master node from the cluster administration machine using the following command:
This command can help us check the registration status of the TaskTrackers in the cluster.
Check the status of the JobTracker safe mode using the following command: hadoop mradmin -safemode get.
We will get the following output: Safe mode is OFF.
The output tells us that the JobTracker is not in safe mode.
If the JobTracker is in safe mode, no jobs can be submitted to the cluster.
Manually let the JobTracker enter safe mode using the following command: hadoop mradmin -safemode enter.
This command is handy when we want to maintain the cluster.
Let the JobTracker leave safe mode using the following command: hadoop mradmin -safemode leave.
When maintenance tasks are done, you need to run this command.
If we want to wait for safe mode to exit, the following command can be used: hadoop mradmin -safemode wait.
Reload the MapReduce queue configuration using the following command: hadoop mradmin -refreshQueues.
Get the usage of the mradmin command using the following:
The meaning of the command options is listed in the following table:
Managing TaskTracker TaskTrackers are MapReduce daemon processes that run on slave nodes.
They accept tasks assigned by the JobTracker on the master node and fork JVM processes/threads to run the tasks.
TaskTracker is also responsible for reporting the progress of the tasks as well as its health status using heartbeat.
Hadoop maintains three lists for TaskTrackers: blacklist, gray list, and excluded list.
TaskTracker black listing is a function that can blacklist a TaskTracker if it is in an unstable state or its performance has been downgraded.
For example, when the ratio of failed tasks for a specific job has reached a certain threshold, the TaskTracker will be blacklisted for this job.
Similarly, Hadoop maintains a gray list of nodes by identifying potential problematic nodes.
For example, when we debug or upgrade a slave node, we want to separate this node from the cluster in case it affects the cluster.
Hadoop supports the live decommission of a TaskTracker from a running cluster.
Getting ready We assume that Hadoop has been properly configured.
Log in to the cluster master node from the administrator machine using the following command:
List the active trackers with the following command on the master node:
Force the JobTracker to reload the TaskTracker list with the following command: hadoop mradmin -refreshNodes.
List all the active trackers again using the following command:
TaskTrackers on slave nodes contact the JobTracker on the master node periodically.
The interval between two consecutive contact communications is called a heartbeat.
More frequent heartbeat configurations can incur higher loads to the cluster.
The value of the heartbeat property should be set based on the size of the cluster.
The JobTracker uses TaskTracker blacklisting to remove those unstable TaskTrackers.
If a TaskTracker is blacklisted, all the tasks currently running on the TaskTracker can still finish and the TaskTracker will continue the connection with JobTracker through the heartbeat mechanism.
But the TaskTracker will not be scheduled for running future tasks.
If a blacklisted TaskTracker is restarted, it will be removed from the blacklist.
The total number of blacklisted TaskTrackers should not exceed 50 percent of the total number of TaskTrackers.
Decommissioning DataNode Similar to TaskTracker, there are situations when we need to temporarily disable a DataNode from the cluster, for example, because the storage space of the DataNode has been used up.
In this recipe, we will outline steps to decommission a DataNode from a live Hadoop cluster.
Getting ready We assume that our Hadoop has been configured properly.
Log in to the master node from the cluster administrator machine with the following command:
For illustration purpose, we assume to decommission DataNode on host slave1 from our running Hadoop cluster.
The dfs-exclude.txt file contains the DataNode hostnames, one per line, that are to be decommissioned from the cluster.
Force the NameNode to reload the active DataNodes using the following command: hadoop dfsadmin -refreshNodes.
Cluster administrators can use the dfsadmin command to manage the DataNodes.
We can get the usage of this command using the following:
Replacing a slave node Sometimes, we need to replace a slave node with new hardware.
For example, the slave node is not stable, more storage space or more powerful CPUs are desired, and so on.
In this recipe, we will outline the steps to replace a slave node.
Getting ready We assume that replacement hardware is ready for use.
And for illustration purposes, we suppose slave2 needs to be replaced in this book.
Decommission the TaskTracker on the slave node with the steps outlined in the Managing TaskTracker recipe of this chapter.
Decommission the DataNode on the slave node with the steps outlined in the Decommission DataNode recipe of this chapter.
Power off the slave node and replace it with the new hardware.
Install and configure the Linux operating system on the new node with the steps outlined in the Installing the Linux operating system, Installing Java and other tools, and Configuring SSH recipes of Chapter 2, Preparing for Hadoop Installation.
Refresh the DataNodes with the following command: hadoop dfsadmin -refreshNodes.
Refresh the TaskTracker with the following command: hadoop mradmin -refreshNodes.
Report the status of the live DataNodes with the following command: hadoop dfsadmin -report.
See also f The Installing the Linux operating system recipe of Chapter 2, Preparing for.
In a multiuser environment, multiple jobs can be submitted and run simultaneously.
The management of Hadoop jobs include checking job status, changing the priority of jobs, killing a running job, and so on.
In this recipe, we will outline the steps to do these job management tasks.
Getting ready We assume that our Hadoop cluster has been configured properly and all the Hadoop daemons are running without any issues.
We also assume that a regular user can submit Hadoop jobs to the cluster.
Log in to the master node from the cluster administrator machine with the following command:
Perform the following steps to check the status of Hadoop jobs:
List all the running jobs using the following command: hadoop job -list.
We will be able to get an output similar to the following:
List all the submitted jobs since the start of the cluster with the following command: hadoop job -list all.
The State column of the output message shows the status of jobs.
Both jobs have normal priority and don't have scheduling information.
We can check the status of the default queue with the following command: hadoop queue -list.
If no queues have been added, we will get an output similar to the following: Queue Name : default.
The output of the command shows the cluster has only one default queue, which in the running state with no scheduling information.
Check the status of a queue ACL with the following command: hadoop queue -showacls.
If no ACLs have been configured, we will get an output similar to the following:
The output shows that the user hduser can submit and administer jobs in the default queue.
Show all the jobs in the default queue using the following command: hadoop queue -info default -showJobs.
Check the status of a job with the following command:
Total time spent by all reduces waiting after reserving slots (ms)=0
Change the status of a job by performing the following steps:
The priority of the job will be HIGH as shown in the following output:
With the job status command, we will get the following output:
The job.xml file is an XML file that specifies the configuration of a job.
In this job configuration file, we specified the job name, the mapper class, the reducer class, the combiner class, the input format, and output format for the job.
Submit the job with the following command: hadoop job -submit job.xml.
And we will get an output similar to the following:
The queue command is a wrapper command for the JobQueueClient class, and the job command is a wrapper command for the JobClient class.
We can get the usage of the queue command with the following:
Similarly, we can get the usage of the job command with the following:
We discussed the most useful commands for Hadoop job management.
Actually, there are even more commands that are related to job management, and alternatively, we can use the web UI to manage Hadoop jobs.
Get the job history including job details, failed and killed jobs, and so on with the following command:
And we will get an output similar to the following:
Managing tasks We will show you how to kill tasks, check task attempts, and so on.
Kill a task with the following command: hadoop job -kill-task <task-id>
After the task is killed, the JobTracker will restart the task on a different node.
The killed tasks can be viewed through the web UI as shown in the following screenshot:
Hadoop JobTracker can automatically kill tasks in the following situations:
Speculative execution can run one task on multiple nodes; if one of these tasks has succeeded, other attempts of the same task will be killed because the attempt results for those attempts will be useless.
Job/Task schedulers, such as fair scheduler and capacity scheduler, need empty slots for other pools or queues.
In many situations, we need a task to fail, which can be done with the following command: hadoop job -fail-task <task-id>
In this command, available task types are map, reduce, setup, and clean; available task states are running and completed.
Managing jobs through the web UI We will show job management from the web UI.
Check the status of a job by opening the JobTracker URL, master:50030/ jobtracker.jsp.
We will get a web page similar to the following screenshot:
From this web page, we can get the cluster summary information, the scheduling information, the running jobs list, the completed jobs list, and the retired jobs list.
By specifying the refresh parameter, we can tell the web page to refresh every 30 seconds.
After a while, the killed job will be listed in the Failed Jobs list as shown in the following screenshot:
Checking job history from the web UI Hadoop keeps track of all the submitted jobs in the logs directory.
The job history logs contain information for each job such as the total run time and the run time of each task.
In this section, we will show you how to check the job history logs through a web UI.
Getting ready We assume that our Hadoop cluster has been properly configured and all daemons are running without any issues.
Perform the following steps to check job history logs from a web UI:
We will be able to get a web page similar to the following screenshot:
On the web UI, we can filter jobs based on the username and job name in the format username:jobname as shown in the screenshot.
From the web UI, we will be able to get a list of jobs in the Available Jobs in History section.
By clicking on the Job Id link of a job, we can get the details of the job as shown in the following screenshot:
This web page shows the details of the job, including task information such as total, successful, failed, and killed tasks.
The information also includes the start time and end time of four phases of a Hadoop job, including setup, map, reduce, and cleanup phases.
The web page also contains information of counters of the job as shown in the lower part of the screenshot.
In addition to the summary of the job information, the web UI provides an interface for us to analyze a job.
By clicking on the link Analyze This Job, we will go to a web page similar to the following screenshot:
The web page contains information of simple time analytics for each task, for example the best performing tasks that take the shortest time, the worst performing tasks, and the average time taken by all the tasks.
To further check the information of a task, we can click on the link for the task, and we will get a web page similar to the following screenshot:
We will be able to get task counters as shown in the following screenshot:
In addition to all these web services, the web UI provides a graphical display of the progress of Hadoop jobs and each phase as shown in the following screenshot:
This screenshot shows the progress of each map and reduce task.
The reduce task is composed of three phases: the shuffle phase, the sort phase, and the reduce phase, with each phase composing of 1/3 of the total reduce task.
Field Description master Hostname of machine that runs the JobTracker daemon.
The following table shows the summary of URLs we can use to check the status of jobs, tasks, and attempts:
The following table lists the naming examples for jobID, taskID, and attemptID:
Importing data to HDFS If our Big Data is on the local filesystem, we need to move it to HDFS.
In this section, we will list steps to move data from the local filesystem to the HDFS filesystem.
Getting ready We assume that our Hadoop cluster has been properly configured and all the Hadoop daemons are running without any issues.
And we assume that the data on the local system is in the directory /data.
Use the following command to create a data directory on HDFS: hadoop fs -mkdir data.
This command will create a directory /user/hduser/data in the HDFS filesystem.
Alternatively, we can use the command hadoop fs -put /data/datafile / user/hduser/data.
Verify the data file on HDFS with the following command: hadoop fs -ls /user/hduser/data.
The local copy will be deleted if you use this command.
Use the distributed copy to copy the large data file to HDFS:
This command will initiate a MapReduce job with a number of mappers to run the copy task in parallel.
We can get the usage of the fs command with the following:
The default <dst> filesystem schema for all these commands is hdfs:///
To copy multiple files from the local directory to HDFS, we can use the following command:
Similarly, we can move files from the local directory to HDFS.
Its only difference from the previous command is that the local files will be deleted.
Although the distributed copy can be faster than the simple data importing commands, it can incur a large load to the node that the data resides on because of the possibility of high data transfer requests.
Manipulating files on HDFS Besides commands to copy files from the local directory, HDFS provides commands to operate on files.
In this section, we will show you how to operate files, such as downloading files from HDFS, checking the content of files, and removing files from HDFS.
Getting ready We assume that our Hadoop cluster has been properly configured and all the daemons are running without any issues.
Perform the following steps to check the status of files and the directory on HDFS:
List files of the user's home directory on HDFS using the following command: hadoop fs -ls.
For example, this command will give the following output on my machine:
To recursively list files in the home directory, we can use the command hadoop fs -lsr ...
Check the space usage of files and folders in the home directory with the following command: hadoop fs -du.
The first column shows the size of the file in bytes and the second column shows the location of files on HDFS.
Sometimes, we can get a summarized usage of a directory with the command hadoop fs -dus ..
It will show us the total space usage of the directory rather than the sizes of individual files and folders in the directory.
For example, we can get a one-line output similar to the following:
Check the content of a file with the following command: hadoop fs -cat file1
This command is handy to check the content of small files.
But when the file is large, it is not recommended.
Instead, we can use the command hadoop fs -tail file1 to check the content of the last few lines.
Perform the following steps to manipulate files and directories on HDFS:
Empty the trash using the following command: hadoop fs -expunge.
Merge files in a directory dir and download it as one big file: hadoop fs -getmerge dir file1
This command is similar to the cat command in Linux.
It is very useful when we want to get the MapReduce output as one file rather than several smaller partitioned files.
To delete a directory, we can use the command hadoop fs -rmr dir.
It is very similar to the Linux command rm -r, which will recursively delete everything in the directory dir and the directory itself.
The file1 file under the directory /user/hduser will be downloaded to the current directory on the local filesystem.
Change the group membership of a regular file with the following command: hadoop fs -chgrp hadoop file1
Also, we can use the command hadoop fs -chgrp -R hadoop dir to change the group membership of a directory dir recursively.
Change the ownership of a regular file with the following command: hadoop fs -chown hduser file1
Similarly, we can use the command hadoop fs -chown hdadmin -R dir to change the ownership of a directory dir recursively.
The mode of files and directories under HDFS follows a similar rule as the mode under Linux.
How it works… We can get the usage of the fs command with the following command:
To get help for each individual command, we can use the -help option.
For example, we can get the help of the list command with the following:
Configuring the HDFS quota In a multiuser environment, quota can enforce the fair share of computing resources.
In this recipe, we will list steps to configure the HDFS quota.
Getting ready We assume that the Hadoop cluster has been configured properly and all the daemons are running without any issues.
Set the name quota on the home directory with the following command: hadoop dfsadmin -setQuota 20 /usr/hduser.
If we reach the quota, an error message similar to the following will be given:
If the space usage under the directory /user/hduser exceeds the specified quota, we will get an error message similar to the following:
Check the quota status with the following command: hadoop fs -count -q /user/hduser.
Clear the name quota with the following command: hadoop dfsadmin -clrQuota /user/hduser.
How it works… We can get the usage of the hadoop fs command with the following command:
The goal is to maximize the Hadoop cluster utilization by sharing the cluster among multiple users.
CapacityScheduler uses queues to guarantee the minimum share of each user.
It has features of being secure, elastic, operable, and supporting job priority.
In this recipe, we will outline steps to configure CapacityScheduler for a Hadoop cluster.
Getting ready We assume that our Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in to the master node from the cluster administrator machine using the following command:
By default, a Hadoop cluster has only one default queue.
From the JobTracker web UI, we can get a queue scheduling information web page similar to the following screenshot:
Alternatively, we can use the command hadoop queue -list to get the same information.
Get the schedule details of each queue by opening the URL, master:50030/ scheduler, and we can get a web page similar to the following:
This screenshot shows the status of each queue in the cluster, including the numbers of running jobs, pending jobs, and so on.
Test the queue configuration by submitting an example wordcount job to the queue hdqueue using the following command:
From the job information web UI, we can get the job scheduling information similar to the following:
CapacityScheduler is available as a JAR file under the $HADOOP_HOME/lib directory.
The following table shows the description of queue configuration properties:
The percentage share of the total number of slots for the hdqueue queue.
The percentage share of the total number of slots for the default queue.
The percentage of minimum resources allocated for each user in the queue hdqueue.
The maximum number of jobs that can be initialized concurrently by CapacityScheduler.
The maximum number of concurrently initialized tasks across all jobs in the queue hdqueue.
The maximum number of concurrently initialized tasks across all jobs in the queue hdqueue for each user.
Whether to support job priority for job scheduling or not.
Hadoop supports access control on the queue using queue ACLs.
Queue ACLs control the authorization of MapReduce job submission to a queue.
Configuring Fair Scheduler Similar to CapacityScheduler, Fair Scheduler was designed to enforce fair shares of cluster resources in a multiuser environment.
In this recipe, we will outline steps to configure Fair Scheduler for a Hadoop cluster.
Getting ready We assume that our Hadoop cluster has been configured properly and all the daemons are running without any problems.
Log in to the master node from the Hadoop administrator machine using the following command:
Verify the setting of Fair Scheduler by opening the URL http://master:50030/ scheduler.
The web page will be similar to the following screenshot:
The Hadoop Fair Scheduler schedules jobs in such a way that all jobs can get an equal share of computing resources.
If the pool for a user is not configured, the default pool will be used.
A pool specifies the amount of resources a user can share on the cluster, for example the number of map slots, reduce slots, the total number of running jobs, and so on.
The minimum share guarantee can be useful when the required number of computing slots is larger than the number of configured slots.
In case the minimum share of a pool is not met, JobTracker will kill tasks on other pools and assign the slots to the starving pool.
In such cases, the JobTracker will restart the killed tasks on other nodes and thus, the job will take a longer time to finish.
Besides computing slots, the Fair Scheduler can limit the number of concurrently running jobs and tasks on a pool.
So, if a user submits more jobs than the configured limit, some jobs have to in-queue until other jobs finish.
In such a case, higher priority jobs will be scheduled by the Fair Scheduler to run earlier than lower priority jobs.
If all jobs in the waiting queue have the same priority, the Fair Scheduler can be configured to schedule these jobs with either Fair Scheduler or FIFO Scheduler.
The following table shows the properties supported by fair scheduler:
Integer Seconds to wait before killing other pool's tasks if a pool's share is under the minimum share.
Integer Default seconds to wait before killing other pools' tasks when a pool's share is under minimum share.
Integer Pre-emption time when a job's resource is below half of the fair share.
Configuring Hadoop daemon logging System logging plays an important role in dealing with performance and security problems.
In addition, the logging information can be used analytically to tune the performance of a Hadoop cluster.
In this recipe, we will show you how to configure Hadoop logging.
Getting ready We assume that our Hadoop cluster has been properly configured.
Log in to the master node with the following command from the Hadoop administrator machine: ssh hduser@master.
We will get an output similar to the following: Connecting to http://master:50030/logLevel?log=org.apache.hadoop.
Now, the logging status of the JobTracker daemon will be similar to the following:
Get the log levels for TaskTracker, NameNode, and DataNode with the following commands:
This file defines both what to log and where to log.
For applications, the default root logger is INFO,console, which logs all messages at level INFO and above the console's stderr.
Hadoop supports a number of log levels for different purposes.
The log level should be tuned based on the purpose of logging.
For example, if we are debugging a daemon, we can set its logging level to be DEBUG rather than something else.
Using a verbose log level can give us more information, while on the other hand will incur overhead to the cluster.
The following table shows all the logging levels provided by Log4j:
Log level Description ALL The lowest logging level, all loggings will be turned on.
We can get the usage of daemonlog with the following command:
Other than configuring Hadoop logging on the fly from command line, we can configure it using configuration files.
The most important file that we need to configure is $HADOOP/conf/ hadoop-env.sh.
Hadoop provides audit logging through Log4j using the INFO logging level.
We will show you how to configure Hadoop audit logging in the next recipe.
Configure the logging directory to /var/log/hadoop by changing the following line:
Additionally, the following table shows other environment variables we can configure for Hadoop logging:
The cluster needs to be restarted for the configuration to take effect.
Configuring Hadoop security logging Security logging can help Hadoop cluster administrators to identify security problems.
By default, the security logging information is appended to the same file as NameNode logging.
We can check the security logs with the following command:
The error message tells that the NameNode is in safe mode, so the file /user/hduser/ test cannot be created.
Similar information can give us a very useful hint to figure out operation errors.
Hadoop logging file naming conventions Hadoop logfiles are kept under the directory $HADOOP_HOME/logs.
The tree structure of this directory on the master node will be similar to the following:
On a slave node, the $HADOOP_HOME/logs folder will have contents similar to the following:
The folder contains one .log file and one .out file for each Hadoop daemon, for example, NameNode, SecondaryNameNode, and JobTracker on the master node and TaskTracker and DataNode on a slave node.
The .out file is used when a daemon is being started.
Its content will be emptied after the daemon has started successfully.
The .log files contain all the log messages for a daemon, including startup logging messages.
On the master node, the logs directory contain a history folder that contains logs of the MapReduce job history.
Similarly, on a slave node, the logs directory contains a userlogs directory, which maintains the history information of the tasks that ran on the node.
In Hadoop, the names of logging files use the following format:
Configuring Hadoop audit logging Audit logging might be required for data processing systems such as Hadoop.
In Hadoop, audit logging has been implemented using the Log4j Java logging library at the INFO logging level.
This recipe will guide you through the steps to configure Hadoop audit logging.
Getting ready We assume that our Hadoop cluster has been configured properly.
Log in to the master node from the administrator machine using the following command:
Try making a directory on HDFS with the following command: hadoop fs -mkdir audittest.
The Hadoop NameNode is responsible for managing audit logging messages, which are forwarded to the NameNode logging facility.
So what we have seen so far is that the audit logging message has been mixed with the normal logging message.
Disable forwarding the audit logging message to the NameNode logger.
Hadoop logs auditing messages of operations, such as creating, changing, or deleting files into a configured log file.
By default, audit logging is set to WARN, which disables audit logging.
To enable it, the logging level needs be changed to INFO.
When a Hadoop cluster has many jobs to run, the log file can become large very quickly.
Log file rotation is a function that periodically rotates a log file to a different name, for example, by appending the date to the filename, so that the original logfile name can be used as an empty file.
Upgrading Hadoop A Hadoop cluster needs to be upgraded when new versions with bug fixes or new features are released.
In this recipe, we will outline steps to upgrade a Hadoop cluster to a newer version.
Getting ready Download the desired Hadoop release from an Apache mirror site: http://www.apache.
We assume that there are no running or pending MapReduce jobs in the cluster.
In the process of upgrading a Hadoop cluster, we want to minimize the damage to the data stored on HDFS, and this procedure is the cause of most of the upgrade problems.
The data damages can be caused by either human operation or software and hardware failures.
But the sheer size of the data on HDFS can be a headache for most of the upgrade experience.
A more practical way is to only back up the HDFS filesystem metadata on the master node, while leaving the data blocks intact.
If some data blocks are lost after upgrade, Hadoop can automatically recover it from other backup replications.
Log in to the master node from the administrator machine with the following command:
A still running DataNode can fail an update if it is not killed because the old version DataNode might register with the newer version NameNode, causing compatibility problems.
You can make changes to the configuration files if necessary.
Update the Hadoop symbolic link to the Hadoop version with the following command: sudo rm -rf /usr/local/hadoop.
Upgrade the NameNode with the following command: hadoop namenode -upgrade.
This command will convert the checkpoint to the new version format.
The two files should have the same content if there is no error in the upgrade.
The two files should have the same content if there is no error.
The result of this command should tell us that the data block locations should be the same.
Now, we can check the status of the cluster either by running a sample MapReduce job such as teragen and terasort, or by using the web user interface.
We can use the following command to get the usage of HDFS upgrade commands:
The following table lists the meanings of the command options:
Introduction The security of a Hadoop cluster is critical for its availability.
Hardening a Hadoop cluster includes configuring access control over resources, such as jobs, queues, and various administrative services.
We will introduce NameNode High Availability (HA) for the problem of single node failure.
In the end, we will introduce Hadoop federation, which federates multiple machines to expand the capacity of a cluster.
Configuring service-level authentication The purpose of service-level authentication (SLA) is to ensure that Hadoop users have the proper permission to access certain services.
One use case of this configuration is to control a list of allowed users who can use the cluster.
In this recipe, we will list steps to configure SLA.
Getting ready Before getting started, we assume that our Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in to the master node from the administrator machine with the following command:
This configuration only allows user hduser and group hadoop to submit jobs to the Hadoop cluster.
This configuration only allows users hduser and hdadmin, and group hadoop to access HDFS.
This configuration only allows the DataNode instances running as users belonging to the group datanode to communicate with the NameNode in the cluster.
Force the NameNode to reload the ACL configurations with the following command: hadoop dfsadmin -refreshServiceAcl.
This command will force the reloading of the HDFS related ACLs from the policy file.
Force the JobTracker to reload the service ACL configurations with the following command:
The users list and the groups list are separated by space.
For example, the generic format of the value should be similar to the following:
Besides the three properties we mentioned in this recipe, a number of other ACL properties are available in Hadoop.
The default value of these properties is *, which means all entities can access the service or, in other words, SLA is disabled.
See also f The Configuring job authorization with ACL recipe in Chapter 5, Hardening a.
Configuring job authorization with ACL Hadoop provides two levels of job authorization: job level and queue level.
When job authorization is enabled, the JobTracker will authenticate users who submit jobs to the cluster.
Users' operations on jobs and queues will also be authenticated by the JobTracker.
In this recipe, we will show steps to configure job authorization with ACLs.
Getting ready We assume that our Hadoop cluster has been properly configured without any problems.
Log in to the master node from the administrator machine with the following command:
Use the following steps to configure job authorization with ACLs:
This property will enable both the queue ACL and the job ACL.
This configuration will only allow user hduser and group hadoop to submit jobs to the queue hdqueue.
This configuration will only allow user hduser and group hadoop to administrate jobs to queue hdqueue.
Check the status of queue ACLs with the following command: hadoop queue -showacls.
Different to queue level ACL, job level ACL specifies access control for any submitted jobs, regardless of the queue a job has been submitted to.
Force the NameNode and the JobTracker to reload the ACL configurations with the following commands:
The user list and group list are separated by a space.
For example, the generic format of the value should be similar to the following:
The job owner, the super user, and the cluster administrators to which the job was submitted, will always have the right to view and modify a job.
Job view ACLs control the access of job status information, including counters, diagnostic information, logs, job configuration, and so on.
When this happens, a user's operation will be granted if the user has been listed in either of these ACLs.
See also f The Configuring service-level authentication recipe in Chapter 5, Hardening a.
Securing a Hadoop cluster with Kerberos Recent Hadoop releases have added the security feature by integrating Kerberos into Hadoop.
Kerberos is a network authentication protocol that provides strong authentication for client/ server applications.
Hadoop uses Kerberos to secure data from unexpected and unauthorized accesses.
It achieves this by authenticating on the underlying Remote Procedure Calls (RPC)
In this recipe, we will outline steps to configure Kerberos authentication for a Hadoop cluster.
It was designed to provide strong authentication for client/ server applications by using secret key cryptography.
The Kerberos protocol requires that a client provide its identity to the server and vice versa.
When their identities are proved by Kerberos, all of their following communication will be encrypted.
Before getting started, we assume that our Hadoop has been properly configured without any problems, and all the Hadoop daemons are running without any issues.
On our CentOS machine, use the following command to install the Kerberos packages:
After this, the kadmin and kinit commands should be accessible from the command line.
If you are also using CentOS or other Red Hat-compatible operating systems, we need to install Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files, by downloading it from the following link:
At the bottom of the web page, you should see options similar to the following:
Click on the download link at the right-hand side of the screen based on your Java version, which we can get with the java -version command.
Use the following steps to configure Kerberos for a Hadoop cluster:
If your account does not have root access, you need to use the kadmin.local command!
Show available keytab file entries with the following command: klist -e -k -t hduser.keytab.
Only the owner is allowed to read the keytab files.
Enable webHDFS by adding the following property to the $HADOOP_HOME/conf/ hdfs-site.xml file:
Set the sticky bit on HDFS directory to prevent the directories or files from being deleted by unauthorized users with the following command: sudo -u hdfs hadoop fs -chmod 1777 /tmp.
Verify the sticky bit setting with the following command: hadoop fs -ls /
Configuring web UI authentication By default, Hadoop users and administrators can access the web UIs of the Hadoop daemons without requiring any authentication.
Hadoop daemon web UIs can be configured to authenticate users with Kerberos.
In this recipe, we will outline steps to configure user authentication for accessing the web UI.
Getting ready We assume that our Hadoop has been properly configured and all the daemons are running without any issues.
We also assume that Kerberos authentication has been configured properly.
Configure the authentication token's valid time length by changing the hadoop.
This property is required for HTTP authentication to work properly.
Configure the Kerberos principal for HTTP endpoint by changing the hadoop.http.
Alternatively, we can test our configuration with the following curl command:
Web UI authentication is implemented with the HTTP SPNEGO protocol.
It is a GSSAPI mechanism used to negotiate one of a number of possible real mechanisms.
The pseudo mechanism uses a protocol to determine what common GSSAPI mechanisms are available, selects one, and then dispatches all further security operations to it.
This can help organizations deploy new security mechanisms in a phased manner.
For more information about SPNEGO, please refer to its wiki page at http://en.wikipedia.org/wiki/SPNEGO.
If this authentication method is used, we must specify the username in the first browser interaction using the user.name parameter in the URL.
If simple authentication is used as the authentication type, anonymous user web UI requests can be configured by changing the following property:
Recovering from NameNode failure The NameNode in a Hadoop cluster keeps track of the metadata for the whole HDFS filesystem.
Unfortunately, as of this book's writing, the NameNode in the current stable version of Hadoop is a single point of failure.
If the metadata of the NameNode is corrupted, for example, due to hard drive failure, the whole cluster will become unavailable.
So, it is important to safeguard the NameNode from these disastrous failures.
There are multiple ways we can increase the resilience of a HDFS cluster.
In this recipe, we will show you how to configure a SecondaryNameNode as a backup NameNode and how to recover from NameNode failures.
Please be prepared that a NameNode failure can cause the cluster to halt.
It might take some time to recover from the failure.
The first method we want to introduce is to configure the NameNode to write edit logs and the filesystem image into two locations—one is on the local directory for the NameNode machine and the other is on the SecondaryNameNode machine.
In this property, we configured two directories for the NameNode to write metadata to.
For brevity purposes, we are not showing you the configuration of NFS in this recipe.
More information about this topic can be obtained from http://www.tldp.org/HOWTO/NFS-HOWTO/
Start the Hadoop cluster in master1 with the following command:
In this configuration, we actually are not starting any daemons on the SecondaryNameNode machine master2
We just use this machine to store the metadata files for the NameNode on master1
Once the NameNode fails, we can quickly start the NameNode on master2 with little effort.
Once the NameNode on master1 fails, we can use the following steps to recover:
Stop the cluster with the following command: ssh master1 -C "stop-all.sh"
In case of NameNode failure, the backup files can be used to recover the HDFS filesystem.
As we have mentioned previously, the failure of a NameNode is mainly caused by the corrupted metadata files.
So the key for NameNode resilience is the recovery of metadata files.
Here, we will introduce two more methods to do this—one is by writing metadata onto multiple hard drives, and the other is to recover from the checkpoint of a SecondaryNameNode.
NameNode resilience with multiple hard drives We can use the following steps to configure the NameNode with multiple hard drives:
Install, format, and mount the hard drive onto the machine; suppose the mount point is /hadoop1/
Create the Hadoop directory with the following command: mkdir /hadoop1/dfs/name.
The second directory is a directory on a separate hard drive.
We can use the following steps to recover from the NameNode failure:
Recovering NameNode from the checkpoint of a SecondaryNameNode We can use the following steps to configure SecondaryNameNode, and recover from the NameNode failure:
By doing this, we configure it to run SecondaryNameNode on master2
In case the NameNode fails, we can use the following steps to recover:
It is recommended that the new NameNode machine has the same configuration as the failed NameNode.
Format the NameNode with the following command: hadoop fs -format.
Convert the checkpoint to the new version format with the following command: hadoop namenode -upgrade.
Configuring NameNode high availability As of this book's writing, the NameNode of the Hadoop stable release is a single point of failure.
In case of either accidental failures or regular maintenance, the cluster will become unavailable.
This is a big problem for a production Hadoop cluster.
In this recipe, we list the steps to configure NameNode HA.
In order for the standby NameNode to automatically recover from the active NameNode failure, the NameNode HA implementation requires that the edit logs of the standby NameNode to be always kept synchronized with the active NameNode.
One is based on Quorum and the other is based on shared storage using NFS.
In this recipe, we will only show you how to configure HA using Quorum.
Be cautious that this Hadoop version is still in alpha status, so it is not recommended for production deployment.
For more development status of MRv2, please refer to the official website at: http://hadoop.
Log in to one of the NameNode machines with the following command: ssh hduser@master1
Specify the NameNode IDs for the configured name service by adding the following property to the file:
This property specifies the NameNode IDs with the property dfs.
Configure the HTTP web UI address of the two NameNodes by adding the following lines to the file:
Configure the NameNode shared edits directory by adding the following property to the file:
This property configures three journal node addresses for Quorum to provide the shared edits storage.
The shared edits logs will be written by the active NameNode and read by the standby NameNode.
Configure the Quorum Journal Node directory for storing edit logs in the local filesystem by adding the following property to the file:
Configure the proxy provider for the NameNode HA by adding the following lines to the file:
Currently, NameNode HA supports two fencing methods, one is sshfence and the other one is shell.
In this recipe, we are assuming to use the sshfence method.
Configure the private key file for the sshfence method by adding the following property to the file:
The value of this property should be a comma-separated list of private key files.
In order for sshfence to work, we are configuring the private key files so that it can log in to the target node without providing a paraphrase.
Configure the SSH connection timeout, in milliseconds, by adding the following property to the file:
This configuration will enable automatic failover for all the name service IDs.
If we want to enable automatic failover for a specific name service ID, for example hdcluster, we can configure the following property:
Initialize the ZooKeeper with the following command: hdfs zkfc -formatZK.
This command will create a znode in ZooKeeper where the automatic failover system will store the data.
This command will start a ZKFC daemon on each NameNode machine, and the active NameNode will be selected after the daemons start.
Alternatively, we can manually start the ZKFC daemon on each NameNode machine with the command hadoop-daemon.sh start zkfc.
We can use the following steps to test the NameNode HA configuration:
Check the status of the NameNode by visiting the NameNode web UI with the following URLs: master1:50070
Kill the NameNode process with the following command on master1:
If the standby NameNode becomes the active NameNode automatically and the Hadoop cluster is still working without any problems, the configuration is successful.
The goal is to guarantee the availability of the cluster.
It addresses the problem by using two NameNodes in the cluster, an active NameNode, and a standby NameNode.
The active NameNode will provide the same service as the NameNode in MRv1
Different from the SecondaryNameNode, which simply copies the NameNode images and edit logs to a backup directory, the standby NameNode is a hot standby node for the active NameNode.
In case the active NameNode fails, the standby NameNode will become the active node in minimum time.
In the NameNode HA implementation, ZooKeeper is playing an important role.
We can use the following steps to configure a secured ZooKeeper:
Log in to the master1 machine with the ssh command.
The special symbol @ specifies that the configuration is pointing to a file rather than inline.
Format the ZooKeeper with the following command: hdfs zkfc -formatZK.
Configuring HDFS federation Hadoop NameNode keeps the metadata in the main memory.
When the HDFS namespace becomes large, the main memory can become a bottleneck of the cluster.
It increases the NameNode capacity and throughput by leveraging the capacity of multiple independent NameNodes, with each NameNode hosting or managing part of the HDFS namespace.
The value of this property is a comma-separated list of NameNode service IDs.
Configure the NameNode RPC and HTTP URI for namenode1 by adding the following into the file:
The previous configurations assume that the NameNode daemons and NameNode HTTP and secondary HTTP daemons locate on the host master1
Specify the NameNode RPC and HTTP URI for namenode2 by adding the following into the file:
In this command, the -clusterId option should be a unique cluster ID in the environment.
A unique cluster ID will be automatically generated if not specified.
Now, we can start or stop the HDFS cluster with the following commands on either of the NameNode hosts:
On a non-federated HDFS cluster, all the DataNodes register with and send heartbeats to the single NameNode.
On a federated HDFS cluster, all the DataNodes will register with all the NameNodes in the cluster, and heartbeats and block reports will be sent to these NameNodes.
A Federated HDFS cluster is composed of one or multiple namespace volumes, which consist of a namespace and a block pool that belongs to the namespace.
A namespace volume is the unit of management in the cluster.
For example, cluster management operations such as delete and upgrade will be operated on a namespace volume.
In addition, federated NameNodes can isolate namespaces for different applications or situations.
The following table shows the properties for configuring NameNode federation:
A NameNode federated Hadoop cluster has different administrative tasks than the old version (MRv1), which does not support federation.
For example, if we want to decommission namenode1 from the cluster, the content of the file should be:
Distribute the exclude file to all the NameNodes with the following command:
We can use the following URL to access the web UI for the HDFS cluster:
Running balancer Similar to the old Hadoop version, balancer is used to balance the data blocks over the cluster.
On a HDFS federated Hadoop cluster, we can run a balancer with the following command:
This command will balance the data blocks on the node level.
Another balancing policy is blockpool, which balances the storage at the block pool level as well as the data node level.
Adding a new NameNode Suppose we have configured a NameNode-federated Hadoop cluster with two running NameNodes.
Tell the DataNodes the change of the NameNodes with the following command:
Introduction System monitoring is critical for maintaining the health and availability for large distributed systems such as Hadoop.
General monitoring tasks include monitoring the health of cluster nodes and networks, for example, the usage of memory, heap, CPU, network, and so on.
For a Hadoop cluster, we may also want to monitor some specific metrics, such as the status of jobs and tasks in the cluster, the status of the JobTracker, TaskTracker, NameNode, and DataNode.
Hadoop is lucky to be born in an open source world! A number of very stable open source tools for system monitoring are there waiting to join the Hadoop family, and many of these systems have been adopted by Hadoop for monitoring purposes.
In this chapter, we will first introduce the management framework, Java Management Extension (JMX) for system monitoring.
Next, we will introduce two famous open source cluster monitoring systems: Ganglia and Nagios.
Monitoring daemons running on monitoring hosts send data to monitoring server hosts for storage, statistical analysis, and visualization.
Next, we will introduce Hadoop-specific monitoring systems Ambari and Chukwa.
Ambari was designed to be a full-fledged system for the deployment, management, and monitoring of Hadoop clusters.
It was developed based on the monitoring framework of Ganglia and Nagios.
But different from Ganglia and Nagios, Apache Chukwa monitors a Hadoop cluster by analyzing system logs collected from the Hadoop cluster.
Apache Flume is another general purpose data analytics framework for streaming data such as system logs.
It can be configured to do system monitoring, which is similar to Chukwa.
Monitoring a Hadoop cluster with JMX JMX is the technology used by Java to build, monitor, and manage distributed systems and network applications.
In this recipe, we will outline steps to configure Hadoop cluster monitoring with JMX.
Getting ready We assume that Oracle JDK has been installed, and our Hadoop cluster has been configured properly, and all the daemons are running without any problems.
Use the following steps to configure JMX for monitoring a Hadoop cluster:
Remove the comment symbol # for the two highlighted lines.
These two lines specify the passwords for monitorRole and controlRole.
Change the permission of the password file to 600 with the following command:
In this file, we will be able to find JMX monitoring configurations for the Hadoop daemons including NameNode, SecondaryNameNode, DataNode, balancer, JobTracker, and TaskTracker.
Now, we need to configure the remote monitoring port for Hadoop daemons by changing the original configuration to the following:
Use the following command to start the monitor user interface: jconsole.
We can see a window that is similar to the following screenshot:
From this window, we can check the status of both local and remote processes.
First, select the desired process to check from the process list.
Then, we only need to click on the Connect button.
We need to specify the location of the service by either specifying the hostname and port or protocol and sap as shown in the screenshot.
We also need to enter the Username and Password values for authentication purposes.
In the following steps, we assume to check the status of local JobTracker processes.
Select the Local Process radio button and then click on the Connect button.
We can get a monitoring window after waiting for a few moments, as shown in the following screenshot:
From the window, we can check the memory usage, threads, classes, summary of JVM, and details of MBeans.
Check the memory of the daemon by clicking on the Memory tab of the jconsole window, and we will get a window similar to the following:
The memory window shows the Heap Memory Usage page as a time serial chart.
From this window, we can select from different charts and time ranges to display.
The bottom part of the window is a summary of the current memory usage.
By clicking on the Threads tab of the window, we can check the running threads of the JobTracker and we can get a window similar to the following screenshot:
The upper part of the window shows the number of peak live threads and the number of current live threads.
On the lower part of the window, we can see a list of threads, the information of which can be viewed by clicking on the desired thread names.
Similarly, we can check the current classes by clicking on the Classes tab of the window and check the summary of the JVM virtual machine by clicking on the VM Summary tab of the window.
The MBeans tab is the most informative one if you want to check the status details of the daemon.
For example, the following screenshot shows more metrics details for JobTracker:
From this window, we can get a number of JobTracker metrics such as number of running jobs, number of map and reduce slots, and number of running map and reduce tasks.
Monitoring a Hadoop cluster with Ganglia Ganglia is an open source, scalable, and distributed monitoring system for clusters and computing grids.
It has three major components: the monitoring daemon, the metadata daemon, and the web UI.
In this recipe, we will outline steps to configure Ganglia for Hadoop cluster monitoring.
Getting ready Log in to the master node from the administrator machine with the following command:
Use the following yum command to install Ganglia on the master machine:
Install Ganglia monitoring daemon on all the slave nodes with the following commands:
In this recipe, we assume that the Ganglia server will run on the master node and the monitoring daemons run on both master and slave nodes.
Use the following steps to configure Ganglia for Hadoop cluster monitoring:
Hadoop supports network communication through both unicast (with normal IP addresses, which is the one we use here) and multicast, which uses multicast addresses, such as 239.2.11.71
To use multicast, all the monitored hosts should be in the same IP segment.
Add all the hostnames in the cluster to the gmetad configuration /etc/ganglia/ gmetad.conf file, for example, this file should contain the following:
Here hdcluster is the name of cluster that Ganglia monitors and master:8649 is the network address of gmetad.
Start the gmond daemon with the following command on the master node: sudo service gmond start.
Run the sudo chkconfig gmond on command if you want the process to survive a system reboot.
Check the status of gmond with the following command: curl master:8649
This command will output the configuration of Ganglia in XML similar to the following:
The highlighted lines show that some HDFS and MapReduce metrics are being monitored by Ganglia.
Start the gmetad daemon with the following command on the master node: sudo service gmetad start.
The access control setting in this file allows everyone to visit the Ganglia web UI.
For security reasons, we can restrict the IP addresses, hosts, or domains with the Allow from and Deny from statements in the configuration.
Start the httpd daemon with the following command: sudo service httpd start.
Check the status of Ganglia by opening URL http://master:80/ganglia, and we can get a web page similar to the following screenshot:
The upper-left portion of the UI is a summary of the cluster, including number of hosts, host status, average load, and so on.
The upper-right part of the screenshot shows an overview of the whole cluster, including cluster total load, CPU usage, memory, and network traffic.
The screenshot contains a clear jump for about 20 minutes when one teragen job is running, which consumes cluster resources.
The lower part of the screenshot shows the status for each node in the cluster, including the master node and all the slave nodes.
We can change the metric to display by selecting from the combo box, as shown in the following screenshot:
The previous screenshot confirms that a DataNode is the actual place where data blocks are stored, and NameNode only keeps track of the metadata for the data blocks.
So the written data size is much smaller for NameNode.
Check the details of each cluster node by selecting the option from the combo box, which has the initial value of --Choose a Node.
For example, if we want to check all the metric values for the master node, we will be able to get a web page similar to the following screenshot:
By scrolling down the window, we can check the Hadoop JobTracker metrics as shown in the following screenshot:
The previous screenshot contains the status information of the JobTracker including the number of running jobs, the number of map reduce slots, and so on.
A Ganglia monitoring system is composed of three parts: the monitoring daemons gmond, the meta-data handling daemon gmetad, and the web UI.
Ganglia gmond daemons run on each node that is being monitored in the cluster.
They continuously collect metrics data and send to the gmetad daemon running on the master node.
The gmetad daemon stores data into a database maintained by rrdtool.
The web UI, including all the graphs, is generated with PHP by pulling the data from the database.
The Ganglia data flow can be described with the following diagram:
Monitoring a Hadoop cluster with Nagios Nagios is a powerful open source cluster monitoring system.
It can monitor not only hosts and servers but also interconnecting devices such as routers and switches.
The alerting services provide notification mechanisms for fast response on system problems.
Designed to be a generic monitoring system, Nagios can be configured to monitor Hadoop clusters.
In this recipe, we will outline steps to configure Nagios for Hadoop cluster monitoring.
Getting ready Perform the following steps before monitoring a Hadoop cluster with Nagios:
To get started with Nagios monitoring, we need to install it first.
On CentOS and other Red Hat-compatible Linux systems, we can use the following yum command to install Nagios: sudo yum install nagios nagios-plugins.
This command will automatically install the dependency software packages such as libgd and libgd-devel.
After installation, the Nagios configuration files will be under the /etc/nagios directory and the Nagios daemon will be under the /etc/init.d/ directory.
Install the Nagios Remote Plugin Executor (NRPE) package with the following command: sudo yum install nrpe.
After this, we want to get the following directory structure: check_jmx.
By doing this, we will be able to use the check_jmx command in the Nagios monitoring configuration.
Use the following steps to configure Nagios for Hadoop cluster monitoring:
It references a number of files with the extension .cfg, which contains specific monitoring configurations.
Other alerting methods such as SMS messages and paging, are also available.
Create an administrator user for the Nagios web UI with the following command:
We are supposed to type in a password for the nagiosadmin user twice.
The username and password will be used to log in to the web UI.
Now, we are ready to specify the Hadoop cluster monitoring configurations with the following steps:
The previous two lines tell the main configuration file nagios.cfg to include two user specific configuration files: hosts.cfg and services.cfg.
The file configures six hosts to monitor in the cluster, including one master node and the five slave nodes.
The configuration file services.cfg specifies the services we want to monitor.
The service configured in this file will be applied to each host claiming to be in the hdgroup group, which is configured in the hosts.cfg file.
Start the Nagios service with the following command: sudo service nagios start.
Check the status of the service with the following command: sudo service nagios status.
If Nagios is running, the output should be similar to: nagios (pid 21542) is running...
If SELinux is on, we need to change the context of the web directory with:
Now, we should be able to check the web UI by opening URL: http://master/nagios.
If you are opening the web UI for the first time, you need to type in the username and password.
The username should be nagiosadmin and the password should be the one you entered for the htpasswd command.
A typical Nagios deployment consists of a monitoring server and a number of hosts that are being monitored.
An administrator defines monitoring specifications (what services on which hosts) using one or more configuration files.
Monitoring a Hadoop cluster with Ambari The Apache Ambari project (http://incubator.apache.org/ambari/) is an open source project aiming to ease the management and monitoring of Hadoop clusters.
In this recipe, we will outline steps to configure Hadoop Ambari for cluster installation, monitoring, and management.
Getting ready We assume that the Hadoop cluster has been configured properly.
Enable the NTP server with the following command: sudo service ntpd start.
SELinux should have been disabled on the servers where Ambari is installed.
We can use the following command to temporarily disable SELinux: sudo setenforce 0
To permanently disable SELinux, we need to edit the SELinux configuration file /etc/ selinux/config by changing the state of the SELINUX attribute to the following: SELINUX=disabled0
Disable the iptables service with the following command: sudo service iptables stop.
Use the following steps to configure Ambari for Hadoop monitoring and management:
Install the epel repository with the following command: sudo yum install epel-release.
Verify the repository list with the following command: yum repolist.
Install Ambari with the following command: sudo yum install ambari-server.
The command will automatically install the PostgreSQL database, which is required by Ambari.
Set up the Ambari server with the following command: sudo ambari-server setup.
We will get a warning if SELinux is not disabled and the iptables service will be disabled if it hasn’t been.
During the configuration process, we will be asked to configure the username and password for the PostgreSQL database.
If you choose not to do so, which is the default option, the default username and password will be ambariserver and bigdata.
The setup process will then prompt for downloading the Oracle JDK, and we should accept the license.
The downloaded JDK will be installed to hosts when deploying packages on the cluster.
The output of the setup process is shown in the following screenshot:
Now, we can start the Ambari server with the following command: sudo service ambari-server start.
Now, we need to log in to the Ambari server with the username and password both as admin.
Ambari web UI, which is similar to the following screenshot:
Next, we need to configure Install Options as shown in the following screenshot:
After specifying the installation options as shown in the previous screenshot, we can click on the Register and Confirm button to start the installation process.
This will lead to the host registration progress page as shown in the following screenshot.
In this step, we need to confirm the hosts to install Hadoop:
By clicking on the Next button, we will allow the web page to choose the services to install, as shown in the following screenshot:
By default, all the services are selected to be installed; we can make changes based on the requirements and then click on the Next button.
We will go to a web page to assign hosts as masters, slaves, and clients, as shown in the following screenshot:
Next, we will go to a web page for customizing services, for example, configuring the location for the NameNode directory.
In this step, some services such as Hive and Nagios may ask you to enter administrative usernames and passwords, which are required for service installation.
After everything is configured, we will get a summary page about our configurations, as shown in the following screenshot:
By clicking on the Deploy button, the cluster deployment will start and a progress bar will appear for each service that is being installed, as shown in the following screenshot:
After the deployment completes, we will get a Summary page as shown in the following screenshot:
By clicking on the Complete button, the cluster installation process will finish, and we will be able to see the status of the cluster as shown in the following screenshot:
Monitoring a Hadoop cluster with Chukwa Chukwa is a project developed for collecting and analyzing Hadoop logs.
It uses HDFS as its storage architecture and contains a number of toolkits for log analysis and cluster monitoring.
In this recipe, we will guide you through steps to configure Chukwa to monitor a Hadoop cluster.
Getting ready The latest release of Chukwa uses HBase for key-value storage.
So, before getting started, we assume that Hadoop and HBase have been installed and properly configured.
Next, we can use the following steps to install Chukwa:
Change the environment variables by adding the following content into the ~/.bashrc file:
Build Chukwa from the source with the following commands: source ~/.bashrc.
When the compilation finishes, the directory structure of Chukwa will be similar to the following: chukwa.
If you are downloading the binary version of Chukwa, the directory structure might be different from this.
Install telnet with the following command: sudo yum install telnet.
Log in to the master node from the administrator machine with the following command:
Use the following steps to configure Chukwa for Hadoop monitoring:
Copy the Hadoop metrics file from the Chukwa directory to the Hadoop configuration directory with the following command:
The command will give output similar to the following: HBase Shell; enter ‘help<RETURN>’ for list of supported commands.
The output shows that six HBase tables have been created.
By doing this, we have configured Chukwa to use HBase for log collection storage.
Multiple collectors can increase the throughput of the data collection process.
The output of this command is similar to the following:
Start the web UI for Hadoop Infrastructure Care Center (HICC) with the following command: chukwa hicc.
Open the URL http://master:4080 and use admin as both the username and password for login.
Chukwa was designed to collect data that are dynamically generated across distributed machines in a cluster.
It has four major components: adaptor, collector, MapReduce or other data processing jobs, and the web UI called HICC.
Adaptors run on these machines in the umbrella of agents, which send the collected data to collectors on local or remote machines.
Chukwa collectors pipeline data from agents to data storage systems such as HDFS.
For performance reasons, data are typically written as sequence files, which will be processed by MapReduce jobs.
The MapReduce jobs will generate key-value pairs for visualization by the last component—HICC.
Sometimes, you might have problem running the HICC and visiting the web UI; the most probable reason is that you have incompatible .jar files for Hadoop, HBase, and Chukwa.
For more information, please check the README.txt file in the source code root directory.
In summary, the data flow of Chukwa can be described with the following diagram:
For more information about the design of Chukwa, please check the URL: http://incubator.apache.org/chukwa/docs/r0.5.0/design.html.
Due to the instability of this software package, you might need to do some debugging when deploying it onto the cluster.
But to get started with Chukwa, it is always advisable to run it in local or pseudo-distributed mode, which will start one agent and one collector on the local machine.
We can check the status of the daemons by tailing on the logfiles with the following commands:
Currently, Chukwa is an incubator project under the Apache Free Software (AFS) foundation.
We can check the development plan and progress on its wiki page at wiki.apache.
Introduction Hadoop performance tuning is a challenging task, mainly due to the distributed feature of the system.
The sheer number of configuration properties can tell, from another perspective, how complicated it is to configure a Hadoop cluster.
Many of the configuration parameters have an effect on the performance of the cluster.
Sometimes, different settings of the properties can lead to dramatic performance differences.
And some properties are more relevant and sensitive than others with regard to the performance of a Hadoop cluster.
A systematic way of performance tuning is to tune the components based on their contribution on the cluster performance.
So, configurations that are closely related to I/O requests should be the first priority for performance tuning.
For example, suboptimal configuration on data replication properties can cause a large number of data block copies over the network, which will pose a negative effect on the performance of a cluster.
Similarly, improper JVM configurations can cause large data swaps for intermediate data.
And, unbalanced data block distribution on the DataNodes can cause the suboptimal execution of map and reduce tasks.
The first step of Hadoop performance tuning is to understand how Hadoop MapReduce works with different configuration property settings.
Based on this, optimized configurations or best practices can be derived.
It requires techniques, such as data collection by running controlled experiments under different parameter settings, data analysis using optimization techniques, and analytical and reasoning skills.
As a matter of fact, due to the challenges and novelty, of Hadoop cluster performance tuning, the research community has recent projects and publications about learning and tuning the performance of a Hadoop cluster (for example, the starfish project at http://www.cs.duke.edu/starfish/)
While the method of finding the optimal parameter configurations is straightforward for tuning the performance of a Hadoop cluster, the implementation is demanding, both theoretically and in practice.
Various tools and strategies for optimizing Hadoop performance have been developed and adopted by the Hadoop community.
For example, balancer is a tool used for balancing skewed data blocks and speculative execution is a strategy for launching a speculative task for a slowly progressing task.
In this chapter, we will introduce the following topics in general:
In other words, the optimal configuration for one cluster might not be optimal for another cluster under different hardware configurations.
So, to find the optimal settings for a specific cluster, real field work is needed.
Benchmarking of a Hadoop cluster is the first step to tune the performance of a Hadoop cluster.
We can also use Hadoop benchmarks to identify configuration problems and use it as reference for performance tuning.
For example, by comparing the local benchmark with clusters with similar configurations, we can have a general understanding of the cluster performance.
Typically, we benchmark a Hadoop cluster after the cluster is newly configured and before putting it to service to accept jobs.
This is because, when clients can submit jobs, the benchmarks can be perplexed by the client's jobs to show the real performance of a Hadoop cluster, and also the benchmark jobs can cause inconveniences for the clients.
In this section, we will introduce how to benchmark and stress test a Hadoop cluster using the tests and examples package included in the Hadoop distribution.
More specifically, we will test the read/write performance of the HDFS cluster.
In addition, we will test the failure resilience of the MapReduce framework and the performance of the MapReduce cluster under stress.
Getting ready To get started with Hadoop cluster benchmarks, we assume that a working Hadoop cluster has been configured and all the daemons are running without any issues.
We also assume that the required environment variables have been properly set in proper locations.
Log in from the Hadoop cluster administrative machine to the cluster master node with the following command:
If there are no errors, the test is considered successful.
Similarly, we can benchmark the distributed read consistency on the distributed filesystem with the following command:
From the output of the two (read/write consistency check) commands, we know that writing is more expensive than reading for the HDFS cluster.
This is because the write operations, in turn, involve more operations, such as computing and recording the checksums of the data blocks and many more.
The following recipe can be used to benchmark a MapReduce cluster:
The mapredtest benchmark does a load test on the MapReduce computing framework.
This benchmark is done with random integers, which are generated, written to files, read read back from files, and tested with the original files.
The command will intentionally cause task and TaskTracker failures on a running job.
So, during the test, we will be able to see messages such as: Killing a few tasks.
The failed tasks will be re-executed on a different TaskTracker.
So, killing one or a few tasks should not fail a job if the cluster is resilient to failures.
If a job fails after a few killed tasks, it is possibly because MapReduce is not reliable enough or not resilient to failures, and hence reliability tuning (such as by adding more computing TaskTrackers) is needed.
This command will generate an output similar to the following:
This output tells us that the average runtime is about 45 seconds.
The result of this command will be similar to the following:
This benchmark will generate an output similar to the following:
By sorting random data, we can peek the health of our Hadoop cluster.
The following steps can be used to benchmark Hadoop sort:
This command will validate the accuracy of the sort algorithm.
If there is no problem on the sort algorithm, we will get the following message:
How it works… We can get the usage of the Hadoop benchmark of the test package with the following command:
An example program must be given as the first argument.
Valid program names are: DFSCIOTest: Distributed i/o benchmark of libhdfs.
MRReliabilityTest: A program that tests the reliability of the MR framework by injecting faults/failures TestDFSIO: Distributed i/o benchmark.
We can get the usage for the testfilesystem benchmark with the following command:
The following table shows the meaning of each option of this benchmark command:
Option Description -files Number of files to generate for the benchmark.
We can get the help for the mrbench benchmark with the following command:
We can get the usage of the loadgen benchmark with the following command:
The create_write operation must be run before running the other operations.
Make sure this is far enough into the future, so all maps (operations) will start at the same time>
The output tells us that only the -operation option is mandatory, all the others are optional.
We can use the testbigmapoutput benchmark with the following command:
The -input and -output options are mandatory for this benchmark and the -create option, which specifies the file size that is to read created, is optional.
We can get the usage of the mapredtest benchmark with the following command:
We can use the testmapredsort benchmark with the following command:
Option Description -sortInput Directory for the input data used for sort.
The specified directory must exist, otherwise the benchmark will fail.
Besides the Hadoop tests package, Hadoop is shipped with an example package, which can also be used to benchmark a Hadoop cluster.
We can get all the examples benchmarks with the following command:
The meaning of each option is shown in the following table:
Analyzing job history with Rumen Rumen is a tool for extracting well-formatted information from job logfiles.
It parses logs and generates statistics for the Hadoop jobs.
The job traces can be used for performance tuning and simulation.
The TraceBuilder takes job history as input and generates easily parsed json files.
The folder is a utility to manipulate on input traces, and, most of the time, it is used to scale the summarized job traces from the TraceBuilder.
For example, we can use the folder tool to scale up (make time longer) or down (make time shorter) the job runtime.
In this recipe, we will outline steps to analyze the job history with Rumen.
Getting ready Before getting started, we assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the cluster master node using the following command:
Use the following steps to analyze job history with Rumen:
Use the TraceBuilder to extract the Gold Trace from the Hadoop job history files.
This command will recursively extract job history traces as well as the topology of the cluster from the Hadoop job history directory $HADOOP_HOME/logs/ history/done.
The -recursive option tells the TraceBuilder to scan the job history directory recursively.
The jobtraces.json output file will contain all the metrics of MapReduce jobs, similar to the following:
The second step of using Rumen is to scale the data generated from the previous step.
In this command, the -output-duration option defines the final runtime of the job trace, and the default value for this option is one hour.
The -input-cycle option is mandatory, and it defines the basic unit of time for the folding operation.
It generates a number of synthetic MapReduce jobs and builds a model based on the performance of these jobs.
Resource profiles of the cluster are modeled based on the job execution metrics.
The profiles can help us find performance bottlenecks of the cluster.
In this section, we will outline steps for benchmarking Hadoop with GridMix.
Getting ready We assume that our Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in to the Hadoop cluster node from the administrator machine using the following command:
All other configurations should follow rules similar to this one.
By default the generated data will be compressed with a block compression ratio of 4
Three jobs will be started as shown in the output message:
We can see an output message similar to the following:
It is generally used to model the performance profile of a Hadoop cluster by running a number of jobs.
We can configure this file to change, for example, the size of the generated data file.
Then, when executing the rungridmix_2 script, a number of jobs will be generated and submitted in batch mode.
In the end, the running time of these jobs will be computed.
GridMix2 is shipped with the following representative jobs: streamSort, javaSort, webdataSort, combiner, and monsterSort.
A GridMix benchmark is a mix of a number of small, medium, and large jobs from different categories.
Based on the specification, a number of jobs will be created and submitted to the Hadoop cluster until it finishes.
So, the generated data and programs will be on HDFS.
Make the generateData.sh script executable with the following command: chmod +x generateData.sh.
GridMix1 is composed of a number of high-level scripts to control how benchmark jobs work.
The tree structure of the GridMix1 directory is similar to the following:
The GridMix1 directory contains a few template jobs with different sizes.
To run a small javasort job, we can use the following command:
We can use similar commands to run medium and large jobs.
In addition, the GridMix3 job mix for a Hadoop cluster is typically described with a job trace file, which is generated from job configuration files using Rumen.
Sometimes, we might get a CRC exception when running this command.
We can delete the latter file to ignore the .crc check.
This command will generate an output similar to the following:
To acquire the usage and available parameters for GridMix3, we can use the following command:
See also f The Benchmarking and profiling a Hadoop cluster recipe.
Hadoop Vaidya is an open source, rule-based performance diagnostic framework for Apache Hadoop.
For example, Hadoop cluster administrators can use Vaidya to identify slow progressing jobs that are wasting cluster resources.
Hadoop clients can use Vaidya to identify configuration mistakes for their submitted jobs.
Hadoop Vaidya is extensible; users can analyze Hadoop job with their own rules.
In this recipe, we will outline steps to configure Hadoop Vaidya for Hadoop cluster performance diagnosis.
Getting ready Before getting started, we assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the master node machine using the following command:
Locate the directory for the job configuration file you want to analyze.
This command assumes that at least one job has been run so that at least one job configuration file can be found.
For purpose of illustration, we assume that a teragen job has finished before running this command.
The first XML file in the output is the job configuration file and the second one is the job logfile.
Use Vaidya to analyze the job trace files with the following command:
Note that the file location should be an absolute path including the schema, which is either hdfs:// or file://
This command will generate the report.txt file with content similar to the following:
We can get the options for Vaidya with the following command:
Path should be available : on local file system and be specified as as an absolute file path.
It has nine typical workloads, including micro, HDFS, web search machine learning, and data analytics benchmarks.
For example, it supports benchmarks for Nutch (a text indexing software package), PageRank (the PageRank algorithm), the Mahout machine learning algorithms, and Hive queries.
The HiBench project and the paper provide good examples of Hadoop benchmarking.
See also f The Benchmarking and profiling a Hadoop cluster recipe.
Balancing data blocks for a Hadoop cluster HDFS stores data blocks on DataNode machines.
Over time, some DataNodes can host much more data blocks than others.
This unbalanced distribution of data on the cluster is called data skew.
Data skew is a big problem for a Hadoop cluster.
We know that when the JobTracker assigns tasks to TaskTrackers, it follows the general rule of being data local, which means the map tasks will be assigned to those hosts where data blocks reside in.
If the data block storage distribution is skewed, or in other words, the data blocks locate only on a small percentage of DataNodes, only those nodes with data blocks can follow the data local rule.
Also, if JobTracker assigns tasks to other nodes that do not have data hosted locally, the data needs to be transferred from remote machines to the TaskTracker machine.
The data transfer will cost a large amount of network bandwidth, downgrading the overall performance of the cluster.
To deal with the data skew problem on HDFS, Hadoop is shipped with a balancer tool.
It can be configured either manually or automatically to rebalance the data blocks.
Getting ready To get started with balancer, we assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the cluster administrator machine to the master node using the following command:
Use the following steps to balance HDFS data blocks with balancer:
Check the data skew through the web UI, for example, by the opening URL http://master:50070/
The web page will be similar to the following screenshot:
Use the following command to balance the data blocks on the DataNode machines: hadoop balancer -threshold 0.2
This command will take some time to finish depending on the status of the distributed filesystem as well as the value for the –threshold option.
The -threshold option specifies the threshold for whether the cluster is balanced.
A smaller value for this option leads to a more even distribution of the data blocks.
On the other hand, it will require more time to finish.
Setting this option to be 0 is not recommended because it is not practical to achieve an ideal balance.
Alternatively, we can start the Hadoop balancer daemon to automatically balance the data blocks on HDFS.
The balancer will move data blocks among the DataNodes according to the space utilization.
For example, it will move data blocks from highly utilized nodes to the less utilized nodes.
We can get the updated DataNode information from the NameNode after each iteration.
If the cluster is already balanced, we will get an output similar to the following:
To stop the balancer, we can use the following command:
How it works… The Hadoop balancer balances data blocks on HDFS according to a preconfigured threshold value, which sets the target for whether the cluster is balanced or not.
A node is considered balanced if the difference between space utilization of the node and space utilization of the cluster is less than the threshold.
Sometimes, we want to limit the percentage of bandwidth used by the balancer.
By configuring this property to be a higher value, the balancing speed will be faster, but more resources will be used.
We need to restart HDFS to make this change take effect.
Choosing a proper block size HDFS stores data as data blocks distributed on multiple machines.
So, when a large file is put onto HDFS, it will first be split into a number of data blocks.
These data blocks are then distributed by the NameNode to the DataNodes in the cluster.
The granularity of the data blocks can affect the distribution and parallel execution of the tasks.
Based on the property of the jobs being executed, one block size might result in better performance than others.
We will guide you through steps to configure a proper block size for the Hadoop cluster.
Getting ready We assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the master node using the following command:
Configure the proper HDFS block size using the following steps:
Now, we can find the block size that achieves the best performance.
For example, by setting block size to be 64 MB, we can get the best performance.
Using compression for input and output A typical MapReduce job uses parallel mapper tasks to load data from external storage devices, such as hard drives to the main memory.
When a job finishes, the reduce tasks write the result data back to the hard drive.
In this way, during the life cycle of a MapReduce job, many data copies are created when data is relayed between the hard drive and the main memory.
Sometimes, the data is copied over the network from a remote node.
Copying data from and to hard drives and transfers over the network are expensive operations.
To reduce the cost of these operations, Hadoop introduced compression on the data.
Data compression in Hadoop is done by a compression codec, which is a program that encodes and decodes data streams.
Although compression and decompression can cause additional cost to the system, the advantages far outweigh the disadvantages.
In this section, we will outline steps to configure data compression on a Hadoop cluster.
Getting ready We assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the cluster master node using the following command:
Use the following steps to configure input and output data compression for a Hadoop cluster:
The property specifies Hadoop to use the Gzip codec for data compression.
This will change the sequence file output compression type from the default type RECORD to BLOCK.
By setting this property to be NONE, we will disable the compression of the sequence file outputs.
Individual records will be compressed with the RECORD compression type and a number of records will be compressed with the BLOCK compression type.
Generally, the BLOCK compression type is more efficient than the RECORD type, so it is recommended.
To disable it, which is the default, we can change the value to be false or remove this configuration property from the configuration file.
The following table is a summary of properties for configuring Hadoop data compression:
Configuring speculative execution Speculative execution is a proactive performance boosting strategy used by JobTracker to execute one task on two TaskTracker instances.
When either of these tasks finishes, the other task will be killed.
Speculative execution can be helpful to improve the performance of MapReduce jobs by reducing the execution time for slowly progressing tasks.
For example, on heterogeneous Hadoop clusters with different hardware configurations, low performance computing nodes can greatly prolong the execution time of a MapReduce job.
Speculative execution can remedy this problem by prioritizing the high performance nodes for MapReduce tasks execution.
On the other hand, speculative execution can negatively affect the performance of the cluster when a lot of resources are used for speculative execution.
For example, many tasks will have to wait for slots that are used for speculative execution.
In this recipe, we will list steps to configure Hadoop speculative execution.
Getting ready We assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the cluster master node using the following command:
In this recipe, we assume all the property configurations will make changes to the $HADOOP_HOME/conf/ mapred-site.xml file.
We can use the following recipe to configure Hadoop speculative execution:
This configures, at maximum, 20 percent of the tasks of a job to run speculatively.
This property is used to test if a task needs to be executed speculatively.
This property is used to test if a TaskTracker qualifies to run speculative tasks.
When speculative execution is enabled, some tasks will get killed.
The web page will be similar to the following screenshot:
If speculative execution has been enabled for a Hadoop cluster, we can still disable it for specific jobs.
For example, when we write MapReduce jobs using Java programming language, we can use the following code snippet to disable speculative execution for this job:
The following table is a summary of the properties we used in this recipe with their default values:
Specifically, a speculative task for a regular task will start when the following conditions are met:
Setting proper number of map and reduce slots for the TaskTracker.
The number of map and reduce slots determines the number of concurrent map/reduce tasks for a TaskTracker, which forks multiple JVMs to run these tasks.
In this recipe, we will give you a general view of setting proper number of these slots for the TaskTracker.
Getting ready We assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the cluster master node using the following command:
Use the following steps to configure map/reduce slots for a TaskTracker:
Tuning the JobTracker configuration In a Hadoop cluster, the JobTracker is responsible for managing jobs and tasks.
The performance of the JobTracker is critical for the whole cluster.
Hadoop provides a few properties for administrators to tune the JobTracker.
In this recipe, we will list the steps to configure the JobTracker.
Getting ready We assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the cluster master node using the following command:
The default value of this property is -1, which ignores the limit.
By default, this property is disabled, and the JobTracker will start afresh.
The job history data, which will be dumped to disk, will be used for job recovery.
This property sets a limit on the maximum number of tasks for each job before it gets preempted by the job scheduler.
It is related to the scheduling of jobs and tasks.
How it works… The following table is a list of properties with descriptions:
Tuning the TaskTracker configuration TaskTrackers accept tasks from the JobTracker in a cluster and forks JVMs to run the tasks.
A couple of TaskTracker properties can be configured based on the configuration of the cluster.
In this section, we will list steps to configure the TaskTracker property.
Getting ready We assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the cluster master node using the following command:
This property specifies the heartbeat time interval, in milliseconds, after which it will be marked lost by the JobTracker.
This property configures the sleep time, in milliseconds that, the TaskTracker waits before sending a SIGKILL signal to a process after it has been sent a SIGTERM signal.
This property configures the maximum memory that a TaskTracker uses for an index cache when serving map output to reducers.
This property configures the interval, in milliseconds, that the TaskTracker monitors the tasks' memory usage.
It is only meaningful when a tasks' memory management has been enabled using the mapred.
The default value for this property is false, which disables the out-of-band heartbeat.
By this configuration, a failed task will be retried up to three times before being declared failed.
Similar to the maximum attempts configuration for a map task, this property configures to retry a failed reduce task up to three times before declaring failed.
How it works… The following table contains a list of properties with descriptions:
Tuning shuffle, merge, and sort parameters In a MapReduce job, map task outputs are aggregated into JVM buffers.
The size of the in-memory buffer determines how large the data can be merged and sorted at once.
Too small a buffer size can cause a large number of swap operations, incurring big overhead.
In this section, we will show best practices for configuring the shuffle, merge, and sort parameters.
Getting ready We assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the cluster master node using the following command:
To minimize seeks, we typically assign 1 MB for each merge stream.
This property configures the number of data streams to merge when sorting files.
This property configures the percentage of memory used for record boundary tracking.
This property enforces a soft limit on the in-memory buffer used for sorting or recording the collection.
A background thread will start to spill data to disk if the limit is reached.
This property configures a threshold with regard to the number of files for the in-memory merge process.
When the threshold number of files has been accumulated, the merge process will start and results will be spilled to the disk.
If the value of this property is set to equal or less than zero, there will be no threshold and the merge process will only be triggered by the memory consumption for data processing.
Configure the percentage of memory to be allocated from the maximum heap size to storing map outputs during the shuffle by changing the following property:
This property configures the percentage, in terms of the maximum heap size, of memory used to store map outputs during the shuffle phase.
The default value of this property is 0.66, or approximately two thirds of the memory.
This property configures a percentage threshold, in terms of the maximum heap size, of memory used to store map outputs during the reduce phase.
To begin the reduce phase, the memory used by the map output should be less than the configured threshold.
The default value for this property is 0.0, which means no map output memory consumption threshold is needed to start the reduce phase.
This property configures the maximum number of reducer retries to fetch map outputs in case of fetch failure.
How it works… The following table shows the description of the properties and their default values:
See also f The Configuring memory for a Hadoop cluster recipe.
Configuring memory for a Hadoop cluster Hadoop has a few memory configuration properties.
Their values should be set according to the configurations of the cluster.
In this recipe, we will outline steps to configure these memory properties.
Getting ready We assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the cluster master node using the following command:
We can use the following steps to configure memory properties for a Hadoop cluster:
This property configures the memory size, in terms of the virtual memory, used by the scheduler for a map slot.
The default value of this property is -1, which disables this property.
The default value for this task is -1, which ignores this property.
See also f The Setting proper number of map and reduce slots for the TaskTracker recipe.
Setting proper number of parallel copies When all or part of the map tasks finish, map outputs will be copied from the map task nodes to the reduce task nodes.
The parallel copying strategy is used to increase the transfer throughput.
By tuning this property, we can boost the performance of our Hadoop cluster.
In this recipe, we will outline steps to configure the number of multiple copies for transferring map outputs to reducers.
Getting ready We assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the cluster master node using the following command:
Use the following recipe to configure the number of parallel copies:
Tuning JVM parameters Configuring JVM properties plays a very important role in the performance tuning of a Hadoop cluster.
In this recipe, we will outline steps to configure the JVM.
Getting ready We assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the cluster master node using the following command:
This property configures the JVM options for TaskTracker child processes, which, by default, will have the same options as the TaskTracker.
Configuring JVM Reuse MapReduce tasks are executed by JVM processes/threads, which are forked by the TaskTracker.
The creation of a JVM, which includes the initialization of execution environments, is costly, especially when the number of tasks is large.
In the default configuration, the number of JVMs needed to finish a job should be equal to the number of the tasks.
In other words, the default setting uses one JVM to execute one task.
When the execution of a task completes, its JVM will be killed by the TaskTracker.
If it is enabled, multiple tasks can be executed sequentially with one JVM.
In this recipe we will outline the steps to configure JVM Reuse.
Getting ready We assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the cluster master node using the following command:
The default value of this property is 1, which disables JVM Reuse.
If this property is set to -1, the number of tasks a JVM can execute is unlimited.
Configuring the reducer initialization time Reduce tasks can be started when a certain percentage of map tasks has been finished.
By setting this property with a smaller number, the reduce tasks will start earlier, occupying the computing slots.
On the other hand, if the number is set too large, for example, very close to 1, the reduce tasks will have to wait for the majority of the map tasks to finish, prolonging the job execution time.
In this recipe, we will outline steps to configure reducer initialization.
Getting ready We assume that the Hadoop cluster has been properly configured and all the daemons are running without any issues.
Log in from the Hadoop cluster administrator machine to the cluster master node using the following command:
EC2 offers platform as a service (PaaS), with which we can start up theoretically an unlimited number of servers on the cloud.
From the previous chapters of this book, we know that the configuration of a Hadoop cluster requires a big amount of hardware investment.
For example, to set up a Hadoop cluster, a number of computing nodes and networking devices are required.
Comparatively, with the help of AWS cloud computing, especially EC2, we can set up a Hadoop cluster with minimum cost and much less efforts.
In this chapter, we are going to discuss topics of configuring a Hadoop cluster in the Amazon cloud.
We will guide you through the recipes of registering with AWS, creating Amazon Machine Image (AMI), configuring a Hadoop cluster with the new AMI, and so on.
In this recipe, we will outline steps to do this.
Getting ready We assume to use a GUI web browser for AWS registration.
So, we are assuming you already have a web browser with Internet access.
In addition, personal information needs to be prepared to fill the online registration forms.
We will use the following steps to register with AWS:
Use a web browser to open the following link: http://aws.amazon.com/
Click on the Sign Up button on the upper-right corner of the window.
You will be directed to a web page as shown in the following screenshot:
Fill in the e-mail address in the text field with the label My e-mail address is:, and select the I am a new user radio button as shown in the previous screenshot.
Click on the Sign in using our secure server button at the bottom as shown in the previous screenshot.
Fill the Login Credentials form, which includes name, e-mail, and password as shown in the following screenshot:
Click on the Continue button at the bottom as shown in the previous screenshot.
Click on the Create Account and Continue button at the bottom as shown in the previous screenshot.
Now we can log in to AWS with the newly created account by using the I am a returning user and my password is: option as shown in the following screenshot:
They are used for remote access of the cloud servers on AWS.
For example, in this chapter, we will use these credentials to log in to the servers remotely from a client machine.
This recipe will guide you through the steps to create, download, and manage these security credentials.
Getting ready Before getting started, we assume that you have successfully registered with AWS; otherwise, you need to follow the steps in the previous recipe to register with AWS.
We also assume that we have a client machine with Linux (such as CentOS) installed.
The machine should be able to access the Internet and has at least one GUI web browser installed.
Create a directory for storing AWS credentials using the following command:
Open a web browser and go to the URL aws.amazon.com.
Click on the My Account / Console drop-down button on the upper-left of the window as shown in the following screenshot:
Click on the Security Credentials option in the drop-down list as shown in the previous screenshot.
If you have logged in to AWS previously, you will be able to visit the Security Credentials Management page.
You need to type in the username and password and log in using the I am a returning user and my password is: option.
Currently, Amazon AWS has a few types of credentials as shown in the following screenshot:
By clicking on the Make Inactive link on the status column of the access keys table, we can make the access keys inactive.
Inactive access keys can be made active again and can be deleted from the list.
Similarly, we can make an X.509 certificate inactive as seen in the preceding screenshot.
Inactive certificates can be made active again or deleted from the list.
By clicking on the Create a new Certificate link, we will be able to create a new certificate as shown in the following screenshot:
We need to download the private key file as well as the X.509 certificate by clicking on the buttons as shown in the previous screenshot.
These files should be kept secured and never shared with any other person.
Key pairs used for EC2 can be managed from the management console as shown in the following screenshot:
New key pairs can be created by clicking on the Create Key Pair button at the top of the window.
A pop-up window will be used to type in the name of the key pair.
And, the newly created key pair will be downloaded to the local machine.
Use the following command to copy the downloaded key pair to the .ec2 folder:
The following table shows the usage of each security credential:
Sign-In Credentials Log in to AWS from the web portal.
In this recipe we will list steps to configure a local machine for EC2 connection.
Getting ready Before getting started, we assume that we have registered with AWS and security credentials have been created.
We also assume that a machine with Linux has been installed.
Use the following steps to configure a local machine for EC2 remote access:
Get the Access Key ID and the Secret Access Key from the security credentials web page as shown in the following screenshot:
If the configuration has no problem, we will get a list of AMIs; otherwise, we will get an error message similar to the following:
An AMI is a template that contains configuration for operating systems and software packages.
We can start EC2 instances from preexisting or personalized AMIs.
Generally, there are two types of AMIs one is EBS-backed AMI (Elastic Block Storage-backed AMI) and the other is instance store-backed AMI.
In this recipe, we will first outline steps to create an instance store-backed AMI and briefly introduce how to create an EBS-backed AMI.
Getting ready Before getting started, we assume that you have successfully registered with AWS.
And, we also assume that a client machine has been configured properly to connect to AWS.
Use the following steps to create an instance store-backed AMI:
In this command, if specifies the input of the data, /dev/zero is a special device on Linux systems, of specifies the output of the command, where we specify a filename as the image name, bs specifies the size of the blocks, and count is the number of blocks for input to output.
The size of the output file, centos.img, is determined by the block size and count.
Check the size of the image file using the following command: ls -lh centos.img.
Create a root filesystem inside the image file using the following command: mke2fs -F -j centos.img.
Create a directory under the /mnt directory using the following command: sudo mkdir -v /mnt/centos.
Mount the image file to the folder using the following command: sudo mount -o loop centos.img /mnt/centos.
Create the /dev directory under the root directory of the mounted filesystem using the following command: sudo mkdir -v /mnt/centos/dev.
Create a minimal set of devices using the following commands:
These commands will give us the following output: MAKEDEV: mkdir: File exists.
The reason for these warning messages is because the parent directories already exist.
When the MAKEDEV command tries to create the folder with the mkdir command, it will fail and display this warning message.
Create the fstab configuration file using the following command: sudo mkdir -pv /etc/fstab.
Create the proc folder under the root filesystem of the image file using the following command: sudo mkdir -pv /mnt/centos/proc.
Mount a proc filesystem to the /mnt/centos/proc directory using the following command: sudo mount -t proc none /mnt/centos/proc.
The --disablerepo option disables all the available repositories, and the-enablerepo option enables only the CentOS repository specified in the previous step.
This command will start the installation of CentOS 6.3 on the mounted directory, which will take a while depending on the network speed and host system hardware configurations.
When the installation is complete, we can verify the installation using the following command:
The directory structure of the installed operating system should be the same as the directory structure of a regularly installed Linux.
For example, the output will be similar to the following:
In this configuration, BOOTPROTO specifies to use DHCP IP address assignment.
These two lines configure the mount points for the swap and root partitions.
Configure to start necessary services using the following commands: sudo chroot /mnt/centos /bin/sh.
When you are prompted to enter the paraphrase, leave it empty by pressing the Enter key.
Download the latest Hadoop distribution from the official mirror website http://www.apache.org/dyn/closer.cgi/hadoop/common/
Download and install the other ecosystem components using the steps outlined in the recipes in Chapter 3, Configuring a Hadoop Cluster.
We will use the following steps to bundle, upload, and register an AMI:
Option -i specifies the image filename, -k specifies the private key file, -c specifies the certificate file, and -u specifies the user account number, which is a 12-digit numeric string.
The account number is on the upper-left of the window as shown in the following screenshot:
The command will ask for the architecture of the image, and then it will bundle the image with the user's security credentials and split the bundled image file into smaller files.
Create a bucket from the S3 web interface as shown in the following screenshot:
Type in the bucket name and select the region based on your location as shown in the following screenshot:
Click on the Create Bucket button and the bucket will be successfully created as shown in the following screenshot:
This command will upload the bundled image parts to the specified bucket (packtbucket in this case), which is specified with the -b option.
The option -m specifies the location of the manifest file, the option -a specifies the access key, and -s specifies the secret key.
Note that for security purposes, the manifest file will be encrypted with the public key before being uploaded.
Uploading bundled image parts to the S3 bucket packt-bucket ...
When the upload completes, we can check the content of the bucket by clicking on the bucket name.
The bucket should now contain all the image parts as well as the manifest file as shown in the following screenshot:
The command will give us an ID for the newly registered AMI similar to the following:
The AMI registration step is required in order for EC2 to find the AMI and run instances with it.
Note that once changes are made on the image part files stored on S3, re-registration is required in order for the changes to take effect.
The product codes, if any, that are attached to the instance.
The ID of the kernel associated with the image (machine images only)
The ID of the RAM disk associated with the image (machine images only)
This command specifies to run the instance with our new AMI, option -n specifies the number of instances to start, and option -k specifies the key pair to use for logging into these instances.
The first line of the output is the reservation information, and the meanings of the columns are:
The name of each security group the instance is in.
The second line shows the instance information, and the meanings of columns are:
The AMI ID of the image on which the instance is based.
This is only present for instances in the running state.
This is only present for instances in the running state.
If a key was associated with the instance at launch, its name will appear.
The output message shows that the ID of the instance is i-0020e06c.
The instance status tells us that it is in the running state.
Alternatively, we can check the status of the instance from the web UI.
For example, we can get the status of the instance similar to the following screenshot:
As we mentioned previously, there are other methods to create AMIs.
One method is to create an AMI from an existing AMI.
Creating an AMI from an existing AMI This section lists steps to create an instance store-backed AMI from an existing AMI.
We assume that you have registered with AWS and have successfully configured the security credentials in a local machine.
We also assume that you have downloaded the key pair and saved it to the proper location.
In this section, we assume the private keys, certificates, and key pairs are all located in the .ec2 folder.
This command will start up one instance from the new AMI.
You will be prompted to enter the paraphrase; leave it empty by pressing the Enter key.
Download and install Java following the steps outlined in the Installing Java and other tools recipe of Chapter 2, Preparing for Hadoop Installation.
Download and install all other Hadoop ecosystem components by following the steps outlined in the recipes in Chapter 3, Configuring a Hadoop Cluster.
Install the AMI tools package using the following command: rpm -ivh http://s3.amazonaws.com/ec2-downloads/ec2-ami-tools.
In this command, -b specifies the name of the bucket on S3, -m specifies the location of the manifest file, -a specifies the access key string, and -p specifies the secret key string.
In this command, the first parameter specifies the location of the manifest file in the S3 bucket, the -n option specifies the name of the AMI, -O specifies the access key string, and -W specifies the secret key string.
This section will guide you through the steps to create an EBS-backed AMI from a running EC2 instance.
For more information, you can visit Amazon's official document at http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/creating-an-amiebs.html#process_creating-an-ami-ebs.
Start an instance by right-clicking on one of the AMIs and then click on Launch as shown in the following screenshot:
When the instance is running, log in to the instance and make changes according to your requirements.
Then, from the Web Management console, right-click on the running instance and then select Create Image (EBS AMI) as shown in the following screenshot:
Go to the AMIs tab of the AWS web console and select Owned By Me; we will see that EBSAMI is being created as shown in the following screenshot:
Similar to the image part files stored in S3, the snapshot stores the physical image of the EBS-backed AMI.
This recipe will outline steps to configure S3 as the distributed data storage system for MapReduce.
Getting ready Before getting started, we assume that you have successfully registered with AWS and the client machine has been successfully configured to access the AWS.
Use the following steps to configure S3 for data storage:
The first property configures Hadoop to use S3 as a distributed filesystem.
As we are using S3 instead of HDFS as the data storage filesystem, there is no need to start the HDFS cluster anymore.
Check the configuration with S3 using the following command: hadoop fs -ls /
We should be able to list all the files in the bucket.
Starting a Hadoop cluster with the new AMI is simple and straightforward.
This recipe will list steps to start up a Hadoop cluster with the new AMI.
Getting ready Before getting started, we assume that you have registered with AWS and have successfully created a new AMI with Hadoop properly configured.
Use the following steps to configure a Hadoop cluster with EC2:
Run a number of instances either from the command line or from the web interface.
The nodes.txt file will have contents similar to the following: ip-10-190-81-210
Use the following commands to create a hosts file: cp nodes.txt nodes.ip.txt.
Move the hosts file to /etc/hosts using the following command: for hosts in 'cat nodes.txt'; do.
When the cluster is running, we can start to submit jobs to the cluster from the master node.
An alternative method of running a MapReduce with the Amazon cloud is to use Amazon Elastic MapReduce (EMR)
In the following recipe, we will use the wordcount job, which is shipped with the Hadoop examples' JAR package, as an example.
We will use the following steps to use EMR for data processing:
Create the input directory (with the name input) and Java library directory (with the name jars) under the bucket from the S3 Web Management console.
Upload data into the input folder from the web console as shown in the following screenshot:
Click on the Create New Job Flow button as shown in the following screenshot:
Next, enter the Job Flow Name, select the Hadoop Version, and select the job flow type as shown in the following screenshot:
To test a simple job flow, you can choose Run a sample application instead.
Click on the Continue button at the bottom; the next window asks for the location of the JAR file and the parameters for running the Hadoop MapReduce job as shown in the following screenshot:
In this step, we need to specify the location of the JAR file and the arguments to run a job.
The specifications should be similar to option specifications from the command line with the only difference that all the files should be specified using the S3 scheme.
You can configure the instance type and the number of instances based on the job properties (for example, big or small input data size, data intensive, or computation intensive)
Click on the Continue button and we will go to the ADVANCED OPTIONS window.
This window asks for instance boot options such as security key pairs.
In this step, we can choose the key pair and use all others as defaults and click on Continue.
We can simply use the default action in this step and click on Continue.
The REVIEW window shows the options we have configured; if there is no problem, we can click on the Create Job Flow button to create an EMR job flow.
The job flow will be started and we can check the output when it completes.
We can get its status from the web console as shown in the following screenshot:
About Packt Publishing Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL Management" in April 2004 and subsequently continued to specialize in publishing highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and customizing today's systems, applications, and frameworks.
Our solution based books give you the knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past.
Our unique business model allows us to bring you more focused information, giving you more of what you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cuttingedge books for communities of developers, administrators, and newbies alike.
This book is part of the Packt Open Source brand, home to books published on software built around Open Source licences, and offering information to anybody from advanced developers to budding web designers.
The Open Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project about whose software a book is sold.
Writing for Packt We welcome all inquiries from people who are interested in authoring.
If your book idea is still at an early stage and you would like to discuss it first before writing a formal book proposal, contact us; one of our commissioning editors will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing experience, our experienced editors can help you develop a writing career, or simply get some additional reward for your expertise.
Learn how to crunch big data to extract meaning from the data avalanche.
Learn tools and techniques that let you approach big data with relish and not fear.
Shows how to build a complete infrastructure to handle your needs as your data grows.
Hands-on examples in each chapter give the big picture while also giving direct experience.
Recipes for analyzing large and complex datasets with Hadoop MapReduce.
Learn to process large and complex data sets, starting simply, then diving in deep.
Solve complex big data problems such as classifications, finding relationships, online marketing and recommendations.
More than 50 Hadoop MapReduce recipes, presented in a simple and straightforward manner, with step-by-step instructions and real world examples.
Realistic, simple code examples to solve problems at scale with Hadoop and related technologies.
Solutions to common problems when working in the Hadoop environment.
In depth code examples demonstrating various analytic models, analytic solutions, and common best practices.
Practical recipes to write your own MapReduce solution patterns for Hadoop programs.
Learn something new in an Instant! A short, fast, focused guide delivering immediate results.
Seven recipes, each describing a particular style of the MapReduce program to give you a good understanding of how to program with MapReduce.
Chapter 2: Preparing for Hadoop Installation Introduction Choosing hardware for cluster nodes Designing the cluster network Configuring the cluster administrator machine Creating the kickstart file and boot media Installing the Linux operating system Installing Java and other tools Configuring SSH.
Chapter 7: Tuning Hadoop Cluster for Best Performance Introduction Benchmarking and profiling a Hadoop cluster Analyzing job history with Rumen Benchmarking a Hadoop cluster with GridMix Using Hadoop Vaidya to identify performance problems Balancing data blocks for a Hadoop cluster Choosing a proper block size Using compression for input and output Configuring speculative execution Setting proper number of map and reduce slots for the TaskTracker Tuning the JobTracker configuration Tuning the TaskTracker configuration Tuning shuffle, merge, and sort parameters Configuring memory for a Hadoop cluster Setting proper number of parallel copies Tuning JVM parameters Configuring JVM Reuse Configuring the reducer initialization time.
