Hadoop: The Definitive Guide, the image of an elephant, and related trade dress are trademarks of O’Reilly Media, Inc.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a trademark claim, the designations have been printed in caps or initial caps.
While every precaution has been taken in the preparation of this book, the publisher and authors assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
A few of us were attempting to build an open source web search engine and having trouble managing computations running on even a handful of computers.
Once Google published its GFS and MapReduce papers, the route became clear.
They’d devised systems to solve precisely the problems we were having with Nutch.
So we started, two of us, half-time, to try to re-create these systems as a part of Nutch.
We managed to get Nutch limping along on 20 machines, but it soon became clear that to handle the Web’s massive scale, we’d need to run it on thousands of machines and, moreover, that the job was bigger than two half-time developers could handle.
Around that time, Yahoo! got interested, and quickly put together a team that I joined.
We split off the distributed computing part of Nutch, naming it Hadoop.
With the help of Yahoo!, Hadoop soon grew into a technology that could truly scale to the Web.
I already knew Tom through an excellent article he’d written about Nutch, so I knew he could present complex ideas in clear prose.
I soon learned that he could also develop software that was as pleasant to read as his prose.
From the beginning, Tom’s contributions to Hadoop showed his concern for users and for the project.
Unlike most open source contributors, Tom is not primarily interested in tweaking the system to better meet his own needs, but rather in making it easier for anyone to use.
Then he moved on to tackle a wide variety of problems, including improving the MapReduce APIs, enhancing the website, and devising an object serialization framework.
In short order, Tom earned the role of Hadoop committer and soon thereafter became a member of the Hadoop Project Management Committee.
Tom is now a respected senior member of the Hadoop developer community.
Though he’s an expert in many technical corners of the project, his specialty is making Hadoop easier to use and understand.
Given this, I was very pleased when I learned that Tom intended to write a book about Hadoop.
Martin Gardner, the mathematics and science writer, once said in an interview: Beyond calculus, I am lost.
It took me so long to understand what I was writing about that I knew how to write in a way most readers would understand.1
In many ways, this is how I feel about Hadoop.
Its inner workings are complex, resting as they do on a mixture of distributed systems theory, practical engineering, and common sense.
If there’s a common theme, it is about raising the level of abstraction—to create building blocks for programmers who just happen to have lots of data to store, or lots of data to analyze, or lots of machines to coordinate, and who don’t have the time, the skill, or the inclination to become distributed systems experts to build the infrastructure to handle it.
With such a simple and generally applicable feature set, it seemed obvious to me when I started using it that Hadoop deserved to be widely used.
However, at the time (in early 2006), setting up, configuring, and writing programs to use Hadoop was an art.
Things have certainly improved since then: there is more documentation, there are more examples, and there are thriving mailing lists to go to when you have questions.
And yet the biggest hurdle for newcomers is understanding what this technology is capable of, where it excels, and how to use it.
Over the course of three years, the Hadoop project has blossomed and spun off half a dozen subprojects.
In this time, the software has made great leaps in performance, reliability, scalability, and manageability.
To gain even wider adoption, however, I believe we need to make Hadoop even easier to use.
This will involve writing more tools; integrating with more systems; and.
I’m looking forward to being a part of this, and I hope this book will encourage and enable others to do so, too.
Administrative Notes During discussion of a particular Java class in the text, I often omit its package name to reduce clutter.
If you need to know which package a class is in, you can easily look it up in Hadoop’s Java API documentation for the relevant subproject, linked to from the Apache Hadoop home page at http://hadoop.apache.org/
Or if you’re using an IDE, it can help using its auto-complete mechanism.
The sample programs in this book are available for download from the website that accompanies this book: http://www.hadoopbook.com/
You will also find instructions there for obtaining the datasets that are used in examples throughout the book, as well as further notes for running the programs in the book, and links to updates, additional resources, and my blog.
What’s in This Book? The rest of this book is organized as follows.
Chapter 1 emphasizes the need for Hadoop and sketches the history of the project.
Chapter 3 looks at Hadoop filesystems, and in particular HDFS, in depth.
Chapter 4 covers the fundamentals of I/O in Hadoop: data integrity, compression, serialization, and file-based data structures.
Chapter 5 goes through the practical steps needed to develop a MapReduce application.
Chapter 6 looks at how MapReduce is implemented in Hadoop, from the point of view of a user.
Chapter 7 is about the MapReduce programming model and the various data formats that MapReduce can work with.
Chapter 8 is on advanced MapReduce topics, including sorting and joining data.
Later chapters are dedicated to projects that build on Hadoop or are related to it.
Finally, Chapter 16 is a collection of case studies contributed by members of the Apache Hadoop community.
This edition continues to describe the 0.20 release series of Apache Hadoop because this was the latest stable release at the time of writing.
New features from later releases are occasionally mentioned in the text, however, with reference to the version that they were introduced in.
Conventions Used in This Book The following typographical conventions are used in this book: Italic.
Indicates new terms, URLs, email addresses, filenames, and file extensions.
Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.
Constant width bold Shows commands or other text that should be typed literally by the user.
Constant width italic Shows text that should be replaced with user-supplied values or by values determined by context.
Using Code Examples This book is here to help you get your job done.
In general, you may use the code in this book in your programs and documentation.
You do not need to contact us for permission unless you’re reproducing a significant portion of the code.
For example, writing a program that uses several chunks of code from this book does not require permission.
Selling or distributing a CD-ROM of examples from O’Reilly books does require permission.
Answering a question by citing this book and quoting example code does not require permission.
Incorporating a significant amount of example code from this book into your product’s documentation does require permission.
An attribution usually includes the title, author, publisher, and ISBN.
Safari Books Online offers a range of product mixes and pricing programs for organizations, government agencies, and individuals.
For more information about Safari Books Online, please visit us online.
How to Contact Us Please address comments and questions concerning this book to the publisher:
We have a web page for this book, where we list errata, examples, and any additional information.
Find us on Facebook: http://facebook.com/oreilly Follow us on Twitter: http://twitter.com/oreillymedia Watch us on YouTube: http://www.youtube.com/oreillymedia.
I am grateful to my editor, Mike Loukides, and his colleagues at O’Reilly for their help in the preparation of this book.
Mike has been there throughout to answer my questions, to read my first drafts, and to keep me on schedule.
Finally, the writing of this book has been a great deal of work, and I couldn’t have done it without the constant support of my family.
My wife, Eliane, not only kept the home going, but also stepped in to help review, edit, and chase case studies.
My daughters, Emilia and Lottie, have been very understanding, and I’m looking forward to spending lots more time with all of them.
In pioneer days they used oxen for heavy pulling, and when one ox couldn’t budge a log, they didn’t try to grow a larger ox.
We shouldn’t be trying for bigger computers, but for more systems of computers.
That’s roughly the same order of magnitude as one disk drive for every person in the world.
The New York Stock Exchange generates about one terabyte of new trade data per day.
Facebook hosts approximately 10 billion photos, taking up one petabyte of storage.
Ancestry.com, the genealogy site, stores around 2.5 petabytes of data.
The Internet Archive stores around 2 petabytes of data and is growing at a rate of.
My wife’s grandfather was an avid photographer and took photographs throughout his adult life.
My family is producing photographic data at 35 times the rate my wife’s grandfather’s did, and the rate is increasing every year as it becomes easier to take more and more photos.
More generally, the digital streams that individuals are producing are growing apace.
Microsoft Research’s MyLifeBits project gives a glimpse of the archiving of personal information that may become commonplace in the near future.
The data gathered included a photo taken every minute, which resulted in an overall data volume of one gigabyte per month.
When storage costs come down enough to make it feasible to store continuous audio and video, the data volume for a future MyLifeBits service will be many times that.
The trend is for every individual’s data footprint to grow, but perhaps more important, the amount of data generated by machines will be even greater than that generated by people.
Machine logs, RFID readers, sensor networks, vehicle GPS traces, retail transactions—all of these contribute to the growing mountain of data.
The volume of data being made publicly available increases every year, too.
Organizations no longer have to merely manage their own data; success in the future will be dictated to a large extent by their ability to extract value from other organizations’ data.
Mashups between different information sources make for unexpected and hitherto unimaginable applications.
Take, for example, the Astrometry.net project, which watches the Astrometry group on Flickr for new photos of the night sky.
It analyzes each image and identifies which part of the sky it is from, as well as any interesting celestial bodies, such as stars or galaxies.
This project shows the kind of things that are possible when data (in this case, tagged photographic images) is made available and used for something (image analysis) that was not anticipated by the creator.
The bad news is that we are struggling to store and analyze it.
This is a long time to read all data on a single drive—and writing is even slower.
The obvious way to reduce the time is to read from multiple disks at once.
Imagine if we had 100 drives, each holding one hundredth of the data.
Working in parallel, we could read the data in under two minutes.
Using only one hundredth of a disk may seem wasteful.
But we can store one hundred datasets, each of which is one terabyte, and provide shared access to them.
We can imagine that the users of such a system would be happy to share access in return for shorter analysis times, and, statistically, that their analysis jobs would be likely to be spread over time, so they wouldn’t interfere with each other too much.
There’s more to being able to read and write data in parallel to or from multiple disks, though.
The first problem to solve is hardware failure: as soon as you start using many pieces of hardware, the chance that one will fail is fairly high.
A common way of avoiding data loss is through replication: redundant copies of the data are kept by the system so that in the event of failure, there is another copy available.
This is how RAID works, for instance, although Hadoop’s filesystem, the Hadoop Distributed Filesystem (HDFS), takes a slightly different approach, as you shall see later.
The second problem is that most analysis tasks need to be able to combine the data in some way, and data read from one disk may need to be combined with the data from any of the other 99 disks.
Various distributed systems allow data to be combined from multiple sources, but doing this correctly is notoriously challenging.
MapReduce provides a programming model that abstracts the problem from disk reads and writes,
We look at the details of this model in later chapters, but the important point for the present discussion is that there are two parts to the computation, the map and the reduce, and it’s the interface between the two where the “mixing” occurs.
This, in a nutshell, is what Hadoop provides: a reliable shared storage and analysis system.
The storage is provided by HDFS and analysis by MapReduce.
There are other parts to Hadoop, but these capabilities are its kernel.
Comparison with Other Systems The approach taken by MapReduce may seem like a brute-force approach.
MapReduce is a batch query processor, and the ability to run an ad hoc query against your whole dataset and get the results in a reasonable time is transformative.
It changes the way you think about data and unlocks data that was previously archived on tape or disk.
Questions that took too long to get answered before can now be answered, which in turn leads to new questions and new insights.
For example, Mailtrust, Rackspace’s mail division, used Hadoop for processing email logs.
One ad hoc query they wrote was to find the geographic distribution of their users.
This data was so useful that we’ve scheduled the MapReduce job to run monthly and we will be using this data to help us decide which Rackspace data centers to place new mail servers in as we grow.
By bringing several hundred gigabytes of data together and having the tools to analyze it, the Rackspace engineers were able to gain an understanding of the data that they otherwise would never have had, and, furthermore, they were able to use what they had learned to improve the service for their customers.
You can read more about how Rackspace uses Hadoop in Chapter 16
The answer to these questions comes from another trend in disk drives: seek time is improving more slowly than transfer rate.
Seeking is the process of moving the disk’s head to a particular place on the disk to read or write data.
It characterizes the latency of a disk operation, whereas the transfer rate corresponds to a disk’s bandwidth.
If the data access pattern is dominated by seeks, it will take longer to read or write large portions of the dataset than streaming through it, which operates at the transfer rate.
On the other hand, for updating a small proportion of records in a database, a traditional B-Tree (the data structure used in relational databases, which is limited by the rate it can perform seeks) works well.
For updating the majority of a database, a B-Tree is less efficient than MapReduce, which uses Sort/Merge to rebuild the database.
In many ways, MapReduce can be seen as a complement to a Rational Database Management System (RDBMS)
The differences between the two systems are shown in Table 1-1.) MapReduce is a good fit for problems that need to analyze the whole dataset in a batch fashion, particularly for ad hoc analysis.
An RDBMS is good for point queries or updates, where the dataset has been indexed to deliver low-latency retrieval and update times of a relatively small amount of data.
MapReduce suits applications where the data is written once and read many times, whereas a relational database is good for datasets that are continually updated.
Updates Read and write many times Write once, read many times.
Another difference between MapReduce and an RDBMS is the amount of structure in the datasets on which they operate.
Structured data is data that is organized into entities that have a defined format, such as XML documents or database tables that conform to a particular predefined schema.
Semi-structured data, on the other hand, is looser, and though there may be a schema, it is often ignored, so it may be used only as a guide to the structure of the data: for example, a spreadsheet, in which the structure is the grid of cells, although the cells themselves may hold any form of data.
Unstructured data does not have any particular internal structure: for example, plain text or image data.
MapReduce works well on unstructured or semistructured data because it is designed to interpret the data at processing time.
In other words, the input keys and values for MapReduce are not intrinsic properties of the data, but they are chosen by the person analyzing the data.
Relational data is often normalized to retain its integrity and remove redundancy.
Normalization poses problems for MapReduce because it makes reading a record a nonlocal operation, and one of the central assumptions that MapReduce makes is that it is possible to perform (high-speed) streaming reads and writes.
A web server log is a good example of a set of records that is not normalized (for example, the client hostnames are specified in full each time, even though the same client may appear many times), and this is one reason that logfiles of all kinds are particularly well-suited to analysis with MapReduce.
These functions are oblivious to the size of the data or the cluster that they are operating on, so they can be used unchanged for a small dataset and for a massive one.
More important, if you double the size of the input data, a job will run twice as slow.
But if you also double the size of the cluster, a job will run as fast as the original one.
Over time, however, the differences between relational databases and MapReduce systems are likely to blur—both as relational databases start incorporating some of the ideas from MapReduce (such as Aster Data’s and Greenplum’s databases) and, from the other direction, as higher-level query languages built on MapReduce (such as Pig and Hive) make MapReduce systems more approachable for traditional database programmers.5
Broadly, the approach in HPC is to distribute the work across a cluster of machines, which access a shared filesystem, hosted by a Storage Area Network (SAN)
This works well for predominantly computeintensive jobs, but it becomes a problem when nodes need to access larger data volumes (hundreds of gigabytes, the point at which MapReduce really starts to shine), since the network bandwidth is the bottleneck and compute nodes become idle.
Many commentators argued that it was a false comparison (see, for example, Mark C.
MapReduce tries to collocate the data with the compute node, so data access is fast because it is local.6 This feature, known as data locality, is at the heart of MapReduce and is the reason for its good performance.
Recognizing that network bandwidth is the most precious resource in a data center environment (it is easy to saturate network links by copying data around), MapReduce implementations go to great lengths to conserve it by explicitly modelling network topology.
Notice that this arrangement does not preclude high-CPU analyses in MapReduce.
MapReduce operates only at the higher level: the programmer thinks in terms of functions of key and value pairs, and the data flow is implicit.
Coordinating the processes in a large-scale distributed computation is a challenge.
MapReduce spares the programmer from having to think about failure, since the implementation detects failed map or reduce tasks and reschedules replacements on machines that are healthy.
MapReduce is able to do this because it is a shared-nothing architecture, meaning that tasks have no dependence on one other.
By contrast, MPI programs have to explicitly manage their own checkpointing and recovery, which gives more control to the programmer but makes them more difficult to write.
MapReduce might sound like quite a restrictive programming model, and in a sense it is: you are limited to key and value types that are related in specified ways, and mappers and reducers run with very limited coordination between one another (the mappers pass keys and values to reducers)
A natural question to ask is: can you do anything useful or nontrivial with it? The answer is yes.
MapReduce was invented by engineers at Google as a system for building production search indexes because they found themselves solving the same problem over and over again (and MapReduce was inspired by older ideas from the functional programming, distributed computing, and database communities), but it has since been used for many other applications in many other industries.
It is pleasantly surprising to see the range of algorithms that can be expressed in MapReduce, from.
You can see a sample of some of the applications that Hadoop has been used for in Chapter 16
Volunteer computing projects work by breaking the problem they are trying to solve into chunks called work units, which are sent to computers around the world to be analyzed.
When the analysis is completed, the results are sent back to the server, and the client gets another work unit.
As a precaution to combat cheating, each work unit is sent to three different machines and needs at least two results to agree to be accepted.
Although SETI@home may be superficially similar to MapReduce (breaking a problem into independent pieces to be worked on in parallel), there are some significant differences.
MapReduce is designed to run jobs that last minutes or hours on trusted, dedicated hardware running in a single data center with very high aggregate bandwidth interconnects.
By contrast, SETI@home runs a perpetual computation on untrusted machines on the Internet with highly variable connection speeds and no data locality.
Apache Mahout (http://mahout.apache.org/) is a project to build machine-learning libraries (such as classification and clustering algorithms) that run on Hadoop.
Hadoop has its origins in Apache Nutch, an open source web search engine, itself a part of the Lucene project.
The Origin of the Name “Hadoop” The name Hadoop is not an acronym; it’s a made-up name.
The project’s creator, Doug Cutting, explains how the name came about:
Short, relatively easy to spell and pronounce, meaningless, and not used elsewhere: those are my naming criteria.
Smaller components are given more descriptive (and therefore more mundane) names.
This is a good principle, as it means you can generally work out what something does from its name.
Building a web search engine from scratch was an ambitious goal, for not only is the software required to crawl and index websites complex to write, but it is also a challenge to run without a dedicated operations team, since there are so many moving parts.
It’s expensive, too: Mike Cafarella and Doug Cutting estimated a system supporting a one-billion-page index would cost around half a million dollars in hardware, with a monthly running cost of $30,000.10 Nevertheless, they believed it was a worthy goal, as it would open up and ultimately democratize search engine algorithms.
Nutch was started in 2002, and a working crawler and search system quickly emerged.
However, they realized that their architecture wouldn’t scale to the billions of pages on the Web.
Help was at hand with the publication of a paper in 2003 that described the architecture of Google’s distributed filesystem, called GFS, which was being used in production at Google.11 GFS, or something like it, would solve their storage needs for the very large files generated as a part of the web crawl and indexing process.
In particular, GFS would free up time being spent on administrative tasks such as managing storage nodes.
In 2004, they set about writing an open source implementation, the Nutch Distributed Filesystem (NDFS)
In this book, we use the lowercase form, “jobtracker,” to denote the entity when it’s being referred to generally, and the CamelCase form JobTracker to denote the Java class that implements it.
There are Hadoop distributions from the large, established enterprise vendors, including EMC, IBM, Microsoft, and Oracle, as well as from specialist Hadoop companies such as Cloudera, Hortonworks, and MapR.
Hadoop at Yahoo! Building Internet-scale search engines requires huge amounts of data and therefore large numbers of machines to process it.
Yahoo! Search consists of four primary components: the Crawler, which downloads pages from web servers; the WebMap, which builds a graph of the known Web; the Indexer, which builds a reverse index to the best pages; and the Runtime, which answers users’ queries.
Creating and analyzing such a large graph requires a large number of computers running for many days.
In early 2005, the infrastructure for the WebMap, named Dreadnaught, needed to be redesigned to scale up to more nodes.
Dreadnaught is similar to MapReduce in many ways, but provides more flexibility and less structure.
In particular, each fragment in a Dreadnaught job can send output to each of the fragments in the next stage of the job, but the sort was all done in library code.
In practice, most of the WebMap phases were pairs that corresponded to MapReduce.
Therefore, the WebMap applications would not require extensive refactoring to fit into MapReduce.
Although the immediate need was for a new framework for WebMap, it was clear that standardization of the batch platform across Yahoo! Search was critical and by making the framework general enough to support other users, we could better leverage investment in the new platform.
At the same time, we were watching Hadoop, which was part of Nutch, and its progress.
The advantage of Hadoop over our prototype and design was that it was already working with a real application (Nutch) on 20 nodes.
That allowed us to bring up a research cluster two months later and start helping real customers use the new framework much sooner than we could have otherwise.
February 2006: Apache Hadoop project officially started to support the standalone.
Apache Hadoop and the Hadoop Ecosystem Although Hadoop is best known for MapReduce and its distributed filesystem (HDFS, renamed from NDFS), the term is also used for a family of related projects that fall under the umbrella of infrastructure for distributed computing and large-scale data processing.
All of the core projects covered in this book are hosted by the Apache Software Foundation, which provides support for a community of open source software projects, including the original HTTP Server from which it gets its name.
As the Hadoop ecosystem grows, more projects are appearing, not necessarily hosted at Apache, that provide complementary services to Hadoop or build on the core to add higher-level abstractions.
The Hadoop projects that are covered in this book are described briefly here: Common.
A set of components and interfaces for distributed filesystems and general I/O (serialization, Java RPC, persistent data structures)
Avro A serialization system for efficient, cross-language RPC and persistent data storage.
MapReduce A distributed data processing model and execution environment that runs on large clusters of commodity machines.
Pig A data flow language and execution environment for exploring very large datasets.
Hive manages data stored in HDFS and provides a query language based on SQL (and which is translated by the runtime engine to MapReduce jobs) for querying the data.
HBase uses HDFS for its underlying storage, and supports both batch-style computations using MapReduce and point queries (random reads)
ZooKeeper provides primitives such as distributed locks that can be used for building distributed applications.
Sqoop A tool for efficient bulk transfer of data between structured data stores (such as relational databases) and HDFS.
Oozie A service for running and scheduling workflows of Hadoop jobs (including MapReduce, Pig, Hive, and Sqoop jobs)
Hadoop Releases Which version of Hadoop should you use? The answer to this question changes over time, of course, and also depends on the features that you need.
Almost all production clusters use these releases or derived versions (such as commercial distributions)
A new MapReduce runtime, called MapReduce 2, implemented on a new system called YARN (Yet Another Resource Negotiator), which is a general resource management system for running distributed applications.
Other projects in the Hadoop ecosystem are continually evolving too, and picking a combination of components that work well together can be a challenge.
The Apache Bigtop project (http://incubator.apache.org/bigtop/) runs interoperability tests on stacks of Hadoop components and provides Linux packages (RPMs and Debian packages) for easy installation.
There are also commercial vendors offering Hadoop distributions containing suites of compatible components.
In the cases where a feature is available only in a particular release, it is noted in the text.
The code in this book is written to work against all these release series, except in a small number of cases, which are called out explicitly.
The example code available on the website has a list of the versions that it was tested against.
Configuration property names have been changed in the releases after 1.x, in order to give them a more regular naming structure.
For properties that exist in version 1.x, the old (deprecated) names are used in this book because they will work in all the versions of Hadoop listed here.
If you are using a release after 1.x, you may wish to use the new property names in your configuration files and code to remove deprecation warnings.
This edition of the book uses the new API for the examples, which will work with all versions listed here, except in a few cases where a MapReduce library using new API is not available in the 1.x releases.
All the examples in this book are available in the old API version (in the oldapi package) from the book’s website.
Where there are material differences between the two APIs, they are discussed in the text.
Compatibility When moving from one release to another, you need to consider the upgrade steps that are needed.
There are several aspects to consider: API compatibility, data compatibility, and wire compatibility.
Hadoop uses a classification scheme for API elements to denote their stability.
Data compatibility concerns persistent data and metadata formats, such as the format in which the HDFS namenode stores its persistent data.
The formats can change across minor or major releases, but the change is transparent to users because the upgrade will automatically migrate the data.
There may be some restrictions about upgrade paths, and these are covered in the release notes.
For example, it may be necessary to upgrade via an intermediate release rather than upgrading directly to the later final release in one step.
Wire compatibility concerns the interoperability between clients and servers via wire protocols such as RPC and HTTP.
There are two types of client: external clients (run by users) and internal clients (run on the cluster as a part of the system, e.g., datanode and tasktracker daemons)
In general, internal clients have to be upgraded in lockstep; an older version of a tasktracker will not work with a newer jobtracker, for example.
In the future, rolling upgrades may be supported, which would allow cluster daemons to be upgraded in phases, so that the cluster would still be available to external clients during the upgrade.
Any exception to this rule should be called out in the release notes.
The model is simple, yet not too simple to express useful programs in.
Hadoop can run MapReduce programs written in various languages; in this chapter, we look at the same program expressed in Java, Ruby, Python, and C++
Most important, MapReduce programs are inherently parallel, thus putting very large-scale data analysis into the hands of anyone with enough machines at her disposal.
MapReduce comes into its own for large datasets, so let’s start by looking at one.
A Weather Dataset For our example, we will write a program that mines weather data.
Weather sensors collect data every hour at many locations across the globe and gather a large volume of log data, which is a good candidate for analysis with MapReduce because it is semistructured and record-oriented.
The data is stored using a line-oriented ASCII format, in which each line is a record.
The format supports a rich set of meteorological elements, many of which are optional or with variable data lengths.
For simplicity, we focus on the basic elements, such as temperature, which are always present and are of fixed width.
Example 2-1 shows a sample line with some of the salient fields highlighted.
The line has been split into multiple lines to show each field; in the real file, fields are packed into one line with no delimiters.
Since there are tens of thousands of weather stations, the whole dataset is made up of a large number of relatively small files.
It’s generally easier and more efficient to process a smaller number of relatively large files, so the data was preprocessed so that each.
The means by which this was carried out is described in Appendix C.)
The script loops through the compressed year files, first printing the year, and then processing each file using awk.
The awk script extracts two fields from the data: the air temperature and the quality code.
The air temperature value is turned into an integer by adding 0
Next, a test is applied to see whether the temperature is valid (the value 9999 signifies a missing value in the NCDC dataset) and whether the quality code indicates that the reading is not suspect or erroneous.
If the reading is OK, the value is compared with the maximum value seen so far, which is updated if a new maximum is found.
The END block is executed after all the lines in the file have been processed, and it prints the maximum value.
To speed up the processing, we need to run parts of the program in parallel.
In theory, this is straightforward: we could process different years in different processes, using all the available hardware threads on a machine.
First, dividing the work into equal-size pieces isn’t always easy or obvious.
In this case, the file size for different years varies widely, so some processes will finish much earlier than others.
Even if they pick up further work, the whole run is dominated by the longest file.
A better approach, although one that requires more work, is to split the input into fixed-size chunks and assign each chunk to a process.
Second, combining the results from independent processes may need further processing.
In this case, the result for each year is independent of other years and may be combined by concatenating all the results and sorting by year.
If using the fixed-size chunk approach, the combination is more delicate.
For this example, data for a particular year will typically be split into several chunks, each processed independently.
We’ll end up with the maximum temperature for each chunk, so the final step is to look for the highest of these maximums for each year.
Third, you are still limited by the processing capacity of a single machine.
If the best time you can achieve is 20 minutes with the number of processors you have, then that’s it.
Also, some datasets grow beyond the capacity of a single machine.
When we start using multiple machines, a whole host of other factors come into play, mainly falling into the category of coordination and reliability.
Using a framework like Hadoop to take care of these issues is a great help.
Analyzing the Data with Hadoop To take advantage of the parallel processing that Hadoop provides, we need to express our query as a MapReduce job.
After some local, small-scale testing, we will be able to run it on a cluster of machines.
Map and Reduce MapReduce works by breaking the processing into two phases: the map phase and the reduce phase.
Each phase has key-value pairs as input and output, the types of which may be chosen by the programmer.
The programmer also specifies two functions: the map function and the reduce function.
The input to our map phase is the raw NCDC data.
We choose a text input format that gives us each line in the dataset as a text value.
The key is the offset of the beginning of the line from the beginning of the file, but as we have no need for this, we ignore it.
We pull out the year and the air temperature because these are the only fields we are interested in.
In this case, the map function is just a data preparation phase, setting up the data in such a way that the reducer function can do its work on it: finding the maximum temperature for each year.
The map function is also a good place to drop bad records: here we filter out temperatures that are missing, suspect, or erroneous.
To visualize the way the map works, consider the following sample lines of input data (some unused columns have been dropped to fit the page, indicated by ellipses):
The keys are the line offsets within the file, which we ignore in our map function.
The map function merely extracts the year and the air temperature (indicated in bold text), and emits them as its output (the temperature values have been interpreted as integers):
The output from the map function is processed by the MapReduce framework before being sent to the reduce function.
This processing sorts and groups the key-value pairs by key.
So, continuing the example, our reduce function sees the following input:
Each year appears with a list of all its air temperature readings.
All the reduce function has to do now is iterate through the list and pick up the maximum reading:
This is the final output: the maximum global temperature recorded in each year.
At the bottom of the diagram is a Unix pipeline, which mimics the whole MapReduce flow and which we will see again later in this chapter when we look at Hadoop Streaming.
Java MapReduce Having run through how the MapReduce program works, the next step is to express it in code.
We need three things: a map function, a reduce function, and some code to run the job.
The map function is represented by the Mapper class, which declares an abstract map() method.
The Mapper class is a generic type, with four formal type parameters that specify the input key, input value, output key, and output value types of the map function.
For the present example, the input key is a long integer offset, the input value is a line of text,
Rather than use built-in Java types, Hadoop provides its own set of basic types that are optimized for network serialization.
The map() method is passed a key and a value.
We convert the Text value containing the line of input into a Java String, then use its substring() method to extract the columns we are interested in.
The map() method also provides an instance of Context to write the output to.
In this case, we write the year as a Text object (since we are just using it as a key), and the temperature is wrapped in an IntWritable.
We write an output record only if the temperature is present and the quality code indicates the temperature reading is OK.
The reduce function is similarly defined using a Reducer, as illustrated in Example 2-4
Again, four formal type parameters are used to specify the input and output types, this time for the reduce function.
The input types of the reduce function must match the output types of the map function: Text and IntWritable.
And in this case, the output types of the reduce function are Text and IntWritable, for a year and its maximum temperature, which we find by iterating through the temperatures and comparing each with a record of the highest found so far.
The third piece of code runs the MapReduce job (see Example 2-5)
A Job object forms the specification of the job and gives you control over how the job is run.
When we run this job on a Hadoop cluster, we will package the code into a JAR file (which Hadoop will distribute around the cluster)
Rather than explicitly specify the name of the JAR file, we can pass a class in the Job’s setJarByClass() method, which Hadoop will use to locate the relevant JAR file by looking for the JAR file containing this class.
Having constructed a Job object, we specify the input and output paths.
An input path is specified by calling the static addInputPath() method on FileInputFormat, and it can be a single file, a directory (in which case the input forms all the files in that directory), or a file pattern.
As the name suggests, addInputPath() can be called more than once to use input from multiple paths.
The output path (of which there is only one) is specified by the static setOutput Path() method on FileOutputFormat.
It specifies a directory where the output files from the reducer functions are written.
The directory shouldn’t exist before running the job because Hadoop will complain and not run the job.
Next, we specify the map and reduce types to use via the setMapperClass() and setReducerClass() methods.
The input types are controlled via the input format, which we have not explicitly set because we are using the default TextInputFormat.
After setting the classes that define the map and reduce functions, we are ready to run the job.
The method’s Boolean argument is a verbose flag, so in this case the job writes information about its progress to the console.
After writing a MapReduce job, it’s normal to try it out on a small dataset to flush out any immediate problems with the code.
First, install Hadoop in standalone mode— there are instructions for how to do this in Appendix A.
This is the mode in which Hadoop runs using the local filesystem with a local job runner.
Then, install and compile the examples using the instructions on the book’s website.
Let’s test it on the five-line sample discussed earlier (the output has been slightly reformatted to fit the page):
When the hadoop command is invoked with a classname as the first argument, it launches a Java Virtual Machine (JVM) to run the class.
It is more convenient to use hadoop than straight java because the former adds the Hadoop libraries (and their dependencies) to the classpath and picks up the Hadoop configuration, too.
To add the application classes to the classpath, we’ve defined an environment variable called HADOOP_CLASSPATH, which the hadoop script picks up.
When running in local (standalone) mode, the programs in this book all assume that you have set the HADOOP_CLASSPATH in this way.
The commands should be run from the directory that the example code is installed in.
The output from running the job provides some useful information.
Knowing the job and task IDs can be very useful when debugging MapReduce jobs.
The last section of the output, titled “Counters,” shows the statistics that Hadoop generates for each job it runs.
These are very useful for checking whether the amount of data processed is what you expected.
For example, we can follow the number of records that went through the system: five map inputs produced five map outputs, then five reduce inputs in two groups produced two reduce outputs.
The output was written to the output directory, which contains one output file per reducer.
The job had a single reducer, so we find a single file, named part-r-00000:
The Java MapReduce API used in the previous section was first released in Hadoop 0.20.0
It is type-incompatible with the old, however, so applications need to be rewritten to take advantage of it.
Previous editions of this book were based on 0.20 releases and used the old API throughout.
In this edition, the new API is used as the primary API, except in a few places.
However, should you wish to use the old API, you can, since the code for all the examples in this book is available for the old API on the book’s website.
The new API favors abstract classes over interfaces, since these are easier to evolve.
This means that you can add a method (with a default implementation) to an abstract class without breaking old implementations of the class.1 For example, the Mapper and Reducer interfaces in the old API are abstract classes in the new API.
The new API makes extensive use of context objects that allow the user code to communicate with the MapReduce system.
The new Context, for example, essentially unifies the role of the JobConf, the OutputCollector, and the Reporter from the old API.
In both APIs, key-value record pairs are pushed to the mapper and reducer, but in addition, the new API allows both mappers and reducers to control the execution flow by overriding the run() method.
For example, records can be processed in batches, or the execution can be terminated before all the records have been processed.
In the old API this is possible for mappers by writing a MapRunnable, but no equivalent exists for reducers.
Job control is performed through the Job class in the new API, rather than the old JobClient, which no longer exists in the new API.
In the new API, job configuration is done through a Configuration, possibly via some of the helper methods on Job.
Output files are named slightly differently: in the old API both map and reduce outputs are named part-nnnnn, whereas in the new API map outputs are named part-m-nnnnn, and reduce outputs are named part-r-nnnnn (where nnnnn is an integer designating the part number, starting from zero)
This means that you can write your code to be responsive to interrupts so that the framework can gracefully cancel long-running operations if it needs to.2
This change makes it easier to iterate over the values using Java’s for-each loop construct:
Example 2-6 shows the MaxTemperature application rewritten to use the old API.
When converting your Mapper and Reducer classes to the new API, don’t forget to change the signature of the map() and reduce() methods to the new form.
Just changing your class to extend the new Mapper or Reducer classes will not produce a compilation error or warning, because these classes provide an identity form of the map() or reduce() method (respectively)
Your mapper or reducer code, however, will not be invoked, which can lead to some hard-to-diagnose errors.
Annotating your map() and reduce() methods with the @Override annotation will allow the Java compiler to catch these errors.
Scaling Out You’ve seen how MapReduce works for small inputs; now it’s time to take a bird’s-eye view of the system and look at the data flow for large inputs.
For simplicity, the examples so far have used files on the local filesystem.
However, to scale out, we need to store the data in a distributed filesystem, typically HDFS (which you’ll learn about in the next chapter), to allow Hadoop to move the MapReduce computation to each machine hosting a part of the data.
A MapReduce job is a unit of work that the client wants to be performed: it consists of the input data, the MapReduce program, and configuration information.
Hadoop runs the job by dividing it into tasks, of which there are two types: map tasks and reduce tasks.
There are two types of nodes that control the job execution process: a jobtracker and a number of tasktrackers.
The jobtracker coordinates all the jobs run on the system by scheduling tasks to run on tasktrackers.
Tasktrackers run tasks and send progress reports to the jobtracker, which keeps a record of the overall progress of each job.
If a task fails, the jobtracker can reschedule it on a different tasktracker.
Hadoop divides the input to a MapReduce job into fixed-size pieces called input splits, or just splits.
Hadoop creates one map task for each split, which runs the userdefined map function for each record in the split.
Having many splits means the time taken to process each split is small compared to the time to process the whole input.
So if we are processing the splits in parallel, the processing is better load-balanced when the splits are small, since a faster machine will be able to process proportionally more splits over the course of the job than a slower machine.
Even if the machines are identical, failed processes or other jobs running concurrently make load balancing desirable, and the quality of the load balancing increases as the splits become more fine-grained.
On the other hand, if splits are too small, the overhead of managing the splits and of map task creation begins to dominate the total job execution time.
For most jobs, a good split size tends to be the size of an HDFS block, 64 MB by default, although this can be changed for the cluster (for all newly created files) or specified when each file is created.
Hadoop does its best to run the map task on a node where the input data resides in HDFS.
This is called the data locality optimization because it doesn’t use valuable cluster bandwidth.
Sometimes, however, all three nodes hosting the HDFS block replicas for a map task’s input split are running other map tasks, so the job scheduler will look for a free map slot on a node in the same rack as one of the blocks.
Very occasionally even this is not possible, so an off-rack node is used, which results in an inter-rack network transfer.
It should now be clear why the optimal split size is the same as the block size: it is the largest size of input that can be guaranteed to be stored on a single node.
If the split spanned two blocks, it would be unlikely that any HDFS node stored both blocks, so some of the split would have to be transferred across the network to the node running.
Map tasks write their output to the local disk, not to HDFS.
Why is this? Map output is intermediate output: it’s processed by reduce tasks to produce the final output, and once the job is complete, the map output can be thrown away.
So storing it in HDFS with replication would be overkill.
If the node running the map task fails before the map output has been consumed by the reduce task, then Hadoop will automatically rerun the map task on another node to re-create the map output.
Reduce tasks don’t have the advantage of data locality; the input to a single reduce task is normally the output from all mappers.
In the present example, we have a single reduce task that is fed by all of the map tasks.
Therefore, the sorted map outputs have to be transferred across the network to the node where the reduce task is running, where they are merged and then passed to the user-defined reduce function.
The output of the reduce is normally stored in HDFS for reliability.
As explained in Chapter 3, for each HDFS block of the reduce output, the first replica is stored on the local node, with other replicas being stored on off-rack nodes.
Thus, writing the reduce output does consume network bandwidth, but only as much as a normal HDFS write pipeline consumes.
The whole data flow with a single reduce task is illustrated in Figure 2-3
The dotted boxes indicate nodes, the light arrows show data transfers on a node, and the heavy arrows show data transfers between nodes.
The number of reduce tasks is not governed by the size of the input, but instead is specified independently.
When there are multiple reducers, the map tasks partition their output, each creating one partition for each reduce task.
There can be many keys (and their associated values) in each partition, but the records for any given key are all in a single partition.
The data flow for the general case of multiple reduce tasks is illustrated in Figure 2-4
In this case, the only off-node data transfer is when the map tasks write to HDFS (see Figure 2-5)
Combiner Functions Many MapReduce jobs are limited by the bandwidth available on the cluster, so it pays to minimize the data transferred between map and reduce tasks.
Hadoop allows the user to specify a combiner function to be run on the map output, and the combiner.
Because the combiner function is an optimization, Hadoop does not provide a guarantee of how many times it will call it for a particular map output record, if at all.
In other words, calling the combiner function zero, one, or many times should produce the same output from the reducer.
The contract for the combiner function constrains the type of function that may be used.
Suppose that for the maximum temperature example, readings for the year 1950 were processed by two maps (because they were in different splits)
We could use a combiner function that, just like the reduce function, finds the maximum temperature for each map output.
More succinctly, we may express the function calls on the temperature values in this case as follows:
Not all functions possess this property.3 For example, if we were calculating mean temperatures, we couldn’t use the mean as our combiner function, because:
How could it? The reduce function is still needed to process records with the same key from different maps.) But it can help cut down the amount of data shuffled between the mappers and the reducers, and for this reason alone it is always worth considering whether you can use a combiner function in your MapReduce job.
Running a Distributed MapReduce Job The same program will run, without alteration, on a full dataset.
This is the point of MapReduce: it scales to the size of your data and the size of your hardware.
Hadoop Streaming Hadoop provides an API to MapReduce that allows you to write your map and reduce functions in languages other than Java.
Hadoop Streaming uses Unix standard streams as the interface between Hadoop and your program, so you can use any language that can read standard input and write to standard output to write your MapReduce program.
Map input data is passed over standard input to your map function, which processes it line by line and writes lines to standard output.
A map output key-value pair is written as a single tab-delimited line.
The reduce function reads lines from standard input, which the framework guarantees are sorted by key, and writes its results to standard output.
Let’s illustrate this by rewriting our MapReduce program for finding maximum temperatures by year in Streaming.
Ruby The map function can be expressed in Ruby as shown in Example 2-8
This is a factor of seven faster than the serial run on one machine using awk.
The main reason it wasn’t proportionately faster is because the input data wasn’t evenly partitioned.
For convenience, the input files were gzipped by year, resulting in large files for later years in the dataset, when the number of weather records was much higher.
The program iterates over lines from standard input by executing a block for each line from STDIN (a global constant of type IO)
The block pulls out the relevant fields from each input line, and, if the temperature is valid, writes the year and the temperature separated by a tab character \t to standard output (using puts)
It’s worth drawing out a design difference between Streaming and the Java MapReduce API.
The Java API is geared toward processing your map function one record at a time.
The framework calls the map() method on your Mapper for each record in the input, whereas with Streaming the map program can decide how to process the input—for example, it could easily read and process multiple lines at a time since it’s in control of the reading.
The user’s Java map implementation is “pushed” records, but it’s still possible to consider multiple lines at a time by accumulating previous lines in an instance variable in the Mapper.5 In this case, you need to implement the close() method so that you know when the last record has been read, so you can finish processing the last group of lines.
Because the script just operates on standard input and output, it’s trivial to test the script without using Hadoop, simply using Unix pipes:
The reduce function shown in Example 2-9 is a little more complex.
Again, the program iterates over lines from standard input, but this time we have to store some state as we process each key group.
In this case, the keys are the years, and we store the last key seen and the maximum temperature seen so far for that key.
The MapReduce framework ensures that the keys are ordered, so we know that if a key is different from the previous one, we have moved into a new key group.
In contrast to the Java API, where you are provided an iterator over each key group, in Streaming you have to find key group boundaries in your program.
For each line, we pull out the key and value.
If we haven’t just finished a group, we just update the maximum temperature for the current key.
The last line of the program ensures that a line is written for the last key group in the input.
We can now simulate the whole MapReduce pipeline with a Unix pipeline (which is equivalent to the Unix pipeline shown in Figure 2-1):
The output is the same as the Java program, so the next step is to run it using Hadoop itself.
The hadoop command doesn’t support a Streaming option; instead, you specify the Streaming JAR file along with the jar option.
Options to the Streaming program specify the input and output paths and the map and reduce scripts.
When running on a large dataset on a cluster, we should use the -combiner option to set the combiner.
In releases after 1.x, the combiner can be any Streaming command.
For earlier releases, the combiner had to be written in Java, so as a workaround it was common to do manual combining in the mapper without having to resort to Java.
In this case, we could change the mapper to be a pipeline:
Note also the use of -file, which we use when running Streaming programs on the cluster to ship the scripts to the cluster.
As an alternative to Streaming, Python programmers should consider Dumbo (http://www.last.fm/ dumbo), which makes the Streaming MapReduce interface more Pythonic and easier to use.
We can test the programs and run the job in the same way we did in Ruby.
Unlike Streaming, which uses standard input and output to communicate with the map and reduce code, Pipes uses sockets as the channel over which the tasktracker communicates with the process running the C++ map or reduce function.
We’ll rewrite this chapter’s temperature example in C++, and then we’ll see how to run it using Pipes.
The application links against the Hadoop C++ library, which is a thin wrapper for communicating with the tasktracker child process.
The map and reduce functions are defined by extending the Mapper and Reducer classes defined in the HadoopPipes namespace and providing implementations of the map() and reduce() methods in each case.
These methods take a context object (of type MapContext or ReduceContext), which provides the means for reading input and writing output, as well as accessing job configuration information via the JobConf class.
The processing in this example is very similar to the Java equivalent.
Unlike the Java interface, keys and values in the C++ interface are byte buffers represented as Standard Template Library (STL) strings.
This makes the interface simpler, although it does put a slightly greater burden on the application developer, who has to convert to and from richer domain-level types.
This is evident in MapTempera tureReducer, where we have to convert the input value into an integer (using a convenience method in HadoopUtils) and then the maximum value back into a string before it’s written out.
In some cases, we can skip the conversion, such as in MaxTemperature Mapper, where the airTemperature value is never converted to an integer because it is never processed as a number in the map() method.
The runTask() method is passed a Factory so that it can create instances of the Mapper or Reducer.
Which one it creates is controlled by the Java parent over the socket connection.
There are overloaded template factory methods for setting a combiner, partitioner, record reader, or record writer.
Compiling and Running Now we can compile and link our program using the makefile in Example 2-13
The makefile expects a couple of environment variables to be set.
I ran it on a 32-bit Linux system with the following:
On successful completion, you’ll find the max_temperature executable in the current directory.
To run a Pipes job, we need to run Hadoop in pseudodistributed mode (where all the daemons run on the local machine), for which there are setup instructions in Appendix A.
Pipes doesn’t run in standalone (local) mode, because it relies on Hadoop’s distributed cache mechanism, which works only when HDFS is running.
With the Hadoop daemons now running, the first step is to copy the executable to HDFS so that it can be picked up by tasktrackers when they launch map and reduce tasks:
For this, we use the Hadoop pipes command, passing the Uniform Resource Identifier (URI) of the executable in HDFS using the -program argument:
Pipes also allows you to set a Java mapper, reducer, combiner, or partitioner.
In fact, you can have a mixture of Java or C++ classes within any one job.
The result is the same as the other versions of the same program that we ran previously.
When a dataset outgrows the storage capacity of a single physical machine, it becomes necessary to partition it across a number of separate machines.
Filesystems that manage the storage across a network of machines are called distributed filesystems.
Since they are network-based, all the complications of network programming kick in, thus making distributed filesystems more complex than regular disk filesystems.
For example, one of the biggest challenges is making the filesystem tolerate node failure without suffering data loss.
Hadoop comes with a distributed filesystem called HDFS, which stands for Hadoop Distributed Filesystem.
The Design of HDFS HDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters of commodity hardware.1 Let’s examine this statement in more detail: Very large files.
Very large” in this context means files that are hundreds of megabytes, gigabytes, or terabytes in size.
There are Hadoop clusters running today that store petabytes of data.2
Streaming data access HDFS is built around the idea that the most efficient data processing pattern is a write-once, read-many-times pattern.
A dataset is typically generated or copied from source, and then various analyses are performed on that dataset over time.
Each analysis will involve a large proportion, if not all, of the dataset, so the time to read the whole dataset is more important than the latency in reading the first record.
It’s designed to run on clusters of commodity hardware (commonly available hardware that can be obtained from multiple vendors)3 for which the chance of node failure across the cluster is high, at least for large clusters.
It is also worth examining the applications for which using HDFS does not work so well.
Although this may change in the future, these are areas where HDFS is not a good fit today: Low-latency data access.
Applications that require low-latency access to data, in the tens of milliseconds range, will not work well with HDFS.
Remember, HDFS is optimized for delivering a high throughput of data, and this may be at the expense of latency.
HBase (Chapter 13) is currently a better choice for low-latency access.
Lots of small files Because the namenode holds filesystem metadata in memory, the limit to the number of files in a filesystem is governed by the amount of memory on the namenode.
As a rule of thumb, each file, directory, and block takes about 150 bytes.
So, for example, if you had one million files, each taking one block, you would need at least 300 MB of memory.
Although storing millions of files is feasible, billions is beyond the capability of current hardware.4
Multiple writers, arbitrary file modifications Files in HDFS may be written to by a single writer.
Writes are always made at the end of the file.
There is no support for multiple writers or for modifications at arbitrary offsets in the file.
These might be supported in the future, but they are likely to be relatively inefficient.)
Blocks A disk has a block size, which is the minimum amount of data that it can read or write.
Filesystems for a single disk build on this by dealing with data in blocks, which are an integral multiple of the disk block size.
Filesystem blocks are typically a few kilobytes in size, whereas disk blocks are normally 512 bytes.
This is generally transparent to the filesystem user who is simply reading or writing a file of whatever length.
However, there are tools to perform filesystem maintenance, such as df and fsck, that operate on the filesystem block level.
HDFS, too, has the concept of a block, but it is a much larger unit—64 MB by default.
Like in a filesystem for a single disk, files in HDFS are broken into block-sized chunks, which are stored as independent units.
Unlike a filesystem for a single disk, a file in HDFS that is smaller than a single block does not occupy a full block’s worth of underlying storage.
When unqualified, the term “block” in this book refers to a block in HDFS.
Why Is a Block in HDFS So Large? HDFS blocks are large compared to disk blocks, and the reason is to minimize the cost of seeks.
By making a block large enough, the time to transfer the data from the disk can be significantly longer than the time to seek to the start of the block.
Thus the time to transfer a large file made of multiple blocks operates at the disk transfer rate.
This figure will continue to be revised upward as transfer speeds grow with new generations of disk drives.
Map tasks in MapReduce normally operate on one block at a time, so if you have too few tasks (fewer than nodes in the cluster), your jobs will run slower than they could otherwise.
Having a block abstraction for a distributed filesystem brings several benefits.
The first benefit is the most obvious: a file can be larger than any single disk in the network.
There’s nothing that requires the blocks from a file to be stored on the same disk, so they can take advantage of any of the disks in the cluster.
In fact, it would be possible, if unusual, to store a single file on an HDFS cluster whose blocks filled all the disks in the cluster.
Second, making the unit of abstraction a block rather than a file simplifies the storage subsystem.
Simplicity is something to strive for in all systems, but is especially important for a distributed system in which the failure modes are so varied.
The storage subsystem deals with blocks, simplifying storage management (because blocks are a fixed size, it is easy to calculate how many can be stored on a given disk) and eliminating metadata concerns (because blocks are just a chunk of data to be stored, file metadata such as permissions information does not need to be stored with the blocks, so another system can handle metadata separately)
Furthermore, blocks fit well with replication for providing fault tolerance and availability.
To insure against corrupted blocks and disk and machine failure, each block is replicated to a small number of physically separate machines (typically three)
If a block becomes unavailable, a copy can be read from another location in a way that is transparent to the client.
A block that is no longer available due to corruption or machine failure can be replicated from its alternative locations to other live machines to bring the replication factor back to the normal level.
Like its disk filesystem cousin, HDFS’s fsck command understands blocks.
Namenodes and Datanodes An HDFS cluster has two types of nodes operating in a master-worker pattern: a namenode (the master) and a number of datanodes (workers)
It maintains the filesystem tree and the metadata for all the files and directories in the tree.
This information is stored persistently on the local disk in the form of two files: the namespace image and the edit log.
The namenode also knows the datanodes on which all the blocks for a given file are located; however, it does not store block locations persistently, because this information is reconstructed from datanodes when the system starts.
A client accesses the filesystem on behalf of the user by communicating with the namenode and datanodes.
The client presents a filesystem interface similar to a Portable Operating System Interface (POSIX), so the user code does not need to know about the namenode and datanode to function.
They store and retrieve blocks when they are told to (by clients or the namenode), and they report back to the namenode periodically with lists of blocks that they are storing.
In fact, if the machine running the namenode were obliterated, all the files on the filesystem would be lost since there would be no way of knowing how to reconstruct the files from the blocks on the datanodes.
For this reason, it is important to make the namenode resilient to failure, and Hadoop provides two mechanisms for this.
The first way is to back up the files that make up the persistent state of the filesystem metadata.
Hadoop can be configured so that the namenode writes its persistent state to multiple filesystems.
The usual configuration choice is to write to local disk as well as a remote NFS mount.
It is also possible to run a secondary namenode, which despite its name does not act as a namenode.
Its main role is to periodically merge the namespace image with the edit log to prevent the edit log from becoming too large.
The secondary namenode usually runs on a separate physical machine because it requires plenty of CPU and as much memory as the namenode to perform the merge.
It keeps a copy of the merged namespace image, which can be used in the event of the namenode failing.
However, the state of the secondary namenode lags that of the primary, so in the event of total failure of the primary, data loss is almost certain.
The usual course of action in this case is to copy the namenode’s metadata files that are on NFS to the secondary and run it as the new primary.
For example, one namenode might manage all the files rooted under /user, say, and a second namenode might handle files under /share.
Under federation, each namenode manages a namespace volume, which is made up of the metadata for the namespace, and a block pool containing all the blocks for the files in the namespace.
Namespace volumes are independent of each other, which means namenodes do not communicate with one another, and furthermore the failure of one namenode does not affect the availability of the namespaces managed by other namenodes.
Block pool storage is not partitioned, however, so datanodes register with each namenode in the cluster and store blocks from multiple block pools.
To access a federated HDFS cluster, clients use client-side mount tables to map file paths to namenodes.
This is managed in configuration using ViewFileSystem and the viewfs:// URIs.
The namenode is still a single point of failure (SPOF)
In such an event the whole Hadoop system would effectively be out of service until a new namenode could be brought online.
To recover from a failed namenode in this situation, an administrator starts a new primary namenode with one of the filesystem metadata replicas and configures datanodes and clients to use this new namenode.
The new namenode is not able to serve requests until it has i) loaded its namespace image into memory, ii) replayed its edit log, and iii) received enough block reports from the datanodes to leave safe mode.
On large clusters with many files and blocks, the time it takes for a namenode to start from cold can be 30 minutes or more.
The long recovery time is a problem for routine maintenance too.
In fact, because unexpected failure of the namenode is so rare, the case for planned downtime is actually more important in practice.
The 2.x release series of Hadoop remedies this situation by adding support for HDFS high-availability (HA)
In this implementation there is a pair of namenodes in an activestandby configuration.
In the event of the failure of the active namenode, the standby takes over its duties to continue servicing client requests without a significant interruption.
A few architectural changes are needed to allow this to happen:
The namenodes must use highly available shared storage to share the edit log.
In the initial implementation of HA this will require an NFS filer, but in future releases more options will be provided, such as a BookKeeper-based system built on ZooKeeper.) When a standby namenode comes up, it reads up to the end of the shared edit log to synchronize its state with the active namenode, and then continues to read new entries as they are written by the active namenode.
Datanodes must send block reports to both namenodes because the block mappings are stored in a namenode’s memory, and not on disk.
Clients must be configured to handle namenode failover, using a mechanism that is transparent to users.
If the active namenode fails, the standby can take over very quickly (in a few tens of seconds) because it has the latest state available in memory: both the latest edit log entries and an up-to-date block mapping.
The actual observed failover time will be longer in practice (around a minute or so), because the system needs to be conservative in deciding that the active namenode has failed.
In the unlikely event of the standby being down when the active fails, the administrator can still start the standby from cold.
This is no worse than the non-HA case, and from an operational point of view it’s an improvement, because the process is a standard operational procedure built into Hadoop.
The transition from the active namenode to the standby is managed by a new entity in the system called the failover controller.
Failover controllers are pluggable, but the first implementation uses ZooKeeper to ensure that only one namenode is active.
Each namenode runs a lightweight failover controller process whose job it is to monitor its namenode for failures (using a simple heartbeating mechanism) and trigger a failover should a namenode fail.
Failover may also be initiated manually by an administrator, for example, in the case of routine maintenance.
This is known as a graceful failover, since the failover controller arranges an orderly transition for both namenodes to switch roles.
In the case of an ungraceful failover, however, it is impossible to be sure that the failed namenode has stopped running.
For example, a slow network or a network partition can trigger a failover transition, even though the previously active namenode is still running and thinks it is still the active namenode.
The HA implementation goes to great lengths to ensure that the previously active namenode is prevented from doing any damage and causing corruption—a method known as fencing.
The system employs a range of fencing mechanisms, including killing the namenode’s process, revoking its access to the shared storage directory (typically by using a vendor-specific NFS command), and disabling its network port via a remote management command.
The HDFS URI uses a logical hostname that is mapped to a pair of namenode addresses (in the configuration file), and the client library tries each namenode address until the operation succeeds.
The Command-Line Interface We’re going to have a look at HDFS by interacting with it from the command line.
There are many other interfaces to HDFS, but the command line is one of the simplest and, to many developers, the most familiar.
We are going to run HDFS on one machine, so first follow the instructions for setting up Hadoop in pseudodistributed mode in Appendix A.
Later we’ll see how to run HDFS on a cluster of machines to give us scalability and fault tolerance.
There are two properties that we set in the pseudodistributed configuration that deserve further explanation.
Filesystems are specified by a URI, and here we have used an hdfs URI to configure Hadoop to use HDFS by default.
The HDFS daemons will use this property to determine the host and port for the HDFS namenode.
We’ll be running it on localhost, on the default HDFS port, 8020
And HDFS clients will use this property to work out where the namenode is running so they can connect to it.
We set the second property, dfs.replication, to 1 so that HDFS doesn’t replicate filesystem blocks by the default factor of three.
When running with a single datanode, HDFS can’t replicate blocks to three datanodes, so it would perpetually warn about blocks being under-replicated.
Basic Filesystem Operations The filesystem is ready to be used, and we can do all of the usual filesystem operations, such as reading files, creating directories, moving files, deleting data, and listing directories.
You can type hadoop fs -help to get detailed help on every command.
Start by copying a file from the local filesystem to HDFS:
This command invokes Hadoop’s filesystem shell command fs, which supports a number of subcommands—in this case, we are running -copyFromLocal.
In fact, we could have omitted the scheme and host of the URI and picked up the default, hdfs://localhost, as specified in core-site.xml:
We also could have used a relative path and copied the file to our home directory in HDFS, which in this case is /user/tom:
The MD5 digests are the same, showing that the file survived its trip to HDFS and is back intact.
We create a directory first just to see how it is displayed in the listing:
The information returned is very similar to the Unix command ls -l, with a few minor differences.
The second column is the replication factor of the file (something a traditional Unix filesystem does not have)
Remember we set the default replication factor in the site-wide configuration to be 1, which is why we see the same value here.
The entry in this column is empty for directories because the concept of replication does not apply to them—directories are treated as metadata and stored by the namenode, not the datanodes.
The third and fourth columns show the file owner and group.
The fifth column is the size of the file in bytes, or zero for directories.
The sixth and seventh columns are the last modified date and time.
Finally, the eighth column is the absolute name of the file or directory.
File Permissions in HDFS HDFS has a permissions model for files and directories that is much like POSIX.
There are three types of permission: the read permission (r), the write permission (w), and the execute permission (x)
The read permission is required to read files or list the contents of a directory.
The write permission is required to write a file, or for a directory, to create or delete files or directories in it.
The execute permission is ignored for a file because you can’t execute a file on HDFS (unlike POSIX), and for a directory this permission is required to access its children.
Each file and directory has an owner, a group, and a mode.
The mode is made up of the permissions for the user who is the owner, the permissions for the users who are members of the group, and the permissions for users who are neither the owners nor members of the group.
By default, a client’s identity is determined by the username and groups of the process it is running in.
Because clients are remote, this makes it possible to become an arbitrary user simply by creating an account of that name on the remote system.
Thus, permissions should be used only in a cooperative community of users, as a mechanism for sharing filesystem resources and for avoiding accidental data loss, and not for securing resources in a hostile environment.
Note, however, that the latest versions of Hadoop support Kerberos authentication, which removes these restrictions; see “Security” on page 325.) Despite these limitations, it is worthwhile having permissions enabled (as it is by default; see the dfs.permissions property), to avoid accidental modification or deletion of substantial parts of the filesystem, either by users or by automated tools or programs.
When permissions checking is enabled, the owner permissions are checked if the client’s username matches the owner, and the group permissions are checked if the client is a member of the group; otherwise, the other permissions are checked.
There is a concept of a super user, which is the identity of the namenode process.
Hadoop Filesystems Hadoop has an abstract notion of filesystem, of which HDFS is just one implementation.
A filesystem providing secure read-write access to HDFS over HTTP.
WebHDFS is intended as a replacement for HFTP and HSFTP.
Hadoop Archives are typically used for archiving files in HDFS to reduce the namenode’s memory usage.
CloudStore (formerly Kosmos filesystem) is a distributed filesystem like HDFS or Google’s GFS, written in C++
Distributed RAID requires that you run a RaidNode daemon on the cluster.
Hadoop provides many interfaces to its filesystems, and it generally uses the URI scheme to pick the correct filesystem instance to communicate with.
For example, the filesystem shell that we met in the previous section operates with all Hadoop filesystems.
To list the files in the root directory of the local filesystem, type:
Interfaces Hadoop is written in Java, and all Hadoop filesystem interactions are mediated through the Java API.
The filesystem shell, for example, is a Java application that uses the Java FileSystem class to provide filesystem operations.
The other filesystem interfaces are discussed briefly in this section.
The original direct HTTP interface (HFTP and HSFTP) was read-only, but the new WebHDFS implementation supports all filesystem operations, including Kerberos authentication.
The second way of accessing HDFS over HTTP relies on one or more standalone proxy servers.
The proxies are stateless so they can run behind a standard load balancer.) All traffic to the cluster passes through the proxy.
This allows for stricter firewall and bandwidth-limiting policies to be put in place.
It’s common to use a proxy for transfers between Hadoop clusters located in different data centers.
From release 1.0.0, there is a new proxy called HttpFS that has read and write capabilities and exposes the same HTTP interface as WebHDFS, so clients can access both using webhdfs URIs.
The HTTP REST API that WebHDFS exposes is formally defined in a specification, so it is expected that over time clients in languages other than Java will be written that use it directly.
Accessing HDFS over HTTP directly and via a bank of HDFS proxies.
It works using the Java Native Interface (JNI) to call a Java filesystem client.
The C API is very similar to the Java one, but it typically lags the Java one, so newer features may not be supported.
You can find the generated documentation for the C API in the libhdfs/docs/api directory of the Hadoop distribution.
Hadoop comes with prebuilt libhdfs binaries for 32-bit Linux, but for other platforms, you will need to build them yourself using the instructions at http://wiki.apache.org/ hadoop/LibHDFS.
Filesystem in Userspace (FUSE) allows filesystems that are implemented in user space to be integrated as a Unix filesystem.
Hadoop’s Fuse-DFS contrib module allows any Hadoop filesystem (but typically HDFS) to be mounted as a standard filesystem.
You can then use Unix utilities (such as ls and cat) to interact with the filesystem, as well as POSIX libraries to access the filesystem from any programming language.
Fuse-DFS is implemented in C using libhdfs as the interface to HDFS.
This is very useful when testing your program, for example, because you can rapidly run tests using data stored on the local filesystem.
Reading Data from a Hadoop URL One of the simplest ways to read a file from a Hadoop filesystem is by using a java.net.URL object to open a stream to read the data from.
In releases after 1.x, there is a new filesystem interface called FileContext with better handling of multiple filesystems (so a single FileContext can resolve multiple filesystem schemes, for example) and a cleaner, more consistent interface.
We make use of the handy IOUtils class that comes with Hadoop for closing the stream in the finally clause, and also for copying bytes between the input stream and the output stream (System.out in this case)
The last two arguments to the copyBytes method are the buffer size used for copying and whether to close the streams when the copy is complete.
We close the input stream ourselves, and System.out doesn’t need to be closed.
Reading Data Using the FileSystem API As the previous section explained, sometimes it is impossible to set a URLStreamHand lerFactory for your application.
In this case, you will need to use the FileSystem API to open an input stream for a file.
A file in a Hadoop filesystem is represented by a Hadoop Path object (and not a java.io.File object, since its semantics are too closely tied to the local filesystem)
FileSystem is a general filesystem API, so the first step is to retrieve an instance for the filesystem we want to use—HDFS in this case.
There are several static factory methods for getting a FileSystem instance:
The second uses the given URI’s scheme and authority to determine the filesystem to use, falling back to the default filesystem if no scheme is specified in the given URI.
In some cases, you may want to retrieve a local filesystem instance, in which case you can use the convenience method, getLocal():
With a FileSystem instance in hand, we invoke an open() method to get the input stream for a file:
The first method uses a default buffer size of 4 KB.
The open() method on FileSystem actually returns a FSDataInputStream rather than a standard java.io class.
The Seekable interface permits seeking to a position in the file and a query method for the current offset from the start of the file (getPos()):
Calling seek() with a position that is greater than the length of the file will result in an IOException.
FSDataInputStream also implements the PositionedReadable interface for reading parts of a file at a given offset:
The read() method reads up to length bytes from the given position in the file into the buffer at the given offset in the buffer.
The return value is the number of bytes actually read; callers should check this value, as it may be less than length.
The readFully() methods will read length bytes into the buffer (or buffer.length bytes for the version.
Finally, bear in mind that calling seek() is a relatively expensive operation and should be used sparingly.
You should structure your application access patterns to rely on streaming data (by using MapReduce, for example) rather than performing a large number of seeks.
Writing Data The FileSystem class has a number of methods for creating a file.
The simplest is the method that takes a Path object for the file to be created and returns an output stream to write to:
There are overloaded versions of this method that allow you to specify whether to forcibly overwrite existing files, the replication factor of the file, the buffer size to use when writing the file, the block size for the file, and file permissions.
The create() methods create any parent directories of the file to be written that don’t already exist.
If you want the write to fail when the parent directory doesn’t exist, you should check for the existence of the parent directory first by calling the exists() method.
There’s also an overloaded method for passing a callback interface, Progressable, so your application can be notified of the progress of the data being written to the datanodes:
As an alternative to creating a new file, you can append to an existing file using the append() method (there are also some other overloaded versions):
The append operation allows a single writer to modify an already written file by opening it and writing data from the final offset in the file.
With this API, applications that produce unbounded files, such as logfiles, can write to an existing file after having.
Currently, none of the other Hadoop filesystems call progress() during writes.
Progress is important in MapReduce applications, as you will see in later chapters.
This is because HDFS allows only sequential writes to an open file or appends to an already written file.
In other words, there is no support for writing to anywhere other than the end of the file, so there is no value in being able to seek while writing.
This method creates all of the necessary parent directories if they don’t already exist, just like the java.io.File’s mkdirs() method.
It returns true if the directory (and all parent directories) was (were) successfully created.
Often, you don’t need to explicitly create a directory, because writing a file by calling create() will automatically create any parent directories.
However, if you are interested only in the existence of a file or directory, the exists() method on FileSys tem is more convenient:
Finding information on a single file or directory is useful, but you also often need to be able to list the contents of a directory.
It is a common requirement to process sets of files in a single operation.
For example, a MapReduce job for log processing might analyze a month’s worth of files contained in a number of directories.
Rather than having to enumerate each file and directory to specify the input, it is convenient to use wildcard characters to match multiple files with a single expression, an operation that is known as globbing.
The globStatus() method returns an array of FileStatus objects whose paths match the supplied pattern, sorted by path.
An optional PathFilter can be specified to restrict the matches further.
Hadoop supports the same set of glob characters as Unix bash (see Table 3-2)
Imagine that logfiles are stored in a directory structure organized hierarchically by date.
Glob patterns are not always powerful enough to describe a set of files you want to access.
For example, it is not generally possible to exclude a particular file using a glob pattern.
The listStatus() and globStatus() methods of FileSystem take an optional PathFilter, which allows programmatic control over matching:
The filter passes only those files that don’t match the regular expression.
After the glob picks out an initial set of files to include, the filter is used to refine the results.
Filters can act only on a file’s name, as represented by a Path.
They can’t use a file’s properties, such as creation time, as the basis of the filter.
Nevertheless, they can perform matching that neither glob patterns nor regular expressions can achieve.
For example, if you store files in a directory structure that is laid out by date (like in the previous section), you can write a PathFilter to pick out files that fall in a given date range.
Deleting Data Use the delete() method on FileSystem to permanently remove files or directories:
If f is a file or an empty directory, the value of recursive is ignored.
A nonempty directory is deleted, along with its contents, only if recursive is true (otherwise, an IOException is thrown)
Anatomy of a File Read To get an idea of how data flows between the client interacting with HDFS, the namenode, and the datanodes, consider Figure 3-2, which shows the main sequence of events when reading a file.
For each block, the namenode returns the addresses of the datanodes that have a copy of that block.
If the client is itself a datanode (in the case of a MapReduce task, for instance), the client will read from the local datanode if that datanode hosts a copy of the block (see also Figure 2-2)
FSDataInputStream in turn wraps a DFSInputStream, which manages the datanode and namenode I/O.
The client then calls read() on the stream (step 3)
DFSInputStream, which has stored the datanode addresses for the first few blocks in the file, then connects to the first (closest) datanode for the first block in the file.
Data is streamed from the datanode back to the client, which calls read() repeatedly on the stream (step 4)
When the end of the block is reached, DFSInputStream will close the connection to the datanode, then find the best datanode for the next block (step 5)
This happens transparently to the client, which from its point of view is just reading a continuous stream.
Blocks are read in order, with the DFSInputStream opening new connections to datanodes as the client reads through the stream.
It will also call the namenode to retrieve the datanode locations for the next batch of blocks as needed.
When the client has finished reading, it calls close() on the FSDataInputStream (step 6)
During reading, if the DFSInputStream encounters an error while communicating with a datanode, it will try the next closest one for that block.
It will also remember datanodes that have failed so that it doesn’t needlessly retry them for later blocks.
The DFSInputStream also verifies checksums for the data transferred to it from the datanode.
If a corrupted block is found, it is reported to the namenode before the DFSInput Stream attempts to read a replica of the block from another datanode.
One important aspect of this design is that the client contacts datanodes directly to retrieve data and is guided by the namenode to the best datanode for each block.
This design allows HDFS to scale to a large number of concurrent clients because the data.
Meanwhile, the namenode merely has to service block location requests (which it stores in memory, making them very efficient) and does not, for example, serve data, which would quickly become a bottleneck as the number of clients grew.
The idea is to use the bandwidth between two nodes as a measure of distance.
Rather than measuring bandwidth between nodes, which can be difficult to do in practice (it requires a quiet cluster, and the number of pairs of nodes in a cluster grows as the square of the number of nodes), Hadoop takes a simple approach in which the network is represented as a tree and the distance between two nodes is the sum of their distances to their closest common ancestor.
Levels in the tree are not predefined, but it is common to have levels that correspond to the data center, the rack, and the node that a process is running on.
The idea is that the bandwidth available for each of the following scenarios becomes progressively less:
Using this notation, here are the distances for the four scenarios:
Mathematically inclined readers will notice that this is an example of a distance metric.) Finally, it is important to realize that Hadoop cannot divine your network topology for you.
For small clusters, this may actually be the case, and no further configuration is required.
At the time of this writing, Hadoop is not suited for running across data centers.
Anatomy of a File Write Next we’ll look at how files are written to HDFS.
Although quite detailed, it is instructive to understand the data flow because it clarifies HDFS’s coherency model.
We’re going to consider the case of creating a new file, writing data to it, then closing the file.
The namenode performs various checks to make sure the file doesn’t already exist and that the client has the right permissions to create the file.
If these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an IOException.
Just as in the read case, FSDataOutputStream wraps a DFSOutput Stream, which handles communication with the datanodes and namenode.
As the client writes data (step 3), DFSOutputStream splits it into packets, which it writes to an internal queue, called the data queue.
The data queue is consumed by the Data Streamer, which is responsible for asking the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas.
The list of datanodes forms a pipeline, and here we’ll assume the replication level is three, so there are three nodes in the pipeline.
The DataStreamer streams the packets to the first datanode in the pipeline, which stores the packet and forwards it to the second datanode in the pipeline.
Similarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipeline (step 4)
DFSOutputStream also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the ack queue.
A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline (step 5)
If a datanode fails while data is being written to it, then the following actions are taken, which are transparent to the client writing the data.
First, the pipeline is closed, and any packets in the ack queue are added to the front of the data queue so that datanodes that are downstream from the failed node will not miss any packets.
The current block on the good datanodes is given a new identity, which is communicated to the namenode, so that the partial block on the failed datanode will be deleted if the failed datanode recovers later on.
The failed datanode is removed from the pipeline, and the remainder of the block’s data is written to the two good datanodes in the pipeline.
The namenode notices that the block is under-replicated, and it arranges for a further replica to be created on another node.
It’s possible, but unlikely, that multiple datanodes fail while a block is being written.
When the client has finished writing data, it calls close() on the stream (step 6)
Replica Placement How does the namenode choose which datanodes to store replicas on? There’s a tradeoff between reliability and write bandwidth and read bandwidth here.
For example, placing all replicas on a single node incurs the lowest write bandwidth penalty since the replication pipeline runs on a single node, but this offers no real redundancy (if the node fails, the data for that block is lost)
At the other extreme, placing replicas in different data centers may maximize redundancy, but at the cost of bandwidth.
Even in the same data center (which is what all Hadoop clusters to date have run in), there are a variety of placement strategies.
Indeed, Hadoop changed its placement strategy in release 0.17.0 to one that helps keep a fairly even distribution of blocks across the cluster.
Hadoop’s default strategy is to place the first replica on the same node as the client (for clients running outside the cluster, a node is chosen at random, although the system tries not to pick nodes that are too full or too busy)
The second replica is placed on a different rack from the first (off-rack), chosen at random.
The third replica is placed on the same rack as the second, but on a different node chosen at random.
Further replicas are placed on random nodes on the cluster, although the system tries to avoid placing too many replicas on the same rack.
Once the replica locations have been chosen, a pipeline is built, taking network topology into account.
Overall, this strategy gives a good balance among reliability (blocks are stored on two racks), write bandwidth (writes only have to traverse a single network switch), read performance (there’s a choice of two racks to read from), and block distribution across the cluster (clients only write a single block on the local rack)
Coherency Model A coherency model for a filesystem describes the data visibility of reads and writes for a file.
After creating a file, it is visible in the filesystem namespace, as expected:
However, any content written to the file is not guaranteed to be visible, even if the stream is flushed.
So the file appears to have a length of zero:
Once more than a block’s worth of data has been written, the first block will be visible to new readers.
This is true of subsequent blocks, too: it is always the current block being written that is not visible to other readers.
After a successful return from sync(), HDFS guarantees that the data written up to that point in the file has reached all the datanodes in the write pipeline and is visible to all new readers:9
Post Hadoop 1.x, sync() is deprecated in favor of the equivalent hflush() method.
Another method, hsync(), has also been added that makes a stronger guarantee that the operating system has flushed the data to the datanodes’ disks (like POSIX fsync)
However, at the time of this writing, this has not been implemented and merely calls hflush()
This behavior is similar to the fsync system call in POSIX that commits buffered data for a file descriptor.
For example, using the standard Java API to write a local file, we are guaranteed to see the content after flushing the stream and synchronizing:
This coherency model has implications for the way you design applications.
With no calls to sync(), you should be prepared to lose up to a block of data in the event of client or system failure.
For many applications, this is unacceptable, so you should call sync() at suitable points, such as after writing a certain number of records or number of bytes.
Though the sync() operation is designed to not unduly tax HDFS, it does have some overhead, so there is a trade-off between data robustness and throughput.
Data Ingest with Flume and Sqoop Rather than writing an application to move data into HDFS, it’s worth considering some of the existing tools for ingesting data because they cover many of the common requirements.
Apache Flume (http://incubator.apache.org/flume/) is a system for moving large quantities of streaming data into HDFS.
Flume supports a large variety of sources; some of the more commonly used ones include tail (which pipes data from a local file being written to into Flume, just like Unix tail), syslog, and Apache log4j (allowing Java applications to write events to files in HDFS via Flume)
Typically there is a node running on each source machine (each web server, for example), with tiers of aggregating nodes that the data flows through on its way to HDFS.
Flume offers different levels of delivery reliability, from best-effort delivery, which doesn’t tolerate any Flume node failures, to end-to-end, which guarantees delivery even in the event of multiple Flume node failures between the source and HDFS.
Apache Sqoop (http://sqoop.apache.org/), on the other hand, is designed for performing bulk imports of data into HDFS from structured data stores, such as relational databases.
An example of a Sqoop use case is an organization that runs a nightly Sqoop import to load the day’s data from a production database into a Hive data warehouse for analysis.
Parallel Copying with distcp The HDFS access patterns that we have seen so far focus on single-threaded access.
Hadoop comes with a useful program called distcp for copying large amounts of data to and from Hadoop filesystems in parallel.
The canonical use case for distcp is for transferring data between two HDFS clusters.
If the clusters are running identical versions of Hadoop, the hdfs scheme is appropriate:
This will copy the /foo directory (and its contents) from the first cluster to the /bar directory on the second cluster, so the second cluster ends up with the directory structure /bar/foo.
You can specify multiple source paths, and all will be copied to the destination.
By default, distcp will skip files that already exist in the destination, but they can be overwritten by supplying the -overwrite option.
You can also update only the files that have changed using the -update option.
Using either (or both) -overwrite or -update changes how the source and destination paths are interpreted.
If we changed a file in the /foo subtree on the first cluster from the previous example, we could synchronize the change with the second cluster by running:
The extra trailing /foo subdirectory is needed on the destination, because now the contents of the source directory are copied to the contents of the destination directory.
If you are familiar with rsync, you can think of the -overwrite or -update options as adding an implicit trailing slash to the source.) If you are unsure of the effect of a distcp operation, it is a good idea to try it out on a small test directory tree first.
There are more options to control the behavior of distcp, including ones to preserve file attributes, ignore failures, and limit the number of files or total data copied.
Run it with no options to see the usage instructions.
Each file is copied by a single map, and distcp tries to give each map approximately the same amount of data by bucketing files into roughly equal allocations.
Because it’s a good idea to get each map to copy a reasonable amount of data to minimize overheads in task setup, each map copies at least 256 MB (unless the total size of the input is less, in which case one map handles it all)
For example, 1 GB of files will be given four map tasks.
When the data size is very large, it becomes necessary to limit the number of maps in order to limit bandwidth and cluster utilization.
By default, the maximum number of maps is 20 per (tasktracker) cluster node.
This can be reduced by specifying the -m argument to distcp.
When you try to use distcp between two HDFS clusters that are running different versions, the copy will fail if you use the hdfs protocol because the RPC systems are incompatible.
To remedy this, you can use the read-only HTTP-based HFTP filesystem to read from the source.
The job must run on the destination cluster so that the HDFS RPC versions are compatible.
Note that you need to specify the namenode’s web port in the source URI.
Using the newer webhdfs protocol (which replaces hftp), it is possible to use HTTP for both the source and destination clusters without hitting any wire incompatibility problems.
Keeping an HDFS Cluster Balanced When copying data into HDFS, it’s important to consider cluster balance.
The second and third replicas would be spread across the cluster, but this one node would be unbalanced.
By having more maps than nodes in the cluster, this problem is avoided.
For this reason, it’s best to start by running distcp with the default of 20 maps per node.
However, it’s not always possible to prevent a cluster from becoming unbalanced.
Perhaps you want to limit the number of maps so that some of the nodes can be used by other jobs.
Hadoop Archives HDFS stores small files inefficiently, since each file is stored in a block, and block metadata is held in memory by the namenode.
Thus, a large number of small files can eat up a lot of memory on the namenode.
Note, however, that small files do not take up any more disk space than is required to store the raw contents of the file.
In particular, Hadoop Archives can be used as input to MapReduce.
Using Hadoop Archives A Hadoop Archive is created from a collection of files using the archive tool.
The tool runs a MapReduce job to process the input files in parallel, so to run it, you need a running MapReduce cluster to use it.
Here are some files in HDFS that we would like to archive:
Now we can run the archive command: % hadoop archive -archiveName files.har /my/files /my.
The first option is the name of the archive, here files.har.
Here we are archiving only one source tree, the files in /my/files in HDFS, but the tool accepts multiple source trees.
The final argument is the output directory for the HAR file.
The directory listing shows what a HAR file is made of: two index files and a collection of part files (this example has just one of the latter)
The part files contain the contents of a number of the original files concatenated together, and the indexes make it possible to look up the part file that an archived file is contained in, as well as its offset and length.
All these details are hidden from the application, however, which uses the har URI scheme to interact with HAR files, using a HAR filesystem that is layered on top of the underlying filesystem (HDFS in this case)
The following command recursively lists the files in the archive:
This is quite straightforward when the filesystem that the HAR file is on is the default filesystem.
On the other hand, if you want to refer to a HAR file on a different filesystem, you need to use a different form of the path URI.
Notice in the second form that the scheme is still har to signify a HAR filesystem, but the authority is hdfs to specify the underlying filesystem’s scheme, followed by a dash and the HDFS host (localhost) and port (8020)
We can now see why HAR files must have a .har extension.
The HAR filesystem translates the har URI into a URI for the underlying filesystem by looking at the authority and path up to and including the component with the .har extension.
The remaining part of the path is the path of the file in the archive: /my/files/dir.
To delete a HAR file, you need to use the recursive form of delete because from the underlying filesystem’s point of view, the HAR file is a directory:
Limitations There are a few limitations to be aware of with HAR files.
Creating an archive creates a copy of the original files, so you need as much disk space as the files you are archiving to create the archive (although you can delete the originals once you have created the archive)
There is currently no support for archive compression, although the files that go into the archive can be compressed (HAR files are like tar files in this respect)
To add or remove files, you must re-create the archive.
In practice, this is not a problem for files that don’t change after being written, since they can be archived in batches on a regular basis, such as daily or weekly.
As noted earlier, HAR files can be used as input to MapReduce.
However, there is no archive-aware InputFormat that can pack multiple files into a single MapReduce split, so processing lots of small files, even in a HAR file, can still be inefficient.
Hadoop comes with a set of primitives for data I/O.
Some of these are techniques that are more general than Hadoop, such as data integrity and compression, but deserve special consideration when dealing with multiterabyte datasets.
Others are Hadoop tools or APIs that form the building blocks for developing distributed systems, such as serialization frameworks and on-disk data structures.
Data Integrity Users of Hadoop rightly expect that no data will be lost or corrupted during storage or processing.
However, because every I/O operation on the disk or network carries with it a small chance of introducing errors into the data that it is reading or writing, when the volumes of data flowing through the system are as large as the ones Hadoop is capable of handling, the chance of data corruption occurring is high.
The usual way of detecting corrupted data is by computing a checksum for the data when it first enters the system, and again whenever it is transmitted across a channel that is unreliable and hence capable of corrupting the data.
The data is deemed to be corrupt if the newly generated checksum doesn’t exactly match the original.
This technique doesn’t offer any way to fix the data—it is merely error detection.
And this is a reason for not using low-end hardware; in particular, be sure to use ECC memory.) Note that it is possible that it’s the checksum that is corrupt, not the data, but this is very unlikely, because the checksum is much smaller than the data.
Data Integrity in HDFS HDFS transparently checksums all data written to it and by default verifies checksums when reading data.
Datanodes are responsible for verifying the data they receive before storing the data and its checksum.
This applies to data that they receive from clients and from other datanodes during replication.
A client writing data sends it to a pipeline of datanodes (as explained in Chapter 3), and the last datanode in the pipeline verifies the checksum.
When clients read data from datanodes, they verify checksums as well, comparing them with the ones stored at the datanode.
Each datanode keeps a persistent log of checksum verifications, so it knows the last time each of its blocks was verified.
When a client successfully verifies a block, it tells the datanode, which updates its log.
Keeping statistics such as these is valuable in detecting bad disks.
Aside from block verification on client reads, each datanode runs a DataBlockScanner in a background thread that periodically verifies all the blocks stored on the datanode.
Because HDFS stores replicas of blocks, it can “heal” corrupted blocks by copying one of the good replicas to produce a new, uncorrupt replica.
The way this works is that if a client detects an error when reading a block, it reports the bad block and the datanode it was trying to read from to the namenode before throwing a ChecksumException.
The namenode marks the block replica as corrupt so it doesn’t direct clients to it or try to copy this replica to another datanode.
It then schedules a copy of the block to be replicated on another datanode, so its replication factor is back at the expected level.
It is possible to disable verification of checksums by passing false to the setVerify Checksum() method on FileSystem before using the open() method to read a file.
The same effect is possible from the shell by using the -ignoreCrc option with the -get or the equivalent -copyToLocal command.
This feature is useful if you have a corrupt file that you want to inspect so you can decide what to do with it.
For example, you might want to see whether it can be salvaged before you delete it.
This means that when you write a file called filename, the filesystem client transparently creates a hidden file, .filename.crc, in the same directory containing the checksums for each chunk of the file.
The chunk size is stored as metadata in the .crc file, so the.
Checksums are verified when the file is read, and if an error is detected, LocalFileSystem throws a ChecksumException.
Checksums are fairly cheap to compute (in Java, they are implemented in native code), typically adding a few percent overhead to the time to read or write a file.For most applications, this is an acceptable price to pay for data integrity.
It is, however, possible to disable checksums, typically when the underlying filesystem supports checksums natively.
This is accomplished by using RawLocalFileSystem in place of Local FileSystem.
To do this globally in an application, it suffices to remap the implementation for file URIs by setting the property fs.file.impl to the value org.apache.
Alternatively, you can directly create a RawLocalFile System instance, which may be useful if you want to disable checksum verification for only some reads, for example:
ChecksumFileSystem LocalFileSystem uses ChecksumFileSystem to do its work, and this class makes it easy to add checksumming to other (nonchecksummed) filesystems, as Checksum FileSystem is just a wrapper around FileSystem.
The underlying filesystem is called the raw filesystem, and may be retrieved using the getRawFileSystem() method on ChecksumFileSystem.
ChecksumFileSystem has a few more useful methods for working with checksums, such as getChecksumFile() for getting the path of a checksum file for any file.
The default implementation does nothing, but LocalFileSystem moves the offending file and its checksum to a side directory on the same device called bad_files.
Administrators should periodically check for these bad files and take action on them.
Compression File compression brings two major benefits: it reduces the space needed to store files, and it speeds up data transfer across the network or to or from disk.
When dealing with large volumes of data, both of these savings can be significant, so it pays to carefully consider how to use compression in Hadoop.
There are many different compression formats, tools and algorithms, each with different characteristics.
Table 4-1 lists some of the more common ones that can be used with Hadoop.
Snappy N/A Snappy .snappy No a DEFLATE is a compression algorithm whose standard implementation is zlib.
Note that the gzip file format is DEFLATE with extra headers and a footer.) The .deflate filename extension is a Hadoop convention.
All compression algorithms exhibit a space/time trade-off: faster compression and decompression speeds usually come at the expense of smaller space savings.
For example, the following command creates a compressed file file.gz using the fastest compression method:
Gzip is a generalpurpose compressor and sits in the middle of the space/time trade-off.
Bzip2’s decompression speed is faster than its compression speed, but it is still slower than the other formats.
In Hadoop, a codec is represented by an implementation of the CompressionCodec interface.
So, for example, GzipCodec encapsulates the compression and decompression algorithm for gzip.
Table 4-2 lists the codecs that are available for Hadoop.
The LzopCodec is compatible with the lzop tool, which is essentially the LZO format with extra headers, and is the one you normally want.
There is also a LzoCodec for the pure LZO format, which uses the .lzo_deflate filename extension (by analogy with DEFLATE, which is gzip without the headers)
CompressionCodec has two methods that allow you to easily compress or decompress data.
Example 4-1 illustrates how to use the API to compress data read from standard input and write it to standard output.
The application expects the fully qualified name of the CompressionCodec implementation as the first command-line argument.
We use ReflectionUtils to construct a new instance of the codec, then obtain a compression wrapper around System.out.
We can try it out with the following command line, which compresses the string “Text” using the StreamCompressor program with the GzipCodec, then decompresses it from standard input using gunzip:
In this way, a file named file.gz is decompressed to file by invoking the program as follows:
By default, this lists all the codecs provided by Hadoop (see Table 4-3), so you would need to alter it only if you have a custom codec that you wish to register (such as the externally hosted LZO codecs)
For performance, it is preferable to use a native library for compression and decompression.
Not all formats have native implementations (bzip2, for example), whereas others are available only as a native implementation (LZO, for example)
The hadoop script in the bin directory sets this property for you, but if you don’t use this script, you will need to set the property in your application.
By default, Hadoop looks for native libraries for the platform it is running on, and loads them automatically if they are found.
This means you don’t have to change any configuration settings to use the native libraries.
In some circumstances, however, you may wish to disable use of native libraries, such as when you are debugging a compressionrelated problem.
By using a finally block, we ensure that the compressor is returned to the pool even if there is an IOException while copying the bytes between the streams.
Compression and Input Splits When considering how to compress data that will be processed by MapReduce, it is important to understand whether the compression format supports splitting.
Consider an uncompressed file stored in HDFS whose size is 1 GB.
Imagine now that the file is a gzip-compressed file whose compressed size is 1 GB.
As before, HDFS will store the file as 16 blocks.
However, creating a split for each block won’t work, because it is impossible to start reading at an arbitrary point in the gzip stream and therefore impossible for a map task to read its split independently of the others.
The gzip format uses DEFLATE to store the compressed data, and DEFLATE stores data as a series of compressed blocks.
The problem is that the start of each block is not distinguished in any way that would allow a reader positioned at an arbitrary point in the stream to advance to the beginning of the next block, thereby synchronizing itself with the stream.
In this case, MapReduce will do the right thing and not try to split the gzipped file, since it knows that the input is gzip-compressed (by looking at the filename extension) and that gzip does not support splitting.
This will work, but at the expense of locality: a single map will process the 16 HDFS blocks, most of which will not be local to the map.
Also, with fewer maps, the job is less granular and so may take longer to run.
If the file in our hypothetical example were an LZO file, we would have the same problem because the underlying compression format does not provide a way for a reader to synchronize itself with the stream.
Which Compression Format Should I Use? Hadoop applications process large datasets, so you should strive to take advantage of compression.
Which compression format you use depends on such considerations as file size, format, and the tools you are using for processing.
Here are some suggestions, arranged roughly in order of most to least effective:
A fast compressor such as LZO, LZ4, or Snappy is generally a good choice.
Split the file into chunks in the application, and compress each chunk separately using any supported compression format (it doesn’t matter whether it is splittable)
In this case, you should choose the chunk size so that the compressed chunks are approximately the size of an HDFS block.
For large files, you should not use a compression format that does not support splitting on the whole file, because you lose locality and make MapReduce applications very inefficient.
We run the program over compressed input (which doesn’t have to use the same compression format as the output, although it does in this example) as follows:
The configuration properties to set compression for MapReduce job outputs are summarized in Table 4-5
Even if your MapReduce application reads and writes uncompressed data, it may benefit from compressing the intermediate output of the map phase.
Since the map output is written to disk and transferred across the network to the reducer nodes, by using a fast compressor such as LZO, LZ4, or Snappy, you can get performance gains simply because the volume of data to transfer is reduced.
The configuration properties to enable compression for map outputs and to set the compression format are shown in Table 4-6
Here are the lines to add to enable gzip map output compression in your job (using the new API):
In the old API, there are convenience methods on the JobConf object for doing the same thing:
Serialization Serialization is the process of turning structured objects into a byte stream for transmission over a network or for writing to persistent storage.
Deserialization is the reverse process of turning a byte stream back into a series of structured objects.
Serialization appears in two quite distinct areas of distributed data processing: for interprocess communication and for persistent storage.
In Hadoop, interprocess communication between nodes in the system is implemented using remote procedure calls (RPCs)
The RPC protocol uses serialization to render the message into a binary stream to be sent to the remote node, which then deserializes the binary stream into the original message.
In general, it is desirable that an RPC serialization format is: Compact.
A compact format makes the best use of network bandwidth, which is the most scarce resource in a data center.
Fast Interprocess communication forms the backbone for a distributed system, so it is essential that there is as little performance overhead as possible for the serialization and deserialization process.
Extensible Protocols change over time to meet new requirements, so it should be straightforward to evolve the protocol in a controlled manner for clients and servers.
For example, it should be possible to add a new argument to a method call and have the new servers accept messages in the old format (without the new argument) from old clients.
Interoperable For some systems, it is desirable to be able to support clients that are written in different languages to the server, so the format needs to be designed to make this possible.
On the face of it, the data format chosen for persistent storage would have different requirements from a serialization framework.
After all, the lifespan of an RPC is less than a second, whereas persistent data may be read years after it was written.
As it turns out, the four desirable properties of an RPC’s serialization format are also crucial for a persistent storage format.
We want the storage format to be compact (to make efficient use of storage space), fast (so the overhead in reading or writing terabytes of data is minimal), extensible (so we can transparently read data written in an older format), and interoperable (so we can read or write persistent data using different languages)
Hadoop uses its own serialization format, Writables, which is certainly compact and fast, but not so easy to extend or use from languages other than Java.
Because Writables are central to Hadoop (most MapReduce programs use them for their key and value.
The Writable Interface The Writable interface defines two methods: one for writing its state to a DataOutput binary stream and one for reading its state from a DataInput binary stream.
Let’s look at a particular Writable to see what we can do with it.
We will use IntWritable, a wrapper for a Java int.
We can create one and set its value using the set() method:
Again, we create a helper method to read a Writable object from a byte array:
We construct a new, value-less IntWritable, and then call deserialize() to read from the output data that we just wrote.
Then we check that its value, retrieved using the get() method, is the original value, 163:
Comparison of types is crucial for MapReduce, where there is a sorting phase during which keys are compared with one another.
This interface permits implementors to compare records read from a stream without deserializing them into objects, thereby avoiding any overhead of object creation.
WritableComparator is a general-purpose implementation of RawComparator for WritableComparable classes.
First, it provides a default implementation of the raw compare() method that deserializes the objects to be compared from the stream and invokes the object compare() method.
For example, to obtain a comparator for IntWritable, we just use:
There are Writable wrappers for all the Java primitive types (see Table 4-7) except char (which can be stored in an IntWritable)
All have a get() and set() method for retrieving and storing the wrapped value.
How do you choose between a fixed-length and a variable-length encoding? Fixedlength encodings are good when the distribution of values is fairly uniform across the whole value space, such as a (well-designed) hash function.
Most numeric variables tend to have nonuniform distributions, and on average the variable-length encoding will save space.
Another advantage of variable-length encodings is that you can switch from VIntWritable to VLongWritable, because their encodings are actually the same.
So by choosing a variable-length representation, you have room to grow without committing to an 8-byte long representation from the beginning.
The Text class uses an int (with a variable-length encoding) to store the number of bytes in the string encoding, so the maximum value is 2 GB.
Because of its emphasis on using standard UTF-8, there are some differences between Text and the Java String class.
Indexing for the Text class is in terms of position in the encoded byte sequence, not the Unicode character in the string or the Java char code unit (as it is for String)
For ASCII strings, these three concepts of index position coincide.
Here is an example to demonstrate the use of the charAt() method:
Notice that charAt() returns an int representing a Unicode code point, unlike the String variant that returns a char.
Text also has a find() method, which is analogous to String’s indexOf():
When we start using characters that are encoded with more than a single byte, the differences between Text and String become clear.
Similarly, the indexOf() method in String returns an index in char code units, and find() for Text is a byte offset.
The charAt() method in String returns the char code unit for the given index, which in the case of a surrogate pair will not represent a whole Unicode character.
The code PointAt() method, indexed by char code unit, is needed to retrieve a single Unicode character represented as an int.
In fact, the charAt() method in Text is more like the codePointAt() method than its namesake in String.
The only difference is that it is indexed by byte offset.
Running the program prints the code points for the four characters in the string:
Another difference with String is that Text is mutable (like all Writable implementations in Hadoop, except NullWritable, which is a singleton)
You can reuse a Text instance by calling one of the set() methods on it.
In some situations, the byte array returned by the getBytes() method may be longer than the length returned by getLength():
This shows why it is imperative that you always call getLength() when calling getBytes(), so you know how much of the byte array is valid data.
This is done in the usual way, using the toString() method:
BytesWritable is a wrapper for an array of binary data.
Its serialized format is an integer field (4 bytes) that specifies the number of bytes to follow, followed by the bytes themselves.
BytesWritable is mutable, and its value may be changed by calling its set() method.
You can determine the size of the BytesWritable by calling get Length()
NullWritable is a special type of Writable, as it has a zero-length serialization.
No bytes are written to or read from the stream.
It is used as a placeholder; for example, in MapReduce, a key or a value can be declared as a NullWritable when you don’t need to use that position, effectively storing a constant empty value.
NullWritable can also be useful as a key in SequenceFile when you want to store a list of values, as opposed to key-value pairs.
ObjectWritable is a general-purpose wrapper for the following: Java primitives, String, enum, Writable, null, or arrays of any of these types.
It is used in Hadoop RPC to marshal and unmarshal method arguments and return types.
ObjectWritable is useful when a field can be of more than one type.
For example, if the values in a SequenceFile have multiple types, you can declare the value type as an ObjectWritable and wrap each type in an ObjectWritable.
Being a general-purpose mechanism, it wasted a fair amount of space because it writes the classname of the wrapped type every time it is serialized.
In cases where the number of types is small and known ahead of time, this can be improved by having a static array of types and using the index into the array as the serialized reference to the type.
This is the approach that GenericWritable takes, and you have to subclass it to specify which types to support.
ArrayWritable and TwoDArrayWritable are Writable implementations for arrays and two-dimensional arrays (array of arrays) of Writable instances.
All the elements of an ArrayWritable or a TwoDArrayWritable must be instances of the same class, which is specified at construction as follows:
In contexts where the Writable is defined by type, such as in SequenceFile keys or values or as input to MapReduce in general, you need to subclass ArrayWritable (or TwoDAr rayWritable, as appropriate) to set the type statically.
ArrayWritable and TwoDArrayWritable both have get() and set() methods, as well as a toArray() method, which creates a shallow copy of the array (or 2D array)
The component type is detected when you call set(), so there is no need to subclass to set the type.
The type of each key and value field is a part of the serialization format for that field.
The type is stored as a single byte that acts as an index into an array of types.
As they are implemented, MapWritable and SortedMapWritable use positive byte values for custom types, so a maximum of 127 distinct nonstandard Writable classes can be used in any particular MapWritable or SortedMapWritable instance.
Here’s a demonstration of using a MapWritable with different types for keys and values:
Conspicuous by their absence are Writable collection implementations for sets and lists.
A general set can be emulated by using a MapWritable (or a SortedMapWritable for a sorted set) with NullWritable values.
For lists of a single type of Writable, ArrayWritable is adequate, but to store different types of Writable in a single list, you can use GenericWritable to wrap the elements in an ArrayWritable.
Alternatively, you could write a general ListWritable using the ideas from MapWritable.
Implementing a Custom Writable Hadoop comes with a useful set of Writable implementations that serve most purposes; however, on occasion, you may need to write your own custom implementation.
With a custom Writable, you have full control over the binary representation and the sort order.
Because Writables are at the heart of the MapReduce data path, tuning the binary representation can have a significant effect on performance.
The stock Writable implementations that come with Hadoop are well-tuned, but for more elaborate structures, it is often better to create a new Writable type rather than compose the stock types.
To demonstrate how to create a custom Writable, we shall write an implementation that represents a pair of strings, called TextPair.
A Writable implementation that stores a pair of Text objects import java.io.*;
The first part of the implementation is straightforward: there are two Text instance variables, first and second, and associated constructors, getters, and setters.
All Writable implementations must have a default constructor so that the MapReduce framework can instantiate them, then populate their fields by calling readFields()
Writable instances are mutable and often reused, so you should take care to avoid allocating objects in the write() or readFields() methods.
TextPair’s write() method serializes each Text object in turn to the output stream by delegating to the Text objects themselves.
Similarly, readFields() deserializes the bytes from the input stream by delegating to each Text object.
The DataOutput and DataInput interfaces have a rich set of methods for serializing and deserializing Java primitives, so, in general, you have complete control over the wire format of your Writable object.
The hash Code() method is used by the HashPartitioner (the default partitioner in MapReduce) to choose a reduce partition, so you should make sure that you write a good hash function that mixes well to ensure reduce partitions are of a similar size.
If you ever plan to use your custom Writable with TextOutputFormat, then you must implement its toString() method.
TextOutputFormat calls toString() on keys and values for their output representation.
For Text Pair, we write the underlying Text objects as strings separated by a tab character.
Notice that TextPair differs from TextArrayWrita ble from the previous section (apart from the number of Text objects it can store), since TextArrayWritable is only a Writable, not a WritableComparable.
We actually subclass WritableComparator rather than implement RawComparator directly, since it provides some convenience methods and default implementations.
Each is made up of the length of the variable-length integer (returned by decodeVIntSize() on WritableUtils) and the value it is encoding (returned by readVInt())
The static block registers the raw comparator so that whenever MapReduce sees the TextPair class, it knows to use the raw comparator as its default comparator.
Serialization Frameworks Although most MapReduce programs use Writable key and value types, this isn’t mandated by the MapReduce API.
In fact, any type can be used; the only requirement is a mechanism that translates to and from a binary representation of each type.
A Serialization defines a mapping from types to Serializer instances (for turning an object into a byte stream) and Deserializer instances (for turning a byte stream into an object)
Here’s what Doug Cutting said in response to that question:
Why didn’t I use Serialization when we first started Hadoop? Because it looked big and hairy and I thought we needed something lean and mean, where we had precise control over exactly how objects are written and read, since that is central to Hadoop.
With Serialization you can get some control, but you have to fight for it.
I felt like we’d need to precisely control how things like connections, timeouts and buffers are handled, and RMI gives you little control over those.
The problem is that Java Serialization doesn’t meet the criteria for a serialization format listed earlier: compact, fast, extensible, and interoperable.
Subsequent instances of the same class write a reference handle to the first occurrence, which occupies only 5 bytes.
However, reference handles don’t work well with random access, because the referent class may occur at any point in the preceding stream—that is, there is state stored in the stream.
Even worse, reference handles play havoc with sorting records in a serialized stream, since the first record of a particular class is distinguished and must be treated as a special case.
All these problems are avoided by not writing the classname to the stream at all, which is the approach that Writable takes.
This makes the assumption that the client knows the expected type.
The result is that the format is considerably more compact than Java Serialization, and random access and sorting work as expected because each record is independent of the others (so there is no stream state)
Java Serialization is a general-purpose mechanism for serializing graphs of objects, so it necessarily has some overhead for serialization and deserialization operations.
What’s more, the deserialization procedure creates a new instance for each object deserialized from the stream.
Writable objects, on the other hand, can be (and often are) reused.
For example, for a MapReduce job, which at its core serializes and deserializes billions of records of just a handful of different types, the savings gained by not having to allocate new objects are significant.
In terms of extensibility, Java Serialization has some support for evolving a type, but it is brittle and hard to use effectively (Writables have no support; the programmer has to manage them himself)
In principle, other languages could interpret the Java Serialization stream protocol (defined by the Java Object Serialization Specification), but in practice there are no widely used implementations in other languages, so it is a Java-only solution.
There are a number of other serialization frameworks that approach the problem in a different way: rather than defining types through code, you define them in a languageneutral, declarative fashion, using an interface description language (IDL)
The system can then generate types for different languages, which is good for interoperability.
They also typically define versioning schemes that make type evolution straightforward.
For whatever reason, however, Record I/O was not widely used, and has been deprecated in favor of Avro.
Apache Thrift  and Google Protocol Buffers are both popular serialization frameworks, and they are commonly used as a format for persistent binary data.
There is limited support for these as MapReduce formats;3 however, they are used internally in parts of Hadoop for RPC and data exchange.
In the next section, we look at Avro, an IDL-based serialization framework designed to work well with large-scale data processing in Hadoop.
The project was created by Doug Cutting (the creator of Hadoop) to address the major downside of Hadoop Writables: lack of language portability.
It is also more future-proof, allowing data to potentially outlive the language used to read and write it.
However, unlike some other systems, code generation is optional in Avro, which means you can read and write data that conforms to a given schema even if your code has not seen that particular schema before.
Avro schemas are usually written in JSON, and data is usually encoded using a binary format, but there are other options, too.
There is a higher-level language called Avro IDL for writing schemas in a C-like language that is more familiar to developers.
There is also a JSON-based data encoder, which, being human-readable, is useful for prototyping and debugging Avro data.
The Avro specification precisely defines the binary format that all implementations must support.
It also specifies many of the other features of Avro that implementations should support.
One area that the specification does not rule on, however, is APIs:
Named after the British aircraft manufacturer from the 20th century.
The fact that there is only one binary format is significant, because it means the barrier for implementing a new language binding is lower and avoids the problem of a combinatorial explosion of languages and formats, which would harm interoperability.
Within certain carefully defined constraints, the schema used to read data need not be identical to the schema that was used to write the data.
This is the mechanism by which Avro supports schema evolution.
For example, a new, optional field may be added to a record by declaring it in the schema used to read the old data.
New and old clients alike will be able to read the old data, while new clients can write new data that uses the new field.
Conversely, if an old client sees newly encoded data, it will gracefully ignore the new field and carry on processing as it would have done with old data.
Avro specifies an object container format for sequences of objects—similar to Hadoop’s sequence file.
An Avro datafile has a metadata section where the schema is stored, which makes the file self-describing.
Avro datafiles support compression and are splittable, which is crucial for a MapReduce data input format.
Furthermore, since Avro was designed with MapReduce in mind, in the future it will be possible to use Avro to bring first-class MapReduce APIs (that is, ones that are richer than Streaming, such as the Java API or C++ Pipes) to languages that speak Avro.
Avro can be used for RPC, too, although this isn’t covered here.
Each primitive type may also be specified using a more verbose form by using the type attribute, such as:
Avro also defines the complex types listed in Table 4-10, along with a representative example of a schema of each type.
All objects in a particular array must have the same schema.
Keys must be strings and values may be any type, although within a particular map, all values must have the same schema.
A union is represented by a JSON array, where each element in the array is a schema.
Data represented by a union must match one of the schemas in the union.
Each Avro language API has a representation for each Avro type that is specific to the language.
For example, Avro’s double type is represented in C, C++, and Java by a double, in Python by a float, and in Ruby by a Float.
What’s more, there may be more than one representation, or mapping, for a language.
All languages support a dynamic mapping, which can be used even when the schema is not known ahead of runtime.
In addition, the Java and C++ implementations can generate code to represent the data for an Avro schema.
Code generation, which is called the specific mapping in Java, is an optimization that is useful when you have a copy of the schema before you read or write data.
Generated classes also provide a more domain-oriented API for user code than generic ones.
Java has a third mapping, the reflect mapping, which maps Avro types onto preexisting Java types using reflection.
It is slower than the generic and specific mappings, and is generally not recommended for new applications.
As the table shows, the specific mapping is the same as the generic one unless otherwise noted (and the reflect one is the same as the specific one unless noted)
The specific mapping differs from the generic one only for record, enum, and fixed, all of which have generated classes (the name of which is controlled by the name and optional namespace attribute)
Avro string can be represented by either Java String or the Avro Utf8 Java type.
In other cases it may be necessary to convert Utf8 instances to String objects by calling its toString() method.
From Avro 1.6.0 onward, there is an option to have Avro always perform the conversion to String.
Alternatively, for the specific mapping, you can generate classes that have String-based getters and setters.
When using the Avro Maven plugin, this is done by setting the configuration property stringType to String (the example code that accompanies the book has a demonstration of this)
Finally, note that the Java reflect mapping always uses String objects, since it is designed for Java compatibility, not performance.
In-Memory Serialization and Deserialization Avro provides APIs for serialization and deserialization, which are useful when you want to integrate Avro with an existing system, such as a messaging system where the framing format is already defined.
Let’s write a Java program to read and write Avro data to and from streams.
We’ll start with a simple Avro schema for representing a pair of strings as a record:
If this schema is saved in a file on the classpath called StringPair.avsc (.avsc is the conventional extension for an Avro schema), we can load it using the following two lines of code:
There are two important objects here: the DatumWriter and the Encoder.
A DatumWriter translates data objects into the types understood by an Encoder, which the latter writes to the output stream.
We pass a null to the encoder factory because we are not reusing a previously constructed encoder here.
In this example only one object is written to the stream, but we could call write() with more objects before closing the stream if we wanted to.
The GenericDatumWriter needs to be passed the schema because it follows the schema to determine which values from the data objects to write out.
After we have called the writer’s write() method, we flush the encoder, then close the output stream.
We can reverse the process and read the object back from the byte buffer:
We pass null to the calls to binaryDecoder() and read() because we are not reusing objects here (the decoder or the record, respectively)
Let’s look now at the equivalent code using the specific API.
We can generate the StringPair class from the schema file by using Avro’s Maven plug-in for compiling schemas.
The following is the relevant part of the Maven Project Object Model (POM):
Avro can be downloaded in both source and binary forms from http://avro.apache.org/releases.html.
Get usage instructions for the Avro tools by typing java -jar avro-tools-*.jar.
Avro Datafiles Avro’s object container file format is for storing sequences of Avro objects.
The main difference is that Avro datafiles are designed to be portable across languages, so, for example, you can write a file in Python and read it in C (we will do exactly this in the next section)
A datafile has a header containing metadata, including the Avro schema and a sync marker, followed by a series of (optionally compressed) blocks containing the serialized Avro objects.
Blocks are separated by a sync marker that is unique to the file (the marker for a particular file is found in the header) and that permits rapid resynchronization with a block boundary after seeking to an arbitrary point in the file, such as an HDFS block boundary.
Thus, Avro datafiles are splittable, which makes them amenable to efficient MapReduce processing.
Writing Avro objects to a datafile is similar to writing to a stream.
We use a DatumWriter as before, but instead of using an Encoder, we create a DataFileWriter instance with the DatumWriter.
Then we can create a new datafile (which, by convention, has a .avro extension) and append objects to it:
The objects that we write to the datafile must conform to the file’s schema; otherwise, an exception will be thrown when we call append()
Reading back objects from a datafile is similar to the earlier case of reading objects from an in-memory stream, with one important difference: we don’t have to specify a schema, since it is read from the file metadata.
Indeed, we can get the schema from the DataFileReader instance, using getSchema(), and verify that it is the same as the one we used to write the original object:
DataFileReader is a regular Java iterator, so we can iterate through its data objects by calling its hasNext() and next() methods.
The following snippet checks that there is only one record and that it has the expected field values:
Rather than using the usual next() method, however, it is preferable to use the overloaded form that takes an instance of the object to be returned (in this case, GenericRecord), since it will reuse the object and save allocation and garbage collection costs for files containing many objects.
Interoperability To demonstrate Avro’s language interoperability, let’s write a datafile using one language (Python) and read it back with another (C)
The program in Example 4-10 reads comma-separated strings from standard input and writes them as StringPair records to an Avro datafile.
Like the Java code for writing a datafile, we create a DatumWriter and a DataFileWriter object.
Notice that we have embedded the Avro schema in the code, although we could equally well have read it from a file.
Python represents Avro records as dictionaries; each line that is read from standard in is turned into a dict object and appended to the DataFileWriter.
A Python program for writing Avro record pairs to a datafile import os import string import sys.
To run the program, we specify the name of the file to write output to (pairs.avro) and send input pairs over standard in, marking the end of file by typing Ctrl-D:
For the general case, the Avro tools JAR file has a tojson command that dumps the contents of a Avro datafile as JSON.
Avro functions and types have a avro_ prefix and are defined in the avro.h header file.
The third argument is a pointer to a avro_datum_t object, which is populated with the contents of the next record read from the file.
Running the program using the output of the Python program prints the original input:
We have successfully exchanged complex data between two Avro implementations.
Schema Resolution We can choose to use a different schema for reading the data back (the reader’s schema) from the one we used to write it (the writer’s schema)
This is a powerful tool because it enables schema evolution.
To illustrate, consider a new schema for string pairs with an added description field:
We can use this schema to read the data we serialized earlier because crucially, we have given the description field a default value (the empty string),9 which Avro will use when there is no field defined in the records it is reading.
Had we omitted the default attribute, we would get an error when trying to read the old data.
See the Avro specification for a description of this encoding for each data type.
To make the default value null rather than the empty string, we would instead define the description field using a union with the null Avro type:
When the reader’s schema is different from the writer’s, we use the constructor for GenericDatumReader that takes two schema objects, the writer’s and the reader’s, in that order:
For datafiles, which have the writer’s schema stored in the metadata, we only need to specify the readers’s schema explicitly, which we can do by passing null for the writer’s schema:
Another common use of a different reader’s schema is to drop fields in a record, an operation called projection.
This is useful when you have records with a large number of fields and you want to read only some of them.
For example, this schema can be used to get only the right field of a StringPair:
The rules for schema resolution have a direct bearing on how schemas may evolve from one version to the next, and are spelled out in the Avro specification for all Avro types.
A summary of the rules for record evolution from the point of view of readers and writers (or servers and clients) is presented in Table 4-12
Added field Old New The reader uses the default value of the new field, since it is not written by the writer.
New Old The reader does not know about the new field written by the writer, so it is ignored (projection)
Removed field Old New The reader ignores the removed field (projection)
New Old The removed field is not written by the writer.
If the old schema had a default defined for the field, the reader uses this; otherwise, it gets an error.
In this case, it is best to update the reader’s schema, either at the same time as or before the writer’s.
Another useful technique for evolving Avro schemas is the use of name aliases.
Aliases allow you to use different names in the schema used to read the Avro data than in the schema originally used to write the data.
For example, the following reader’s schema can be used to read StringPair data with the new field names first and second instead of left and right (which is what it was written with)
Note that the aliases are used to translate (at read time) the writer’s schema into the reader’s, but the alias names are not available to the reader.
In this example, the reader cannot use the field names left and right, because they have already been translated to first and second.
For most Avro types, the order is the natural one you would expect—for example, numeric types are ordered by ascending numeric value.
For instance, enums are compared by the order in which the symbol is defined and not by the value of the symbol string.
All types except record have preordained rules for their sort order, as described in the Avro specification, that cannot be overridden by the user.
For records, however, you can control the sort order by specifying the order attribute for a field.
It takes one of three values: ascending (the default), descending (to reverse the order), or ignore (so the field is skipped for comparison purposes)
The left field is ignored for the purposes of ordering, but it is still present in the projection:
The record’s fields are compared pairwise in the document order of the reader’s schema.
Thus, by specifying an appropriate reader’s schema, you can impose an arbitrary ordering on data records.
That is to say, Avro does not have to deserialize a binary data into objects to perform the comparison, because it can instead work directly on the byte streams.10 In the case of the original StringPair schema (with no order attributes), for example, Avro implements the binary comparison as follows.
The first field, left, is a UTF-8-encoded string, for which Avro can compare the bytes lexicographically.
If they differ, the order is determined, and Avro can stop the comparison there.
Otherwise, if the two-byte sequences are the same, it compares the second two (right) fields, again lexicographically at the byte level because the field is another UTF-8 string.
The great thing is that Avro provides the comparator for us, so we don’t have to write and maintain this code.
It’s also easy to change the sort order just by changing the reader’s schema.
The differences are which fields are considered, the order in which they are considered, and whether the sort order is ascending or descending.
Later in the chapter we’ll use Avro’s sorting logic in conjunction with MapReduce to sort Avro datafiles in parallel.
Avro MapReduce Avro provides a number of classes for making it easy to run MapReduce programs on Avro data.
A useful consequence of this property is that you can compute an Avro datum’s hash code from either the object or the binary representation (the latter by using the static hashCode() method on BinaryData) and get the same result in both cases.
The MaxTemperatureRe ducer iterates through the records for each key (year) and finds the one with the maximum temperature.
It is necessary to make a copy of the record with the highest temperature found so far, since the iterator reuses the instance for reasons of efficiency (and only the fields are updated)
The second major difference from regular MapReduce is the use of AvroJob for configuring the job.
AvroJob is a convenience class for specifying the Avro schemas for the input, map output, and final output data.
The map output schema is a pair schema whose key schema is an Avro int and whose value schema is the weather record schema.
The final output schema is the weather record schema, and the output format is the default, AvroOutputFormat, which writes to Avro datafiles.
The following command line shows how to run the program on a small sample dataset:
On completion we can look at the output using the Avro tools JAR to render the Avro datafile as JSON, one record per line:
In this example, we used an AvroMapper and an AvroReducer, but the API supports a mixture of regular MapReduce mappers and reducers with Avro-specific ones, which is useful for converting between Avro formats and other formats, such as SequenceFiles.
See the documentation for the Avro MapReduce package for details.
This program (which uses the generic Avro mapping and hence does not require any code generation) can sort Avro records of any type, represented in Java by the generic type parameter K.
The reducer acts as an identity, passing the values through to the (singlevalued) output, which will get written to an Avro datafile.
The sorting happens in the MapReduce shuffle, and the sort function is determined by the Avro schema that is passed to the program.
First, we inspect the input using the Avro tools JAR:
Finally, we inspect the output and see that it is sorted correctly.
Avro MapReduce in Other Languages For languages other than Java, there are a few choices for working with Avro data.
Each datum in the file is converted to a string, which is the JSON representation of the datum, or just to the raw bytes if the type is Avro bytes.
At the time of this writing, there are no bindings for other languages, but a Python implementation will be available in a future release.
It’s also worth considering Pig and Hive for doing Avro processing, since both can read and write Avro datafiles by specifying the appropriate storage formats.
File-Based Data Structures For some applications, you need a specialized data structure to hold your data.
For doing MapReduce-based processing, putting each blob of binary data into its own file doesn’t scale, so Hadoop developed a number of higher-level containers for these situations.
SequenceFile Imagine a logfile where each log record is a new line of text.
If you want to log binary types, plain text isn’t a suitable format.
Hadoop’s SequenceFile class fits the bill in this situation, providing a persistent data structure for binary key-value pairs.
To use it as a logfile format, you would choose a key, such as timestamp represented by a LongWritable, and the value would be Writable that represents the quantity being logged.
Which one you use depends on the serialization framework you are using.
If you are using Writable types, you can use the next() method that takes a key.
The return value is true if a key-value pair was read and false if the end of the file has been reached.
For other, non-Writable serialization frameworks (such as Apache Thrift), you should use these two methods:
Another feature of the program is that it displays the position of the sync points in the sequence file.
A sync point is a point in the stream that can be used to resynchronize with a record boundary if the reader is “lost”—for example, after seeking to an arbitrary position in the stream.
Running the program in Example 4-15 shows the sync points in the sequence file as asterisks.
There are two ways to seek to a given position in a sequence file.
The first is the seek() method, which positions the reader at the given point in the file.
For example, seeking to a record boundary works as expected:
But if the position in the file is not at a record boundary, the reader fails when the next() method is called:
The second way to find a record boundary makes use of sync points.
This is not to be confused with the identically named but otherwise unrelated sync() method defined by the Syncable interface for synchronizing buffers to the underlying device.
Sync points come into their own when using sequence files as input to MapReduce, since they permit the file to be split and different portions of it can be processed independently by separate map tasks.
The hadoop fs command has a -text option to display sequence files in textual form.
It looks at a file’s magic number so that it can attempt to detect the type of the file and appropriately convert it to text.
It can recognize gzipped files and sequence files; otherwise, it assumes the input is plain text.
For sequence files, this command is really useful only if the keys and values have a meaningful string representation (as defined by the toString() method)
Also, if you have your own key or value classes, you will need to make sure they are on Hadoop’s classpath.
Running it on the sequence file we created in the previous section gives the following output:
The most powerful way of sorting (and merging) one or more sequence files is to use MapReduce.
MapReduce is inherently parallel and will let you specify the number of reducers to use, which determines the number of output partitions.
For example, by specifying one reducer, you get a single output file.
We can use the sort example that comes with Hadoop by specifying that the input and output are sequence files and by setting the key and value types:
These functions predate MapReduce and are lower-level functions than MapReduce (for example, to get parallelism, you need to partition your data manually), so in general MapReduce is the preferred approach to sort and merge sequence files.
A sequence file consists of a header followed by one or more records (see Figure 4-2)
The first three bytes of a sequence file are the bytes SEQ, which acts as a magic number, followed by a single byte representing the version number.
The header contains other fields, including the names of the key and value classes, compression details, userdefined metadata, and the sync marker.14 Recall that the sync marker is used to allow a reader to synchronize to a record boundary from any position in the file.
Each file has a randomly generated sync marker, whose value is stored in the header.
They are designed to incur less than a 1% storage overhead, so they don’t necessarily appear between every pair of records (such is the case for short records)
Full details of the format of these fields may be found in SequenceFile’s documentation and source code.
The internal format of the records depends on whether compression is enabled, and if it is, whether it is record compression or block compression.
If no compression is enabled (the default), each record is made up of the record length (in bytes), the key length, the key, and then the value.
The length fields are written as four-byte integers adhering to the contract of the writeInt() method of java.io.Data Output.
Keys and values are serialized using the Serialization defined for the class being written to the sequence file.
The format for record compression is almost identical to no compression, except the value bytes are compressed using the codec defined in the header.
Block compression compresses multiple records at once; it is therefore more compact than and should generally be preferred over record compression because it has the opportunity to take advantage of similarities between records.
A sync marker is written before the start of every block.
The format of a block is a field indicating the number of records in the block, followed by four compressed fields: the key lengths, the keys, the value lengths, and the values.
MapFile A MapFile is a sorted SequenceFile with an index to permit lookups by key.
MapFile can be thought of as a persistent form of java.util.Map (although it doesn’t implement this interface), which is able to grow beyond the size of a Map that is kept in memory.
The internal structure of a sequence file with no compression and with record compression.
The internal structure of a sequence file with block compression.
Let’s use this program to build a MapFile: % hadoop MapFileWriteDemo numbers.map.
If we look at the MapFile, we see it’s actually a directory containing two files called data and index:
The index file contains a fraction of the keys and contains a mapping from the key to that key’s offset in the data file:
A reason to increase the index interval would be to decrease the amount of memory that the MapFile needs to store the index.
Conversely, you might decrease the interval to improve the time for random selection (since fewer records need to be skipped on average) at the expense of memory usage.
Because the index is only a partial index of keys, MapFile is not able to provide methods to enumerate, or even count, all the keys it contains.
The only way to perform these operations is to read the whole file.
Iterating through the entries in order in a MapFile is similar to the procedure for a SequenceFile: you create a MapFile.Reader, then call the next() method until it returns false, signifying that no entry was read because the end of the file was reached:
The return value is used to determine whether an entry was found in the MapFile; if it’s null, no value exists for the given key.
If key was found, the value for that key is read into val, as well as being returned from the method call.
It might be helpful to understand how this is implemented.
Here is a snippet of code that retrieves an entry for the MapFile we created in the previous section:
For this operation, the MapFile.Reader reads the index file into memory (this is cached so that subsequent random access calls will use the same in-memory index)
The reader then performs a binary search on the in-memory index to find the key in the index that is less than or equal to the search key, 496
Next, the reader seeks to this offset in the data file and reads entries until the key is greater than or equal to the search key (496)
In this case, a match is found and the value is read from the data file.
Overall, a lookup takes a single disk seek and a scan through up to 128 entries on disk.
The getClosest() method is like get(), except it returns the “closest” match to the specified key rather than returning null on no match.
More precisely, if the MapFile contains the specified key, then that is the entry returned; otherwise, the key in the MapFile that is immediately after (or before, according to a boolean argument) the specified key is returned.
A very large MapFile’s index can take up a lot of memory.
Larger skip values save memory but at the expense of lookup time, since on average, more entries have to be scanned on disk.
Hadoop comes with a few variants on the general key-value MapFile interface: • SetFile is a specialization of MapFile for storing a set of Writable keys.
ArrayFile is a MapFile where the key is an integer representing the index of the.
BloomMapFile is a MapFile that offers a fast version of the get() method, especially.
The implementation uses a dynamic bloom filter for testing whether a given key is in the map.
The test is very fast because it is inmemory, but it has a nonzero probability of false positives, in which case the regular get() method is called.
The fix() method is usually used for recreating corrupted indexes, but because it creates a new index from scratch, it’s exactly what we need here.
Sort the sequence file numbers.seq into a new directory called number.map that will become the MapFile (if the sequence file is already sorted, you can skip this step; instead, copy it to a file number.map/data, and go to step 3):
In this chapter, we look at the practical aspects of developing a MapReduce application in Hadoop.
You start by writing your map and reduce functions, ideally with unit tests to make sure they do what you expect.
Then you write a driver program to run a job, which can run from your IDE using a small subset of the data to check that it is working.
If it fails, you can use your IDE’s debugger to find the source of the problem.
With this information, you can expand your unit tests to cover this case and improve your mapper or reducer as appropriate to handle such input correctly.
When the program runs as expected against the small dataset, you are ready to unleash it on a cluster.
Running against the full dataset is likely to expose some more issues, which you can fix as before, by expanding your tests and mapper or reducer to handle the new cases.
Debugging failing programs in the cluster is a challenge, so we look at some common techniques to make it easier.
After the program is working, you may wish to do some tuning, first by running through some standard checks for making MapReduce programs faster and then by doing task profiling.
Profiling distributed programs is not easy, but Hadoop has hooks to aid the process.
Before we start writing a MapReduce program, we need to set up and configure the development environment.
And to do that, we need to learn a bit about how Hadoop does configuration.
Each property is named by a String, and the type of a value may be one of several types, including Java primitives such as boolean, int, long, and float, other useful types such as String, Class, and java.io.File, and collections of Strings.
Configurations read their properties from resources—XML files with a simple structure for defining name-value pairs.
There are a couple of things to note: type information is not stored in the XML file; instead, properties can be interpreted as a given type when they are read.
Also, the get() methods allow you to specify a default value, which is used if the property is not defined in the XML file, as in the case of breadth here.
Combining Resources Things get interesting when more than one resource is used to define a configuration.
The file in Example 5-2 defines the size and weight properties.
Properties defined in resources that are added later override the earlier definitions.
However, properties that are marked as final cannot be overridden in later definitions.
The weight property is final in the first configuration file, so the attempt to override it in the second fails, and it takes the value from the first:
Attempting to override final properties usually indicates a configuration error, so this results in a warning message being logged to aid diagnosis.
Administrators mark properties as final in the daemon’s site files that they don’t want users to change in their client-side configuration files or job submission parameters.
This feature is useful for overriding properties on the command line by using -Dproperty=value JVM arguments.
Note that although configuration properties can be defined in terms of system properties, unless system properties are redefined using configuration properties, they are not accessible through the configuration API.
Setting Up the Development Environment The first step is to create a project so you can build MapReduce programs and run them in local (standalone) mode from the command line or within your IDE.
The Maven POM in Example 5-3 shows the dependencies needed for building and testing MapReduce programs.
The dependencies section is the interesting part of the POM.
It is straightforward to use another build tool, such as Gradle or Ant with Ivy, as long as you use the same set of dependencies defined here.) For building MapReduce jobs you only need to have the hadoop-core dependency, which contains all the Hadoop classes.
For running unit tests we use junit, as well as a couple of helper libraries: hamcrest-all provides useful matchers for writing test assertions, and mrunit is used for writing MapReduce tests.
The hadoop-test library contains the “mini-” clusters that are useful for testing with Hadoop clusters running in a single JVM (we also pull in jersey-core because it is missing from the Hadoop POM)
The JAR packaging has changed in releases after 1.x, so unfortunately you can’t just change the version number of the hadoop-core dependency and expect it to work.
The example code on the book’s website contains up-to-date dependency declarations for different Hadoop releases.
Many IDEs can read Maven POMs directly, so you can just point them at the directory containing the pom.xml file and start writing code.1 Alternatively, you can use Maven to generate configuration files for your IDE.
For example, the following creates Eclipse configuration files so you can import the project into Eclipse:
Managing Configuration When developing Hadoop applications, it is common to switch between running the application locally and running it on a cluster.
One way to accommodate these variations is to have Hadoop configuration files containing the connection settings for each cluster you run against and specify which one you are using when you run Hadoop applications or tools.
As a matter of best practice, it’s recommended to keep these files outside Hadoop’s installation directory tree, as this makes it easy to switch between Hadoop versions without duplicating or losing settings.
Note that there is nothing special about the names of these files; they are just convenient ways to package up some configuration settings.
Compare this to Table A-1 in Appendix A, which sets out the equivalent server-side configurations.)
Karmasphere also provides Eclipse and NetBeans plug-ins for developing and running MapReduce jobs and browsing Hadoop clusters.
The hadoop-local.xml file contains the default Hadoop configuration for the default filesystem and the jobtracker:
In practice, you would name the file after the name of the cluster, rather than “cluster” as we have here:
You can add other configuration properties to these files as needed.
For example, if you wanted to set your Hadoop username for a particular cluster, you could do it in the appropriate file.
Setting User Identity The user identity that Hadoop uses for permissions in HDFS is determined by running the whoami command on the client system.
Similarly, the group names are derived from the output of running groups.
If, however, your Hadoop user identity is different from the name of your user account on your client machine, you can explicitly set your Hadoop username and group names by setting the hadoop.job.ugi property.
You can set the user identity that the HDFS web interface runs as by setting dfs.web.ugi using the same syntax.
By default, it is webuser,webgroup, which is not a super user, so system files are not accessible through the web interface.
Notice that, by default, there is no authentication with this system.
With this setup, it is easy to use any configuration with the -conf command-line switch.
For example, the following command shows a directory listing on the HDFS server running in pseudodistributed mode on localhost:
If you omit the -conf option, you pick up the Hadoop configuration in the conf subdirectory under $HADOOP_INSTALL.
Depending on how you set this up, this may be for a standalone setup or a pseudodistributed cluster.
Tools that come with Hadoop support the -conf option, but it’s also straightforward to make your programs (such as programs that run MapReduce jobs) support it, too, using the Tool interface.
All implementations of Tool need to implement Configurable (since Tool extends it), and subclassing Configured is often the easiest way to achieve this.
The run() method obtains the Configuration using Configurable’s getConf() method and then iterates over it, printing each property to standard output.
The static block makes sure that the HDFS and MapReduce configurations are picked up in addition to the core ones (which Configuration knows about already)
Instead, we call ToolRunner’s static run() method, which takes care of creating a Configuration object for the Tool before calling its run() method.
Each property has a description that explains what it is for and what values it can be set to.
Be aware that some properties have no effect when set in the client configuration.
This is not a hard and fast rule, however, so in some cases you may need to resort to trial and error, or even to reading the source.
We discuss many of Hadoop’s most important configuration properties throughout this book.
The -D option is used to set the configuration property with key color to the value yellow.
Options specified with -D take priority over properties from the configuration files.
This is very useful because you can put defaults into configuration files and then override them with the -D option as needed.
This will override the number of reducers set on the cluster or set in any client-side configuration files.
If you want to be able to set configuration through system properties, you need to mirror the system properties of interest in the configuration file.
Overrides any default or site properties in the configuration and any properties set via the -conf option.
Adds the given files to the list of resources in the configuration.
This is a convenient way to set site properties or to set a number of properties at once.
Copies the specified files from the local filesystem (or any filesystem if a scheme is specified) to the shared filesystem used by the jobtracker (usually HDFS) and makes them available to MapReduce programs in the task’s working directory.
Copies the specified archives from the local filesystem (or any filesystem if a scheme is specified) to the shared filesystem used by the jobtracker (usually HDFS), unarchives them, and makes them available to MapReduce programs in the task’s working directory.
Copies the specified JAR files from the local filesystem (or any filesystem if a scheme is specified) to the shared filesystem used by the jobtracker (usually HDFS), and adds them to the MapReduce task’s classpath.
This option is a useful way of shipping JAR files that a job is dependent on.
Writing a Unit Test with MRUnit The map and reduce functions in MapReduce are easy to test in isolation, which is a consequence of their functional style.
MRUnit (http://incubator.apache.org/mrunit/) is a testing library that makes it easy to pass known inputs to a mapper or a reducer and check that the outputs are as expected.
MRUnit is used in conjunction with a standard test execution framework, such as JUnit, so you can run the tests for MapReduce jobs as a part of your normal development environment.
Mapper The test for the mapper is shown in Example 5-5
This is a very simple implementation that pulls the year and temperature fields from the line and writes them to the Context.
Let’s add a test for missing values, which in the raw data are represented by a temperature of +9999:
A MapDriver can be used to check for zero, one, or more output records, according to the number of times that withOutput() is called.
In our application, since records with missing temperatures should be filtered out, this test asserts that no output is produced for this particular input value.
With the test for the mapper passing, we move on to writing the reducer.
Reducer The reducer has to find the maximum value for a given key.
Here’s a simple test for this feature, which uses a ReduceDriver:
Running Locally on Test Data Now that we have the mapper and reducer working on controlled inputs, the next step is to write a job driver and run it on some test data on a development machine.
The run() method constructs a Job object based on the tool’s configuration, which it uses to launch a job.
It’s also a good idea to set a name for the job (Max temperature) so that you can pick it out in the job list during execution and after it has completed.
By default, the name is the name of the JAR file, which normally is not particularly descriptive.
Now we can run this application against some local files.
Hadoop comes with a local job runner, a cut-down version of the MapReduce execution engine for running MapReduce jobs in a single JVM.
It’s designed for testing and is very convenient for use in an IDE, since you can run it in a debugger to step through the code in your mapper and reducer.
The local job runner is designed only for simple testing of MapReduce programs, so inevitably it differs from the full MapReduce implementation.
The biggest difference is that it can’t run more than one reducer.
It can support the zero reducer case, too.) Normally, this is not a problem, as most applications can work with one reducer, although on a cluster you would choose a larger number to take advantage of parallelism.
The thing to watch out for is that even if you set the number of reducers to a value over one, the local runner will silently ignore the setting and use a single reducer.
This limitation may be removed in a future version of Hadoop.
The local job runner is enabled by a configuration setting.
From the command line, we can run the driver by typing:
Note that although we’ve set -fs so we use the local filesystem (file:///), the local job runner will actually work fine against any filesystem, including HDFS (and it can be handy to do this if you have a few files that are on HDFS)
When we run the program, it fails and prints the following exception:
It just calls the parser’s parse() method, which parses the fields of interest from a line of input, checks whether.
At the end of the test, the checkOut put() method is called to compare the actual output with the expected output, line by line.
The second way of testing the driver is to run it using a “mini-” cluster.
Hadoop has a set of testing classes, called MiniDFSCluster, MiniMRCluster, and MiniYARNCluster, that provide a programmatic way of creating in-process clusters.
Unlike the local job runner, these allow testing against the full HDFS and MapReduce machinery.
Bear in mind, too, that tasktrackers in a mini-cluster launch separate JVMs to run tasks in, which can make debugging more difficult.
Mini-clusters are used extensively in Hadoop’s own automated test suite, but they can be used for testing user code, too.
Subclasses need only populate data in HDFS (perhaps by copying from a local file), run a MapReduce job, and confirm the output is as expected.
Refer to the MaxTemperature DriverMiniTest class in the example code that comes with this book for the listing.
Tests like this serve as regression tests, and are a useful repository of input edge cases and their expected results.
As you encounter more test cases, you can simply add them to the input file and update the file of expected output accordingly.
Running on a Cluster Now that we are happy with the program running on a small test dataset, we are ready to try it on the full dataset on a Hadoop cluster.
Chapter 9 covers how to set up a fully distributed cluster, although you can also work through this section on a pseudodistributed cluster.
Packaging a Job The local job runner uses a single JVM to run a job, so as long as all the classes that your job needs are on its classpath, then things will just work.
In a distributed setting, things are a little more complex.
For a start, a job’s classes must be packaged into a job JAR file to send to the cluster.
Hadoop will find the job JAR automatically by searching for the JAR on the driver’s classpath that contains the class set in the setJarByClass() method (on JobConf or Job)
Alternatively, if you want to set an explicit JAR file by its file path, you can use the setJar() method.
Creating a job JAR file is conveniently achieved using a build tool such as Ant or Maven.
If you have a single job per JAR, you can specify the main class to run in the JAR file’s manifest.
If the main class is not in the manifest, it must be specified on the command line (as we will see shortly when we run the job)
Any dependent JAR files can be packaged in a lib subdirectory in the job JAR file, although there are other ways to include dependencies, discussed later.
Similarly, resource files can be packaged in a classes subdirectory.
This is analogous to a Java Web application archive, or WAR file, except in that case the JAR files go in a WEB-INF/ lib subdirectory and classes go in a WEB-INF/classes subdirectory in the WAR file.)
Incidentally, this explains why you have to set HADOOP_CLASSPATH to point to dependent classes and libraries if you are running using the local job runner without a job JAR (hadoop CLASSNAME)
On a cluster (and this includes pseudodistributed mode), map and reduce tasks run in separate JVMs, and their classpaths are not controlled by HADOOP_CLASSPATH.
HADOOP_CLASSPATH is a client-side setting and only sets the classpath for the driver JVM, which submits the job.
Given these different ways of controlling what is on the client and task classpaths, there are corresponding options for including library dependencies for a job.
Unpack the libraries and repackage them in the job JAR.
Package the libraries in the lib directory of the job JAR.
Keep the libraries separate from the job JAR, and add them to the client classpath.
The last option, using the distributed cache, is simplest from a build point of view because dependencies don’t need rebundling in the job JAR.
Also, the distributed cache can mean fewer transfers of JAR files around the cluster, since files may be cached on a node between tasks.
You can read more about the distributed cache on page 289.)
User JAR files are added to the end of both the client classpath and the task classpath, which in some cases can cause a dependency conflict with Hadoop’s built-in libraries if Hadoop uses a different, incompatible version of a library that your code uses.
Sometimes you need to be able to control the task classpath order so that your classes are picked up first.
Note that by setting these options you change the class loading for Hadoop framework dependencies (but only in your job), which could potentially cause the job submission or task to fail, so use these options with caution.
Launching a Job To launch the job, we need to run the driver, specifying the cluster that we want to run the job on with the -conf option (we equally could have used the -fs and -jt options):
We unset the HADOOP_CLASSPATH environment variable because we don’t have any third-party dependencies for this job.
Here’s the output (some lines have been removed for clarity):
When the job is complete, its statistics (known as counters) are printed out.
These are very useful for confirming that the job did what you expected.
The input was broken into 101 gzipped files of reasonable size, so there was no problem with not being able to split them.
Job, Task, and Task Attempt IDs The format of a job ID is composed of the time that the jobtracker (not the job) started and an incrementing counter maintained by the jobtracker to uniquely identify the job to that instance of the jobtracker.
The counter is formatted with leading zeros to make job IDs sort nicely—in directory listings, for example.
However, when the counter reaches 10000, it is not reset, resulting in longer job IDs (which don’t sort so well)
Tasks belong to a job, and their IDs are formed by replacing the job prefix of a job ID with a task prefix and adding a suffix to identify the task within the job.
The task IDs are created for a job when it is initialized, so they do not necessarily dictate the order in which the tasks will be executed.
Task attempts are allocated during the job run as needed, so their ordering represents the order in which they were created for tasktrackers to run.
The MapReduce Web UI Hadoop comes with a web UI for viewing information about your jobs.
It is useful for following a job’s progress while it is running, as well as finding job statistics and logs after the job has completed.
A screenshot of the home page is shown in Figure 5-1
The first section of the page gives details of the Hadoop installation, such as the version number and when it was compiled, and the current state of the jobtracker (in this case, running) and when it was started.
Next is a summary of the cluster, which has measures of cluster capacity and utilization.
Below the summary, there is a section about the job scheduler that is running (here, the default)
Further down, we see sections for running, (successfully) completed, and failed jobs.
Finally, at the foot of the page, there are links to the jobtracker’s logs and the jobtracker’s history, which contains information on all the jobs that the jobtracker has run.
Note also that the job history is persistent, so you can find jobs here from previous runs of the jobtracker.
Job History Job history refers to the events and configuration for a completed job.
It is retained regardless of whether the job was successful, in an attempt to provide interesting information for the user running a job.
Job history files are stored on the local filesystem of the jobtracker in a history subdirectory of the logs directory.
The jobtracker’s history files are kept for 30 days before being deleted by the system.
A second copy is also stored for the user in the _logs/history subdirectory of the job’s output directory.
By setting it to the special value none, no user job.
A user’s job history files are never deleted by the system.
The history log includes job, task, and attempt events, all of which are stored in a plaintext file.
The history for a particular job may be viewed through the web UI or via the command line using hadoop job -history (which you point at the job’s output directory)
Clicking on a job ID brings you to a page for the job, illustrated in Figure 5-2
At the top of the page is a summary of the job, with basic information such as job owner and name and how long the job has been running for.
The job file is the consolidated configuration file for the job, containing all the properties and their values that were in effect during the job run.
If you are unsure of what a particular property was set to, you can click through to inspect the file.
While the job is running, you can monitor its progress on this page, which periodically updates itself.
Below the summary is a table that shows the map progress and the reduce progress.
Num Tasks” shows the total number of map and reduce tasks for this job (a row for each)
The final column shows the total number of failed and killed task attempts for all the map or reduce tasks for the job (task attempts may be marked as killed if they are a speculative execution duplicate, if the tasktracker they are running on dies, or if they are killed by a user)
Farther down the page, you can find completion graphs for each task that show their progress graphically.
The reduce completion graph is divided into the three phases of the reduce task: copy (when the map outputs are being transferred to the reduce’s tasktracker), sort (when the reduce inputs are being merged), and reduce (when the reduce function is being run to produce the final output)
In the middle of the page is a table of job counters.
These are dynamically updated during the job run and provide another useful window into the job’s progress and general health.
Retrieving the Results Once the job is finished, there are various ways to retrieve the results.
Usually, if a file is in this partitioned form, it can still be used easily enough —as the input to another MapReduce job, for example.
This job produces a very small amount of output, so it is convenient to copy it from HDFS to our development machine.
The -getmerge option to the hadoop fs command is useful here, as it gets all the files in the directory specified in the source pattern and merges them into a single file on the local filesystem:
We sorted the output, as the reduce output partitions are unordered (owing to the hash partition function)
Doing a bit of postprocessing of data from MapReduce is very common, as is feeding it into analysis tools such as R, a spreadsheet, or even a relational database.
Another way of retrieving the output if it is small is to use the -cat option to print the output files to the console:
Debugging a Job The time-honored way of debugging programs is via print statements, and this is certainly possible in Hadoop.
However, there are complications to consider: with programs running on tens, hundreds, or thousands of nodes, how do we find and examine the output of the debug statements, which may be scattered across these nodes? For.
The web UI makes this easy, as we will see.
We also create a custom counter to count the total number of records with implausible temperatures in the whole dataset.
This gives us valuable information about how to deal with the condition.
If it turns out to be a common occurrence, we might need to learn more about the condition and how to extract the temperature in these cases, rather than simply dropping the record.
In fact, when trying to debug a job, you should always ask yourself if you can use a counter to get the information you need to find out what’s happening.
Even if you need to use logging or a status message, it may be useful to use a counter to gauge the extent of the problem.
The first is to write the information to the map’s output, rather than to standard error, for analysis and aggregation by the reduce.
This approach usually necessitates structural changes to your program, so start with the other techniques first.
Alternatively, you can write a program (in MapReduce, of course) to analyze the logs produced by your job.
We add our debugging to the mapper (version 4), as opposed to the reducer, as we want to find out what the source data causing the anomalous output looks like:
The job page has a number of links for viewing the tasks in a job in more detail.
For example, by clicking on the “map” link, you are brought to a page that lists information for all of the map tasks on one page.
The screenshot in Figure 5-3 shows a portion of this page for the job run with our debugging statements.
Each row in the table is a task, and it provides such information as the start and end times for each task, any errors reported back from the tasktracker, and a link to view the counters for an individual task.
The “Status” column can be helpful for debugging because it shows a task’s latest status message.
Before a task starts, it shows its status as “initializing,” and then once it starts reading records, it shows the split information for the split it is reading as a filename with a byte offset and length.
Notice, too, that there is an extra counter for this task because our user counter has a nonzero count for this task.)
From the tasks page, you can click on any task to get more information about it.
The task details page, shown in Figure 5-4, shows each task attempt.
In this case, there was one task attempt, which completed successfully.
The table provides further useful data, such as the node the task attempt ran on and links to task logfiles and counters.
The “Actions” column contains links for killing a task attempt.
By default, this is disabled, making the web UI a read-only interface.
The dfs.web.ugi property determines the user that the HDFS web UI runs as, thus controlling which files may be viewed and deleted.
For map tasks, there is also a section showing which nodes the input split was located on.
The -counter option takes the job ID, counter group name (which is the fully qualified classname here), and the counter name (the enum name)
There are only three malformed records in the entire dataset of over a billion records.
Throwing out bad records is standard for many big data problems, although we need to be careful in this case because we are looking for an extreme value—the maximum temperature rather than an aggregate measure.
Still, throwing away three records is probably not going to change the result.
Capturing input data that causes a problem is valuable, as we can use it in a test to check that the mapper does the right thing.
In this MRUnit test, we check that the counter is updated for the malformed input:
The record that was causing the problem is of a different format from the other lines we’ve seen.
We’ve also introduced a counter to measure the number of records that we are ignoring for this reason.
Hadoop Logs Hadoop produces logs in various places, and for various audiences.
System daemon logs Administrators Each Hadoop daemon produces a logfile (using log4j) and another file that combines standard out and error.
Written in the directory defined by the HADOOP_LOG_DIR environment variable.
MapReduce job history logs Users A log of the events (such as task completion) that occur in the course of running a job.
Saved centrally on the jobtracker and in the job’s output directory in a _logs/history subdirectory.
MapReduce task logs Users Each tasktracker child process produces a logfile using log4j (called syslog), a file for data sent to standard out (stdout), and a file for standard error (stderr)
Logs Primary audience Description Further information userlogs subdirectory of the directory defined by the HADOOP_LOG_DIR environment variable.
As we have seen in the previous section, MapReduce task logs are accessible through the web UI, which is the most convenient way to view them.
You can also find the logfiles on the local filesystem of the tasktracker that ran the task attempt, located in a directory named by the task attempt.
The web UI hides this by showing only the portion that is relevant for the task attempt being viewed.
Anything written to standard output or standard error is directed to the relevant logfile.
Of course, in Streaming, standard output is used for the map or reduce output, so it will not show up in the standard output log.) In Java, you can write to the task’s syslog file if you wish by using the Apache Commons Logging API.
The default log level is INFO, so DEBUG level messages do not appear in the syslog task logfile.
For example, in this case we could set it for the mapper to see the map values in the log as follows:
There are some controls for managing the retention and size of task logs.
Sometimes you may need to debug a problem that you suspect is occurring in the JVM running a Hadoop command, rather than on the cluster.
You can send DEBUG level logs to the console by using an invocation like this:
Remote Debugging When a task fails and there is not enough information logged to diagnose the error, you may want to resort to running a debugger for that task.
This is hard to arrange when running the job on a cluster, as you don’t know which node is going to process which part of the input, so you can’t set up your debugger ahead of the failure.
However, there are a few other options available: Reproduce the failure locally.
Often the failing task fails consistently on a particular input.
You can try to reproduce the problem locally by downloading the file that the task is failing on and running the job locally, possibly using a debugger such as Java’s VisualVM.
Use JVM debugging options A common cause of failure is a Java out of memory error in the task JVM.
This setting produces a heap dump that can be examined afterward with tools such as jhat or the Eclipse Memory Analyzer.
Use task profiling Java profilers give a lot of insight into the JVM, and Hadoop provides a mechanism to profile a subset of the tasks in a job.
Use IsolationRunner Older versions of Hadoop provided a special task runner called IsolationRunner that could rerun failed tasks in situ on the cluster.
In some cases it’s useful to keep the intermediate files for a failed task attempt for later inspection, particularly if supplementary dump or profile files are created in the task’s working directory.
You can keep the intermediate files for successful tasks, too, which may be handy if you want to examine a task that isn’t failing.
To examine the intermediate files, log into the node that the task failed on and look for the directory for that task attempt.
If this property is a comma-separated list of directories (to spread load across the physical disks on a machine), you may need to look in all of the directories before you find the directory for that particular task attempt.
You should run through the checklist in Table 5-3 before you start trying to profile or optimize at the task level.
How long are your mappers running for? If they are only running for a few seconds on average, you should see whether there’s a way to have fewer mappers and make them all run longer, a minute or so, as a rule of thumb.
The extent to which this is possible depends on the input format you are using.
Number of reducers For maximum performance, the number of reducers should be slightly less than the number of reduce slots in the cluster.
This allows the reducers to finish in one wave and fully utilizes the cluster during the reduce phase.
Combiners Check whether your job can take advantage of a combiner to reduce the amount of data passing through the shuffle.
Job execution time can almost always benefit from enabling map output compression.
If you are using your own custom Writable objects or custom comparators, make sure you have implemented RawComparator.
Shuffle tweaks The MapReduce shuffle exposes around a dozen tuning parameters for memory management, which may help you wring out the last bit of performance.
Profiling Tasks Like debugging, profiling a job running on a distributed system such as MapReduce presents some challenges.
Hadoop allows you to profile a fraction of the tasks in a job, and as each task completes, pulls down the profile information to your machine for later analysis with standard profiling tools.
Of course, it’s possible, and somewhat easier, to profile a job running in the local job runner.
And provided you can run with enough input data to exercise the map and reduce tasks, this can be a valuable way of improving the performance of your mappers and reducers.
The local job runner is a very different environment from a cluster, and the data flow patterns are very different.
Optimizing the CPU performance of your code may be pointless if your MapReduce job is I/O-bound (as many jobs are)
To be sure that any tuning is effective, you should compare the new execution time with the old one running on a real cluster.
Even this is easier said than done, since job execution times can vary due to resource contention with other jobs and the decisions the scheduler makes regarding task placement.
To get a good idea of job execution time under these circumstances, perform a series of runs (with and without the change) and check whether any improvement is statistically significant.
It’s unfortunately true that some problems (such as excessive memory use) can be reproduced only on the cluster, and in these cases the ability to profile in situ is indispensable.
There are a number of configuration properties to control profiling, which are also exposed via convenience methods on JobConf.
The first line enables profiling, which by default is turned off.
A set of ranges is permitted, using a notation that allows open ranges.
When we run a job with the modified driver, the profile output turns up at the end of the job in the directory from which we launched the job.
Because we are profiling only a few tasks, we can run the job on a subset of the dataset.
Here’s a snippet of one of the mapper’s profile files, which shows the CPU sampling information:
So it looks like the mapper is spending 3% of its time constructing IntWritable objects.
This observation might lead us to try reusing the Writable instances being output to see whether there is any performance gain on our particular cluster and dataset.
In one experiment, I changed the mapper to store the Writable objects as instance variables.
When I ran the modified program on an 11-node cluster, I did not see a statistically significant difference in overall job execution time.
The mechanism for retrieving profile output is HPROF-specific, so if you use another profiler, you will need to manually retrieve the profiler’s output from tasktrackers for analysis.
MapReduce Workflows So far in this chapter, you have seen the mechanics of writing a program using MapReduce.
We haven’t yet considered how to turn a data processing problem into the MapReduce model.
The data processing you have seen so far in this book is to solve a fairly simple problem: finding the maximum recorded temperature for given years.
When the processing gets more complex, this complexity is generally manifested by having more MapReduce jobs, rather than having more complex map and reduce functions.
In other words, as a rule of thumb, think about adding more jobs, rather than adding complexity to jobs.
For more complex problems, it is worth considering a higher-level language than MapReduce, such as Pig, Hive, Cascading, Cascalog, or Crunch.
One immediate benefit is that it frees you from having to do the translation into MapReduce jobs, allowing you to concentrate on the analysis you are performing.
Decomposing a Problem into MapReduce Jobs Let’s look at an example of a more complex problem that we want to translate into a MapReduce workflow.
Imagine that we want to find the mean maximum recorded temperature for every day of the year and every weather station.
How can we compute this using MapReduce? The computation decomposes most naturally into two stages:
The MapReduce program in this case is a variant of the maximum temperature program, except that the keys in this case are a composite station-date pair, rather than just the year.
Compute the mean of the maximum daily temperatures for every station-daymonth key.
The reduce function then takes the mean of the maximum temperatures for each station-day-month key.
The first two fields form the key, and the final column is the maximum temperature from all the readings for the given station and date.
The second stage averages these daily maxima over years to yield:
However, there is a case for splitting these into distinct mappers and chaining them into a single mapper using the ChainMapper library class that comes with Hadoop.
Combined with a ChainReducer, you can run a chain of mappers, followed by a reducer and another chain of mappers, in a single MapReduce job.
JobControl When there is more than one job in a MapReduce workflow, the question arises: how do you manage the jobs so they are executed in order? There are several approaches, and the main consideration is whether you have a linear chain of jobs or a more complex directed acyclic graph (DAG) of jobs.
For a linear chain, the simplest approach is to run each job one after another, waiting until a job completes successfully before running the next:
If a job fails, the runJob() method will throw an IOException, so later jobs in the pipeline don’t get executed.
Depending on your application, you might want to catch the exception and clean up any intermediate data that was produced by any previous jobs.
For anything more complex than a linear chain, there are libraries that can help orchestrate your workflow (although they are also suited to linear chains, or even oneoff jobs)
You add the job configurations, then tell the JobControl instance the dependencies between jobs.
You run the JobControl in a thread, and it runs the jobs in dependency order.
You can poll for progress, and when the jobs have finished, you can query for all the jobs’ statuses and the associated errors for any failures.
Apache Oozie Apache Oozie is a system for running workflows of dependent jobs.
It is composed of two main parts: a workflow engine that stores and runs workflows composed of different types of Hadoop jobs (MapReduce, Pig, Hive, and so on), and a coordinator engine that runs workflow jobs based on predefined schedules and data availability.
Oozie has been designed to scale, and it can manage the timely execution of thousands of workflows in a Hadoop cluster, each composed of possibly dozens of constituent jobs.
Oozie makes rerunning failed workflows more tractable, since no time is wasted running successful parts of a workflow.
Anyone who has managed a complex batch system knows how difficult it can be to catch up from jobs missed due to downtime or failure, and will appreciate this feature.
Furthermore, coordinator applications representing a single data pipeline may be packaged into a bundle and run together as a unit.) Unlike JobControl, which runs on the client machine submitting the jobs, Oozie runs as a service in the cluster, and clients submit workflow definitions for immediate or later execution.
In Oozie parlance, a workflow is a DAG of action nodes and controlflow nodes.
An action node performs a workflow task, such as moving files in HDFS, running a MapReduce, Streaming, Pig, or Hive job, performing a Sqoop import, or running an arbitrary shell script or Java program.
A control-flow node governs the workflow execution between actions by allowing such constructs as conditional logic (so different execution branches may be followed depending on the result of an earlier action node) or parallel execution.
When the workflow completes, Oozie can make an HTTP callback to the client to inform it of the workflow status.
It is also possible to receive callbacks every time the workflow enters or exits an action node.
This workflow has three control-flow nodes and one action node: a start control node, a map-reduce action node, a kill control node, and an end control node.
The nodes and allowed transitions between them are shown in Figure 5-5
All workflows must have one start and one end node.
When the workflow job starts, it transitions to the node specified by the start node (the max-temp-mr action in this example)
A workflow job succeeds when it transitions to the end node.
However, if the workflow job transitions to a kill node, it is considered to have failed and reports the appropriate error message specified by the message element in the workflow definition.
A workflow application is made up of the workflow definition plus all the associated resources (such as MapReduce JAR files, Pig scripts, and so on) needed to run it.
Applications must adhere to a simple directory structure, and are deployed to HDFS so that they can be accessed by Oozie.
For this workflow application, we’ll put all of the files in a base directory called max-temp-workflow, as shown diagrammatically here:
The workflow definition file workflow.xml must appear in the top level of this directory.
Workflow applications that conform to this layout can be built with any suitable build tool, such as Ant or Maven; you can find an example in the code that accompanies this.
Once an application has been built, it should be copied to HDFS using regular Hadoop tools.
Next, let’s see how to run a workflow job for the application we just uploaded.
For this we use the oozie command-line tool, a client program for communicating with an Oozie server.
For convenience we export the OOZIE_URL environment variable to tell the oozie command which Oozie server to use (here we’re using one running locally):
There are lots of subcommands for the oozie tool (type oozie help to get a list), but we’re going to call the job subcommand with the -run option to run the workflow job:
To get information about the status of the workflow job, we use the -info option, specifying the job ID that was printed by the run command earlier (type oozie job to get a list of all jobs):
You can find all this information via Oozie’s web UI too, available at http://localhost:11000/oozie.
When the job has succeeded, we can inspect the results in the usual way:
This example only scratched the surface of writing Oozie workflows.
The documentation on Oozie’s website has information about creating more complex workflows, as well as writing and running coordinator jobs.
In this chapter, we look at how MapReduce in Hadoop works in detail.
This knowledge provides a good foundation for writing more advanced MapReduce programs, which we will cover in the following two chapters.
This section uncovers the steps Hadoop takes to run a job.
We saw in Chapter 5 that the way Hadoop executes a MapReduce program depends on a couple of configuration settings.
In versions of Hadoop up to and including the 0.20 release series, mapred.
If this configuration property is set to local (the default), the local job runner is used.
This runner runs the whole job in a single JVM.
It’s designed for testing and for running MapReduce programs on small datasets.
The whole process is described in detail in the next section.
The APIs are user-facing client-side features and determine how you write MapReduce programs, whereas the implementations are just different ways of running MapReduce programs.
Table 1-2 lists which of these combinations are supported in the different Hadoop releases.
The tasktrackers, which run the tasks that the job has been split into.
The distributed filesystem (normally HDFS, covered in Chapter 3), which is used.
Having submitted the job, waitForCom pletion() polls the job’s progress once per second and reports the progress to the console if it has changed since the last report.
When the job completes successfully, the job counters are displayed.
Otherwise, the error that caused the job to fail is logged to the console.
The job submission process implemented by JobSummitter does the following:
Asks the jobtracker for a new job ID (by calling getNewJobId() on JobTracker) (step 2)
For example, if the output directory has not been specified or it already exists, the job is not submitted and an error is thrown to the MapReduce program.
If the splits cannot be computed (because the input paths don’t exist, for example), the job is not submitted and an error is thrown to the MapReduce program.
How Hadoop runs a MapReduce job using the classic framework.
Copies the resources needed to run the job, including the job JAR file, the configuration file, and the computed input splits, to the jobtracker’s filesystem in a directory named after the job ID.
Tells the jobtracker that the job is ready for execution by calling submitJob() on JobTracker (step 4)
When the JobTracker receives a call to its submitJob() method, it puts it into an internal queue from where the job scheduler will pick it up and initialize it.
Initialization involves creating an object to represent the job being run, which encapsulates its tasks, and bookkeeping information to keep track of the status and progress of its tasks (step 5)
To create the list of tasks to run, the job scheduler first retrieves the input splits computed by the client from the shared filesystem (step 6)
In addition to the map and reduce tasks, two further tasks are created: a job setup task and a job cleanup task.
These are run by tasktrackers and are used to run code to set up the job before any map tasks run, and to cleanup after all the reduce tasks are complete.
For the job setup task it will create the final output directory for the job and the temporary working space for the task output, and for the job cleanup task it will delete the temporary working space for the task output.
Tasktrackers run a simple loop that periodically sends heartbeat method calls to the jobtracker.
Heartbeats tell the jobtracker that a tasktracker is alive, but they also double as a channel for messages.
As a part of the heartbeat, a tasktracker will indicate whether it is ready to run a new task, and if it is, the jobtracker will allocate it a task, which it communicates to the tasktracker using the heartbeat return value (step 7)
Before it can choose a task for the tasktracker, the jobtracker must choose a job to select the task from.
Having chosen a job, the jobtracker now chooses a task for the job.
Tasktrackers have a fixed number of slots for map tasks and for reduce tasks, and these are set independently.
For example, a tasktracker may be configured to run two map tasks and two reduce tasks simultaneously.
So if the tasktracker has at least one empty map task slot, the jobtracker will select a map task; otherwise, it will select a reduce task.
To choose a reduce task, the jobtracker simply takes the next in its list of yet-to-be-run reduce tasks, since there are no data locality considerations.
For a map task, however, it takes into account the tasktracker’s network location and picks a task whose input split is as close as possible to the tasktracker.
In the optimal case, the task is datalocal, that is, running on the same node that the split resides on.
Alternatively, the task may be rack-local: on the same rack, but not the same node, as the split.
Some tasks are neither data-local nor rack-local and retrieve their data from a different rack than the one they are running on.
Now that the tasktracker has been assigned a task, the next step is for it to run the task.
First, it localizes the job JAR by copying it from the shared filesystem to the tasktracker’s filesystem.
Second, it creates a local working directory for the task and “un-jars” the contents of the JAR into this directory.
Third, it creates an instance of TaskRunner to run the task.
The child process communicates with its parent through the umbilical interface.
It informs the parent of the task’s progress every few seconds until the task is complete.
The cleanup action is used to commit the task, which in the case of file-based jobs means that its output is written to the final location for that task.
Both Streaming and Pipes run special map and reduce tasks for the purpose of launching the user-supplied executable and communicating with it (Figure 6-2)
In the case of Streaming, the Streaming task communicates with the process (which may be written in any language) using standard input and output streams.
In both cases, during execution of the task, the Java process passes input key-value pairs to the external process, which runs it through the user-defined map or reduce function and passes the output key-value pairs back to the Java process.
From the tasktracker’s point of view, it is as if the tasktracker child process ran the map or reduce code itself.
MapReduce jobs are long-running batch jobs, taking anything from minutes to hours to run.
Because this is a significant length of time, it’s important for the user to get feedback on how the job is progressing.
A job and each of its tasks have a status, which includes such things as the state of the job or task (e.g., running, successfully completed, failed), the progress of maps and reduces, the values of the job’s counters, and a status.
These statuses change over the course of the job, so how do they get communicated back to the client? When a task is running, it keeps track of its progress, that is, the proportion of the task completed.
For map tasks, this is the proportion of the input that has been processed.
For reduce tasks, it’s a little more complex, but the system can still estimate the proportion of the reduce input processed.
The relationship of the Streaming and Pipes executable to the tasktracker and its child.
What Constitutes Progress in MapReduce? Progress is not always measurable, but nevertheless, it tells Hadoop that a task is doing something.
For example, a task writing output records is making progress, even when it cannot be expressed as a percentage of the total number that will be written (because the latter figure may not be known), even by the task producing the output.
Progress reporting is important, as Hadoop will not fail a task that’s making progress.
If a task reports progress, it sets a flag to indicate that the status change should be sent to the tasktracker.
The flag is checked in a separate thread every three seconds, and if set, it notifies the tasktracker of the current task status.
Meanwhile, the tasktracker is sending heartbeats to the jobtracker every five seconds (this is a minimum, as the heartbeat interval is actually dependent on the size of the cluster; for larger clusters, the interval is longer), and the status of all the tasks being run by the tasktracker is sent in the call.
Counters are sent less frequently than every five seconds because they can be relatively high-bandwidth.
The jobtracker combines these updates to produce a global view of the status of all the jobs being run and their constituent tasks.
Finally, as mentioned earlier, the Job receives the latest status by polling the jobtracker every second.
Clients can also use Job’s getStatus() method to obtain a JobStatus instance, which contains all of the status information for the job.
Job statistics and counters are printed to the console at this point.
The jobtracker also sends an HTTP job notification if it is configured to do so.
Finally, the jobtracker cleans up its working state for the job and instructs tasktrackers to do the same (so intermediate output is deleted, for example)
How status updates are propagated through the MapReduce 1 system.
You can read more about the motivation for and development of YARN in Arun C.
The jobtracker takes care of both job scheduling (matching tasks with tasktrackers) and task progress monitoring (keeping track of tasks, restarting failed or slow tasks, and doing task bookkeeping, such as maintaining counter totals)
This model is actually closer to the original Google MapReduce paper, which describes how a master process is started to coordinate map and reduce tasks running on a set of workers.
As described, YARN is more general than MapReduce, and in fact MapReduce is just one type of YARN application.
There are a few other YARN applications, such as a distributed shell that can run a script on a set of nodes in the cluster, and others are actively being developed (some are listed at http://wiki.apache.org/hadoop/Powered ByYarn)
Furthermore, it is even possible for users to run different versions of MapReduce on the same YARN cluster, which makes the process of upgrading MapReduce more manageable.
Note that some parts of MapReduce, such as the job history server and the shuffle handler, as well as YARN itself, still need to be upgraded across the cluster.) MapReduce on YARN involves more entities than classic MapReduce.
The YARN resource manager, which coordinates the allocation of compute resources on the cluster.
At the time of this writing, memory is the only resource that is managed, and node managers will kill any container that exceeds its allocated memory limits.
Not discussed in this section are the job history server daemon (for retaining job history data) and the shuffle handler auxiliary service (for serving map outputs to reduce tasks), which are a part of the jobtracker and the tasktracker (respectively) in classic MapReduce, but are independent entities in YARN.
The YARN node managers, which launch and monitor the compute containers on machines in the cluster.
The MapReduce application master, which coordinates the tasks running the MapReduce job.
The application master and the MapReduce tasks run in containers that are scheduled by the resource manager and managed by the node managers.
The distributed filesystem (normally HDFS, covered in Chapter 3), which is used for sharing job files between the other entities.
The process of running a job is shown in Figure 6-4 and described in the following sections.
The application master for MapReduce jobs is a Java application whose main class is MRAppMaster.
It initializes the job by creating a number of bookkeeping objects to keep track of the job’s progress, as it will receive progress and completion reports from the tasks (step 6)
Next, it retrieves the input splits computed in the client from the shared filesystem (step 7)
The next thing the application master does is decide how to run the tasks that make up the MapReduce job.
If the job is small, the application master may choose to run the tasks in the same JVM as itself.
This happens when it judges the overhead of allocating and running tasks in new containers outweighs the gain to be had in running them in parallel, compared to running them sequentially on one node.
This is different from MapReduce 1, where small jobs are never run on a single tasktracker.) Such a job is said to be uberized, or run as an uber task.
Before any tasks can be run, the job setup method is called (for the job’s OutputCommit ter) to create the job’s output directory.
In contrast to MapReduce 1, where it is called in a special task that is run by the tasktracker, in the YARN implementation the method is called directly by the application master.
If the job does not qualify for running as an uber task, then the application master requests containers for all the map and reduce tasks in the job from the resource manager (step 8)
All requests, which are piggybacked on heartbeat calls, includes information about each map task’s data locality, in particular the hosts and corresponding racks that the input split resides on.
The scheduler uses this information to make scheduling decisions (just like a jobtracker’s scheduler does)
It attempts to place tasks on data-local nodes in the ideal case, but if this is not possible, the scheduler prefers rack-local placement to nonlocal placement.
Slots have a maximum memory allowance, which again is fixed for a cluster, leading to both problems of underutilization when tasks use less memory (because other waiting tasks are not able to take advantage of the unused memory) and problems of job failure when a task can’t complete since it can’t get enough memory to run correctly and therefore can’t complete.
In YARN, resources are more fine-grained, so both of these problems can be avoided.
In particular, applications may request a memory capability that is anywhere between the minimum allocation and a maximum allocation, and that must be a multiple of the minimum allocation.
The task is executed by a Java application whose main class is YarnChild.
Before it can run the task it localizes the resources that the task needs, including the job configuration and JAR file, and any files from the distributed cache (step 10)
Finally, it runs the map or reduce task (step 11)
The YarnChild runs in a dedicated JVM, for the same reason that tasktrackers spawn new JVMs for tasks in MapReduce 1: to isolate user code from long-running system daemons.
Unlike MapReduce 1, however, YARN does not support JVM reuse, so each task runs in a new JVM.
Streaming and Pipes programs work in the same way as MapReduce 1
The Yarn Child launches the Streaming or Pipes process and communicates with it using standard input/output or a socket (respectively), as shown in Figure 6-2 (except the child and subprocesses run on node managers, not tasktrackers)
When running under YARN, the task reports its progress and status (including counters) back to its application master, which has an aggregate view of the job, every three seconds over the umbilical interface.
Contrast this to MapReduce 1, where progress updates flow from the child through the tasktracker to the jobtracker for aggregation.
How status updates are propagated through the MapReduce 2 system.
In MapReduce 1, the job tracker web UI has a list of running jobs and their progress.
In YARN, the resource manager web UI displays all the running applications with links to the web UIs of their respective application masters, each of which displays further details on the MapReduce job, including its progress.
Notification of job completion via an HTTP callback is also supported, as it is in MapReduce 1
In MapReduce 2, however, the application master initiates the callback.
On job completion, the application master and the task containers clean up their working state, and the OutputCommitter’s job cleanup method is called.
Job information is archived by the job history server to enable later interrogation by users if desired.
Failures In the real world, user code is buggy, processes crash, and machines fail.
One of the major benefits of using Hadoop is its ability to handle such failures and allow your job to complete.
Failures in Classic MapReduce In the MapReduce 1 runtime, there are three failure modes to consider: failure of the running task, failure of the tasktracker, and failure of the jobtracker.
The most common occurrence of this failure is when user code in the map or reduce task throws a runtime exception.
If this happens, the child JVM reports the error back to its parent tasktracker before it exits.
The tasktracker marks the task attempt as failed, freeing up a slot to run another task.
For Streaming tasks, if the Streaming process exits with a nonzero exit code, it is marked as failed.
Another failure mode is the sudden exit of the child JVM—perhaps there is a JVM bug that causes the JVM to exit for a particular set of circumstances exposed by the.
In this case, the tasktracker notices that the process has exited and marks the attempt as failed.
The tasktracker notices that it hasn’t received a progress update for a while and proceeds to mark the task as failed.
Setting the timeout to a value of zero disables the timeout, so long-running tasks are never marked as failed.
In this case, a hanging task will never free up its slot, and over time there may be cluster slowdown as a result.
When the jobtracker is notified of a task attempt that has failed (by the tasktracker’s heartbeat call), it will reschedule execution of the task.
The jobtracker will try to avoid rescheduling the task on a tasktracker where it has previously failed.
Furthermore, if a task fails four times (or more), it will not be retried again.
By default, if any task fails four times (or whatever the maximum number of attempts is configured to), the whole job fails.
For some applications, it is undesirable to abort the job when a few tasks fail, as it may be possible to use the results of the job despite some failures.
In this case, the maximum percentage of tasks that are allowed to fail without triggering job failure can be set for the job.
A task attempt may also be killed, which is different from failing.
In any other case, orphaned Streaming or Pipes processes will accumulate on the system, which will impact utilization over time.
Users may also kill or fail task attempts using the web UI or the command line (type hadoop job to see the options)
If a tasktracker fails by crashing or running very slowly, it will stop sending heartbeats to the jobtracker (or send them very infrequently)
The jobtracker arranges for map tasks that were run and completed successfully on that tasktracker to be rerun if they belong to incomplete jobs, since their intermediate output residing on the failed tasktracker’s local filesystem may not be accessible to the reduce task.
A tasktracker can also be blacklisted by the jobtracker, even if the tasktracker has not failed.
Blacklisted tasktrackers are not assigned tasks, but they continue to communicate with the jobtracker.
Faults expire over time (at the rate of one per day), so tasktrackers get the chance to run jobs again simply by continuing to run.
Alternatively, if there is an underlying fault that can be fixed (by replacing hardware, for example), the tasktracker will be removed from the jobtracker’s blacklist after it restarts and rejoins the cluster.
Failure of the jobtracker is the most serious failure mode.
However, this failure mode is unlikely because the chance of a particular machine failing is low.
The good news is that the situation is improved with YARN, since one of its design goals is to eliminate single points of failure in MapReduce.
After restarting a jobtracker, any jobs that were running at the time it was stopped will need to be resubmitted.
Failures in YARN For MapReduce programs running on YARN, we need to consider the failure of any of the following entities: the task, the application master, the node manager, and the resource manager.
Failure of the running task is similar to the classic case.
Runtime exceptions and sudden exits of the JVM are propagated back to the application master, and the task attempt is marked as failed.
Just like MapReduce tasks are given several attempts to succeed (in the face of hardware or network failures), applications in YARN are tried multiple times in the event of failure.
An application master sends periodic heartbeats to the resource manager, and in the event of application master failure, the resource manager will detect the failure and start a new instance of the master running in a new container (managed by a node manager)
In the case of the MapReduce application master, it can recover the state of the tasks that were already run by the (failed) application so they don’t have to be rerun.
The client polls the application master for progress reports; if its application master fails, the client needs to locate the new instance.
During job initialization, the client asks the resource manager for the application master’s address, and then caches it so it doesn’t overload the resource manager with a request every time it needs to poll the application master.
If the application master fails, however, the client will experience a timeout when it issues a status update, at which point the client will go back to the resource manager to ask for the new application master’s address.
If a node manager fails, it will stop sending heartbeats to the resource manager, and the node manager will be removed from the resource manager’s pool of available nodes.
Any task or application master running on the failed node manager will be recovered using the mechanisms described in the previous two sections.
Node managers may be blacklisted if the number of failures for the application is high.
Blacklisting is done by the application master, and for MapReduce the application master will try to reschedule tasks on different nodes if more than three tasks fail on a node manager.
Note that the resource manager does not do blacklisting (at the time of this writing), so tasks from new jobs may be scheduled on bad nodes, even if they have been blacklisted by an application master running an earlier job.
Failure of the resource manager is serious, because without it, neither jobs nor task containers can be launched.
The resource manager was designed from the outset to be able to recover from crashes by using a checkpointing mechanism to save its state to persistent storage, although at the time of this writing, the latest release did not have a complete implementation.
After a crash, a new resource manager instance is brought up (by an administrator), and it recovers from the saved state.
The state consists of the node managers in the system as well as the running applications.
Note that tasks are not part of the resource manager’s state, since they are managed by the application master.
However, there is a ZooKeeper-based store in the works that will support reliable recovery from resource manager failures in the future.
Job Scheduling Early versions of Hadoop had a very simple approach to scheduling users’ jobs: they ran in order of submission, using a FIFO scheduler.
Typically, each job would use the whole cluster, so jobs had to wait their turn.
Although a shared cluster offers great potential for offering large resources to many users, the problem of sharing resources fairly between users requires a better scheduler.
Production jobs need to complete in a timely manner, while allowing users who are making smaller ad hoc queries to get results back in a reasonable time.
When the job scheduler is choosing the next job to run, it selects one with the highest priority.
However, with the FIFO scheduler, priorities do not support preemption, so a high-priority job can still be blocked by a long-running, low-priority job that started before the high-priority job was scheduled.
The default in MapReduce 1 is the original FIFO queue-based scheduler, and there are also multiuser schedulers called the Fair Scheduler and the Capacity Scheduler.
The Fair Scheduler The Fair Scheduler aims to give every user a fair share of the cluster capacity over time.
If a single job is running, it gets all of the cluster.
As more jobs are submitted, free task slots are given to the jobs in such a way as to give each user a fair share of the cluster.
A short job belonging to one user will complete in a reasonable time even while another user’s long job is running, and the long job will still make progress.
Jobs are placed in pools, and by default, each user gets her own pool.
A user who submits more jobs than a second user will not get any more cluster resources than the second, on average.
It is also possible to define custom pools with guaranteed minimum capacities specified in terms of the number of map and reduce slots, and to set weightings for each pool.
The Fair Scheduler supports preemption, so if a pool has not received its fair share for a certain period of time, the scheduler will kill tasks in pools running over capacity in order to give more slots to the pool running under capacity.
A cluster is made up of a number of queues (like the Fair Scheduler’s pools), which may be hierarchical (so a queue may be the child of another queue), and each queue has an allocated capacity.
This is like the Fair Scheduler, except that within each queue, jobs are scheduled using FIFO scheduling (with priorities)
In contrast, the Fair Scheduler (which actually also supports FIFO job scheduling within pools as an option, making it like the Capacity Scheduler) enforces fair sharing within each pool, so running jobs share the pool’s resources.
Shuffle and Sort MapReduce makes the guarantee that the input to every reducer is sorted by key.
The shuffle is an area of the codebase where refinements and improvements are continually being made, so the following description necessarily conceals many details (and may change over time; this is for version 0.20)
In many ways, the shuffle is the heart of MapReduce and is where the “magic” happens.
The Map Side When the map function starts producing output, it is not simply written to disk.
The process is more involved, and takes advantage of buffering writes in memory and doing some presorting for efficiency reasons.
The term shuffle is actually imprecise, since in some contexts it refers to only the part of the process where map outputs are fetched by reduce tasks.
In this section, we take it to mean the whole process, from the point where a map produces output to where a reduce consumes input.
Each map task has a circular memory buffer that it writes the output to.
The buffer is 100 MB by default, a size that can be tuned by changing the io.sort.mb property.
Map outputs will continue to be written to the buffer while the spill takes place, but if the buffer fills up during this time, the map will block until the spill is complete.
Before it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to.
Within each partition, the background thread performs an in-memory sort by key, and if there is a combiner function, it is run on the output of the sort.
Running the combiner function makes for a more compact map output, so there is less data to write to local disk and to transfer to the reducer.
Each time the memory buffer reaches the spill threshold, a new spill file is created, so after the map task has written its last output record, there could be several spill files.
Before the task is finished, the spill files are merged into a single partitioned and sorted output file.
The configuration property io.sort.factor controls the maximum number of streams to merge at once; the default is 10
Recall that combiners may be run repeatedly over the input without affecting the final result.
If there are only one or two spills, the potential reduction in map output size is not worth the overhead in invoking the combiner, so it is not run again for this map output.
It is often a good idea to compress the map output as it is written to disk because doing so makes it faster to write to disk, saves disk space, and reduces the amount of data to transfer to the reducer.
The output file’s partitions are made available to the reducers over HTTP.
The default of 40 may need to be increased for large clusters running large jobs.
In MapReduce 2, this property is not applicable because the maximum number of threads used is set automatically based on the number of processors on the machine.
MapReduce 2 uses Netty, which by default allows up to twice as many threads as there are processors.)
The Reduce Side Let’s turn now to the reduce part of the process.
The map output file is sitting on the local disk of the machine that ran the map task (note that although map outputs always get written to local disk, reduce outputs may not be), but now it is needed by the machine that is about to run the reduce task for the partition.
Furthermore, the reduce task needs the map output for its particular partition from several map tasks across the cluster.
The map tasks may finish at different times, so the reduce task starts copying their outputs as soon as each completes.
This is known as the copy phase of the reduce task.
The reduce task has a small number of copier threads so that it can fetch map outputs in parallel.
How do reducers know which machines to fetch map output from? As map tasks complete successfully, they notify their parent tasktracker of the status update, which in turn notifies the jobtracker.
In MapReduce 2, the tasks notify their application master directly.) These notifications are transmitted over the heartbeat communication mechanism described earlier.
Therefore, for a given job, the jobtracker (or application master) knows the mapping between map outputs and hosts.
A thread in the reducer periodically asks the master for map output hosts until it has retrieved them all.
Hosts do not delete map outputs from disk as soon as the first reducer has retrieved them, as the reducer may subsequently fail.
Instead, they wait until they are told to delete them by the jobtracker (or application master), which is after the job has completed.
Each round would merge 10 files into one, so at the end there would be five intermediate files.
Rather than have a final round that merges these five files into a single sorted file, the merge saves a trip to disk by directly feeding the reduce function in what is the last phase: the reduce phase.
This final merge can come from a mixture of in-memory and on-disk segments.
The number of files merged in each round is actually more subtle than this example suggests.
The goal is to merge the minimum number of files to get to the merge factor for the final round.
Note that this does not change the number of rounds; it’s just an optimization to minimize the amount of data that is written to disk, since the final round always merges directly into the reduce.
During the reduce phase, the reduce function is invoked for each key in the sorted output.
The output of this phase is written directly to the output filesystem, typically HDFS.
In the case of HDFS, because the tasktracker node (or node manager) is also running a datanode, the first block replica will be written to the local disk.
Configuration Tuning We are now in a better position to understand how to tune the shuffle to improve MapReduce performance.
The general principle is to give the shuffle as much memory as possible.
However, there is a trade-off, in that you need to make sure that your map and reduce functions get enough memory to operate.
This is why it is best to write your map and reduce functions to use as little memory as possible—certainly they should not use an unbounded amount of memory (by avoiding accumulating values in a map, for example)
On the map side, the best performance can be obtained by avoiding multiple spills to disk; one is optimal.
If you can estimate the size of your map outputs, you can set the io.sort.* properties appropriately to minimize the number of spills.
Note that the counter includes both map- and reduce-side spills.
On the reduce side, the best performance is obtained when the intermediate data can reside entirely in memory.
This does not happen by default, since for the general case all the memory is reserved for the reduce function.
The remaining space is used for the map output records themselves.
This property was removed in releases after 1.x, as the shuffle code was improved to do a better job of using all the available memory for map output and accounting information.
This is a cluster-wide setting and cannot be set by individual jobs.
The reducer may repeatedly reattempt a transfer within this time if it fails (using exponential backoff)
For the reduce phase to begin, the size of map outputs in memory must be no more than this size.
By default, all map outputs are merged to disk before the reduce begins, to give the reducers as much memory as possible.
However, if your reducers require less memory, this value may be increased to minimize the number of trips to disk.
In this section, we’ll look at some more controls that MapReduce users have over task execution.
The Task Execution Environment Hadoop provides information to a map or reduce task about the environment in which it is running.
The properties in Table 6-3 can be accessed from the job’s configuration, obtained in the old MapReduce API by providing an implementation of the configure() method for Mapper or Reducer, where the configuration is passed in as an argument.
In the new API, these properties can be accessed from the context object passed to all methods of the Mapper or Reducer.
Hadoop sets job configuration parameters as environment variables for Streaming programs.
However, it replaces nonalphanumeric characters with underscores to make sure they are valid names.
The following Python expression illustrates how you can retrieve the value of the mapred.job.id property from within a Python Streaming script:
You can also set environment variables for the Streaming processes launched by MapReduce by supplying the -cmdenv option to the Streaming launcher program (once for each variable you wish to set)
Speculative Execution The MapReduce model is to break jobs into tasks and run the tasks in parallel to make the overall job execution time smaller than it would be if the tasks ran sequentially.
This makes the job execution time sensitive to slow-running tasks, as it takes only one slow task to make the whole job take significantly longer than it would have done otherwise.
When a job consists of hundreds or thousands of tasks, the possibility of a few straggling tasks is very real.
Tasks may be slow for various reasons, including hardware degradation or software misconfiguration, but the causes may be hard to detect because the tasks still complete successfully, albeit after a longer time than expected.
Hadoop doesn’t try to diagnose and fix slow-running tasks; instead, it tries to detect when a task is running slower than expected and launches another equivalent task as a backup.
It’s important to understand that speculative execution does not work by launching two duplicate tasks at about the same time so they can race each other.
Rather, a speculative task is launched only after all the tasks for a job have been launched, and then only for tasks that have been running for some time (at least a minute) and have failed to make as much progress, on average, as the other tasks from the job.
When a task completes successfully, any duplicate tasks that are running are killed since they are no longer needed.
So if the original task completes before the speculative task, the speculative task is killed; on the other hand, if the speculative task finishes first, the original is killed.
Speculative execution is an optimization, and not a feature to make jobs run more reliably.
If there are bugs that sometimes cause a task to hang or slow down, relying on speculative execution to avoid these problems is unwise and won’t work reliably, since the same bugs are likely to affect the speculative task.
You should fix the bug so that the task doesn’t hang or slow down.
It can be enabled or disabled independently for map tasks and reduce tasks, on a cluster-wide basis, or on a per-job basis.
The Speculator class implementing the speculative execution policy (MapReduce 2 only)
An implementation of TaskRuntimeEstima tor used by Specula tor instances that provides estimates for task runtimes (MapReduce 2 only)
Why would you ever want to turn off speculative execution? The goal of speculative execution is to reduce job execution time, but this comes at the cost of cluster efficiency.
On a busy cluster, speculative execution can reduce overall throughput, since redundant tasks are being executed in an attempt to bring down the execution time for a single job.
For this reason, some cluster administrators prefer to turn it off on the cluster and have users explicitly turn it on for individual jobs.
This was especially relevant for older versions of Hadoop, when speculative execution could be overly aggressive in scheduling speculative tasks.
There is a good case for turning off speculative execution for reduce tasks, since any duplicate reduce tasks have to fetch the same map outputs as the original task, and this can significantly increase network traffic on the cluster.
Another reason for turning off speculative execution is for tasks that are not idempotent.
However, in many cases it is possible to write tasks to be idempotent and use an OutputCommitter to promote the output to its final location when the task succeeds.
This technique is explained in more detail in the next section.
Output Committers Hadoop MapReduce uses a commit protocol to ensure that jobs and tasks either succeed or fail cleanly.
In the new MapReduce API, the OutputCommitter is determined by the OutputFormat, via its getOut putCommitter() method.
You can customize an existing OutputCommitter or even write a new implementation if you need to do special setup or cleanup for jobs or tasks.
The OutputCommitter API is as follows (in both old and new MapReduce APIs):
The usual way of writing output from map and reduce tasks is by using the OutputCol lector to collect key-value pairs.
As we saw in the previous section, the OutputCommitter protocol solves this problem.
If applications write side files in their tasks’ working directories, the side files for tasks that successfully complete will be promoted to the output directory automatically, whereas failed tasks will have their side files deleted.
The framework creates the working directory before executing the task, so you don’t need to create it.
To take a simple example, imagine a program for converting image files from one format to another.
If a map task writes the converted images into its working directory, they will be promoted to the output directory when the task successfully finishes.
Task JVM Reuse Hadoop runs tasks in their own Java Virtual Machine to isolate them from other running tasks.
Starting a new JVM for each task can take around a second, which for jobs that run for a minute or so is insignificant.
However, jobs that have a large number of very short-lived tasks (these are usually map tasks) or that have a lengthy initialization can see performance gains when the JVM is reused for subsequent tasks.7 Note that with task JVM reuse enabled, tasks are not run concurrently in a single JVM; rather, the JVM runs tasks sequentially.
Tasktrackers can, however, run more than one task at a time, but this is always done in separate JVMs.
No distinction is made between map or reduce tasks; however, tasks from different jobs are always run in separate JVMs.
A value of –1 indicates no limit, which means the same JVM may be used for all tasks for a job.
Tasks that are CPU-bound may also benefit from task JVM reuse by taking advantage of runtime optimizations applied by the HotSpot JVM.
This works well for long-running processes, but JVMs that run for seconds or a few minutes may not gain the full benefit of HotSpot.
They often have records that are in a different format.
In an ideal world, your code would cope gracefully with all of these conditions.
In practice, it is often expedient to ignore the offending records.
Depending on the analysis being performed, if only a small percentage of records are affected, then skipping them may not significantly affect the result.
If it is the data that is causing the task to throw an exception, rerunning the task won’t help, since it will fail in exactly the same way each time.
Corruption in a file can manifest itself as a very long line, which can cause out-of-memory errors and then task failure.
The best way to handle corrupt records is in your mapper or reducer code.
You can detect the bad record and ignore it, or you can abort the job by throwing an exception.
You can also count the total number of bad records in the job using counters to see how widespread the problem is.
In rare cases, though, you can’t handle the problem because there is a bug in a thirdparty library that you can’t work around in your mapper or reducer.
In these cases, you can use Hadoop’s optional skipping mode for automatically skipping bad records.8 When skipping mode is enabled, tasks report the records being processed back to the tasktracker.
When the task fails, the tasktracker retries the task, skipping the records that caused the failure.
Because of the extra network traffic and bookkeeping to maintain the failed record ranges, skipping mode is turned on for a task only after it has failed twice.
Thus, for a task consistently failing on a bad record, the tasktracker runs the following task attempts with these outcomes:
Task fails, but the failed record is stored by the tasktracker.
Skipping mode is off by default; you enable it independently for map and reduce tasks using the SkipBadRecords class.
It’s important to note that skipping mode can detect only one bad record per task attempt, so this mechanism is appropriate only for detecting occasional bad records (a few per task, say)
Bad records that have been detected by Hadoop are saved as sequence files in the job’s output directory under the _logs/skip subdirectory.
These can be inspected for diagnostic purposes after the job has completed (using hadoop fs -text, for example)
Skipping mode is not supported in the new MapReduce API.
MapReduce has a simple model of data processing: inputs and outputs for the map and reduce functions are key-value pairs.
This chapter looks at the MapReduce model in detail and, in particular, how data in various formats, from simple text to structured binary objects, can be used with this model.
MapReduce Types The map and reduce functions in Hadoop MapReduce have the following general form:
The context objects are used for emitting key-value pairs, and so they are parameterized by the output types so that the signature of the write() method is:
Since Mapper and Reducer are separate classes, the type parameters have different scopes, and the actual type argument of KEYIN (say) in the Mapper may be different from the type of the type parameter of the same name (KEYIN) in the Reducer.
For instance, in the maximum temperature example from earlier chapters, KEYIN is replaced by LongWrita ble for the Mapper and by Text for the Reducer.
Similarly, even though the map output types and the reduce input types must match, this is not enforced by the Java compiler.
The type parameters are named differently from the abstract types (KEYIN versus K1, and so on), but the form is the same.
In practice, the partition is determined solely by the key (the value is ignored):
It is divided into the properties that determine the types and those that have to be compatible with the configured types.
So, for instance, a TextInputFormat generates keys of type LongWritable and values of type Text.
The other types are set explicitly by calling the methods on the Job (or JobConf in the old API)
If not set explicitly, the intermediate types default to the (final) output types, which default to LongWritable and Text.
It may seem strange that these methods for setting the intermediate and final output types exist at all.
After all, why can’t the types be determined from a combination of the mapper and the reducer? The answer has to do with a limitation in Java generics: type erasure means that the type information isn’t always present at runtime, so Hadoop has to be given it explicitly.
This also means that it’s possible to configure a MapReduce job with incompatible types, because the configuration isn’t checked at compile time.
The settings that have to be compatible with the MapReduce types are listed in the lower part of Table 7-1
Type conflicts are detected at runtime during job execution, and for this reason, it is wise to run a test job using a small amount of data to flush out and fix any type incompatibilities.
The Default MapReduce Job What happens when you run MapReduce without setting a mapper or a reducer? Let’s try it by running this minimal MapReduce program:
The only configuration that we set is an input path and an output path.
We run it over a subset of our weather data with the following:
We do get some output: one file named part-r-00000 in the output directory.
Here’s what the first few lines look like (truncated to fit the page):
Each line is an integer followed by a tab character, followed by the original weather data record.
Admittedly, it’s not a very useful program, but understanding how it produces its output does provide some insight into the defaults that Hadoop uses when running MapReduce jobs.
Example 7-1 shows a program that has exactly the same effect as MinimalMapReduce, but explicitly sets the job settings to their defaults.
We’ve simplified the first few lines of the run() method by extracting the logic for printing usage and setting the input and output paths into a helper method.
Almost all MapReduce drivers take these two arguments (input and output), so reducing the boilerplate code here is a good thing.
Here are the relevant methods in the JobBuilder class for reference:
The default input format is TextInputFormat, which produces keys of type LongWrita ble (the offset of the beginning of the line in the file) and values of type Text (the line of text)
This explains where the integers in the final output come from: they are the line offsets.
The default mapper is just the Mapper class, which writes the input key and value unchanged to the output:
Mapper is a generic type, which allows it to work with any key or value types.
In this case, the map input and output key is of type LongWritable, and the map input and output value is of type Text.
The default partitioner is HashPartitioner, which hashes a record’s key to determine which partition the record belongs in.
Each partition is processed by a reduce task, so the number of partitions is equal to the number of reduce tasks for the job:
The key’s hash code is turned into a nonnegative integer by bitwise ANDing it with the largest integer value.
It is then reduced modulo the number of partitions to find the index of the partition that the record belongs in.
By default, there is a single reducer, and therefore a single partition, so the action of the partitioner is irrelevant in this case since everything goes into one partition.
However, it is important to understand the behavior of HashPartitioner when you have more than one reduce task.
Assuming the key’s hash function is a good one, the records.
You may have noticed that we didn’t set the number of map tasks.
The reason for this is that the number is equal to the number of splits that the input is turned into, which is driven by the size of the input and the file’s block size (if the file is in HDFS)
Choosing the Number of Reducers The single reducer default is something of a gotcha for new users to Hadoop.
Almost all real-world jobs should set this to a larger number; otherwise, the job will be very slow since all the intermediate data flows through a single reduce task.
Note that when running under the local job runner, only zero or one reducers are supported.) The optimal number of reducers is related to the total number of available reducer slots in your cluster.
One common setting is to have slightly fewer reducers than total slots, which gives one wave of reduce tasks (and tolerates a few failures, without extending job execution time)
If your reduce tasks are very big, it makes sense to have a larger number of reducers (resulting in two waves, for example) so that the tasks are more fine-grained and failure doesn’t affect job execution time significantly.
The default reducer is Reducer, again a generic type, which simply writes all its input to its output:
For this job, the output key is LongWritable and the output value is Text.
In fact, all the keys for this MapReduce program are LongWritable, and all the values are Text, since these are the input keys and values, and the map and reduce functions are both identity functions, which by definition preserve type.
Most MapReduce programs, however, don’t use the same key or value types throughout, so you need to configure the job to declare the types you are using, as described in the previous section.
Records are sorted by the MapReduce system before being presented to the reducer.
In this case, the keys are sorted numerically, which has the effect of interleaving the lines from the input files into one combined output file.
The default output format is TextOutputFormat, which writes out records, one per line, by converting keys and values to strings and separating them with a tab character.
This is why the output is tab-separated: it is a feature of TextOutputFormat.
In Streaming, the default job is similar, but not identical, to the Java equivalent.
Notice that you have to supply a mapper; the default identity mapper will not work.
The reason has to do with the default input format, TextInputFormat, which generates LongWritable keys and Text values.
However, Streaming output keys and values (including the map keys and values) are always both of type Text.1 The identity mapper cannot change LongWritable keys to Text keys, so it fails.
When we specify a non-Java mapper and the input format is TextInputFormat, Streaming does something special.
It doesn’t pass the key to the mapper process; it just passes the value.
The overall effect of this job is to perform a sort of the input.
With more of the defaults spelled out, the command looks like this (notice that Streaming uses the old MapReduce API classes):
The mapper and reducer arguments take a command or a Java class.
A combiner may optionally be specified using the -combiner argument.
Except when used in binary mode (not available in 1.x) via the -io rawbytes or -io typedbytes options.
A Streaming application can control the separator that is used when a key-value pair is turned into a series of bytes and sent to the map or reduce process over standard input.
The default is a tab character, but it is useful to be able to change it in the case that the keys or values themselves contain tab characters.
Similarly, when the map or reduce writes out key-value pairs, they may be separated by a configurable separator.
For example, if the output from a Streaming process were a,b,c (with a comma as the separator,) and n were two, the key would be parsed as a,b and the value as c.
These settings do not have any bearing on the input and output formats.
With the standard TextOutputFormat, this record would be written to the output file with a tab separating a and b.
A list of Streaming configuration parameters can be found on the Hadoop website at http://hadoop.apache.org/mapreduce/docs/current/streaming.html#Configurable+pa rameters.
String \t The separator to use when passing the input key and value strings to the stream map process as a stream of bytes.
String \t The separator to use when splitting the output from the stream map process into key and value strings for the map output.
String \t The separator to use when passing the input key and value strings to the stream reduce process as a stream of bytes.
String \t The separator to use when splitting the output from the stream reduce process into key and value strings for the final reduce output.
Input Formats Hadoop can process many different types of data formats, from flat text files to databases.
Input Splits and Records As we saw in Chapter 2, an input split is a chunk of the input that is processed by a single map.
Splits and records are logical: there is nothing that requires them to be tied to files, for example, although in their most common incarnations, they are.
In a database context, a split might correspond to a range of rows from a table and a record to a row in that range (this is precisely what DBInputFormat does, which is an input format for reading data from a relational database)
An InputSplit has a length in bytes and a set of storage locations, which are just hostname strings.
Notice that a split doesn’t contain the input data; it is just a reference to the data.
The storage locations are used by the MapReduce system to place map tasks as close to the split’s data as possible, and the size is used to order the splits so that the largest get processed first, in an attempt to minimize the job runtime (this is an instance of a greedy approximation algorithm)
As a MapReduce application writer, you don’t need to deal with InputSplits directly, as they are created by an InputFormat.
An InputFormat is responsible for creating the input splits and dividing them into records.
Before we see some concrete examples of InputFormat, let’s briefly examine how it is used in MapReduce.
The client running the job calculates the splits for the job by calling getSplits(), then sends them to the jobtracker, which uses their storage locations to schedule map tasks that will process them on the tasktrackers.
A RecordReader is little more than an iterator over records, and the map task uses one to generate record key-value pairs, which it passes to the map function.
We can see this by looking at the Mapper’s run() method:
After running setup(), the nextKeyValue() is called repeatedly on the Context (which delegates to the identically-named method on the RecordReader) to populate the key and value objects for the mapper.
The key and value are retrieved from the Record Reader by way of the Context and are passed to the map() method for it to do its work.
When the reader gets to the end of the stream, the nextKeyValue() method returns false, and the map task runs its cleanup() method and then completes.
It’s not shown in the code snippet, but for reasons of efficiency Record Reader implementations will return the same key and value objects on each call to getCurrentKey() and getCurrentValue()
Only the contents of these objects are changed by the reader’s nextKeyValue() method.
This can be a surprise to users, who might expect keys and values to be immutable, and not to be reused.
This causes problems when a reference to a key or value object is retained outside the map() method, as its value can change without warning.
If you need to do this, make a copy of the object you want to hold on to.
For example, for a Text object, you can use its copy constructor: new Text(value)
In this case, the value objects in the reducer’s iterator are reused, so you need to copy any that you need to retain between calls to the iterator (see Example 8-14)
Finally, note that the Mapper’s run() method is public and may be customized by users.
For most data processing tasks, it confers no advantage over the default implementation.
FileInputFormat is the base class for all implementations of InputFormat that use files as their data source (see Figure 7-2)
It provides two things: a place to define which files are included as the input to a job, and an implementation for generating splits for the input files.
The job of dividing splits into records is performed by subclasses.
The input to a job is specified as a collection of paths, which offers great flexibility in constraining the input to a job.
FileInputFormat offers four static convenience methods for setting a Job’s input paths:
The addInputPath() and addInputPaths() methods add a path or paths to the list of inputs.
You can call these methods repeatedly to build the list of paths.
The setInput Paths() methods set the entire list of paths in one go (replacing any paths set on the Job in previous calls)
A path may represent a file, a directory, or, by using a glob, a collection of files and directories.
A path representing a directory includes all the files in the directory as input to the job.
The contents of a directory specified as an input path are not processed recursively.
If the directory contains a subdirectory, it will be interpreted as a file, which will cause an error.
The way to handle this case is to use a file glob or a filter to select only the files in the directory based on a name pattern.
The add and set methods allow files to be specified by inclusion only.
Even if you don’t set a filter, FileInputFormat uses a default filter that excludes hidden files (those whose names begin with a dot or an underscore)
In other words, only nonhidden files that are accepted by your filter get through.
Paths and filters can be set through configuration properties, too (Table 7-4), which can be handy for Streaming and Pipes.
Setting paths is done with the -input option for both Streaming and Pipes interfaces, so setting paths directly usually is not needed.
None The filter to apply to the input files for a job.
Given a set of files, how does FileInputFormat turn them into splits? FileInputFormat splits only large files.
The split size is normally the size of an HDFS block, which is appropriate for most applications; however, it is possible to control this value by setting various Hadoop properties, as shown in Table 7-5
The largest valid size in bytes for a file split.
The minimum split size is usually 1 byte, although some formats have a lower bound on the split size.
For example, sequence files insert sync entries every so often in the stream, so the minimum split size has to be large enough to ensure that every split has a sync point to allow the reader to resynchronize with a record boundary.)
By setting this to a value larger than the block size, they can force splits to be larger than a block.
There is no good reason for doing this when using HDFS, because doing so will increase the number of blocks that are not local to a map task.
The maximum split size defaults to the maximum value that can be represented by a Java long type.
It has an effect only when it is less than the block size, forcing splits to be smaller than a block.
The split size is calculated by the formula (see the computeSplitSize() method in FileInputFormat):
Various settings for these parameters and how they affect the final split size are illustrated in Table 7-6
Hadoop works better with a small number of large files than a large number of small files.
One reason for this is that FileInputFormat generates splits in such a way that each split is all or part of a single file.
If the file is very small (“small” means significantly smaller than an HDFS block) and there are a lot of them, each map task will process very little input, and there will be a lot of them (one per file), each of which imposes extra bookkeeping overhead.
Of course, if possible, it is still a good idea to avoid the many small files case because MapReduce works best when it can operate at the transfer rate of the disks in the cluster, and processing many small files increases the number of seeks that are needed to run a job.
Also, storing large numbers of small files in HDFS is wasteful of the namenode’s memory.
One technique for avoiding the many small files case is to merge small files into larger files by using a SequenceFile; with this approach, the keys can act as filenames (or a constant such as NullWritable, if not needed) and the values as file contents.
In return, the overall processing time falls, since proportionally fewer mappers run, which reduces the overhead in task bookkeeping and startup time associated with a large number of shortlived mappers.
Some applications don’t want files to be split, as this allows a single mapper to process each input file in its entirety.
For example, a simple way to check if all the records in a file are sorted is to go through the records in order, checking whether each record is not less than the preceding one.
Implemented as a map task, this algorithm will work only if one map processes the whole file.3 There are a couple of ways to ensure that an existing file is not split.
The first (quick and dirty) way is to increase the minimum split size to be larger than the largest file in your system.
Setting it to its maximum value, Long.MAX_VALUE, has this effect.
The second is to subclass the concrete subclass of FileInputFormat that you want to use, to override the isSplitable() method4 to return false.
A mapper processing a file input split can find information about the split by calling the getInputSplit() method on the Mapper’s Context object.
When the input format derives from FileInputFormat, the InputSplit returned by this method can be cast to a FileSplit to access the file information listed in Table 7-7
In the old MapReduce API, Streaming, and Pipes, the same file split information is made available through properties that can be read from the mapper’s configuration.
In the next section, you shall see how to use a FileSplit when we need to access the split’s filename.
If the file has not been processed when the nextKey.
It does this by casting the InputSplit from the context to a FileSplit, which has a method to retrieve the file path.
The path is stored in a Text object for the key.
We’ve chosen to use two reducers, so we get two output sequence files:
Two part files are created, each of which is a sequence file.
We can inspect these with the -text option to the filesystem shell:
We can see this in the textual rendering of the sequence files, which prints the filename followed by the hex representation of the file.
There’s at least one way we could improve this program.
In this section, we discuss the different InputFormats that Hadoop provides to process text.
The key, a LongWritable, is the byte offset within the file of the beginning of the line.
The value is the contents of the line, excluding any line terminators (e.g., newline or carriage return), and is packaged as a Text object.
This would be impossible to implement in general, in that a file is broken into splits at byte, not line, boundaries.
You have to keep a count of lines as you consume them, so knowing the line number within a split would be possible, but not within the file.
However, the offset within the file of each line is known by each split independently of the other splits, since each split knows the size of the preceding splits and just adds this onto the offsets within the split to produce a global file offset.
The offset is usually sufficient for applications that need a unique identifier for each line.
Combined with the file’s name, it is unique within the filesystem.
Of course, if all the lines are a fixed width, calculating the line number is simply a matter of dividing the offset by the width.
For example, a TextInputFormat’s logical records are lines, which will cross HDFS boundaries more often than not.
A single file is broken into lines, and the line boundaries do not correspond with the HDFS block boundaries.
Splits honor logical record boundaries, in this case lines, so we see that the first split contains line 5, even though it spans the first and second block.
Like in the TextInputFormat case, the input is in a single split comprising four records, although this time the keys are the Text sequences before the tab in each line:
The number depends on the size of the split and the length of the lines.
If you want your mappers to receive a fixed number of lines of input, then NLineInputFormat is the InputFormat to use.
Like TextInputFormat, the keys are the byte offsets within the file and the values are the lines themselves.
With N set to one (the default), each mapper receives exactly one line of input.
If, for example, N is two, then each split contains two lines.
The keys and values are the same as those that TextInputFormat produces.
The difference is in the way the splits are constructed.
Usually, having a map task for a small number of lines of input is inefficient (due to the overhead in task setup), but there are applications that take a small amount of input data and run an extensive (that is, CPU-intensive) computation for it, then emit their output.
By creating an input file that specifies input parameters, one per line, you can perform a parameter sweep: run a set of simulations in parallel to find how a model varies as the parameter changes.
If you have long-running simulations, you may fall afoul of task timeouts.
The best way to guard against this is to report progress periodically, by writing a status message or incrementing a counter, for example.
Another example is using Hadoop to bootstrap data loading from multiple data sources, such as databases.
You create a “seed” input file that lists the data sources, one per line.
Then each mapper is allocated a single data source, and it loads the data from that source into HDFS.
Furthermore, MapReduce jobs can be run to process the data loaded into HDFS.
Most XML parsers operate on whole XML documents, so if a large XML document is made up of multiple input splits, it is a challenge to parse these individually.
Large XML documents that are composed of a series of “records” (XML document fragments) can be broken into these records using simple string or regular-expression matching to find the start and end tags of records.
This alleviates the problem when the document is split by the framework because the next start tag of a record is easy to find by simply scanning from the start of the split, just like TextInputFormat finds newline boundaries.
The reader is configured by setting job configuration properties to tell it the patterns for the start and end tags (see the class documentation for details).5 To take an example, Wikipedia provides dumps of its content in XML form, which are appropriate for processing in parallel with MapReduce using this approach.
The data is contained in one large XML wrapper document, which contains a series of elements, such as page elements that contain a page’s content and associated metadata.
Binary Input Hadoop MapReduce is not restricted to processing textual data.
Hadoop’s sequence file format stores sequences of binary key-value pairs.
Sequence files are well suited as a format for MapReduce data because they are splittable (they have sync points so that readers can synchronize with record boundaries from an arbitrary point in the file, such as the start of a split), they support compression as a part of the format, and they can store arbitrary types using a variety of serialization frameworks.
The keys and values are determined by the sequence file, and you need to make sure that your map input types correspond.
See Mahout’s XmlInputFormat (available from http://mahout.apache.org/) for an improved XML input format.
The conversion is performed by calling toString() on the keys and values.
They are encapsulated as BytesWritable objects, and the application is free to interpret the underlying byte array as it pleases.
Multiple Inputs Although the input to a MapReduce job may consist of multiple input files (constructed by a combination of file globs, filters, and plain paths), all of the input is interpreted by a single InputFormat and a single Mapper.
What often happens, however, is that the data format evolves over time, so you have to write your mapper to cope with all of your legacy formats.
Or you have data sources that provide the same type of data but in different formats.
For instance, one might be tab-separated plain text, and the other a binary sequence file.
Even if they are in the same format, they may have different representations and, therefore, need to be parsed differently.
These cases are handled elegantly by using the MultipleInputs class, which allows you to specify which InputFormat and Mapper to use on a per-path basis.
For example, if we had weather data from the UK Met Office6 that we wanted to combine with the NCDC data for our maximum temperature analysis, we might set up the input as follows:
Met Office data is generally available only to the research and academic community.
However, there is a small amount of monthly weather station data available at http://www.metoffice.gov.uk/climate/uk/ stationdata/
Both the Met Office and NCDC data is text-based, so we use TextInput Format for each.
But the line format of the two data sources is different, so we use two different mappers.
The important thing is that the map outputs have the same types, since the reducers (which are all of the same type) see the aggregated map outputs and are not aware of the different mappers used to produce them.
The MultipleInputs class has an overloaded version of addInputPath() that doesn’t take a mapper:
This is useful when you only have one mapper (set using the Job’s setMapperClass() method) but multiple input formats.
Database Input (and Output) DBInputFormat is an input format for reading data from a relational database, using JDBC.
Because it doesn’t have any sharding capabilities, you need to be careful not to overwhelm the database from which you are reading by running too many mappers.
For this reason, it is best used for loading relatively small datasets, perhaps for joining with larger datasets from HDFS using MultipleInputs.
HBase’s TableInputFormat is designed to allow a MapReduce program to operate on data stored in an HBase table.
TableOutputFormat is for writing MapReduce outputs into an HBase table.
Output Formats Hadoop has output data formats that correspond to the input formats covered in the previous section.
Text Output The default output format, TextOutputFormat, writes records as lines of text.
Its keys and values may be of any type, since TextOutputFormat turns them to strings by calling toString() on them.
You can suppress the key or the value (or both, making this output format equivalent to NullOutputFormat, which emits nothing) from the output using a NullWritable type.
This also causes no separator to be written, which makes the output suitable for reading in using TextInputFormat.
This is a good choice of output if it forms the input to a further MapReduce job, since.
The keys in a MapFile must be added in order, so you need to ensure that your reducers emit keys in sorted order.
The reduce input keys are guaranteed to be sorted, but the output keys are under the control of the reduce function, and there is nothing in the general MapReduce contract that states that the reduce output keys have to be ordered in any way.
Multiple Outputs FileOutputFormat and its subclasses generate a set of files in the output directory.
Sometimes there is a need to have more control over the naming of the files or to produce multiple files per reducer.
MapReduce comes with the MultipleOut puts class to help you do this.8
Consider the problem of partitioning the weather dataset by weather station.
We would like to run a job whose output is one file per station, with each file containing all the records for that station.
One way of doing this is to have a reducer for each weather station.
First, write a partitioner that puts records from the same weather station into the same partition.
Second, set the number of reducers on the job to be the number of weather stations.
MultipleOutputs in the new API combines the best features of the two multiple output classes in the old API.
To do this, it needs a list of all the station IDs and then just returns the index of the station ID in the list.
The first is that since the number of partitions needs to be known before the job is run, so does the number of weather stations.
Although the NCDC provides metadata about its stations, there is no guarantee that the IDs encountered in the data match those in the metadata.
A station that appears in the metadata but not in the data wastes a reducer slot.
Worse, a station that appears in the data but not in the metadata doesn’t get a reducer slot; it has to be thrown away.
One way of mitigating this problem would be to write a job to extract the unique station IDs, but it’s a shame that we need an extra job to do this.
It is generally a bad idea to allow the number of partitions to be rigidly fixed by the application, since this can lead to small or unevensized partitions.
Having many reducers doing a small amount of work isn’t an efficient way of organizing a job; it’s much better to get reducers to do more work and have fewer of them, as the overhead in running a task is then reduced.
Different weather stations will have gathered a widely varying amount of data; for example, compare a station that opened one year ago to one that has been gathering data for one century.
If a few reduce tasks take significantly longer than the others, they will dominate the job execution time and cause it to be longer than it needs to be.
There are two special cases when it does make sense to allow the application to set the number of partitions (or equivalently, the number of reducers): Zero reducers.
This is a vacuous case: there are no partitions, as the application needs to run only map tasks.
One reducer It can be convenient to run small jobs to combine the output of previous jobs into a single file.
This should be attempted only when the amount of data is small enough to be processed comfortably by one reducer.
It is much better to let the cluster drive the number of partitions for a job, the idea being that the more cluster reduce slots are available, the faster the job can complete.
This is why the default HashPartitioner works so well, as it works with any number of partitions and ensures each partition has a good mix of keys, leading to more even-sized partitions.
If we go back to using HashPartitioner, each partition will contain multiple stations, so to create a file per station, we need to arrange for each reducer to write multiple files, which is where MultipleOutputs comes in.
In the reducer, which is where we generate the output, we construct an instance of MultipleOutputs in the setup() method and assign it to an instance variable.
We then use the MultipleOutputs instance in the reduce() method to write to the output, in place of the context.
The write() method takes the key and value, as well as a name.
In one run, the first few output files were named as follows:
The base path specified in the write() method of MultipleOutputs is interpreted relative to the output directory, and because it may contain file path separator characters (/), it’s possible to create subdirectories of arbitrary depth.
MultipleOutputs delegates to the mapper’s OutputFormat, which in this example is a TextOutputFormat, but more complex setups are possible.
For example, you can create named outputs, each with its own OutputFormat and key and value types (which may differ from the output types of the mapper or reducer)
Furthermore, the mapper or reducer (or both) may write to multiple output files for each record processed.
Lazy Output FileOutputFormat subclasses will create output (part-r-nnnnn) files, even if they are empty.
Some applications prefer that empty files not be created, which is where Lazy OutputFormat helps.
It is a wrapper output format that ensures that the output file is.
Streaming and Pipes support a -lazyOutput option to enable LazyOutputFormat.
This chapter looks at some of the more advanced features of MapReduce, including counters and sorting and joining datasets.
Counters There are often things you would like to know about the data you are analyzing but that are peripheral to the analysis you are performing.
For example, if you were counting invalid records and discovered that the proportion of invalid records in the whole dataset was very high, you might be prompted to check why so many records were being marked as invalid—perhaps there is a bug in the part of the program that detects invalid records? Or if the data were of poor quality and genuinely did have very many invalid records, after discovering this, you might decide to increase the size of the dataset so that the number of good records was large enough for meaningful analysis.
Counters are a useful channel for gathering statistics about the job: for quality control or for application-level statistics.
If you are tempted to put a log message into your map or reduce task, it is often better to see whether you can use a counter instead to record that a particular condition occurred.
In addition to counter values being much easier to retrieve than log output for large distributed jobs, you get a record of the number of times that condition occurred, which is more work to obtain from a set of logfiles.
Built-in Counters Hadoop maintains some built-in counters for every job, and these report various metrics.
For example, there are counters for the number of bytes and records processed, which allows you to confirm that the expected amount of input was consumed and the expected amount of output was produced.
Counters are divided into groups, and there are several groups for the built-in counters, listed in Table 8-1
Each group either contains task counters (which are updated as a task progresses) or job counters (which are updated as a job progresses)
Task counters gather information about tasks over the course of their execution, and the results are aggregated over all the tasks in a job.
For example, the MAP_INPUT_RECORDS counter counts the input records read by each map task and aggregates over all map tasks in a job, so that the final figure is the total number of input records for the whole job.
Task counters are maintained by each task attempt, and periodically sent to the tasktracker and then to the jobtracker so they can be globally aggregated.
Furthermore, during a job run, counters may go down if a task fails.
Counter values are definitive only once a job has successfully completed.
However, some counters provide useful diagnostic information as a task is progressing, and it can be useful to monitor them with the web UI.
The number of input records consumed by all the maps in the job.
Incremented every time a record is read from a RecordReader and passed to the map’s map() method by the framework.
The number of input records skipped by all the maps in the job.
The number of bytes of uncompressed input consumed by all the maps in the job.
Incremented every time a record is read from a RecordReader and passed to the map’s map() method by the framework.
The number of bytes of input-split objects read by maps.
These objects represent the split metadata (that is, the offset and length within a file) rather than the split data itself, so the total size should be small.
The number of map output records produced by all the maps in the job.
Incremented every time the collect() method is called on a map’s OutputCollector.
The number of bytes of uncompressed output produced by all the maps in the job.
Incremented every time the collect() method is called on a map’s OutputCollector.
The number of bytes of map output actually written to disk.
If map output compression is enabled, this is reflected in the counter value.
The number of input records consumed by all the combiners (if any) in the job.
Incremented every time a value is read from the combiner’s iterator over values.
The number of output records produced by all the combiners (if any) in the job.
Incremented every time the collect() method is called on a combiner’s OutputCollector.
The number of distinct key groups consumed by all the reducers in the job.
Incremented every time the reducer’s reduce() method is called by the framework.
The number of input records consumed by all the reducers in the job.
Incremented every time a value is read from the reducer’s iterator over values.
If reducers consume all of their inputs, this count should be the same as the count for map output records.
The number of reduce output records produced by all the maps in the job.
Incremented every time the collect() method is called on a reducer’s OutputCollector.
The number of distinct key groups skipped by all the reducers in the job.
Reduce skipped records The number of input records skipped by all the reducers in the job.
The number of bytes of map output copied by the shuffle to reducers.
The number of records spilled to disk in all map and reduce tasks in the job.
The cumulative CPU time for a task in milliseconds, as reported by /proc/cpuinfo.
The physical memory being used by a task in bytes, as reported by /proc/meminfo.
The virtual memory being used by a task in bytes, as reported by /proc/meminfo.
The number of map output copy failures during the shuffle.
The number of map outputs that have been merged on the reduce side of the shuffle.
The number of bytes read by each filesystem by map and reduce tasks.
There is a counter for each filesystem, and Filesystem, which may be Local, HDFS, S3, KFS, etc.
The number of bytes written by each filesystem by map and reduce tasks.
The number of bytes read by map tasks via the FileInputFormat.
The number of bytes written by map tasks (for map-only jobs) or reduce tasks via the FileOutputFormat.
Job counters (Table 8-6) are maintained by the jobtracker (or application master in YARN), so they don’t need to be sent across the network, unlike all other counters, including user-defined ones.
They measure job-level statistics, not values that change while a task is running.
The number of map tasks that ran on the same node as their input data.
The number of map tasks that ran on a node in the same rack as their input data, but that are not data-local.
The number of map tasks that ran on a node in a different rack to their input data.
Inter-rack bandwidth is scarce, and Hadoop tries to place map tasks close to their input data, so this count should be low.
The total time in milliseconds spent waiting after reserving slots for map tasks.
The total time in milliseconds spent waiting after reserving slots for reduce tasks.
When the job has successfully completed, it prints out the counters at the end (this is done by the job client)
The code makes use of a dynamic counter—one that isn’t defined by a Java enum.
Because a Java enum’s fields are defined at compile time, you can’t create new counters on the fly using enums.
Here we want to count the distribution of temperature quality codes, and though the format specification defines the values that the temperature quality code can take, it is more convenient to use a dynamic counter to emit the values.
The method we use on the Reporter object takes a group and counter name using String names:
Enums are slightly easier to work with, provide type safety, and are suitable for most jobs.
For the odd occasion when you need to create counters dynamically, you can use the String interface.
By default, a counter’s name is the enum’s fully qualified Java classname.
These names are not very readable when they appear on the web UI or in the console, so Hadoop provides a way to change the display names using resource bundles.
Create a properties file named after the enum, using an underscore as a separator for nested classes.
The properties file should be in the same directory as the top-level class containing the enum.
The properties file should contain a single property named CounterGroupName, whose value is the display name for the whole group.
Then each field in the enum should have a corresponding property defined for it, whose name is the name of the field suffixed with .name and whose value is the display name for the counter.
Hadoop uses the standard Java localization mechanisms to load the correct properties for the locale you are running in.
In addition to being available via the web UI and the command line (using hadoop job -counter), you can retrieve counter values using the Java API.
You can do this while the job is running, although it is more usual to get counters at the end of a job run, when they are stable.
Example 8-2 shows a program that calculates the proportion of records that have missing temperature fields.
First we retrieve a RunningJob object from a JobClient by calling the getJob() method with the job ID.
We check whether there is actually a job with the given ID.
After confirming that the job has completed, we call the RunningJob’s getCounters() method, which returns a Counters object, encapsulating all the counters for a job.
The Counters class provides various methods for finding the names and values of counters.
We use the getCounter() method, which takes an enum to find the number of records.
Finally, we print the proportion of records that had a missing temperature field.
We used the old API in this example because the new equivalent for retrieving counters after a job has finished is not available in Hadoop 1.x.
Note that it is possible to retrieve the counters from a job that ran using the new API with retrieval code that uses the old API.) The main difference is the use of a Cluster object to retrieve a Job (rather than a RunningJob), and then calling its getCounters() method:
User-Defined Streaming Counters A Streaming MapReduce program can increment counters by sending a specially formatted line to the standard error stream, which is co-opted as a control channel in this case.
Sorting The ability to sort data is at the heart of MapReduce.
Even if your application isn’t concerned with sorting per se, it may be able to use the sorting stage that MapReduce provides to organize its data.
In this section, we examine different ways of sorting datasets and how you can control the sort order in MapReduce.
Preparation We are going to sort the weather dataset by temperature.
The MapReduce job in Example 8-3 is a map-only job that also filters the input to remove records that don’t have a valid temperature reading.
Each map creates a single block-compressed sequence file as output.
Controlling Sort Order The sort order for keys is controlled by a RawComparator, which is found as follows:
If there is no registered comparator, then a RawComparator is used that deserializes the byte streams being compared into objects and delegates to the WritableCompar able’s compareTo() method.
This command produces 30 output files, each of which is sorted.
However, there is no easy way to combine the files (by concatenation, for example, in the case of plain-text files) to produce a globally sorted file.
For example, having a partially sorted set of files is fine when you want to do lookups.
Equivalent old API code for the examples in this section can be found on this book’s website.
We can also use the readers directly to get all the records for a given key.
The array of readers that is returned is ordered by partition, so that the reader for a given key may be found using the same partitioner that was used in the MapReduce job:
Although we could use this information to construct a very even set of partitions, the fact that we needed to run a job that used the entire dataset to construct them is not ideal.
It’s possible to get a fairly even set of partitions by sampling the key space.
The idea behind sampling is that you look at a small subset of the keys to approximate the key distribution, which is then used to construct partitions.
Luckily, we don’t have to write the code to do this ourselves, as Hadoop comes with a selection of samplers.
The InputSampler class defines a nested Sampler interface whose implementations return a sample of keys given an InputFormat and Job:
Instead, the writePartition File() static method on InputSampler is used, which creates a sequence file to store the keys that define the partitions:
Samplers run on the client, making it important to limit the number of splits that are downloaded so the sampler runs quickly.
In practice, the time taken to run the sampler is a small fraction of the overall job time.
For example, SplitSampler, which samples only the first n records in a split, is not so good for sorted data,5 because it doesn’t select keys from throughout the split.
On the other hand, IntervalSampler chooses keys at regular intervals through the split and makes a better choice for sorted data.
If none of these suits your application (and remember that the point of sampling is to produce partitions that are approximately equal in size), you can write your own implementation of the Sampler interface.
This choice is normally driven by the number of reducer slots in your cluster (choose a number slightly fewer than the total, to allow for failures)
One problem with choosing a high number is that you may get collisions if you have a small key space.
Secondary Sort The MapReduce framework sorts the records by key before they reach the reducers.
For any particular key, however, the values are not sorted.
The order in which the values appear is not even stable from one run to the next, because they come from different map tasks, which may finish at different times from run to run.
Generally speaking, most MapReduce programs are written so as not to depend on the order in which the.
In some applications, it’s common for some of the input to already be sorted, or at least partially sorted.
For example, the weather dataset is ordered by time, which may introduce certain biases, making the RandomSampler a safer choice.
However, it is possible to impose an order on the values by sorting and grouping the keys in a particular way.
To illustrate the idea, consider the MapReduce program for calculating the maximum temperature for each year.
If we arranged for the values (temperatures) to be sorted in descending order, we wouldn’t have to iterate through them to find the maximum; instead, we could take the first for each year and ignore the rest.
This approach isn’t the most efficient way to solve this particular problem, but it illustrates how secondary sort works in general.) To achieve this, we change our keys to be composite: a combination of year and temperature.
We want the sort order for keys to be by year (ascending) and then by temperature (descending):
The final piece of the puzzle is the setting to control the grouping.
If we group values in the reducer by the year part of the key, we will see all the records for the same year in one reduce group.
And because they are sorted by temperature in descending order, the first is the maximum temperature:
To summarize, there is a recipe here to get the effect of sorting by value: • Make the key a composite of the natural key and the natural value.
The sort comparator should order by the composite key, that is, the natural key.
The partitioner and grouping comparator for the composite key should consider.
In the mapper, we create a key representing the year and temperature, using an IntPair Writable implementation.
The reducer emits the first key, which due to the secondary sorting, is an IntPair for the year and its maximum temperature.
IntPair’s toString() method creates a tab-separated string, so the output is a set of tab-separated year-temperature pairs.
Many applications need to access all the sorted values, not just the first value as we have provided here.
To do this, you need to populate the value fields since in the reducer you can retrieve only the first key.
This necessitates some unavoidable duplication of information between key and value.
We set the partitioner to partition by the first field of the key (the year) using a custom partitioner called FirstPartitioner.
To do a secondary sort in Streaming, we can take advantage of a couple of library classes that Hadoop provides.
Here’s the driver that we can use to do a secondary sort:
Our map function (Example 8-10) emits records with year and temperature fields.
This means that values will be empty, just like in the Java case.
However, we don’t want to partition by the entire key, so we use the KeyFieldBased Partitioner partitioner, which allows us to partition by a part of the key.
Next, we want a comparator that sorts the year field in ascending order and the temperature field in descending order, so that the reduce function can simply return the first record in each group.
The comparison order is defined by a specification that is like the one used for GNU sort.
In the Java version, we had to set the grouping comparator; however, in Streaming, groups are not demarcated in any way, so in the reduce function we have to detect the group boundaries ourselves by looking for when the year changes (Example 8-11)
When we run the Streaming program, we get the same output as the Java version.
Joins MapReduce can perform joins between large datasets, but writing the code to do joins from scratch is fairly involved.
Rather than writing MapReduce programs, you might consider using a higher-level framework such as Pig, Hive, or Cascading, in which join operations are a core part of the implementation.
Let’s briefly consider the problem we are trying to solve.
Let’s say we want to see each station’s history, with the station’s metadata inlined in each output row.
How we implement the join depends on how large the datasets are and how they are partitioned.
If one dataset is large (the weather records) but the other one is small enough to be distributed to each node in the cluster (as the station metadata is), the join can be effected by a MapReduce job that brings the records for each station together (a partial sort on station ID, for example)
The mapper or reducer uses the smaller dataset to look up the station metadata for a station ID, so it can be written out with each record.
If the join is performed by the mapper, it is called a map-side join, whereas if it is performed by the reducer it is called a reduce-side join.
If both datasets are too large for either to be copied to each node in the cluster, we can still join them using MapReduce with a map-side or reduce-side join, depending on how the data is structured.
One common example of this case is a user database and a.
For a popular service, it is not feasible to distribute the user database (or the logs) to all the MapReduce nodes.
Map-Side Joins A map-side join between large inputs works by performing the join before the data reaches the map function.
For this to work, though, the inputs to each map must be partitioned and sorted in a particular way.
Each input dataset must be divided into the same number of partitions, and it must be sorted by the same key (the join key) in each source.
All the records for a particular key must reside in the same partition.
This may sound like a strict requirement (and it is), but it actually fits the description of the output of a MapReduce job.
A map-side join can be used to join the outputs of several jobs that had the same number of reducers, the same keys, and output files that are not splittable (by being smaller than an HDFS block or by virtue of being gzip compressed, for example)
In the context of the weather example, if we ran a partial sort on the stations file by station ID, and another, identical sort on the records, again by station ID and with the same number of reducers, then the two outputs would satisfy the conditions for running a map-side join.
The input sources and join type (inner or outer) for CompositeIn putFormat are configured through a join expression that is written according to a simple grammar.
Reduce-Side Joins A reduce-side join is more general than a map-side join, in that the input datasets don’t have to be structured in any particular way, but it is less efficient because both datasets have to go through the MapReduce shuffle.
The basic idea is that the mapper tags each record with its source and uses the join key as the map output key, so that the records with the same key are brought together in the reducer.
We use several ingredients to make this work in practice: Multiple inputs.
Secondary sort As described, the reducer will see the records from both sources that have the same key, but they are not guaranteed to be in any particular order.
However, to perform the join, it is important to have the data from one source before another.
For the weather data join, the station record must be the first of the values seen for each key, so the reducer can fill in the weather records with the station name and emit them straightaway.
Of course, it would be possible to receive the records in any order if we buffered them in memory, but this should be avoided because the number of records in any group may be very large and exceed the amount of memory available to the reducer.7
The data_join package in the contrib directory implements reduce-side joins by buffering records in memory, so it suffers from this limitation.
The code assumes that every station ID in the weather records has exactly one matching record in the station dataset.
If this were not the case, we would need to generalize the code to put the tag into the value objects, by using another TextPair.
The reduce() method would then be able to tell which entries were station names and detect (and handle) missing or duplicate entries before processing the weather records.
Because objects in the reducer’s values iterator are reused (for efficiency purposes), it is vital that the code makes a copy of the first Text object from the values iterator:
If the copy is not made, the stationName reference will refer to the value just read when it is turned into a string, which is a bug.
Side Data Distribution Side data can be defined as extra read-only data needed by a job to process the main dataset.
The challenge is to make side data available to all the map or reduce tasks (which are spread across the cluster) in a convenient and efficient fashion.
Using the Job Configuration You can set arbitrary key-value pairs in the job configuration using the various setter methods on Configuration (or JobConf in the old MapReduce API)
This is very useful when you need to pass a small piece of metadata to your tasks.
In the task you can retrieve the data from the configuration returned by Context’s getConfiguration() method.
In the old API, it’s a little more involved: override the configure() method in the Mapper or Reducer and use a getter method on the JobConf object passed in to retrieve the data.
It’s very common to store the data in an instance field so it can be used in the map() or reduce() method.) Usually a primitive type is sufficient to encode your metadata, but for arbitrary objects you can either handle the serialization yourself (if you have an existing mechanism for turning objects to strings and back) or use Hadoop’s Stringifier class.
You shouldn’t use this mechanism for transferring more than a few kilobytes of data,because it can put pressure on the memory usage in the Hadoop daemons, particularly in a system running hundreds of jobs.
The job configuration is read by the jobtracker, the tasktracker, and the child JVM, and each time the configuration is read, all of its entries are read into memory, even if they are not used.
User properties are not used by the jobtracker or the tasktracker, so they just waste time and memory.
Distributed Cache Rather than serializing side data in the job configuration, it is preferable to distribute datasets using Hadoop’s distributed cache mechanism.
This provides a service for copying files and archives to the task nodes in time for the tasks to use them when they run.
To save network bandwidth, files are normally copied to any particular node once per job.
Files can be on the local filesystem, on HDFS, or on another Hadoopreadable filesystem (such as S3)
If no scheme is supplied, then the files are assumed to be local.
This is true even when the default filesystem is not the local filesystem.) You can also copy archive files (JAR files, ZIP files, tar files, and gzipped tar files) to your tasks using the -archives option; these are unarchived on the task node.
The -libjars option will add JAR files to the classpath of the mapper and reducer tasks.
This is useful if you haven’t bundled library JAR files in your job JAR file.
Streaming doesn’t use the distributed cache for copying the streaming scripts across the cluster.
You specify a file to be copied using the -file option (note the singular), which should be repeated for each file to be copied.
Furthermore, files specified using the -file option must be file paths only, not URIs, so they must be accessible from the local filesystem of the client launching the Streaming job.
Streaming also accepts the -files and -archives options for copying files into the distributed cache for use by your Streaming scripts.
Let’s see how to use the distributed cache to share a metadata file for station names.
We use the reducer’s setup() method to retrieve the cache file using its original name, relative to the working directory of the task.
You can use the distributed cache for copying files that do not fit in memory.
Because MapFiles are a collection of files with a defined directory structure, you should put them into an archive format (JAR, ZIP, TAR, or gzipped TAR) and add them to the cache using the -archives option.
Here’s a snippet of the output, showing some maximum temperatures for a few weather stations:
However, some applications may need to use more advanced features of the distributed cache, and for this they can use its API directly.
The API is in two parts: methods for putting data into the cache (found in Job) and methods for retrieving data from the cache (found in JobContext).8 Here are the pertinent methods in Job for putting data into the cache:
Recall that there are two types of objects that can be placed in the cache: files and archives.
Files are left intact on the task node, whereas archives are unarchived on the task node.
Add files to the distributed cache to be copied to the task node.
Add archives to the distributed cache to be copied to the task node and unarchived there.
Add files to the distributed cache to be added to the MapReduce task’s classpath.
The files are not unarchived, so this is a useful way to add JAR files to the classpath.
This can be useful when you want to add a directory of files to the classpath, since you can create an archive containing the files.
The URIs referenced in the add() or set() methods must be files in a shared filesystem that exist when the job is run.
The remaining distributed cache API method on Job is createSymlink(), which creates symbolic links for all the files for the current job when they are localized on the task node.
The symbolic link name is set by the fragment identifier of the file’s URI.
There’s an example of using this API in Example 8-8.) If there is no fragment identifier, then no symbolic link is created.
The second part of the distributed cache API is found on JobContext, and it is used from the map or reduce task code when you want to access files from the distributed cache.
If the files from the distributed cache have symbolic links in the task’s working directory, you can access the localized file directly by name, as we did in Example 8-16
In the case of archives, the paths returned are to the directory containing the unarchived files.
To read the files, you can use a Hadoop local FileSystem instance, retrieved using its getLocal() method.
When using the old MapReduce API, we use the static method on DistributedCache instead, as follows:
MapReduce Library Classes Hadoop comes with a library of mappers and reducers for commonly used functions.
For further information on how to use them, please consult their Java documentation.
ChainMapper, ChainReducer Runs a chain of mappers in a single mapper, and runs a reducer followed by a chain of mappers in a single reducer.
Symbolically, M+RM*, where M is a mapper and R is a reducer.) This can substantially reduce the amount of disk I/O incurred compared to running multiple MapReduce jobs.
A mapper and a reducer that can select fields (like the Unix cut command) from the input keys and values and emit them as output keys and values.
Reducers that sum integer values to produce a total for every key.
A mapper (or map runner in the old API) that runs mappers concurrently in separate threads.
TokenCounterMapper A mapper that tokenizes the input value into words (using Java’s StringTokenizer) and emits each word along with a count of one.
RegexMapper A mapper that finds matches of a regular expression in the input value and emits the matches along with a count of one.
This chapter explains how to set up Hadoop to run on a cluster of machines.
Running HDFS and MapReduce on a single machine is great for learning about these systems, but to do useful work they need to run on multiple nodes.
There are a few options when it comes to getting a Hadoop cluster, from building your own, to running on rented hardware or using an offering that provides Hadoop as a service in the cloud.
This chapter and the next give you enough information to set up and operate your own cluster, but even if you are using a Hadoop service in which a lot of the routine maintenance is done for you, these chapters still offer valuable information about how Hadoop works from an operations point of view.
Cluster Specification Hadoop is designed to run on commodity hardware.
That means that you are not tied to expensive, proprietary offerings from a single vendor; rather, you can choose standardized, commonly available hardware from any of a large range of vendors to build your cluster.
When you are operating tens, hundreds, or thousands of machines, cheap components turn out to be a false economy, as the higher failure rate incurs a greater maintenance cost.
On the other hand, large database-class machines are not recommended either, since they don’t score well on the price/performance curve.
And even though you would need fewer of them to build a cluster of comparable performance than one built of mid-range commodity hardware, when one did fail, it would have a bigger impact on the cluster because a larger proportion of the cluster hardware would be unavailable.
Hardware specifications rapidly become obsolete, but for the sake of illustration, a typical choice of machine for running a Hadoop datanode and tasktracker in mid-2010 would have the following specifications: Processor.
Gigabit Ethernet Although the hardware specification for your cluster will assuredly be different, Hadoop is designed to use multiple cores and disks, so it will be able to take full advantage of more powerful hardware.
Why Not Use RAID? HDFS clusters do not benefit from using RAID (Redundant Array of Independent Disks) for datanode storage (although RAID is recommended for the namenode’s disks, to protect against corruption of its metadata)
The redundancy that RAID provides is not needed, since HDFS handles it by replication between nodes.
Furthermore, RAID striping (RAID 0), which is commonly used to increase performance, turns out to be slower than the JBOD (Just a Bunch Of Disks) configuration used by HDFS, which round-robins HDFS blocks between all disks.
This is because RAID 0 read and write operations are limited by the speed of the slowest disk in the RAID array.
In JBOD, disk operations are independent, so the average speed of operations is greater than that of the slowest disk.
Disk performance often shows considerable variation in practice, even for disks of the same model.
Finally, if a disk fails in a JBOD configuration, HDFS can continue to operate without the failed disk, whereas with RAID, failure of a single disk causes the whole array (and hence the node) to become unavailable.
The bulk of Hadoop is written in Java and can therefore run on any platform with a JVM, although there are enough parts that harbor Unix assumptions (the control scripts, for example) to make it unwise to run on a non-Unix platform in production.
In fact, Windows operating systems are not supported production platforms (although they can be used with Cygwin as a development platform; see Appendix A)
In many ways, a better question is this: how fast does my cluster need to grow? You can get a good feel for this by considering storage capacity.
In practice, you wouldn’t buy a new machine each week and add it to the cluster.
In this example, a cluster that holds two years of data needs 100 machines.
For a small cluster (on the order of 10 nodes), it is usually acceptable to run the namenode and the jobtracker on a single master machine (as long as at least one copy of the namenode’s metadata is stored on a remote filesystem)
As the cluster and the number of files stored in HDFS grow, the namenode needs more memory, so the namenode and jobtracker should be moved onto separate machines.
The secondary namenode can be run on the same machine as the namenode, but again for reasons of memory usage (the secondary has the same memory requirements as the primary), it is best to run it on a separate piece of hardware, especially for larger clusters.
Network Topology A common Hadoop cluster architecture consists of a two-level network topology, as illustrated in Figure 9-1
The salient point is that the aggregate bandwidth between nodes on the same rack is much greater than that between nodes on different racks.
The names parameter is a list of IP addresses, and the return value is a list of corresponding network location strings.
The script must accept a variable number of arguments that are the hostnames or IP addresses to be mapped, and it must emit the corresponding network locations to standard output, separated by whitespace.
If no script location is specified, the default behavior is to map all nodes to a single network location, called /default-rack.
The next steps are to get it racked up and install the software needed to run Hadoop.
This chapter describes how to do it from scratch using the Apache Hadoop distribution and provides background information on the things you need to think about when setting up Hadoop.
Alternatively, if you would like to use RPMs or Debian packages for managing your Hadoop installation, then you might want to start with Cloudera’s Distribution, described in Appendix B.
To ease the burden of installing and maintaining the same software on each node, it is normal to use an automated installation method such as Red Hat Linux’s Kickstart or Debian’s Fully Automatic Installation.
These tools allow you to automate the operating system installation by recording the answers to questions that are asked during the installation process (such as the disk partition layout), as well as which packages to install.
Crucially, they also provide hooks to run scripts at the end of the process, which are invaluable for doing the final system tweaks and customization that are not covered by the standard installer.
The following sections describe the customizations that are needed to run Hadoop.
Installing Java Java 6 or later is required to run Hadoop.
The latest stable Sun JDK is the preferred option, although Java distributions from other vendors may work, too.
Creating a Hadoop User It’s good practice to create a dedicated Hadoop user account to separate the Hadoop installation from other services running on the same machine.
For small clusters, some administrators choose to make this user’s home directory an NFS-mounted drive, to aid with SSH key distribution (see the following discussion)
If you use NFS, it is worth considering autofs, which allows you to mount the NFS filesystem on demand when the system accesses it.
Autofs provides some protection against the NFS server failing and allows you to use replicated filesystems for failover.
There are other NFS gotchas to watch out for, such as synchronizing UIDs and GIDs.
For help setting up NFS on Linux, refer to the HOWTO at http://nfs.sourceforge.net/nfs-howto/index.html.
Note that Hadoop is not installed in the hadoop user’s home directory, as that may be an NFS-mounted directory:
We also need to change the owner of the Hadoop files to be the hadoop user and group: % sudo chown -R hadoop:hadoop hadoop-x.y.z.
Some administrators like to install HDFS and MapReduce in separate locations on the same system.
At the time of this writing, only HDFS and MapReduce from the same Hadoop release are compatible with one another; however, in future releases, the compatibility requirements will be loosened.
Note that separate installations of HDFS and MapReduce can still share configuration by using the --config option (when starting daemons) to refer to a common configuration directory.
They can also log to the same directory because the logfiles they produce are named in such a way as to avoid clashes.
Testing the Installation Once you’ve created an installation script, you are ready to test it by installing it on the machines in your cluster.
This will probably take a few iterations as you discover kinks in the install.
When it’s working, you can proceed to configure Hadoop and give it a test run.
For example, there is a script for stopping and starting all the daemons in the cluster.
To work seamlessly, SSH needs to be set up to allow password-less login for the hadoop user from machines in the cluster.
The simplest way to achieve this is to generate a public/private key pair and place it in an NFS location that is shared across the cluster.
First, generate an RSA key pair by typing the following in the hadoop user account:
Even though we want password-less logins, keys without passphrases are not considered good practice (it’s OK to have an empty passphrase when running a local pseudodistributed cluster, as described in Appendix A), so we specify a passphrase when prompted for one.
We use ssh-agent to avoid the need to enter a password for each connection.
If the hadoop user’s home directory is an NFS filesystem, as described earlier, the keys can be shared across the cluster by typing:
If the home directory is not shared using NFS, the public keys will need to be shared by some other means (such as ssh-copy-id)
Test that you can SSH from the master to a worker machine by making sure sshagent is running,3 and then run ssh-add to store your passphrase.
You should be able to ssh to a worker without entering the passphrase again.
Hadoop Configuration There are a handful of files for controlling the configuration of a Hadoop installation; the most important ones are listed in Table 9-1
This section covers MapReduce 1, which employs the jobtracker and tasktracker daemons.
Configuration settings for Hadoop Core, such as I/O settings that are common to HDFS and MapReduce.
Configuration settings for HDFS daemons: the namenode, the secondary namenode, and the datanodes.
Configuration settings for MapReduce daemons: the jobtracker, and the tasktrackers.
These files are all found in the conf directory of the Hadoop distribution.
The configuration directory can be relocated to another part of the filesystem (outside the Hadoop.
See its main page for instructions on how to start ssh-agent.
Hadoop comes with scripts for running commands and starting and stopping daemons across the whole cluster.
To use these scripts (which can be found in the bin directory), you need to tell Hadoop which machines are in the cluster.
There are two files for this purpose, called masters and slaves, each of which contains a list of the machine hostnames or IP addresses, one per line.
The masters file is actually a misleading name, in that it determines which machine or machines should run a secondary namenode.
The slaves file lists the machines that the datanodes and tasktrackers should run on.
Both masters and slaves files reside in the configuration directory, although the slaves file may be placed elsewhere (and given another name) by changing the HADOOP_SLAVES.
Also, these files do not need to be distributed to worker nodes, since they are used only by the control scripts running on the namenode or jobtracker.
You don’t need to specify which machine (or machines) the namenode and jobtracker run on in the masters file, as this is determined by the machine the scripts are run on.
In fact, specifying these in the masters file would cause a secondary namenode to run there, which isn’t always what you want.) For example, the start-dfs.sh script, which starts all the HDFS daemons in the cluster, runs the namenode on the machine that the script is run on.
Starts a namenode on the local machine (the machine that the script is run on)
There is a similar script called start-mapred.sh, which starts all the MapReduce daemons in the cluster.
Note that masters is not used by the MapReduce control scripts.
Also provided are stop-dfs.sh and stop-mapred.sh scripts to stop the daemons started by the corresponding start script.
These scripts start and stop Hadoop daemons using the hadoop-daemon.sh script.
If you use the aforementioned scripts, you shouldn’t call hadoop-daemon.sh directly.
But if you need to control Hadoop daemons from another system or from your own scripts, the hadoop-daemon.sh script is a good integration point.
Likewise, hadoopdaemons.sh (with an “s”) is handy for starting the same daemon on a set of hosts.
Depending on the size of the cluster, there are various configurations for running the master daemons: the namenode, secondary namenode, and jobtracker.
On a small cluster (a few tens of nodes), it is convenient to put them on a single machine; however, as the cluster gets larger, there are good reasons to separate them.
The namenode has high memory requirements, as it holds file and block metadata for the entire namespace in memory.
The secondary namenode, although idle most of the time, has a comparable memory footprint to the primary when it creates a checkpoint.
The secondary namenode keeps a copy of the latest checkpoint of the filesystem metadata that it creates.
Keeping this (stale) backup on a different node from the namenode.
This is discussed further in Chapter 10.) On a busy cluster running lots of MapReduce jobs, the jobtracker uses considerable memory and CPU resources, so it should run on a dedicated node.
Whether the master daemons run on one or more nodes, the following instructions apply:
The masters file should contain the address of the secondary namenode.
When the namenode and jobtracker are on separate nodes, their slaves files need to be kept in sync, since each node in the cluster should run a datanode and a tasktracker.
Environment Settings In this section, we consider how to set the variables in hadoop-env.sh.
In addition, the task tracker launches separate child JVMs to run map and reduce tasks in, so we need to factor these into the total memory footprint of a worker machine.
The tasktracker is said to have two map slots and two reduce slots.
The number of tasks that can be run simultaneously on a tasktracker is related to the number of processors available on the machine.
Because MapReduce jobs are normally I/O-bound, it makes sense to have more tasks than processors to get better utilization.
The amount of oversubscription depends on the CPU utilization of jobs you run, but a good rule of thumb is to have a factor of between one and two more tasks (counting both map and reduce tasks) than processors.
Whether this Java memory allocation will fit into 8 GB of physical memory depends on the other processes that are running on the machine.
If you are running Streaming or Pipes programs, this allocation will probably be inappropriate (and the memory allocated to the child should be dialed down), since it doesn’t allow enough memory for users’ (Streaming or Pipes) processes to run.
The thing to avoid is processes being swapped out, as this leads to severe performance degradation.
The precise memory settings are necessarily very cluster-dependent and can be optimized over time with experience gained from monitoring the memory usage across the cluster.
Hadoop also provides settings to control how much memory is used for MapReduce operations.
You probably also want to run the secondary namenode on a different machine in this case.
There are corresponding environment variables for the other Hadoop daemons, so you can customize their memory allocations, if desired.
It’s a good idea to set the value in hadoop-env.sh, so that it is clearly defined in one place and to ensure that the whole cluster is using the same version of Java.
This can be changed using the HADOOP_LOG_DIR setting in hadoop-env.sh.
It’s a good idea to change this so that logfiles are kept out of the directory that Hadoop is installed in.
Changing this keeps logfiles in one place, even after the installation directory changes due to an upgrade.
A common choice is /var/log/hadoop, set by including the following line in hadoop-env.sh:
The log director will be created if it doesn’t already exist.
If it does not exist, confirm that the Hadoop user has permission to create it.) Each Hadoop daemon running on a machine produces two logfiles.
This file, which ends in .log, should be the first port of call when diagnosing problems because most application log messages are written here.
Old logfiles are never deleted, so you should arrange for them to be periodically deleted or archived, so as to not run out of disk space on the local node.
The second logfile is the combined standard output and standard error log.
This logfile, which ends in .out, usually contains little or no output, since Hadoop uses log4j for logging.
It is rotated only when the daemon is restarted, and only the last five logs are retained.
Logfile names (of both types) are a combination of the name of the user running the daemon, the daemon name, and the machine hostname.
This naming structure makes it possible to archive logs from all machines in the cluster in a single directory, if needed, since the filenames are unique.
The control scripts allow you to run commands on (remote) worker nodes from the master node using SSH.
It can be useful to customize the SSH settings, for various reasons.
For example, you may want to reduce the connection timeout (using the ConnectTimeout option) so the control scripts don’t hang around waiting to see whether a dead node is going to respond.
If the timeout is too low, then busy nodes will be skipped, which is bad.
See the ssh and ssh_config manual pages for more SSH settings.
The Hadoop control scripts can distribute configuration files to all nodes of the cluster using rsync.
In fact, you could use any machine, even one outside the Hadoop cluster, to rsync from.
For larger clusters, tools such as dsh can do the copies in parallel.
Alternatively, a suitable hadoop-env.sh can be created as a part of the automated installation script (such as Kickstart)
When starting a large cluster with rsyncing enabled, the worker nodes start at around the same time and can overwhelm the master node with rsync requests.
When running commands on all nodes of the cluster, the master will sleep for this period between invoking the command on each worker machine in turn.
In this section, we address the ones that you need to define (or at least understand why the default is appropriate) for any real-world working cluster.
These properties are set in the Hadoop site files: core-site.xml, hdfs-site.xml, and mapred-site.xml.
Notice that most properties are marked as final in order to prevent them from being overridden by job configurations.
To run HDFS, you need to designate one machine as a namenode.
If no port is specified, the default of 8020 is used.
The masters file that is used by the control scripts is not used by the HDFS (or MapReduce) daemons to determine hostnames.
In fact, because the masters file is used only by the scripts, you can ignore it if you don’t use them.
The default filesystem is used to resolve relative paths, which are handy to use because they save typing (and avoid hardcoding knowledge of a particular namenode’s address)
Bear in mind, however, that it is possible to specify a different filesystem as the default in the client configuration, for convenience.
For example, if you use both HDFS and S3 filesystems, then you have a choice of specifying either as the default in the client configuration, which allows you to refer to the default with a relative URI and the other with an absolute URI.
There are a few other configuration properties you should set for HDFS: those that set the storage directories for the namenode and for datanodes.
The property dfs.name.dir specifies a list of directories where the namenode stores persistent filesystem metadata (the edit log and the filesystem image)
A copy of each metadata file is stored in each directory for redundancy.
It’s common to configure dfs.name.dir so that the namenode metadata is written to one or two local disks, as well as a remote disk, such as an NFS-mounted directory.
Such a setup guards against failure of a local disk and failure of the entire namenode, since in both cases the files can be recovered and used to start a new namenode.
The secondary namenode takes only periodic checkpoints of the namenode, so it does not provide an up-to-date backup of the namenode.) You should also set the dfs.data.dir property, which specifies a list of directories for a datanode to store its blocks.
Unlike the namenode, which uses multiple directories for redundancy, a datanode round-robins writes between its storage directories, so for performance you should specify a storage directory for each local disk.
Read performance also benefits from having multiple disks for storage, because blocks will be spread across them and concurrent reads for distinct blocks will be correspondingly spread across disks.
For maximum performance, you should mount storage disks with the noatime option.
This setting means that last-accessed time information is not written on file reads, which gives significant performance gains.
Finally, you should configure where the secondary namenode stores its checkpoints of the filesystem.
The URI defines the hostname and port that the namenode’s RPC server runs on.
The list of directories where the namenode stores its persistent metadata.
The namenode stores a copy of the metadata in each directory in the list.
Each block is stored in only one of these directories.
A list of directories where the secondary namenode stores checkpoints.
It stores a copy of the checkpoint in each directory in the list.
To run MapReduce, you need to designate one machine as a jobtracker, which on small clusters may be the same machine as the namenode.
Note that this property is not a URI, but instead a host-port pair, separated by a colon.
During a MapReduce job, intermediate data and working files are written to temporary local files.
Typically, you will use the same disks and partitions (but different directories) for MapReduce temporary data as you.
MapReduce uses a distributed filesystem to share files (such as the job JAR file) with the tasktrackers that run the MapReduce tasks.
If set to the default value of local, the jobtracker is run in-process on demand when you run a MapReduce job (you don’t need to start the jobtracker in this case, and in fact you will get an error if you try to start it in this mode)
A list of directories where MapReduce stores intermediate data for jobs.
This property can be set on a per-job basis, which can be useful for setting JVM properties for debugging, for example.
String -Xmx200m The JVM options used for the child process that runs map tasks.
String -Xmx200m The JVM options used for the child process that runs reduce tasks.
Each server is configured by setting the network address and port number to listen on.
By specifying the network address as 0.0.0.0, Hadoop will bind to all addresses on the machine.
Alternatively, you can specify a single address to bind to.
A port number of 0 instructs the server to start on a free port, but this is generally discouraged because it is incompatible with setting cluster-wide firewall policies.
This is used by the tasktracker’s child JVM to communicate with the tasktracker.
Using any free port is acceptable in this case, as the server only binds to the loopback address.
You should change this setting only if the machine has no loopback address.
In addition to an RPC server, datanodes run a TCP/IP server for block transfers.
There are also settings for controlling which network interfaces the datanodes and tasktrackers report as their IP addresses (for HTTP and RPC servers)
You can set this explicitly to report the address of a particular interface (eth0, for example)
Other Hadoop Properties This section discusses some other properties that you might consider setting.
To aid the addition and removal of nodes in the future, you can specify a file containing a list of authorized machines that may join the cluster as datanodes or tasktrackers.
By default, datanodes will try to use all of the space available in their storage directories.
Hadoop filesystems have a trash facility, in which deleted files are not actually deleted, but rather are moved to a trash folder, where they remain for a minimum period before being permanently deleted by the system.
By default, the trash interval is zero, which disables trash.
Like in many operating systems, Hadoop’s trash facility is a user-level feature, meaning that only files that are deleted using the filesystem shell are put in the trash.
It is possible to use the trash programmatically, however, by constructing a Trash instance, then calling its moveToTrash() method with the Path of the file intended for deletion.
The method returns a value indicating success; a value of false means either that trash is not enabled or that the file is already in the trash.
When trash is enabled, each user has her own trash directory called .Trash in her home directory.
File recovery is simple: you look for the file in a subdirectory of .Trash and move it out of the trash subtree.
You can expunge the trash, which will delete files that have been in the trash longer than their minimum period, using the filesystem shell:
The Trash class exposes an expunge() method that has the same effect.
Particularly in a multiuser MapReduce setting, consider changing the default FIFO job scheduler to one of the more fully featured alternatives.
By default, schedulers wait until 5% of the map tasks in a job have completed before scheduling reduce tasks for the same job.
For large jobs this can cause problems with cluster utilization, since they take up reduce slots while waiting for the map tasks to complete.
On a shared cluster, it shouldn’t be possible for one user’s errant MapReduce program to bring down nodes in the cluster.
This can happen if the map or reduce task has a memory leak, for example, because the machine on which the tasktracker is running will run out of memory and may affect the other running processes.
Marking this property as final on the cluster would prevent it from being changed by users in their jobs, but there are legitimate reasons to allow some jobs to use more memory, so this is not always an acceptable solution.
To prevent cases like these, some way of enforcing a limit on a task’s memory usage is needed.
The second mechanism is Hadoop’s task memory monitoring feature.5 The idea is that an administrator sets a range of allowed virtual memory limits for tasks on the cluster, and users specify the maximum memory requirements for their jobs in the job configuration.
This approach has a couple of advantages over the ulimit approach.
First, it enforces the memory usage of the whole task process tree, including spawned processes.
Second, it enables memory-aware scheduling, where tasks are scheduled on tasktrackers that have enough free memory to run them.
To enable task memory monitoring, you need to set all six of the properties in Table 9-7
The default values are all -1, which means the feature is disabled.
Map tasks that require more than this amount of memory will use more than one map slot.
Reduce tasks that require more than this amount of memory will use more than one reduce slot.
If a map task exceeds this limit, it may be terminated and marked as failed.
If a reduce task exceeds this limit, it may be terminated and marked as failed.
User Account Creation Once you have a Hadoop cluster up and running, you need to give users access to it.
This involves creating a home directory for each user and setting ownership permissions on it:
This is a good time to set space limits on the directory.
The following sets a 1 TB limit on the given user directory:
It has a different set of daemons and configuration options than classic MapReduce (also called MapReduce 1), and in this section we look at these differences and discuss how to run MapReduce on YARN.
Under YARN, you no longer run a jobtracker or tasktrackers.
Instead, there is a single resource manager running on the same machine as the HDFS namenode (for small clusters) or on a dedicated machine, and node managers running on each worker node in the cluster.
The YARN start-yarn.sh script (in the sbin directory) starts the YARN daemons in the cluster.
This script will start a resource manager (on the machine the script is run on) and a node manager on each machine listed in the slaves file.
In the case of MapReduce, the web UI served by the proxy provides information about the current job you are running, similar to the one.
By default, the web app proxy server runs in the same process as the resource manager, but it may be configured to run as a standalone daemon.
The JVM options specified in this way are used to launch the YARN child process that runs map or reduce tasks.
The configuration files in Example 9-4 show some of the important configuration properties for running MapReduce on YARN.
The YARN resource manager address is controlled via yarn.resourceman ager.address, which takes the form of a host-port pair.
It is specified by a comma-separated list of local directory paths, which are used in a round-robin fashion.
Hostname and port 0.0.0.0:8032 The hostname and port that the resource manager’s RPC server runs on.
A list of directories where node managers allow containers to store intermediate data.
A list of auxiliary services run by the node manager.
Virtual memory usage may exceed the allocation by this amount.
Rather than specifying a fixed maximum number of map and reduce slots that may run on a tasktracker node at once, YARN allows applications to request an arbitrary amount of memory (within limits) for a task.
In the YARN model, node managers allocate memory from a pool, so the number of tasks that are running on a particular node depends on the sum of their memory requirements, and not simply on a fixed number of slots.
The slot-based model can lead to cluster underutilization, since the proportion of map slots to reduce slots is fixed as a cluster-wide configuration.
However, the number of map versus reduce slots that are in demand changes over time: at the beginning of a job only map slots are needed, whereas at the end of the job only reduce slots are needed.
On larger clusters with many concurrent jobs, the variation in demand for a particular type of slot may be less pronounced, but there is still wastage.
The default is 8,192 MB.) The next step is to determine how to set memory options for individual jobs.
The latter setting is used by the application master when negotiating for resources in the cluster, and also by the node manager, which runs and monitors the task containers.
Note that the JVM process will have a larger memory footprint than the heap size, and the overhead will depend on such things as the native.
This is used by the client (typically outside the cluster) to communicate with the resource manager.
This is used by the admin client (invoked with yarn rmadmin, typically run outside the cluster) to communicate with the resource manager.
This is used by (in-cluster) application masters to communicate with the resource manager.
This is used by the (in-cluster) node managers to communicate with the resource manager.
This is used by (in-cluster) application masters to communicate with node managers.
This is used by the client (typically outside the cluster) to query job history.
If not set (the default), then the web app proxy server will run in the resource manager process.
This is used for serving map outputs, and is not a user-accessible web UI.
Security Early versions of Hadoop assumed that HDFS and MapReduce clusters would be used by a group of cooperating users within a secure environment.
The measures for restricting access were designed to prevent accidental data loss, rather than to prevent unauthorized access to data.
In security parlance, what was missing was a secure authentication mechanism to assure Hadoop that the user seeking to perform an operation on the cluster is who she claims to be and therefore can be trusted.
For example, a file may be readable only by a certain group of users, so anyone not in that group is not authorized to read it.
It’s common to restrict access to data that contains personally identifiable information (such as an end user’s full name or IP address) to a small set of users (of the cluster) within the organization who are authorized to access such information.
Less sensitive (or anonymized) data may be made available to a larger set of users.
It is convenient to host a mix of datasets with different security levels on the same cluster (not least because it means the datasets with lower security levels can be shared)
However, to meet regulatory requirements for data protection, secure authentication must be in place for shared clusters.
In their design, Hadoop itself does not manage user credentials; instead, it relies on Kerberos, a mature open-source network authentication protocol, to authenticate the user.
Kerberos says that a user is who he says he is; it’s Hadoop’s job to determine whether that user has permission to perform a given action.
There’s a lot to Kerberos, so here we only cover enough to use it in the context of Hadoop, referring readers who want more background to Kerberos: The Definitive Guide by Jason Garman (O’Reilly, 2003)
See Table 1-2 for which recent release series support this feature.
Kerberos and Hadoop At a high level, there are three steps that a client must take to access a service when using Kerberos, each of which involves a message exchange with a server:
The client authenticates itself to the Authentication Server and receives a timestamped Ticket-Granting Ticket (TGT)
The client uses the service ticket to authenticate itself to the server that is providing the service the client is using.
In the case of Hadoop, this might be the namenode or the jobtracker.
The authorization and service request steps are not user-level actions; the client performs these steps on the user’s behalf.
The authentication step, however, is normally carried out explicitly by the user using the kinit command, which will prompt for a password.
However, this doesn’t mean you need to enter your password every time you run a job or access HDFS, since TGTs last for 10 hours by default (and can be renewed for up to a week)
It’s common to automate authentication at operating system login time, thereby providing single sign-on to Hadoop.
In cases where you don’t want to be prompted for a password (for running an unattended MapReduce job, for example), you can create a Kerberos keytab file using the ktutil command.
A keytab is a file that stores passwords and may be supplied to kinit with the -t option.
Let’s look at an example of the process in action.
Services are defined at the protocol level, so there are ones for MapReduce job submission, namenode communication, and so on.
By default, all ACLs are set to *, which means that all users have permission to access each service, but on a real cluster you should lock the ACLs down to only those users and groups that should have access.
The format for an ACL is a comma-separated list of usernames, followed by whitespace, followed by a comma-separated list of group names.
With Kerberos authentication turned on, let’s see what happens when we try to copy a local file to HDFS:
The operation fails because we don’t have a Kerberos ticket.
We can get one by authenticating to the KDC, using kinit:
And we see that the file is successfully written to HDFS.
Notice that even though we carried out two filesystem commands, we only needed to call kinit once, since the Kerberos ticket is valid for 10 hours (use the klist command to see the expiry time of your tickets and kdestroy to invalidate your tickets)
After we get a ticket, everything works just as it normally would.
Delegation Tokens In a distributed system such as HDFS or MapReduce, there are many client-server interactions, each of which must be authenticated.
For example, an HDFS read operation will involve multiple calls to the namenode and calls to one or more datanodes.
Instead of using the three-step Kerberos ticket exchange protocol to authenticate each call, which would present a high load on the KDC on a busy cluster, Hadoop uses delegation tokens to allow later authenticated access without having to contact the KDC.
Delegation tokens are created and used transparently by Hadoop on behalf of users, so there’s no action you need to take as a user beyond using kinit to sign in, but it’s useful to have a basic idea of how they are used.
A delegation token is generated by the server (the namenode in this case) and can be thought of as a shared secret between the client and the server.
On the first RPC call to the namenode, the client has no delegation token, so it uses Kerberos to authenticate, and as a part of the response it gets a delegation token from the namenode.
In subsequent calls, it presents the delegation token, which the namenode can verify (since it generated it using a secret key), and hence the client is authenticated to the server.
When it wants to perform operations on HDFS blocks, the client uses a special kind of delegation token, called a block access token, that the namenode passes to the client in response to a metadata request.
The client uses the block access token to authenticate itself to datanodes.
This is possible only because the namenode shares its secret key used to generate the block access token with datanodes (which it sends in heartbeat messages), so that they can verify block access tokens.
Thus, an HDFS block may be accessed only by a client with a valid block access token from a namenode.
This closes the security hole in unsecured Hadoop where only the block ID was needed to gain access to a block.
Delegation tokens are used by the jobtracker and tasktrackers to access HDFS during the course of the job.
When the job has finished, the delegation tokens are invalidated.
Other Security Enhancements Security has been tightened throughout HDFS and MapReduce to protect against unauthorized access to resources.7 The more notable changes are listed here:
Tasks can be run using the operating system account for the user who submitted the job, rather than the user running the tasktracker.
This means that the operating system is used to isolate running tasks, so they can’t send signals to each other (to.
At the time of this writing, other projects, such as HBase and Hive, had not been integrated with this security model.
Users can view and modify only their own jobs, not others.
The shuffle is secure, preventing a malicious user from requesting another user’s map outputs.
However, the shuffle is not encrypted, so it is subject to malicious sniffing.
When appropriately configured, it’s no longer possible for a malicious user to run a rogue secondary namenode, datanode, or tasktracker that can join the cluster and potentially compromise data stored in the cluster.
This is enforced by requiring daemons to authenticate with the master node they are connecting to.
To enable this feature, you first need to configure Hadoop to use a keytab previously generated with the ktutil command.
A datanode may be run on a privileged port (one lower than 1024), so a client may be reasonably sure that it was started securely.
A task may communicate only with its parent tasktracker, thus preventing an attacker from obtaining MapReduce data from another user’s job.
One area that hasn’t yet been addressed in the security work is encryption: neither RPC nor block transfers are encrypted.
These features are planned for a future release, and in fact, encrypting the data stored in HDFS could be carried out in existing versions of Hadoop by the application itself (by writing an encryption CompressionCodec, for example)
You should ensure that this binary is owned by root and has the setuid bit set (with chmod +s)
Benchmarking a Hadoop Cluster Is the cluster set up correctly? The best way to answer this question is empirically: run some jobs and confirm that you get the expected results.
Benchmarks make good tests because you also get numbers that you can compare with other clusters as a sanity check on whether your new cluster is performing roughly as expected.
And you can tune a cluster using benchmark results to squeeze the best performance out of it.
To get the best results, you should run benchmarks on a cluster that is not being used by others.
In practice, this is just before it is put into service and users start relying on it.
Once users have scheduled periodic jobs on a cluster, it is generally impossible to find a time when the cluster is not being used (unless you arrange downtime with users), so you should run benchmarks to your satisfaction before this happens.
Experience has shown that most hardware failures for new systems are hard drive failures.
Hadoop Benchmarks Hadoop comes with several benchmarks that you can run very easily with minimal setup cost.
Benchmarks are packaged in the test JAR file, and you can get a list of them, with descriptions, by invoking the JAR file with no arguments:
Most of the benchmarks show usage instructions when invoked with no arguments.
It does this by using a MapReduce job as a convenient way to read or write files in parallel.
Each file is read or written in a separate map task, and the output of the map is used for collecting statistics related to the file just processed.
The statistics are accumulated in the reduce to produce a summary.
At the end of the run, the results are written to the console and also recorded in a local file (which is appended to, so you can rerun the benchmark and not lose old results):
Note that these files must already exist (having been written by TestDFSIO -write):
When you’ve finished benchmarking, you can delete all the generated files from HDFS using the -clean argument:
Hadoop comes with a MapReduce program that does a partial sort of its input.
It is very useful for benchmarking the whole MapReduce system, as the full input dataset is transferred through the shuffle.
The three steps are: generate some random data, perform the sort, then validate the results.
There are also settings for the size ranges of the keys and values; see RandomWriter for details.
Here’s how to invoke RandomWriter (found in the example JAR file, not the test one) to write its output to a directory called random-data:
The overall execution time of the sort is the metric we are interested in, but it’s instructive to watch the job’s progress via the web UI (http://jobtracker-host:50030/), where you can get a feel for how long each phase of the job takes.
As a final sanity check, we validate that the data in sorted-data is, in fact, correctly sorted:
This command runs the SortValidator program, which performs a series of checks on the unsorted and sorted data to check whether the sort is accurate.
It reports the outcome to the console at the end of its run:
There are many more Hadoop benchmarks, but the following are widely used: • MRBench (invoked with mrbench) runs a small job a number of times.
NNBench (invoked with nnbench) is useful for load-testing namenode hardware.
Gridmix is a suite of benchmarks designed to model a realistic cluster workload by.
User Jobs For tuning, it is best to include a few jobs that are representative of the jobs that your users run, so your cluster is tuned for these and not just for the standard benchmarks.
If this is your first Hadoop cluster and you don’t have any user jobs yet, then Gridmix is a good substitute.
When running your own jobs as benchmarks, you should select a dataset for your user jobs and use it each time you run the benchmarks to allow comparisons between runs.
When you set up a new cluster or upgrade a cluster, you will be able to use the same dataset to compare the performance with previous runs.
Hadoop in the Cloud Although many organizations choose to run Hadoop in-house, it is also popular to run Hadoop in the cloud on rented hardware or as a service.
For instance, Cloudera offers tools for running Hadoop in a public or private cloud (see Appendix B), and Amazon has a Hadoop cloud service called Elastic MapReduce.
In this section, we look at running Hadoop on Amazon EC2, which is a great way to try out your own Hadoop cluster on a low-commitment trial basis.
Apache Whirr Amazon Elastic Compute Cloud (EC2) is a computing service that allows customers to rent computers (instances) on which they can run their own applications.
A customer can launch and terminate instances on demand, paying by the hour for active instances.
Running Hadoop on EC2 is especially appropriate for certain workflows.
First, install Whirr by downloading a recent release tarball and unpacking it on the machine you want to launch the cluster from, as follows:
Whirr uses SSH to communicate with machines running in the cloud, so it’s a good idea to generate an SSH keypair for exclusive use with Whirr.
Here we create an RSA keypair with an empty passphrase, stored in a file called id_rsa_whirr in the current user’s .ssh directory:
There are also bash scripts in the src/contrib/ec2 subdirectory of the Hadoop distribution, but these are deprecated in favor of Whirr.
Do not confuse the Whirr SSH keypair with any certificates, private keys, or SSH keypairs associated with your Amazon Web Services account.
Whirr is designed to work with many cloud providers, and it must have access to both the public and private SSH key of a passphraseless keypair that’s read from the local filesystem.
In practice, it’s simplest to generate a new keypair for Whirr, as we did here.
We can export them as environment variables as follows, although you can alternatively specify them on the command line or in the configuration file for the service.
Whirr comes with several recipes files for launching common service configurations, and here we use the recipe to run Hadoop on EC2:
The launch-cluster command provisions the cloud instances and starts the services running on them before returning control to the user.
Before we start using the cluster, let’s look at Whirr configuration in more detail.
Configuration parameters are passed to Whirr commands as bundles in a configuration file specified by the --config option, or individually using command-line arguments, like the --private-key-file argument we used to indicate the location of the SSH private key file.
The recipe file is actually just a Java properties file that defines a number of Whirr properties.
The name must be unique within the cloud account that the cluster is running in.
An instance template specifies a cardinality and a set of roles that run on each instance of.
Thus, we have one instance running in both the hadoop-namenode role and the hadoop-jobtracker role.
There are also 5 instances running a hadoop-datanode and a hadoop-tasktracker.
There are plenty of other services you can run in addition to Hadoop, and you can discover them by running bin/whirr with no arguments.
The whirr.provider property defines the cloud provider, here EC2 (other supported providers are listed in the Whirr documentation)
The final three parameters offer control over the cluster hardware (instance capabilities, such as memory, disk, CPU, and network speed), the machine image (operating system), and geographic location (data center)
Properties in the file are prefixed with whirr., but if they are passed as arguments on the command line, the prefix is dropped.
So, for example, you could set the cluster name by adding --cluster-name hadoop on the command line, and this would take precedence over any value set in the properties file.
Conversely, we could have set the private key file in the properties file by adding a line like:
There are also properties for specifying the version of Hadoop to run on the cluster and for setting Hadoop configuration properties across the cluster (details are in the recipe file)
To use the cluster, network traffic from the client needs to be proxied through the master node of the cluster using an SSH tunnel, which we can set up using the following command:
You should keep the proxy running as long as the cluster is running.
When you have finished with the cluster, stop the proxy with Ctrl-C.
You can run MapReduce jobs either from within the cluster or from an external machine.
Here we show how to run a job from the machine we launched the cluster on.
Note that the Hadoop version that has been installed locally must be the same as the one running on the cluster.
When we launched the cluster, Hadoop site configuration files were created in the directory ~/.whirr/hadoop.
We can use this to connect to the cluster by setting the HADOOP_CONF_DIR environment variable as follows:
The cluster’s filesystem is empty, so before we run a job, we need to populate it with data.
The easiest way to achieve that is to log into the master node (its address is printed to the console during launch) with:
You can track the progress of the job using the jobtracker’s web UI, found at http:// master_host:50030/
To access web pages running on worker nodes, you need to set up a proxy auto-config (PAC) file in your browser.
See the Whirr documentation for details on how to do this.
This will terminate all the running instances in the cluster and delete all the data stored in the cluster.
The previous chapter was devoted to setting up a Hadoop cluster.
In this chapter, we look at the procedures to keep a cluster running smoothly.
Knowing which files are which can help you diagnose problems or spot that something is awry.
Recall from Chapter 9 that the dfs.name.dir property is a list of directories, with the same contents mirrored in each directory.
This mechanism provides resilience, particularly if one of the directories is an NFS mount, as is recommended.
The VERSION file is a Java properties file that contains information about the version of HDFS that is running.
When a filesystem client performs a write operation (such as creating or moving a file), it is first recorded in the edit log.
The namenode also has an in-memory representation of the filesystem metadata, which it updates after the edit log has been modified.
The edit log is flushed and synced after every write before a success code is returned to the client.
For namenodes that write to multiple directories, the write must be flushed and synced to every copy before returning successfully.
This ensures that no operation is lost due to machine failure.
The fsimage file is a persistent checkpoint of the filesystem metadata.
However, it is not updated for every filesystem write operation, because writing out the fsimage file, which can grow to be gigabytes in size, would be very slow.
This does not compromise resilience, however, because if the namenode fails, then the latest state of its metadata can be reconstructed by loading the fsimage from disk into memory, and then applying each of the operations in the edit log.
The fsimage file contains a serialized form of all the directory and file inodes in the filesystem.
Each inode is an internal representation of a file or directory’s metadata and contains such information as the file’s replication level, modification and access times, access permissions, block size, and the blocks a file is made up of.
For directories, the modification time, permissions, and quota metadata is stored.
The fsimage file does not record the datanodes on which the blocks are stored.
Instead, the namenode keeps this mapping in memory, which it constructs by asking the datanodes for their block lists when they join the cluster and periodically afterward to ensure the namenode’s block mapping is up-to-date.
Though this state of affairs would have no impact on the system while the namenode is running, if the namenode were restarted, it would take a long time to apply each of the operations in its (very long) edit log.
During this time, the filesystem would be offline, which is generally undesirable.
The secondary asks the primary to roll its edits file, so new edits go to a new file.
The secondary retrieves fsimage and edits from the primary (using HTTP GET)
The secondary sends the new fsimage back to the primary (using HTTP POST)
It also updates the fstime file to record the time that the checkpoint was taken.
At the end of the process, the primary has an up-to-date fsimage file and a shorter edits file (it is not necessarily empty, as it may have received some edits while the checkpoint was being taken)
It is possible for an administrator to run this process manually while the namenode is in safe mode, using the hadoop dfsadmin -saveNamespace command.
From Hadoop version 0.22.0 onward you can start a namenode with the -checkpoint option so that it runs the checkpointing process against another (primary) namenode.
This is functionally equivalent to running a secondary namenode, but at the time of this writing offers no advantages over the secondary namenode (and indeed the secondary namenode is the most tried and tested option)
This procedure makes it clear why the secondary has similar memory requirements to the primary (since it loads the fsimage into memory), which is the reason that the secondary needs a dedicated machine on large clusters.
The schedule for checkpointing is controlled by two configuration parameters.
This can be used as a source for making (stale) backups of the namenode’s metadata:
The layout of this directory and of the secondary’s current directory is identical to the namenode’s.
This is by design, since in the event of total namenode failure (when there are no recoverable backups, even from NFS), it allows recovery from a secondary namenode.
This can be achieved either by copying the relevant storage directory to a new namenode or, if the secondary is taking over as the new primary namenode, by using the -importCheckpoint option when starting the namenode daemon.
Unlike namenodes, datanodes do not need to be explicitly formatted, because they create their storage directories automatically on startup.
The namespaceID, cTime, and layoutVersion are all the same as the values in the namenode (in fact, the namespaceID is retrieved from the namenode when the datanode first connects)
The storageID is unique to the datanode (it is the same across all storage directories) and is used by the namenode to uniquely identify the datanode.
The storageType identifies this directory as a datanode storage directory.
The other files in the datanode’s current storage directory are the files with the blk_ prefix.
There are two types: the HDFS blocks themselves (which just consist of the file’s raw bytes) and the metadata for a block (with a .meta suffix)
A block file just consists of the raw bytes of a portion of the file being stored; the metadata file is made up of a header with version and type information, followed by a series of checksums for sections of the block.
When the number of blocks in a directory grows to a certain size, the datanode creates a new subdirectory in which to place new blocks and their accompanying metadata.
The effect is to have a tree with high fan-out, so even for systems with a very large number of blocks, the directories will be only a few levels deep.
By taking this measure, the datanode ensures that there is a manageable number of files per directory, which avoids the problems that most operating systems encounter when there are a large number of files (tens or hundreds of thousands) in a single directory.
If the configuration property dfs.data.dir specifies multiple directories on different drives, blocks are written to each in a round-robin fashion.
Note that blocks are not replicated on each drive on a single datanode; instead, block replication is across distinct datanodes.
Safe Mode When the namenode starts, the first thing it does is load its image file (fsimage) into memory and apply the edits from the edit log (edits)
Once it has reconstructed a consistent in-memory image of the filesystem metadata, it creates a new fsimage file (effectively doing the checkpoint itself, without recourse to the secondary namenode) and an empty edit log.
Only at this point does the namenode start listening for RPC and HTTP requests.
However, the namenode is running in safe mode, which means that it offers only a read-only view of the filesystem to clients.
Strictly speaking, in safe mode, only filesystem operations that access the filesystem metadata (such as producing a directory listing) are guaranteed to work.
Reading a file will work only when the blocks are available on the current set of datanodes in the cluster, and file modifications (writes, deletes, or renames) will always fail.
Recall that the locations of blocks in the system are not persisted by the namenode; this information resides with the datanodes, in the form of a list of the blocks it is storing.
During normal operation of the system, the namenode has a map of block locations stored in memory.
Safe mode is needed to give the datanodes time to check in to the namenode with their block lists, so the namenode can be informed of enough block locations to run the filesystem effectively.
If the namenode didn’t wait for enough datanodes to check in, then it would start the process of replicating blocks to new datanodes, which would be unnecessary in most cases (because it only needed to wait for the extra datanodes to check in) and would put a great strain on the cluster’s resources.
Indeed, while in safe mode, the namenode does not issue any block-replication or deletion instructions to datanodes.
Safe mode is exited when the minimal replication condition is reached, plus an extension time of 30 seconds.
When you are starting a newly formatted HDFS cluster, the namenode does not go into safe mode, since there are no blocks in the system.
Setting this value to 0 or less forces the namenode not to start in safe mode.
Setting this value to more than 1 means the namenode never exits safe mode.
For small clusters (tens of nodes), it can be set to 0
To see whether the namenode is in safe mode, you can use the dfsadmin command: % hadoop dfsadmin -safemode get Safe mode is ON.
The front page of the HDFS web UI provides another indication of whether the namenode is in safe mode.
Sometimes you want to wait for the namenode to exit safe mode before carrying out a command, particularly in scripts.
An administrator has the ability to make the namenode enter or leave safe mode at any time.
It is sometimes necessary to do this when carrying out maintenance on the cluster or after upgrading a cluster to confirm that data is still readable.
You can use this command when the namenode is still in safe mode while starting up to ensure that it never leaves safe mode.
You can make the namenode leave safe mode by using:
Audit Logging HDFS can log all filesystem access requests, a feature that some organizations require for auditing purposes.
Audit logging is implemented using log4j logging at the INFO level.
You can enable audit logging by replacing WARN with INFO, and the result will be a log line written to the namenode’s log for every HDFS event.
Here’s an example for a list status request on /user/tom:
It is a good idea to configure log4j so that the audit log is written to a separate file and isn’t mixed up with the namenode’s other log entries.
An example of how to do this can be found on the Hadoop wiki at http://wiki.apache.org/hadoop/HowToConfigure.
The dfsadmin tool is a multipurpose tool for finding information about the state of HDFS, as well as for performing administration operations on HDFS.
It is invoked as hadoop dfsadmin and requires superuser privileges.
Some of the available commands to dfsadmin are described in Table 10-2
Used after an upgrade has been applied and the cluster is running successfully on the new version.
Directory quotas set a limit on the number of names (files or directories) in the directory tree.
Directory quotas are useful for preventing users from creating large numbers of small files, a measure that helps preserve the namenode’s memory (recall that accounting information for every file, directory, and block in the filesystem is stored in memory)
Space quotas set a limit on the size of files that may be stored in a directory tree.
They are useful for giving users a limited amount of storage.
Hadoop provides an fsck utility for checking the health of files in HDFS.
The tool looks for blocks that are missing from all datanodes, as well as under- or over-replicated blocks.
Here is an example of checking the whole filesystem for a small cluster:
To check a file, fsck retrieves the metadata for the file’s blocks and looks for problems or inconsistencies.
Note that fsck retrieves all of its information from the namenode; it does not communicate with any datanodes to actually retrieve any block data.
Most of the output from fsck is self-explanatory, but here are some of the conditions it looks for: Over-replicated blocks.
These are blocks that exceed their target replication for the file they belong to.
Normally, over-replication is not a problem, and HDFS will automatically delete excess replicas.
Under-replicated blocks These are blocks that do not meet their target replication for the file they belong to.
You can get information about the blocks being replicated (or waiting to be replicated) using hadoop dfsadmin -metasave.
For example, for a replication level of three in a multirack cluster, if all three replicas of a block are on the same rack, then the block is misreplicated because the replicas should be spread across at least two racks for resilience.
Corrupt blocks These are blocks whose replicas are all corrupt.
Blocks with at least one noncorrupt replica are not reported as corrupt; the namenode will replicate the noncorrupt replica until the target replication is met.
Missing replicas These are blocks with no replicas anywhere in the cluster.
Corrupt or missing blocks are the biggest cause for concern, as it means data has been lost.
By default, fsck leaves files with corrupt or missing blocks, but you can tell it to perform one of the following actions on them:
Move the affected files to the /lost+found directory in HDFS, using the -move option.
Files are broken into chains of contiguous blocks to aid any salvaging efforts you may attempt.
The fsck tool provides an easy way to find out which blocks are in any particular file.
The -files option shows the line with the filename, size, number of blocks, and its health (whether there are any missing blocks)
The -blocks option shows information about each block in the file, one line per block.
The -racks option displays the rack location and the datanode addresses for each block.
Running hadoop fsck without any arguments displays full usage instructions.
Every datanode runs a block scanner, which periodically verifies all the blocks stored on the datanode.
This allows bad blocks to be detected and fixed before they are read by clients.
The DataBlockScanner maintains a list of blocks to verify and scans them one by one for checksum errors.
The scanner employs a throttling mechanism to preserve disk bandwidth on the datanode.
Corrupt blocks are reported to the namenode to be fixed.
You can get a block verification report for a datanode by visiting the datanode’s web interface at http://datanode:50075/blockScannerReport.
Here’s an example of a report, which should be self-explanatory:
Here is a snippet of the block list (lines are split to fit the page):
The first column is the block ID, followed by some key-value pairs.
The status can be one of failed or ok according to whether the last scan of the block detected a checksum error.
The type of scan is local if it was performed by the background thread, remote if it was performed by a client or a remote datanode, or none if a scan of this block has yet to be made.
Over time, the distribution of blocks across datanodes can become unbalanced.
An unbalanced cluster can affect locality for MapReduce, and it puts a greater strain on the highly utilized datanodes, so it’s best avoided.
It moves blocks until the cluster is deemed to be balanced, which means that the utilization of every datanode (ratio of used space on the node to total capacity of the node) differs from the utilization of the.
The -threshold argument specifies the threshold percentage that defines what it means for the cluster to be balanced.
The flag is optional, in which case the threshold is 10%
At any one time, only one balancer may be running on the cluster.
The balancer runs until the cluster is balanced; it cannot move any more blocks, or it loses contact with the namenode.
It produces a logfile in the standard log directory, where it writes a line for every iteration of redistribution that it carries out.
Here is the output from a short run on a small cluster:
The balancer is designed to run in the background without unduly taxing the cluster or interfering with other clients using the cluster.
It limits the bandwidth that it uses to copy a block from one node to another.
In this section, we look at the monitoring facilities in Hadoop and how they can hook into external monitoring systems.
The purpose of monitoring is to detect when the cluster is not providing the expected level of service.
The master daemons are the most important to monitor: the namenodes (primary and secondary) and the jobtracker.
Failure of datanodes and tasktrackers is to be expected, particularly on larger clusters, so you should provide extra capacity so that the cluster can tolerate having a small percentage of dead nodes at any time.
In addition to the facilities described next, some administrators run test jobs on a periodic basis as a test of the cluster’s health.
Though it is not covered here, there is a lot of work going on to add more monitoring capabilities to Hadoop.
For example, Chukwa, a data collection and monitoring system built on HDFS and MapReduce, excels at mining log data for finding large-scale trends.
Logging All Hadoop daemons produce logfiles that can be very useful for finding out what is happening in the system.
When debugging a problem, it is very convenient to be able to change the log level temporarily for a particular component in the system.
Hadoop daemons have a web page for changing the log level for any log4j log name, which can be found at /logLevel in the daemon’s web UI.
By convention, log names in Hadoop correspond to the classname doing the logging, although there are exceptions to this rule, so you should consult the source code to find log names.
The same thing can be achieved from the command line as follows:
Log levels changed in this way are reset when the daemon restarts, which is usually what you want.
However, to make a persistent change to a log level, simply change the log4j.properties file in the configuration directory.
Hadoop daemons expose a web page (/stacks in the web UI) that produces a thread dump for all running threads in the daemon’s JVM.
For example, you can get a thread dump for a jobtracker from http://jobtracker-host:50030/stacks.
Metrics The HDFS and MapReduce daemons collect information about events and measurements that are collectively known as metrics.
For example, datanodes collect the following metrics (and many more): the number of bytes written, the number of blocks replicated, and the number of read requests from clients (both local and remote)
They have different audiences, too: broadly speaking, metrics are for administrators, and counters are for MapReduce users.
The way they are collected and aggregated is also different.
Counters are a MapReduce feature, and the MapReduce system ensures that counter values are propagated from the tasktrackers where they are produced, back to the jobtracker, and finally back to the client running the MapReduce job.
The collection mechanism for metrics is decoupled from the component that receives the updates, and there are various pluggable outputs, including local files, Ganglia, and JMX.
The daemon collecting the metrics performs aggregation on them before they are sent to the output.
This is the contents of the default configuration file (minus the comments):
Each line in this file configures a different context and specifies the class that handles the metrics for that context.
The class must be an implementation of the MetricsCon text interface; and, as the name suggests, the NullContext class neither publishes nor updates metrics.2 The other implementations of MetricsContext are covered in the following sections.
You can view raw metrics gathered by a particular Hadoop daemon by connecting to its /metrics web page.
For example, you can view jobtracker metrics in plain text at http://jobtracker-host:50030/metrics.
To retrieve metrics in JSON format, you would use http://jobtracker-host:50030/metrics?format=json.
It exposes two configuration properties: fileName, which specifies the absolute name of the file to write to, and period, for the.
Both properties are optional; if not set, the metrics will be written to standard output every five seconds.
Configuration properties apply to a context name and are specified by appending the property name to the context name (separated by a dot)
For example, to dump the “jvm” context to a file, we alter its configuration to be the following:
Here are two lines of output from the logfile, split over several lines to fit the page:
FileContext can be useful on a local system for debugging purposes, but is unsuitable on a larger cluster because the output files are spread across the cluster, which makes analyzing them difficult.
Ganglia (http://ganglia.info/) is an open source distributed monitoring system for very large clusters.
It is designed to impose very low resource overheads on each node in the cluster.
Ganglia itself collects metrics, such as CPU and memory usage, and by using GangliaContext, you can inject Hadoop metrics into Ganglia.
GangliaContext has one required property, servers, which takes a space- and/or comma-separated list of Ganglia server host-port pairs.
Further details on configuring this context can be found on the Hadoop wiki.
For a flavor of the kind of information you can get out of Ganglia, see Figure 10-2, which shows how the number of tasks in the jobtracker’s queue varies over time.
Both FileContext and a GangliaContext push metrics to an external system.
Like NullContext, it doesn’t publish any metrics, but in addition it runs a timer that periodically updates the metrics stored in memory.
This ensures that the metrics are up-to-date when they are fetched by another system.
If you were using GangliaContext, for example, then it would ensure the metrics are updated, so you would be able to use JMX in addition with no further configuration of the metrics system.
CompositeContext allows you to output the same set of metrics to multiple contexts, such as a FileContext and a GangliaContext.
The configuration is slightly tricky and is best shown by an example:
The arity property is used to specify the number of subcontexts; in this case, there are two.
Hadoop includes several managed beans (MBeans), which expose Hadoop metrics to JMX-aware applications.
Ganglia plot of the number of tasks in the jobtracker queue.
FSDatasetMBean Datanode Datanode storage metrics, such as capacity and free storage space.
The JDK comes with a tool called JConsole for viewing MBeans in a running JVM.
It’s useful for browsing Hadoop metrics, as demonstrated in Figure 10-3
JConsole view of a locally running namenode, showing metrics for the filesystem state.
Although you can see Hadoop metrics via JMX using the default metrics configuration, they will not be updated unless you change the MetricsContext implementation to something other than NullContext.
Many third-party monitoring and alerting systems (such as Nagios or Hyperic) can query MBeans, making JMX the natural way to monitor your Hadoop cluster from an existing monitoring system.
You will need to enable remote access to JMX, however, and choose a level of security that is appropriate for your cluster.
See the official Java documentation3 for an in-depth guide on configuring these options.
With this configuration, we can use JConsole to browse MBeans on a remote namenode.
Alternatively, we can use one of the many JMX tools to retrieve MBean attribute values.
Here is an example of using the “jmxquery” command-line tool (and Nagios plug-in, available from http://code.google.com/p/jmxquery/) to retrieve the number of under-replicated blocks:
The -w and -c options specify warning and critical levels for the value.
The appropriate values for these are normally determined after operating a cluster for a while.
It’s common to use Ganglia in conjunction with an alerting system such as Nagios for monitoring a Hadoop cluster.
Ganglia is good for efficiently collecting a large number of metrics and graphing them, whereas Nagios and similar systems are good at sending alerts when a critical threshold is reached in any of a smaller set of metrics.
If the namenode’s persistent metadata is lost or damaged, the entire filesystem is rendered unusable, so it is critical that backups are made of these files.
You should keep multiple copies of different ages (one hour, one day, one week, and one month, say) to protect against corruption, either in the copies themselves or in the live files running on the namenode.
The script should additionally test the integrity of the copy.
This can be done by starting a local namenode daemon and verifying that it has successfully read the fsimage and edits files into memory (by scanning the namenode log for the appropriate success message, for example).4
Although HDFS is designed to store data reliably, data loss can occur, just like in any storage system, and thus a backup strategy is essential.
With the large data volumes that Hadoop can store, deciding what data to back up and where to store it is a challenge.
The highest priority is the data that cannot be regenerated and that is critical to the business; however, data that is either straightforward to regenerate or essentially disposable because it is of limited business value is the lowest priority, and you may choose not to make backups of this low-priority data.
Do not make the mistake of thinking that HDFS replication is a substitute for making backups.
Bugs in HDFS can cause replicas to be lost, and so can hardware failures.
Although Hadoop is expressly designed so that hardware failure is very unlikely to result in data loss, the possibility can never be completely ruled out, particularly when combined with software bugs or human error.
When it comes to backups, think of HDFS in the same way as you would RAID.
Although the data will survive the loss of an individual RAID disk, it may not if the RAID controller fails or is buggy (perhaps overwriting some data), or the entire array is damaged.
Hadoop 2.0 comes with an Offline Image Viewer and Offline Edits Viewer, which can be used to check the integrity of the fsimage and edits files.
Note that both viewers support older formats of these files, so you can use them to diagnose problems in these files generated by previous releases of Hadoop.
Type hdfs oiv and hdfs oev to invoke these tools.
It’s common to have a policy for user directories in HDFS.
For example, they may have space quotas and be backed up nightly.
Whatever the policy, make sure your users know what it is, so they know what to expect.
The distcp tool is ideal for making backups to other HDFS clusters (preferably running on a different version of the software, to guard against loss due to bugs in HDFS) or other Hadoop filesystems (such as S3 or KFS) because it can copy files in parallel.
It is advisable to run HDFS’s fsck tool regularly (for example, daily) on the whole filesystem to proactively look for missing or corrupt blocks.
Commissioning and Decommissioning Nodes As an administrator of a Hadoop cluster, you will need to add or remove nodes from time to time.
For example, to grow the storage available to a cluster, you commission new nodes.
Conversely, sometimes you may wish to shrink a cluster, and to do so, you decommission nodes.
Sometimes it is necessary to decommission a node if it is misbehaving, perhaps because it is failing more often than it should or its performance is noticeably slow.
Nodes normally run both a datanode and a tasktracker, and both are typically commissioned or decommissioned in tandem.
Although commissioning a new node can be as simple as configuring the hdfssite.xml file to point to the namenode, configuring the mapred-site.xml file to point to the jobtracker, and starting the datanode and jobtracker daemons, it is generally best to have a list of authorized nodes.
It is a potential security risk to allow any machine to connect to the namenode and act as a datanode, because the machine may gain access to data that it is not authorized to see.
Furthermore, because such a machine is not a real datanode, it is not under your control and may stop at any time, causing potential data loss.
Datanodes that are permitted to connect to the namenode are specified in a file whose name is specified by the dfs.hosts property.
The file resides on the namenode’s local filesystem, and it contains a line for each datanode, specified by network address (as reported by the datanode; you can see what this is by looking at the namenode’s web UI)
If you need to specify multiple network addresses for a datanode, put them on one line, separated by whitespace.
Similarly, tasktrackers that may connect to the jobtracker are specified in a file whose name is specified by the mapred.hosts property.
In most cases, there is one shared file, referred to as the include file, that both dfs.hosts and mapred.hosts refer to, since nodes in the cluster run both datanode and tasktracker daemons.
The file (or files) specified by the dfs.hosts and mapred.hosts properties is different from the slaves file.
The former is used by the namenode and jobtracker to determine which worker nodes may connect.
The slaves file is used by the Hadoop control scripts to perform cluster-wide operations, such as cluster restarts.
Add the network addresses of the new nodes to the include file.
Update the jobtracker with the new set of permitted tasktrackers using: % hadoop mradmin -refreshNodes.
Update the slaves file with the new nodes, so that they are included in future operations performed by the Hadoop control scripts.
Check that the new datanodes and tasktrackers appear in the web UI.
Although HDFS is designed to tolerate datanode failures, this does not mean you can just terminate datanodes en masse with no ill effect.
With a replication level of three, for example, the chances are very high that you will lose data by simultaneously shutting down three datanodes if they are on different racks.
The way to decommission datanodes is to inform the namenode of the nodes that you wish to take out of circulation, so that it can replicate the blocks to other datanodes before the datanodes are shut down.
If you shut down a tasktracker that is running tasks, the jobtracker will notice the failure and reschedule the tasks on other tasktrackers.
It is often the case that these properties refer to the same file.
The exclude file lists the nodes that are not permitted to connect to the cluster.
The rules for whether a tasktracker may connect to the jobtracker are simple: a tasktracker may connect only if it appears in the include file and does not appear in the exclude file.
An unspecified or empty include file is taken to mean that all nodes are in the include file.
If a datanode appears in both the include and the exclude file, then it may connect, but only to be decommissioned.
As for tasktrackers, an unspecified or empty include file means all nodes are included.
Node appears in include file Node appears in exclude file Interpretation.
Add the network addresses of the nodes to be decommissioned to the exclude file.
Update the jobtracker with the new set of permitted tasktrackers using: % hadoop mradmin -refreshNodes.
They will start copying their blocks to other datanodes in the cluster.
When all the datanodes report their state as “Decommissioned,” all the blocks have been replicated.
Remove the nodes from the include file, and run: % hadoop dfsadmin -refreshNodes.
Upgrades Upgrading an HDFS and MapReduce cluster requires careful planning.
If the layout version of the filesystem has changed, then the upgrade will automatically migrate the filesystem data and metadata to a format that is compatible with the new version.
Part of the planning process should include a trial run on a small test cluster with a copy of data that you can afford to lose.
A trial run will allow you to familiarize yourself with the process, customize it to your particular cluster configuration and toolset, and iron out any snags before running the upgrade procedure on a production cluster.
A test cluster also has the benefit of being available to test client upgrades on.
Upgrading a cluster when the filesystem layout has not changed is fairly straightforward: install the new versions of HDFS and MapReduce on the cluster (and on clients at the same time), shut down the old daemons, update configuration files, and then start up the new daemons and switch clients to use the new libraries.
This process is reversible, so rolling back an upgrade is also straightforward.
After every successful upgrade, you should perform a couple of final cleanup steps:
Remove the old installation and configuration files from the cluster.
If you use the procedure just described to upgrade to a new version of HDFS and it expects a different layout version, then the namenode will refuse to run.
A message like the following will appear in its log:
The most reliable way of finding out whether you need to upgrade the filesystem is by performing a trial on a test cluster.
An upgrade of HDFS makes a copy of the previous version’s metadata and data.
Doing an upgrade does not double the storage requirements of the cluster, as the datanodes use hard links to keep two references (for the current and previous version) to the same block of data.
This design makes it straightforward to roll back to the previous version of the filesystem, if you need to.
You should understand that any changes made to the data on the upgraded system will be lost after the rollback completes.
You can keep only the previous version of the filesystem, which means you can’t roll back several versions.
Therefore, to carry out another upgrade to HDFS data and metadata, you will need to delete the previous version, a process called finalizing the upgrade.
Once an upgrade is finalized, there is no procedure for rolling back to a previous version.
The release notes make it clear when this is required.
As an extra precaution, you can keep a copy of the fsck output that lists all the files and blocks in the system, so you can compare it with the output of running fsck after the upgrade.
It’s also worth clearing out temporary files before doing the upgrade, both from the MapReduce system directory on HDFS and local temporary files.
With these preliminaries out of the way, here is the high-level procedure for upgrading a cluster when the filesystem layout needs to be migrated:
Make sure that any previous upgrade is finalized before proceeding with another upgrade.
Shut down MapReduce, and kill any orphaned task processes on the tasktrackers.
While running the upgrade procedure, it is a good idea to remove the Hadoop scripts from your PATH environment variable.
To perform the upgrade, run the following command (this is step 5 in the high-level upgrade procedure):
This causes the namenode to upgrade its metadata, placing the previous version in a new directory called previous:
Similarly, datanodes upgrade their storage directories, preserving the old copy in a directory called previous.
The upgrade process is not instantaneous, but you can check the progress of an upgrade using dfsadmin (step 6; upgrade events also appear in the daemons’ logfiles):
At this stage, you should run some sanity checks (step 7) on the filesystem (e.g., check files and blocks using fsck, test basic file operations)
You might choose to put HDFS into safe mode while you are running some of these checks (the ones that are read-only) to prevent others from making changes.
If you find that the new version is not working correctly, you may choose to roll back to the previous version (step 9)
This is possible only if you have not finalized the upgrade.
A rollback reverts the filesystem state to before the upgrade was performed, so any changes made in the meantime will be lost.
In other words, it rolls back to the previous state of the filesystem, rather than downgrading the current state of the filesystem to a former version.
Then start up the old version of HDFS with the -rollback option:
This command gets the namenode and datanodes to replace their current storage directories with their previous copies.
When you are happy with the new version of HDFS, you can finalize the upgrade (step 9) to remove the previous storage directories.
After an upgrade has been finalized, there is no way to roll back to the previous version.
Pig raises the level of abstraction for processing large datasets.
MapReduce allows you, as the programmer, to specify a map function followed by a reduce function, but working out how to fit your data processing into this pattern, which often requires multiple MapReduce stages, can be a challenge.
With Pig, the data structures are much richer, typically being multivalued and nested, and the set of transformations you can apply to the data are much more powerful.
They include joins, for example, which are not for the faint of heart in MapReduce.
The language used to express data flows, called Pig Latin.
A Pig Latin program is made up of a series of operations, or transformations, that are applied to the input data to produce output.
Taken as a whole, the operations describe a data flow, which the Pig execution environment translates into an executable representation and then runs.
Under the covers, Pig turns the transformations into a series of MapReduce jobs, but as a programmer you are mostly unaware of this, which allows you to focus on the data rather than the nature of the execution.
One criticism of MapReduce is that the development cycle is very long.
Writing the mappers and reducers, compiling and packaging the code, submitting the job(s), and retrieving the results is a timeconsuming business, and even with Streaming, which removes the compile and package step, the experience is still involved.
Pig’s sweet spot is its ability to process terabytes of data simply by issuing a half-dozen lines of Pig Latin from the console.
Indeed, it was created at Yahoo! to make it easier for researchers and engineers to mine the huge datasets there.
Pig is very supportive of a programmer writing a query, since it provides several commands for introspecting the data structures in your program as it is written.
Even more useful, it can perform a sample run on a representative subset of your input.
Virtually all parts of the processing path are customizable: loading, storing, filtering, grouping, and joining can all be altered by userdefined functions (UDFs)
These functions operate on Pig’s nested data model, so they can integrate very deeply with Pig’s operators.
As another benefit, UDFs tend to be more reusable than the libraries developed for writing MapReduce programs.
Like MapReduce, it is designed for batch processing of data.
If you want to perform a query that touches only a small amount of data in a large dataset, then Pig will not perform well, because it is set up to scan the whole dataset, or at least large portions of it.
In some cases, Pig doesn’t perform as well as programs written in MapReduce.
However, the gap is narrowing with each release, as the Pig team implements sophisticated algorithms for applying Pig’s relational operators.
It’s fair to say that unless you are willing to invest a lot of effort optimizing Java MapReduce code, writing queries in Pig Latin will save you time.
For a more detailed guide, see Programming Pig by Alan Gates (O’Reilly, 2011)
Installing and Running Pig Pig runs as a client-side application.
Even if you want to run Pig on a Hadoop cluster, there is nothing extra to install on the cluster: Pig launches jobs and interacts with HDFS (or other Hadoop filesystems) from your workstation.
Java 6 is a prerequisite (and on Windows, you will need Cygwin)
Download a stable release from http://pig.apache.org/releases.html, and unpack the tarball in a suitable place on your workstation:
It’s convenient to add Pig’s binary directory to your command-line path.
You also need to set the JAVA_HOME environment variable to point to a suitable Java installation.
Execution Types Pig has two execution types or modes: local mode and MapReduce mode.
In local mode, Pig runs in a single JVM and accesses the local filesystem.
This mode is suitable only for small datasets and when trying out Pig.
The execution type is set using the -x or -exectype option.
To run in local mode, set the option to local:
This starts Grunt, the Pig interactive shell, which is discussed in more detail shortly.
In MapReduce mode, Pig translates queries into MapReduce jobs and runs them on a Hadoop cluster.
The cluster may be a pseudo- or fully distributed cluster.
MapReduce mode (with a fully distributed cluster) is what you use when you want to run Pig on large datasets.
To use MapReduce mode, you first need to check that the version of Pig you downloaded is compatible with the version of Hadoop you are using.
Pig releases will only work against particular versions of Hadoop; this is documented in the release notes.
Pig honors the HADOOP_HOME environment variable for finding which Hadoop client to run.
However, if it is not set, Pig will use a bundled copy of the Hadoop libraries.
Note that these may not match the version of Hadoop running on your cluster, so it is best to explicitly set HADOOP_HOME.
Next, you need to point Pig at the cluster’s namenode and jobtracker.
If the installation of Hadoop at HADOOP_HOME is already configured for this, then there is nothing more to do.
Alternatively, you can set these two properties in the pig.properties file in Pig’s conf directory (or the directory specified by PIG_CONF_DIR)
Once you have configured Pig to connect to a Hadoop cluster, you can launch Pig, setting the -x option to mapreduce or omitting it entirely, as MapReduce mode is the default:
As you can see from the output, Pig reports the filesystem and jobtracker that it has connected to.
Running Pig Programs There are three ways of executing Pig programs, all of which work in both local and MapReduce mode: Script.
Pig can run a script file that contains Pig commands.
For example, pig script.pig runs the commands in the local file script.pig.
Alternatively, for very short scripts, you can use the -e option to run a script specified as a string on the command line.
Grunt Grunt is an interactive shell for running Pig commands.
Grunt is started when no file is specified for Pig to run and the -e option is not used.
It is also possible to run Pig scripts from within Grunt using run and exec.
Embedded You can run Pig programs from Java using the PigServer class, much like you can use JDBC to run SQL programs from Java.
Grunt Grunt has line-editing facilities like those found in GNU Readline (used in the bash shell and many other command-line applications)
For instance, the Ctrl-E key combination will move the cursor to the end of the line.
Grunt remembers command history, too,1 and you can recall lines in the history buffer using Ctrl-P or Ctrl-N (for previous and next) or, equivalently, the up or down cursor keys.
Another handy feature is Grunt’s completion mechanism, which will try to complete Pig Latin keywords and functions when you press the Tab key.
You can customize the completion tokens by creating a file named autocomplete and placing it on Pig’s classpath (such as in the conf directory in Pig’s install directory) or in the directory you invoked Grunt from.
The file should have one token per line, and tokens must not contain any whitespace.
History is stored in a file called .pig_history in your home directory.
You can get a list of commands using the help command.
When you’ve finished your Grunt session, you can exit with the quit command.
Pig Latin Editors PigPen is an Eclipse plug-in that provides an environment for developing Pig programs.
It includes a Pig script text editor, an example generator (equivalent to the ILLUSTRATE command), and a button for running the script on a Hadoop cluster.
There is also an operator graph window, which shows a script in graph form, for visualizing the data flow.
There are also Pig Latin syntax highlighters for other editors, including Vim and TextMate.
An Example Let’s look at a simple example by writing the program to calculate the maximum recorded temperature by year for the weather dataset in Pig Latin (just like we did using MapReduce in Chapter 2)
To explore what’s going on, we’ll use Pig’s Grunt interpreter, which allows us to enter lines and interact with the program to understand what it’s doing.
Start up Grunt in local mode, and then enter the first line of the Pig script:
For simplicity, the program assumes that the input is tab-delimited text, with each line having just year, temperature, and quality fields.
Pig actually has more flexibility than this with regard to the input formats it accepts, as you’ll see later.) This line describes the input data we want to process.
The year:chararray notation describes the field’s name and type; chararray is like a Java string, and an int is like a Java int.
The LOAD operator takes a URI argument; here we are just using a local file, but we could refer to an HDFS URI.
The AS clause (which is optional) gives the fields names to make it convenient to refer to them in subsequent statements.
The result of the LOAD operator, and indeed any operator in Pig Latin, is a relation, which is just a set of tuples.
A tuple is just like a row of data in a database table, with multiple fields in a particular order.
In this example, the LOAD function produces a set of (year, temperature, quality) tuples that are present in the input file.
We write a relation with one tuple per line, where tuples are represented as comma-separated items in parentheses:
Relations are given names, or aliases, so they can be referred to.
We can examine the contents of an alias using the DUMP operator:
This tells us that records has three fields, with aliases year, temperature, and quality, which are the names we gave them in the AS clause.
The fields have the types given to them in the AS clause, too.
The second statement removes records that have a missing temperature (indicated by a value of 9999) or an unsatisfactory quality reading.
The third statement uses the GROUP function to group the records relation by the year field.
We now have two rows, or tuples, one for each year in the input data.
The first field in each tuple is the field being grouped by (the year), and the second field is a bag of tuples for that year.
A bag is just an unordered collection of tuples, which in Pig Latin is represented using curly braces.
By grouping the data in this way, we have created a row per year, so now all that remains is to find the maximum temperature for the tuples in each bag.
Before we do this, let’s understand the structure of the grouped_records relation:
This tells us that the grouping field is given the alias group by Pig, and the second field is the same structure as the filtered_records relation that was being grouped.
In this example, the first field is group, which is just the year.
In this case, it calculates the maximum temperature for the fields in each filtered_records bag.
So we’ve successfully calculated the maximum temperature for each year.
Generating Examples In this example, we’ve used a small sample dataset with just a handful of rows to make it easier to follow the data flow and aid debugging.
Creating a cut-down dataset is an art, as ideally it should be rich enough to cover all the cases to exercise your queries (the completeness property), yet small enough to make sense to the programmer (the conciseness property)
Using a random sample doesn’t work well in general because join and filter operations tend to remove all random data, leaving an empty result, which is not illustrative of the general data flow.
With the ILLUSTRATE operator, Pig provides a tool for generating a reasonably complete and concise sample dataset.
Here is the output from running ILLUSTRATE (slightly reformatted to fit the page):
Comparison with Databases Having seen Pig in action, it might seem that Pig Latin is similar to SQL.
The presence of such operators as GROUP BY and DESCRIBE reinforces this impression.
However, there are several differences between the two languages, and between Pig and relational database management systems (RDBMSs) in general.
The most significant difference is that Pig Latin is a data flow programming language, whereas SQL is a declarative programming language.
In other words, a Pig Latin program is a step-by-step set of operations on an input relation, in which each step is a single transformation.
By contrast, SQL statements are a set of constraints that, taken together, define the output.
In many ways, programming in Pig Latin is like working at the level of an RDBMS query planner, which figures out how to turn a declarative statement into a system of steps.
Pig is more relaxed about the data that it processes: you can define a schema at runtime, but it’s optional.
Essentially, it will operate on any source of tuples (although the source should support.
Unlike a traditional database, there is no data import process to load the data into the RDBMS.
The data is loaded from the filesystem (usually HDFS) as the first step in the processing.
Pig’s support for complex, nested data structures differentiates it from SQL, which operates on flatter data structures.
Also, Pig’s ability to use UDFs and streaming operators that are tightly integrated with the language and Pig’s nested data structures makes Pig Latin more customizable than most SQL dialects.
RDBMSs have several features to support online, low-latency queries, such as transactions and indexes, that are absent in Pig.
As mentioned earlier, Pig does not support random reads or queries in the order of tens of milliseconds.
Nor does it support random writes to update small portions of data; all writes are bulk streaming writes, just like MapReduce.
Hive (covered in Chapter 12) sits between Pig and conventional RDBMSs.
Like Pig, Hive is designed to use HDFS for storage, but otherwise there are some significant differences.
Its query language, HiveQL, is based on SQL, and anyone who is familiar with SQL would have little trouble writing queries in HiveQL.
Like RDBMSs, Hive mandates that all data be stored in tables, with a schema under its management; however, it can associate a schema with preexisting data in HDFS, so the load step is optional.
Hive does not support low-latency queries, a characteristic it shares with Pig.
Pig Latin does not have a formal language definition as such, but there is a comprehensive guide to the language that you can find through a link on the Pig website at http://pig.apache.org/
Structure A Pig Latin program consists of a collection of statements.
A statement can be thought of as an operation or a command.5 For example, a GROUP operation is a type of statement:
The command to list the files in a Hadoop filesystem is another example of a statement: ls /
Statements are usually terminated with a semicolon, as in the example of the GROUP statement.
In fact, this is an example of a statement that must be terminated with a semicolon; it is a syntax error to omit it.
The ls command, on the other hand, does not have to be terminated with a semicolon.
As a general guideline, statements or commands for interactive use in Grunt do not need the terminating semicolon.
This group includes the interactive Hadoop commands, as well as the diagnostic operators such as DESCRIBE.
It’s never an error to add a terminating semicolon, so if in doubt, it’s simplest to add one.
Statements that have to be terminated with a semicolon can be split across multiple lines for readability:
Everything from the first hyphen to the end of the line is ignored by the Pig Latin interpreter:
They can span lines or be embedded in a single line:
Pig Latin has a list of keywords that have a special meaning in the language and cannot be used as identifiers.
Statements As a Pig Latin program is executed, each statement is parsed in turn.
If there are syntax errors or other (semantic) problems, such as undefined aliases, the interpreter will halt and display an error message.
The interpreter builds a logical plan for every relational operation, which forms the core of a Pig Latin program.
The logical plan for the statement is added to the logical plan for the program so far, and then the interpreter moves on to the next statement.
It’s important to note that no data processing takes place while the logical plan of the program is being constructed.
For example, consider again the Pig Latin program from the first example:
When the Pig Latin interpreter sees the first line containing the LOAD statement, it confirms that it is syntactically and semantically correct, and adds it to the logical plan, but it does not load the data from the file (or even check whether the file exists)
The point is that it makes no sense to start any processing until the whole flow is defined.
The trigger for Pig to start execution is the DUMP statement.
At that point, the logical plan is compiled into a physical plan and executed.
Multiquery Execution Because DUMP is a diagnostic tool, it will always trigger execution.
In interactive mode, STORE acts like DUMP and will always trigger execution (this includes the run command), but in batch mode it will not (this includes the exec command)
In batch mode, Pig will parse the whole script to see whether there are any optimizations that could be.
Relations B and C are both derived from A, so to save reading A twice, Pig can run this script as a single MapReduce job by reading A once and writing two output files from the job, one for each of B and C.
In previous versions of Pig that did not have multiquery execution, each STORE statement in a script run in batch mode triggered execution, resulting in a job for each STORE statement.
It is possible to restore the old behavior by disabling multiquery execution with the -M or -no_multiquery option to pig.
The physical plan that Pig prepares is a series of MapReduce jobs, which in local mode Pig runs in the local JVM, and in MapReduce mode Pig runs on a Hadoop cluster.
You can see the logical and physical plans created by Pig using the EXPLAIN command on a relation (EXPLAIN max_temp; for example)
This is a good way to find out how many MapReduce jobs Pig will run for your query.
The relational operators that can be a part of a logical plan in Pig are summarized in Table 11-1
Loading and storing LOAD Loads data from the filesystem or other storage into a relation.
Sorting ORDER Sorts a relation by one or more fields.
Combining and splitting UNION Combines two or more relations into one.
There are other types of statements that are not added to the logical plan.
The STORE statement should be used when the size of the output is more than a few lines, as it writes to a file rather than to the console.
Because they do not process relations, commands are not added to the logical plan; instead, they are executed immediately.
Pig provides commands to interact with Hadoop filesystems (which are very handy for moving data around before or after processing with Pig) and MapReduce, as well as a few utility commands (described in Table 11-4)
Hadoop Filesystem cat Prints the contents of one or more files.
Utility exec Runs a script in a new Grunt shell in batch mode.
The filesystem commands can operate on files or directories in any Hadoop filesystem, and they are very similar to the hadoop fs commands (which is not surprising, as both are simple wrappers around the Hadoop FileSystem interface)
You can access all of the Hadoop filesystem shell commands using Pig’s fs command.
For example, fs -ls will show a file listing, and fs -help will show help on all the available commands.
These commands are mostly self-explanatory, except set, which is used to set options that control Pig’s behavior, including arbitrary MapReduce job properties.
The debug option is used to turn debug logging on or off from within a script (you can also control the log level when launching Pig, using the -d or -debug option):
Another useful option is the job.name option, which gives a Pig job a meaningful name, making it easier to pick out your Pig MapReduce jobs when running on a shared Hadoop cluster.
If Pig is running a script (rather than operating as an interactive query from Grunt), its job name defaults to a value based on the script name.
There are two commands in Table 11-4 for running a Pig script, exec and run.
The difference is that exec runs the script in batch mode in a new Grunt shell, so any aliases defined in the script are not accessible to the shell after the script has completed.
On the other hand, when running a script with run, it is as if the contents of the script had been entered manually, so the command history of the invoking shell contains all the statements from the script.
The recommended approach for writing programs that have conditional logic or loop constructs is to embed Pig Latin in another language, such as Python, JavaScript, or Java, and manage the control flow from there.
In this model the host script uses a compile-bind-run API to execute Pig scripts and retrieve their status.
Embedded Pig programs always run in a JVM, so for Python and JavaScript you use the pig command followed by the name of your script, and the appropriate Java scripting engine will be selected (Jython for Python, Rhino for JavaScript)
Expressions An expression is something that is evaluated to yield a value.
Expressions can be used in Pig as a part of a statement containing a relational operator.
Pig has a rich variety of expressions, many of which will be familiar from other programming languages.
They are listed in Table 11-5, with brief descriptions and examples.
We will see examples of many of these expressions throughout the chapter.
Projection c.$n, c.f Field in container c (relation, bag, or tuple) by position, by name.
Cast (t) f Cast of field f to type t (int) year.
Conditional x ? y : z Bincond/ternary; y if x evaluates to true, z otherwise.
Flatten FLATTEN(f) Removal of a level of nesting from bags and tuples.
Types So far you have seen some of the simple types in Pig, such as int and chararray.
Here we will discuss Pig’s built-in types in more detail.
Pig has four numeric types: int, long, float, and double, which are identical to their Java counterparts.
Pig does not have types corresponding to Java’s boolean,6 byte, short, or char primitive types.
These are all easily represented using Pig’s int type, or chararray for char.
The numeric, textual, and binary types are simple atomic types.
Pig Latin also has three complex types for representing nested structures: tuple, bag, and map.
All of Pig Latin’s types are listed in Table 11-6
Although there is no Boolean type for data (until version 0.10.0), Pig has the concept of an expression evaluating to true or false for testing conditions (such as in a FILTER statement)
However, Pig does not allow a Boolean expression to be stored in a field.
The simplest workaround in this case is to load the data from a file using the LOAD statement.
As another example, you can’t treat a relation like a bag and project a field into a new relation ($0 refers to the first field of A, using the positional notation):
It’s possible that a future version of Pig Latin will remove these inconsistencies and treat relations and bags in the same way.
Schemas A relation in Pig may have an associated schema, which gives the fields in the relation names and types.
We’ve seen how an AS clause in a LOAD statement is used to attach a schema to a relation:
This time we’ve declared the year to be an integer rather than a chararray, even though the file it is being loaded from is the same.
An integer may be more appropriate if we need to manipulate the year arithmetically (to turn it into a timestamp, for example), whereas the chararray representation might be more appropriate when it’s being used as a simple identifier.
Pig’s flexibility in the degree to which schemas are declared contrasts with schemas in traditional SQL databases, which are declared before the data is loaded into to the system.
Pig is designed for analyzing plain input files with no associated type information, so it is quite natural to choose types for fields later than you would with an RDBMS.
In this case, we have specified only the names of the fields in the schema: year, temperature, and quality.
The types default to bytearray, the most general type, representing a binary string.
You don’t need to specify types for every field; you can leave some to default to byte array, as we have done for year in this declaration:
However, if you specify a schema in this way, you do need to specify every field.
Also, there’s no way to specify the type of a field without specifying the name.
On the other hand, the schema is entirely optional and can be omitted by not specifying an AS clause:
Although it can be convenient not to assign types to fields (particularly in the first stages of writing a query), doing so can improve the clarity and efficiency of Pig Latin programs and is generally recommended.
Declaring a schema as a part of the query is flexible but doesn’t lend itself to schema reuse.
A set of Pig queries over the same input data will often have the same schema repeated in each query.
If the query processes a large number of fields, this repetition can become hard to maintain.
The Apache HCatalog project (http://incubator.apache.org/hcatalog/) solves this problem by providing a table metadata service, based on Hive’s metastore, so that Pig queries can reference schemas by name, rather than specifying them in full each time.
An SQL database will enforce the constraints in a table’s schema at load time; for example, trying to load a string into a column that is declared to be a numeric type will fail.
In Pig, if the value cannot be cast to the type declared in the schema, it will substitute a null value.
Let’s see how this works when we have the following input for the weather data, which has an “e” character in place of an integer:
Pig handles the corrupt line by producing a null for the offending value, which is displayed as the absence of a value when dumped to screen (and also when saved using STORE):
Pig produces a warning for the invalid field (not shown here) but does not halt its processing.
For large datasets, it is very common to have corrupt, invalid, or merely unexpected data, and it is generally infeasible to incrementally fix every unparsable record.
Instead, we can pull out all of the invalid records in one go, so we can take action on them, perhaps by fixing our program (because they indicate that we have made a mistake) or by filtering them out (because the data is genuinely unusable):
Note the use of the is null operator, which is analogous to SQL.
In practice, we would include more information from the original record, such as an identifier and the value that could not be parsed, to help our analysis of the bad data.
We can find the number of corrupt records using the following idiom for counting the number of rows in a relation:
Going back to the case in which temperature’s type was left undeclared, the corrupt data cannot be detected easily, since it doesn’t surface as a null:
What happens in this case is that the temperature field is interpreted as a bytearray, so the corrupt field is not detected when the input is loaded.
When passed to the MAX function, the temperature field is cast to a double, since MAX works only with numeric types.
The corrupt field cannot be represented as a double, so it becomes a null, which MAX silently ignores.
The best approach is generally to declare types for your data on loading and look for missing or corrupt values in the relations themselves before you do your main processing.
Sometimes corrupt data shows up as smaller tuples because fields are simply missing.
You can filter these out by using the SIZE function as follows:
In Pig, you don’t declare the schema for every new relation in the data flow.
In most cases, Pig can figure out the resulting schema for the output of a relational operation by considering the schema of the input relation.
How are schemas propagated to new relations? Some relational operators don’t change the schema, so the relation produced by the LIMIT operator (which restricts a relation to a maximum number of tuples), for example, has the same schema as the relation it operates on.
If the schemas are incompatible, due to different types or number of fields, then the schema of the result of the UNION is unknown.
You can find out the schema for any relation in the data flow using the DESCRIBE operator.
Functions Functions in Pig come in four types: Eval function.
A function that takes one or more expressions and returns another expression.
An example of a built-in eval function is MAX, which returns the maximum value of the entries in a bag.
Some eval functions are aggregate functions, which means they operate on a bag of data to produce a scalar value; MAX is an example of an aggregate function.
Furthermore, many aggregate functions are algebraic, which means that the result of the function may be calculated incrementally.
Filter function A special type of eval function that returns a logical Boolean result.
As the name suggests, filter functions are used in the FILTER operator to remove unwanted rows.
They can also be used in other relational operators that take Boolean conditions and, in general, expressions using Boolean or conditional expressions.
An example of a built-in filter function is IsEmpty, which tests whether a bag or a map contains any items.
Load function A function that specifies how to load data into a relation from external storage.
Store function A function that specifies how to save the contents of a relation to external storage.
Often, load and store functions are implemented by the same type.
For example, PigStorage, which loads data from delimited text files, can store data in the same format.
Pig comes with a collection of built-in functions, a selection of which are listed in Table 11-7
The complete list of built-in functions, which includes a large number of standard math and string functions, can be found in the documentation for each Pig release.
Eval AVG Calculates the average (mean) value of entries in a bag.
COUNT_STAR Calculates the number of entries in a bag, including those that are null.
If the two arguments are not bags, returns a bag containing both if they are equal; otherwise, returns an empty bag.
The size of numeric types is always one; for character arrays, it is the number of characters; for byte arrays, the number of bytes; and for containers (tuple, bag, map), it is the number of entries.
Filter IsEmpty Tests whether a bag or map is empty.
Load/Store PigStorage Loads or stores relations using a field-delimited text format.
Each line is broken into fields using a configurable field delimiter (defaults to a tab character) to be stored in the tuple’s fields.
BinStorage Loads or stores relations from or to binary files in a Pig-specific format that uses Hadoop Writable objects.
Each line corresponds to a tuple whose single field is the line of text.
JsonLoader, JsonStorage Loads or stores relations from or to a (Pig-defined) JSON format.
HBaseStorage Loads or stores relations from or to HBase tables.
If the function you need is not available, you can write your own.
Before you do that, however, have a look in the Piggy Bank, a repository of Pig functions shared by the Pig community.
For example, there are load and store functions in the Piggy Bank for Avro data files, CSV files, Hive RCFiles, SequenceFiles, and XML files.
The Pig website has instructions on how to browse and obtain the Piggy Bank functions.
If the Piggy Bank doesn’t have what you need, you can write your own function (and if it is sufficiently.
Macros Macros provide a way to package reusable pieces of Pig Latin code from within Pig Latin itself.
For example, we can extract the part of our Pig Latin program that performs grouping on a relation and then finds the maximum value in each group by defining a macro as follows:
At runtime, Pig will expand the macro using the macro definition.
After expansion, the program looks like the following, with the expanded section in bold.
Normally you don’t see the expanded form, because Pig creates it internally; however, in some cases it is useful to see it when writing and debugging macros.
You can get Pig to perform macro expansion only (without executing the script) by passing the -dryrun argument to pig.
Notice that the parameters that were passed to the macro (filtered_records, year, and temperature) have been substituted for the names in the macro definition.
Aliases in the macro definition that don’t have a $ prefix, such as A in this example, are local to the macro definition and are rewritten at expansion time to avoid conflicts with aliases in other parts of the program.
To foster reuse, macros can be defined in separate files to Pig scripts, in which case they need to be imported into any script that uses them.
User-Defined Functions Pig’s designers realized that the ability to plug in custom code is crucial for all but the most trivial data processing jobs.
For this reason, they made it easy to define and use user-defined functions.
We only cover Java UDFs in this section, but be aware that you can write UDFs in Python or JavaScript too, both of which are run using the Java Scripting API.
A Filter UDF Let’s demonstrate by writing a filter function for filtering out weather records that do not have a temperature quality reading of satisfactory (or better)
This achieves two things: it makes the Pig script more concise, and it encapsulates the logic in one place so that it can be easily reused in other scripts.
If we were just writing an ad hoc query, we probably wouldn’t bother to write a UDF.
It’s when you start doing the same kind of processing over and over again that you see opportunities for reusable UDFs.
Filter UDFs are all subclasses of FilterFunc, which itself is a subclass of EvalFunc.
We’ll look at EvalFunc in more detail later, but for the moment just note that, in essence, EvalFunc looks like the following class:
EvalFunc’s only abstract method, exec(), takes a tuple and returns a single value, the (parameterized) type T.
The fields in the input tuple consist of the expressions passed to the function—in this case, a single integer.
For FilterFunc, T is Boolean, so the method should return true only for those tuples that should not be filtered out.
For the quality filter, we write a class, IsGoodQuality, that extends FilterFunc and implements the exec() method.
The Tuple class is essentially a list of objects with associated types.
Here we are concerned only with the first field (since the function only has a single argument), which we extract by index using the get() method on Tuple.
The field is an integer, so if it’s not null, we cast it and check whether the.
To use the new function, we first compile it and package it in a JAR file (the example code that accompanies this book comes with build instructions for how to do this)
Then we tell Pig about the JAR file with the REGISTER operator, which is given the local path to the filename (and is not enclosed in quotes):
Pig resolves function calls by treating the function’s name as a Java classname and attempting to load a class of that name.
This, incidentally, is why function names are case-sensitive: because Java classnames are.) When searching for classes, Pig uses a.
When running in distributed mode, Pig will ensure that your JAR files get shipped to the cluster.
Resolution of built-in functions proceeds in the same way, except for one difference: Pig has a set of built-in package names that it searches, so the function call does not have to be a fully qualified name.
Alternatively, we can shorten the function name by defining an alias, using the DEFINE operator:
Defining an alias is a good idea if you want to use the function several times in the same script.
It’s also necessary if you want to pass arguments to the constructor of the UDF’s implementation class.
The filter works when the quality field is declared to be of type int, but if the type information is absent, the UDF fails! This happens because the field is the default type, bytearray, represented by the DataByteArray class.
The obvious way to fix this is to convert the field to an integer in the exec() method.
However, there is a better way, which is to tell Pig the types of the fields that the function expects.
We can override it to tell Pig that the first field should be an integer:
This method returns a FuncSpec object corresponding to each of the fields of the tuple that are passed to the exec() method.
Here there is a single field, and we construct an anonymous FieldSchema (the name is passed as null, since Pig ignores the name when doing type conversion)
The type is specified using the INTEGER constant on Pig’s DataType class.
With the amended function, Pig will attempt to convert the argument passed to the function to an integer.
If the field cannot be converted, then a null is passed for the field.
The exec() method always returns false when the field is null.
For this application, this behavior is appropriate, as we want to filter out records whose quality field is unintelligible.
When you write an eval function, you need to consider what the output’s schema looks like.
In the following statement, the schema of B is determined by the function udf:
If udf creates tuples with scalar fields, then Pig can determine B’s schema through reflection.
For complex types such as bags, tuples, or maps, Pig needs more help, and you should implement the outputSchema() method to give Pig the information about the output schema.
The Trim UDF returns a string, which Pig translates as a chararray, as can be seen from the following session:
A has chararray fields that have leading and trailing spaces.
We create B from A by applying the Trim function to the first field in A (named fruit)
B’s fields are correctly inferred to be of type chararray.
Sometimes you want to use a function that is provided by a Java library, but without going to the effort of writing a UDF.
Dynamic invokers allow you to do this by calling Java methods directly from a Pig script.
The trade-off is that method calls are made via reflection, which can impose significant overhead when called for every record in a large dataset.
So for scripts that are run repeatedly, a dedicated UDF is normally preferred.
Pig’s Algebraic or Accumulator interfaces for more efficient processing of the bag in chunks.
The following snippet shows how we could define and use a trim UDF that uses the Apache Commons Lang StringUtils class.
The InvokeForString invoker is used because the return type of the method is a String.
There are also InvokeForInt, InvokeForLong, InvokeForDouble, and InvokeFor Float invokers.) The first argument to the invoker constructor is the fully qualified method to be invoked.
The second is a space-separated list of the method argument classes.
A Load UDF We’ll demonstrate a custom load function that can read plain-text column ranges as fields, very much like the Unix cut command.
From Pig 0.7.0, the load and store function interfaces have been overhauled to be more closely aligned with Hadoop’s InputFormat and OutputFormat classes.
Functions written for previous versions of Pig will need rewriting (guidelines for doing so are provided at http://wiki.apache.org/pig/LoadStoreMigrationGuide)
A LoadFunc will typically use an existing underlying InputFormat to create records, with the LoadFunc providing the logic for turning the records into Pig tuples.
CutLoadFunc is constructed with a string that specifies the column ranges to use for each field.
The logic for parsing this string and creating a list of internal Range objects that.
Pig calls setLocation() on a LoadFunc to pass the input location to the loader.
Since CutLoadFunc uses a TextInputFormat to break the input into lines, we just pass the location to set the input path using a static method on FileInputFormat.
Next, Pig calls the getInputFormat() method to create a RecordReader for each split, just like in MapReduce.
Pig passes each RecordReader to the prepareToRead() method of CutLoadFunc, which we store a reference to, so we can use it in the getNext() method for iterating through the records.
The Pig runtime calls getNext() repeatedly, and the load function reads tuples from the reader until the reader reaches the last record in its split.
At this point, it returns null to signal that there are no more tuples to be read.
It is the responsibility of the getNext() implementation to turn lines of the input file into Tuple objects.
It does this by means of a TupleFactory, a Pig class for creating Tuple instances.
The newTuple() method creates a new tuple with the required number of fields, which is just the number of Range classes, and the fields are populated using substrings of the line, which are determined by the Range objects.
We need to think about what to do when the line is shorter than the range asked for.
One option is to throw an exception and stop further processing.
This is appropriate if your application cannot tolerate incomplete or corrupt records.
In many cases, it is better to return a tuple with null fields and let the Pig script handle the incomplete data as it sees fit.
This is the approach we take here; by exiting the for loop if the range end is past the end of the line, we leave the current field and any subsequent fields in the tuple with their default value of null.
Let’s now consider the type of the fields being loaded.
If the user has specified a schema, then the fields need to be converted to the relevant types.
However, this is performed lazily by Pig, and so the loader should always construct tuples of type bytearrary, using the DataByteArray type.
The loader function still has the opportunity to do the conversion, however, by overriding getLoadCaster() to return a custom implementation of the LoadCaster interface, which provides a collection of conversion methods for this purpose:
In some cases, the load function itself can determine the schema.
For example, if we were loading self-describing data such as XML or JSON, we could create a schema for Pig by looking at the data.
Alternatively, the load function may determine the schema in another way, such as an external file, or by being passed information in its constructor.
To support such cases, the load function should implement the LoadMetadata interface (in addition to the LoadFunc interface) so it can supply a schema to the Pig runtime.
Note, however, that if a user supplies a schema in the AS clause of LOAD, then it takes precedence over the schema one specified through the LoadMetadata interface.
A load function may additionally implement the LoadPushDown interface as a means for finding out which columns the query is asking for.
This can be a useful optimization for column-oriented storage, so that the loader loads only the columns that are needed by the query.
There is no obvious way for CutLoadFunc to load only a subset of columns, because it reads the whole line for each tuple, so we don’t use this optimization.
Loading and Storing Data Throughout this chapter, we have seen how to load data from external storage for processing in Pig.
Here’s an example of using PigStorage to store tuples as plain-text values separated by a colon character:
Filtering Data Once you have some data loaded into a relation, often the next step is to filter it to remove the data that you are not interested in.
By filtering early in the processing pipeline, you minimize the amount of data flowing through the system, which can improve efficiency.
We have already seen how to remove rows from a relation using the FILTER operator with simple expressions and a UDF.
It can be used to remove fields or to generate new ones.
Here we have created a new relation B with three fields.
Its first field is a projection of the first field ($0) of A.
B’s second field is the third field of A ($2) with one added to it.
B’s third field is a constant field (every row in B has the same third field) with the chararray value Constant.
In the following example, we compute various statistics for the weather dataset:
Using the cut UDF we developed earlier, we load various fields from the input dataset into the records relation.
Notice the PARALLEL keyword for setting the number of reducers to use; this is vital when running on a cluster.
The first nested statement creates a relation for the distinct USAF identifiers for stations using the DISTINCT operator.
The second nested statement creates a relation for the records with “good” readings using the FILTER operator and a UDF.
Running it on a few years of data, we get the following:
The fields are year, number of unique stations, total number of good readings, and total number of readings.
We can see how the number of weather stations and readings grew over time.
The STREAM operator allows you to transform data in a relation using an external program or script.
Here is an example that uses the Unix cut command to extract the second field of each tuple in A.
Note that the command and its arguments are enclosed in backticks:
The STREAM operator uses PigStorage to serialize and deserialize relations to and from the program’s standard input and output streams.
Tuples in A are converted to tabdelimited lines that are passed to the script.
The output of the script is read one line at a time and split on tabs to create new tuples for the output relation C.
You can provide a custom serializer and deserializer, which implement PigToStream and StreamToPig, respectively (both in the org.apache.pig package), using the DEFINE operator.
Pig streaming is most powerful when you write custom processing scripts.
To use the script, you need to ship it to the cluster.
This is achieved via a DEFINE clause, which also creates an alias for the STREAM command.
The STREAM statement can then refer to the alias, as the following Pig script shows:
Since the large datasets that are suitable for analysis by Pig (and MapReduce in general) are not normalized, joins are used more infrequently in Pig than they are in SQL.
This is a classic inner join, where each match between the two relations corresponds to a row in the result.
It’s actually an equijoin because the join predicate is equality.) The result’s fields are made up of all the fields of all the input relations.
You should use the general join operator when all the relations being joined are too large to fit in memory.
If one of the relations is small enough to fit in memory, there is a special type of join called a fragment replicate join, which is implemented by distributing the small input to all the mappers and performing a map-side join using an inmemory lookup table against the (fragmented) larger relation.
There is a special syntax for telling Pig to use a fragment replicate join:8
The first relation must be the large one, followed by one or more small ones (all of which must fit in memory)
The COGROUP statement is similar to JOIN, but instead creates a nested set of output tuples.
This can be useful if you want to exploit the structure in subsequent statements:
The first field of each tuple is the key, and the remaining fields are bags of tuples from the relations with a matching key.
The first bag contains the matching tuples from relation A with the same key.
There are more keywords that may be used in the USING clause, including "skewed" (for large datasets with a skewed keyspace) and "merge" (to effect a merge join for inputs that are already sorted on the join key)
See Pig’s documentation for details on how to use these specialized joins.
Similarly, the second bag contains the matching tuples from relation B with the same key.
If for a particular key a relation has no matching key, the bag for that relation is empty.
For example, since no one has bought a scarf (with ID 1), the second bag in the tuple for that row is empty.
This is an example of an outer join, which is the default type for COGROUP.
It can be made explicit using the OUTER keyword, making this COGROUP statement the same as the previous one:
You can suppress rows with empty bags by using the INNER keyword, which gives the COGROUP inner join semantics.
The INNER keyword is applied per relation, so the following suppresses rows only when relation A has no match (dropping the unknown product 0 here):
Using a combination of COGROUP, INNER, and FLATTEN (which removes nesting) it’s possible to simulate an (inner) JOIN:
If the join key is composed of several fields, you can specify them all in the BY clauses of the JOIN or COGROUP statement.
Make sure that the number of fields in each BY clause is the same.
Here’s another example of a join in Pig, in a script for calculating the maximum temperature for every station over a time period controlled by the input:
We use the cut UDF we developed earlier to load one relation holding the station IDs (USAF and WBAN identifiers) and names, and one relation holding all the weather records, keyed by station ID.
We group the filtered weather records by station ID and aggregate by maximum temperature before joining with the stations.
Finally, we project out the fields we want in the final result: USAF, WBAN, station name, and maximum temperature.
This query could be made more efficient by using a fragment replicate join, as the station metadata is small.
Pig Latin includes the cross-product operator (also known as the cartesian product), which joins every tuple in a relation with every tuple in a second relation (and with every tuple in further relations if supplied)
The size of the output is the product of the size of the inputs, potentially making the output very large:
When dealing with large datasets, you should try to avoid operations that generate intermediate representations that are quadratic (or worse) in size.
Computing the crossproduct of the whole input dataset is rarely needed, if ever.
For example, at first blush one might expect that calculating pairwise document similarity in a corpus of documents would require every document pair to be generated before calculating their similarity.
However, if one starts with the insight that most document pairs have a similarity score of zero (that is, they are unrelated), then we can find a way to a better algorithm.
In this case, the key idea is to focus on the entities that we are using to calculate similarity (terms in a document, for example) and make them the center of the algorithm.
In practice, we also remove terms that don’t help discriminate between documents (stopwords), and this reduces the problem space still further.
Although COGROUP groups the data in two or more relations, the GROUP statement groups the data in a single relation.
Let’s group by the number of characters in the second field:
The second field is a bag containing the grouped fields with the same schema as the original relation (in this case, A)
There are also two special grouping operations: ALL and ANY.
Note that there is no BY in this form of the GROUP statement.
The ANY keyword is used to group the tuples in a relation randomly, which can be useful for sampling.
There is no guarantee which order the rows will be processed in.
In particular, when retrieving the contents of A using DUMP or STORE, the rows may be written in any order.
If you want to impose an order on the output, you can use the ORDER operator to sort a relation by one or more fields.
The following example sorts A by the first field in ascending order and by the second field in descending order:
Any further processing on a sorted relation is not guaranteed to retain its order.
Even though relation C has the same contents as relation B, its tuples may be emitted in any order by a DUMP or a STORE.
It is for this reason that it is usual to perform the ORDER operation just before retrieving the output.
The LIMIT statement is useful for limiting the number of results as a quick and dirty way to get a sample of a relation; prototyping (the ILLUSTRATE command) should be preferred for generating more representative samples of the data.
It can be used immediately after the ORDER statement to retrieve the first n tuples.
Usually, LIMIT will select any n tuples from a relation, but when used immediately after an ORDER statement, the order is retained (in an exception to the rule that processing a relation does not retain its order):
If the limit is greater than the number of tuples in the relation, all tuples are returned (so LIMIT has no effect)
Using LIMIT can improve the performance of a query because Pig tries to apply the limit as early as possible in the processing pipeline, to minimize the amount of data that needs to be processed.
For this reason, you should always use LIMIT if you are not interested in the entire output.
Combining and Splitting Data Sometimes you have several relations that you would like to combine into one.
Also, it’s possible to form the union of two relations with different schemas or with different numbers of fields, as we have done here.
Pig attempts to merge the schemas from the relations that UNION is operating on.
In this case, they are incompatible, so C has no schema:
If the output relation has no schema, your script needs to be able to handle tuples that vary in the number of fields and/or types.
The SPLIT operator is the opposite of UNION: it partitions a relation into two or more relations.
Pig in Practice There are some practical techniques that are worth knowing about when you are developing and running Pig programs.
Parallelism When running in MapReduce mode, it’s important that the degree of parallelism matches the size of the dataset.
To explicitly set the number of reducers you want for each job, you can use a PARALLEL clause for operators that run in the reduce phase.
These include all the grouping and joining operators (GROUP, COGROUP, JOIN, CROSS), as well as DISTINCT and ORDER.
The following line sets the number of reducers to 30 for the GROUP:
Alternatively, you can set the default_parallel option, and it will take effect for all subsequent jobs:
A good setting for the number of reduce tasks is slightly fewer than the number of reduce slots in the cluster.
The number of map tasks is set by the size of the input (with one map per HDFS block) and is not affected by the PARALLEL clause.
Parameter Substitution If you have a Pig script that you run on a regular basis, it’s quite common to want to be able to run the same script with different parameters.
For example, a script that runs daily may use the date to determine which input files it runs over.
Pig supports parameter substitution, where parameters in the script are substituted with values supplied at runtime.
Parameters can be specified when launching Pig, using the -param option, one for each parameter:
You can also put parameters in a file and pass them to Pig using the -param_file option.
For example, we can achieve the same result as the previous command by placing the parameter definitions in a file:
You can also use a combination of -param and -param_file options, and if any parameter is defined in both a parameter file and on the command line, the last value on the command line takes precedence.
For parameters that are supplied using the -param option, it is easy to make the value dynamic by running a command or script.
Many Unix shells support command substitution for a command enclosed in backticks, and we can use this to make the output directory date-based:
Pig also supports backticks in parameter files by executing the enclosed command in a shell and using the shell output as the substituted value.
If the command or scripts exit with a nonzero exit status, then the error message is reported and execution halts.
Backtick support in parameter files is a useful feature; it means that parameters can be defined in the same way in a file or on the command line.
Parameter substitution occurs as a preprocessing step before the script is run.
You can see the substitutions that the preprocessor made by executing Pig with the -dryrun option.
In dry run mode, Pig performs parameter substitution (and macro expansion) and generates a copy of the original script with substituted values, but does not execute the script.
You can inspect the generated script and check that the substitutions look sane (because they are dynamically generated, for example) before running it in normal mode.
At the time of this writing, Grunt does not support parameter substitution.
Hive grew from a need to manage and learn from the huge volumes of data that Facebook was producing every day from its burgeoning social network.
After trying a few different systems, the team chose Hadoop for storage and processing, since it was cost-effective and met their scalability needs.2 Hive was created to make it possible for analysts with strong SQL skills (but meager Java programming skills) to run queries on the huge volumes of data that Facebook stored in HDFS.
Today, Hive is a successful Apache project used by many organizations as a general-purpose, scalable data processing platform.
What’s more, SQL is the lingua franca in business intelligence tools (ODBC is a common bridge, for example), so Hive is well placed to integrate with these products.
It assumes that you have working knowledge of SQL and general database architecture; as we go through Hive’s features, we’ll often compare them to the equivalent in a traditional RDBMS.
Installing Hive In normal use, Hive runs on your workstation and converts your SQL query into a series of MapReduce jobs for execution on a Hadoop cluster.
Hive organizes data into tables, which provide a means for attaching structure to data stored in HDFS.
When starting out with Hive, it is convenient to run the metastore on your local machine.
In this configuration, which is the default, the Hive table definitions that you create will be local to your machine, so you can’t share them with other users.
Java 6 is a prerequisite, and on Windows, you will need Cygwin, too.
You also need to have the same version of Hadoop installed locally that your cluster is running.3 Of course, you may choose to run Hadoop locally, either in standalone or pseudodistributed mode, while getting started with Hive.
Generally, Hive works with the latest stable release of Hadoop, as well as supporting a number of older versions, listed in the release notes.
You don’t need to do anything special to tell Hive which version of Hadoop you are using, beyond making sure that the hadoop executable is on the path or setting the HADOOP_HOME environment variable.
Download a release at http://hive.apache.org/releases.html, and unpack the tarball in a suitable place on your workstation:
It is assumed that you have network connectivity from your workstation to the Hadoop cluster.
You can test this before running Hive by installing Hadoop locally and performing some HDFS operations with the hadoop fs command.
The Hive Shell The shell is the primary way that we will interact with Hive, by issuing commands in HiveQL.
It is heavily influenced by MySQL, so if you are familiar with MySQL, you should feel at home using Hive.
When starting Hive for the first time, we can check that it is working by listing its tables —there should be none.
The command must be terminated with a semicolon to tell Hive to execute it:
Like SQL, HiveQL is generally case-insensitive (except for string comparisons), so show tables; works equally well here.
For a fresh install, the command takes a few seconds to run as it lazily creates the metastore database on your machine.
The database stores its files in a directory called metastore_db, which is relative to the location from which you ran the hive command.) You can also run the Hive shell in noninteractive mode.
The -f option runs the commands in the specified file, which is script.q in this example:
For short scripts, you can use the -e option to specify the commands inline, in which case the final semicolon is not required:
You can suppress these messages using the -S option at launch time, which has the effect of showing only the output result for queries:
Other useful Hive shell features include the ability to run commands on the host operating system by using a ! prefix to the command and the ability to access Hadoop filesystems using the dfs command.
An Example Let’s see how to use Hive to run a query on the weather dataset we explored in earlier chapters.
The first step is to load the data into Hive’s managed storage.
Here we’ll have Hive use the local filesystem for storage; later we’ll see how to store tables in HDFS.
Just like an RDBMS, Hive organizes its data into tables.
We create a table to hold the weather data using the CREATE TABLE statement:
The first line declares a records table with three columns: year, temperature, and quality.
Here the year is a string, while the other two columns are integers.
This declaration is saying that each row in the data file is tab-delimited text.
Hive expects there to be three fields in each row, corresponding to the table columns, with fields separated by tabs and rows by newlines.
Running this command tells Hive to put the specified local file in its warehouse directory.
There is no attempt, for example, to parse the file and store it in an internal database format, because Hive does not mandate any particular file format.
Files are stored verbatim; they are not modified by Hive.
In this case, there is only one file, sample.txt, but in general there can be more, and Hive will read all of them when querying the table.
The OVERWRITE keyword in the LOAD DATA statement tells Hive to delete any existing files in the directory for the table.
If it is omitted, the new files are simply added to the table’s directory (unless they have the same names, in which case they replace the old files)
Now that the data is in Hive, we can run a query against it:
It is a SELECT statement with a GROUP BY clause for grouping rows into years, which uses the MAX() aggregate function to find the maximum temperature for each year group.
But the remarkable thing is that Hive transforms this query into a MapReduce job, which it executes on our behalf, then prints the results to the console.
Running Hive In this section, we look at some more practical aspects of running Hive, including how to set up Hive to run against a Hadoop cluster and a shared metastore.
In doing so, we’ll see Hive’s architecture in some detail.
The file is called hive-site.xml and is located in Hive’s conf directory.
This file is where you can set properties that you want to set every time you run Hive.
The same directory contains hivedefault.xml, which documents the properties that Hive exposes and their default values.
You can override the configuration directory that Hive looks for in hive-site.xml by passing the --config option to the hive command:
Note that this option specifies the containing directory, not hive-site.xml itself.
Alternatively, you can set the HIVE_CONF_DIR environment variable to the configuration directory for the same effect.
Hive also permits you to set properties on a per-session basis, by passing the -hiveconf option to the hive command.
For example, the following command sets the cluster (in this case, to a pseudodistributed cluster) for the duration of the session:
If you plan to have more than one Hive user sharing a Hadoop cluster, you need to make the directories that Hive uses writable by all users.
The following commands will create the directories and set their permissions appropriately:
If all users are in the same group, then permissions g+w are sufficient on the warehouse directory.
You can change settings from within a session, too, using the SET command.
This is useful for changing Hive or MapReduce job settings for a particular query.
By itself, SET will list all the properties (and their values) set by Hive.
Note that the list will not include Hadoop defaults, unless they have been explicitly overridden in one of the ways covered in this section.
Use SET -v to list all the properties in the system, including Hadoop defaults.
In the following list, lower numbers take precedence over higher numbers:
It can be very useful when trying to diagnose configuration problems or other types of error.
However, often it’s more convenient to set logging configuration for the session.
For example, the following handy invocation will send debug messages to the console:
Hive Services The Hive shell is only one of several services that you can run using the hive command.
You can specify the service to run using the --service option.
Type hive --service help to get a list of available service names; the most useful are described in the following list.
Runs Hive as a server exposing a Thrift service, enabling access from a range of clients written in different languages.
Applications using the Thrift, JDBC, and ODBC connectors need to run a Hive server to communicate with Hive.
Using this service, it is possible to run the metastore as a standalone (remote) process.
Set the METASTORE_PORT environment variable to specify the port the server will listen on.
The Hive Web Interface (HWI) As an alternative to the shell, you might want to try Hive’s simple web interface.
From there, you can browse Hive database schemas and create sessions for issuing commands and queries.
It’s possible to run the web interface as a shared service to give users within an organization access to Hive without having to install any client software.
If you run Hive as a server (hive --service hiveserver), there are a number of different mechanisms for connecting to it from applications.
The relationship between Hive clients and Hive services is illustrated in Figure 12-1
Thrift Client The Hive Thrift Client makes it easy to run Hive commands from a wide range of programming languages.
They can be found in the src/service/src subdirectory in the Hive distribution.
The driver makes calls to an interface implemented by the Hive Thrift Client using the Java Thrift bindings.) You may alternatively choose to connect to Hive via JDBC in embedded mode using the URI jdbc:hive://
In this mode, Hive runs in the same JVM as the application invoking it, so there is no need to launch it as a standalone server, since it does not use the Thrift service or the Hive Thrift Client.
Like the JDBC driver, the ODBC driver uses Thrift to communicate with the Hive server.) The ODBC driver is still in development, so you should refer to the latest instructions on the Hive wiki for how to build and run it.
The Metastore The metastore is the central repository of Hive metadata.
The metastore is divided into two pieces: a service and the backing store for the data.
By default, the metastore service runs in the same JVM as the Hive service and contains an embedded Derby database instance backed by the local disk.
This is called the embedded metastore configuration (see Figure 12-2)
Using an embedded metastore is a simple way to get started with Hive; however, only one embedded Derby database can access the database files on disk at any one time, which means you can have only one Hive session open at a time that shares the same metastore.
The solution to supporting multiple sessions (and therefore multiple users) is to use a standalone database.
This configuration is referred to as a local metastore, since the metastore service still runs in the same process as the Hive service, but connects to a database running in a separate process, either on the same machine or on a remote.
The username and password should be set, too, of course.) The JDBC driver JAR file for MySQL (Connector/J) must be on Hive’s classpath, which is simply achieved by placing it in Hive’s lib directory.
Going a step further, there’s another metastore configuration called a remote metastore, where one or more metastore servers run in separate processes to the Hive service.
This brings better manageability and security because the database tier can be completely firewalled off, and the clients no longer need the database credentials.
Not set The URIs specifying the remote metastore servers to connect to.
Clients connect in a round-robin fashion when there are multiple remote servers.
Comparison with Traditional Databases Although Hive resembles a traditional database in many ways (such as supporting an SQL interface), its HDFS and MapReduce underpinnings mean that there are a number of architectural differences that directly influence the features that Hive supports, which in turn affects the uses that Hive can be put to.
Schema on Read Versus Schema on Write In a traditional database, a table’s schema is enforced at data load time.
If the data being loaded doesn’t conform to the schema, then it is rejected.
Hive, on the other hand, doesn’t verify the data when it is loaded, but rather when a query is issued.
Schema on read makes for a very fast initial load, since the data does not have to be read, parsed, and serialized to disk in the database’s internal format.
The load operation is just a file copy or move.
It is more flexible, too: consider having two schemas for the same underlying data, depending on the analysis being performed.
The trade-off, however, is that it takes longer to load data into the database.
Furthermore, there are many scenarios where the schema is not known at load time, so there are no indexes to apply, because the queries have not been formulated yet.
Updates, Transactions, and Indexes Updates, transactions, and indexes are mainstays of traditional databases.
Yet, until recently, these features have not been considered a part of Hive’s feature set.
This is because Hive was built to operate over HDFS data using MapReduce, where full-table scans are the norm and a table update is achieved by transforming the data into a new table.
For a data warehousing application that runs over large portions of the dataset, this works well.
Hive doesn’t support updates (or deletes), but it does support INSERT INTO, so it is possible to add new rows to an existing table.
With release 0.7.0, Hive introduced indexes, which can speed up queries in certain cases.
The index implementation was designed to be pluggable, so it’s expected that a variety of implementations will emerge for different use cases.) Compact indexes store the HDFS block numbers of each value, rather than each file offset, so they don’t take up much disk space but are still effective for the case where values are clustered together in nearby rows.
Bitmap indexes use compressed bitsets to efficiently store the rows that a particular value appears in, and they are usually appropriate for low-cardinality columns (such as gender or country)
Release 0.7.0 also saw the introduction of table- and partition-level locking in Hive.
Locks prevent, for example, one process from dropping a table while another is reading from it.
Locks are managed transparently using ZooKeeper, so the user doesn’t have.
HBase (Chapter 13) has different storage characteristics than HDFS, such as the ability to do row updates and column indexing, so we can expect to see these features used by Hive in future releases.
Rather, as an open source project, features are added by developers to meet their users’ needs, resulting in that Hive’s SQL support becoming richer over time.
Furthermore, Hive has some extensions that are not in SQL-92, inspired by syntax from other database systems, notably MySQL.
In fact, to a first-order approximation, HiveQL most closely resembles MySQL’s SQL dialect.
Instead, we focus on commonly used features and pay particular attention to features that diverge from either SQL-92 or popular databases such as MySQL.
Table 12-2 provides a high-level comparison of SQL and HiveQL.
Data types Integral, floating-point, fixedpoint, text and binary strings, temporal.
Integral, floating-point, Boolean, text and binary strings, timestamp, array, map, struct.
Select SQL-92 Single table or view in the FROM clause; SORT BY for partial ordering.
Joins SQL-92 or variants (join tables in the FROM clause, join condition in the WHERE clause)
Inner joins, outer joins; semi joins, map joins (SQL-92 syntax, with hinting)
Data Types Hive supports both primitive and complex data types.
For a particular map, the keys must be the same type, and the values must be the same type.
That is, array(), map(), and struct() are built-in Hive functions.
Hive’s primitive types correspond roughly to Java’s, although some names are influenced by MySQL’s type names (some of which, in turn, overlap with SQL-92)
Unlike some databases, there is no option to control the number of significant digits or decimal places stored for floatingpoint values.
Hive supports a BOOLEAN type for storing true and false values.
There is a single Hive data type for storing text, STRING, which is a variable-length character string.
Hive’s STRING type is like VARCHAR in other databases, although there is no declaration of the maximum number of characters to store with STRING.
The theoretical maximum-size STRING that may be stored is 2GB, although in practice it may be inefficient to materialize such large values.
Hive comes with UDFs for converting between Hive timestamps, Unix timestamps (seconds since the Unix epoch), and strings, which makes most common date operations tractable.
Complex type declarations must specify the type of the fields in the collection, using an angled bracket notation, as illustrated in this table definition with three columns, one for each complex type:
Use the concat function for the latter in both MySQL and Hive.
You can retrieve a list of functions from the Hive shell by typing SHOW FUNCTIONS.5 To get brief usage instructions for a particular function, use the DESCRIBE command:
Primitive types form a hierarchy that dictates the implicit type conversions Hive will perform in function and operator expressions.
For example, a TINYINT will be converted to an INT if an expression expects an INT; however, the reverse conversion will not occur, and Hive will return an error unless the CAST operator is used.
Any integral numeric type can be implicitly converted to a wider type.
All the integral numeric types, FLOAT, and (perhaps surprisingly) STRING can be implicitly converted to DOUBLE.
Tables A Hive table is logically made up of the data being stored and the associated metadata describing the layout of the data in the table.
The data typically resides in HDFS, although it may reside in any Hadoop filesystem, including the local filesystem or S3
In this section, we look in more detail at how to create tables, the different physical storage formats that Hive offers, and how to import data into them.
Multiple Database/Schema Support Many relational databases have a facility for multiple namespaces, which allow users and applications to be segregated into different databases or schemas.
Hive supports the same facility and provides commands such as CREATE DATABASE dbname, USE dbname, and DROP DATABASE dbname.
If no database is specified, tables belong to the default database.
Managed Tables and External Tables When you create a table in Hive, by default Hive will manage the data, which means that Hive moves the data into its warehouse directory.
The difference between the two table types is seen in the LOAD and DROP semantics.
When you load data into a managed table, it is moved into Hive’s warehouse directory.
The load operation is very fast because it is just a move or rename within a filesystem.
However, bear in mind that Hive does not check that the files in the table directory conform to the schema declared for the table, even for managed tables.
If there is a mismatch, this will become apparent at query time, often by the query returning NULL for a missing field.
You can check that the data is being parsed correctly by issuing a simple SELECT statement to retrieve a few rows directly from the table.
If the table is later dropped, using: DROP TABLE managed_table;
It bears repeating that since the initial LOAD performed a move operation, and the DROP performed a delete operation, the data no longer exists anywhere.
This is what it means for Hive to manage the data.
The location of the external data is specified at table creation time:
With the EXTERNAL keyword, Hive knows that it is not managing the data, so it doesn’t move it to its warehouse directory.
Indeed, it doesn’t even check whether the external location exists at the time it is defined.
This is a useful feature because it means you can create the data lazily after creating the table.
When you drop an external table, Hive will leave the data untouched and only delete the metadata.
The move will succeed only if the source and target filesystems are the same.
Also, there is a special case when the LOCAL keyword is used, where Hive will copy the data from the local filesystem into Hive’s warehouse directory (even if it, too, is on the same local filesystem)
In all other cases, though, LOAD is a move operation and is best thought of as such.
So how do you choose which type of table to use? In most cases, there is not much difference between the two (except of course for the difference in DROP semantics), so it is a just a matter of preference.
As a rule of thumb, if you are doing all your processing with Hive, then use managed tables, but if you wish to use Hive and other tools on the same dataset, then use external tables.
A common pattern is to use an external table to access an initial dataset stored in HDFS (created by another process), then use a Hive transform to move the data into a managed Hive table.
This works the other way around, too; an external table (not necessarily on HDFS) can be used to export data from Hive for other applications to use.7 Another reason for using external tables is when you wish to associate multiple schemas with the same dataset.
Partitions and Buckets Hive organizes tables into partitions, a way of dividing a table into coarse-grained parts based on the value of a partition column, such as a date.
Using partitions can make it faster to do queries on slices of the data.
Tables or partitions may be subdivided further into buckets to give extra structure to the data that may be used for more efficient queries.
For example, bucketing by user ID means we can quickly evaluate a user-based query by running it on a randomized sample of the total set of users.
To take an example where partitions are commonly used, imagine logfiles where each record includes a timestamp.
If we partitioned by date, then records for the same date would be stored in the same partition.
The advantage to this scheme is that queries that are restricted to a particular date or set of dates can be answered much more efficiently because they only need to scan the files in the partitions that the query pertains to.
Notice that partitioning doesn’t preclude more wide-ranging queries: it is still feasible to query the entire dataset across many partitions.
For example, in addition to partitioning logs by date, we might also subpartition each date partition by country to permit efficient queries by location.
Partitions are defined at table creation time using the PARTITIONED BY clause,8 which takes a list of column definitions.
For the hypothetical logfiles example, we might define a table with records comprising a timestamp and the log line itself:
However, partitions may be added to or removed from a table after creation using an ALTER TABLE statement.
At the filesystem level, partitions are simply nested subdirectories of the table directory.
After loading a few more files into the logs table, the directory structure might look like this:
We can ask Hive for the partitions in a table using SHOW PARTITIONS:
One thing to bear in mind is that the column definitions in the PARTITIONED BY clause are full-fledged table columns, called partition columns; however, the datafiles do not contain values for these columns, since they are derived from the directory names.
You can use partition columns in SELECT statements in the usual way.
Hive performs input pruning to scan only the relevant partitions.
Notice, too, that the query returns the values of the dt partition column, which Hive reads from the directory names since they are not in the datafiles.
There are two reasons why you might want to organize your tables (or partitions) into buckets.
Bucketing imposes extra structure on the table, which Hive can take advantage of when performing certain queries.
The second reason to bucket a table is to make sampling more efficient.
When working with large datasets, it is very convenient to try out queries on a fraction of your dataset while you are in the process of developing or refining them.
We will see how to do efficient sampling at this end of this section.
First, let’s see how to tell Hive that a table should be bucketed.
We use the CLUSTERED BY clause to specify the columns to bucket on and the number of buckets:
Here we are using the user ID to determine the bucket (which Hive does by hashing the value and reducing modulo the number of buckets), so any particular bucket will effectively have a random set of users in it.
In the map-side join case, where the two tables are bucketed in the same way, a mapper processing a bucket of the left table knows that the matching rows in the right table are in its corresponding bucket, so it need only retrieve that bucket (which is a small fraction of all the data stored in the right table) to effect the join.
This optimization also works when the number of buckets in the two tables are multiples of each other; they do not have to have exactly the same number of buckets.
The data within a bucket may additionally be sorted by one or more columns.
This allows even more efficient map-side joins, since the join of each bucket becomes an efficient merge-sort.
The syntax for declaring that a table has sorted buckets is:
How can we make sure the data in our table is bucketed? Although it’s possible to load data generated outside Hive into a bucketed table, it’s often easier to get Hive to do the bucketing, usually from an existing table.
Hive does not check that the buckets in the datafiles on disk are consistent with the buckets in the table definition (either in number or on the basis of bucketing columns)
If there is a mismatch, you may get an error or undefined behavior at query time.
For this reason, it is advisable to get Hive to perform the bucketing.
Then it is a matter of just using the INSERT command:
Physically, each bucket is just a file in the table (or partition) directory.
The filename is not important, but bucket n is the nth file when arranged in lexicographic order.
In fact, buckets correspond to MapReduce output file partitions: a job will produce as many buckets (output files) as reduce tasks.
We can see this by looking at the layout of the bucketed_users table we just created.
We can see the same thing by sampling the table using the TABLESAMPLE clause, which restricts the query to a fraction of the buckets in the table rather than the whole table:
The fields appear to run together when displaying the raw file because the separator character in the output is a nonprinting control character.
The control characters used are explained in the next section.
Bucket numbering is 1-based, so this query retrieves all the users from the first of four buckets.
For a large, evenly distributed dataset, approximately one quarter of the table’s rows would be returned.
It’s possible to sample a number of buckets by specifying a different proportion (which need not be an exact multiple of the number of buckets, as sampling is not intended to be a precise operation)
Sampling a bucketed table is very efficient because the query only has to read the buckets that match the TABLESAMPLE clause.
Contrast this with sampling a nonbucketed table using the rand() function, where the whole input dataset is scanned, even if only a very small sample is needed:
Storage Formats There are two dimensions that govern table storage in Hive: the row format and the file format.
The row format dictates how rows, and the fields in a particular row, are stored.
When acting as a deserializer, which is the case when querying a table, a SerDe will deserialize a row of data from the bytes in the file to objects used internally by Hive to operate on that row of data.
The file format dictates the container format for fields in a row.
The simplest format is a plain-text file, but there are row-oriented and column-oriented binary formats available, too.
When you create a table with no ROW FORMAT or STORED AS clauses, the default format is delimited text with one row per line.
The default row delimiter is not a tab character, but the Control-A character from the set of ASCII control codes (it has ASCII code 1)
The choice of Control-A, sometimes written as ^A in documentation, came about because it is less likely to be a part of the.
There is no means for escaping delimiter characters in Hive, so it is important to choose ones that don’t occur in data fields.
The default collection item delimiter is a Control-B character, used to delimit items in an ARRAY or STRUCT, or in key-value pairs in a MAP.
The default map key delimiter is a Control-C character, used to delimit the key and value in a MAP.
Rows in a table are delimited by a newline character.
The preceding description of delimiters is correct for the usual case of flat data structures, where the complex types contain only primitive types.
For nested types, however, this isn’t the whole story, and in fact the level of the nesting determines the delimiter.
For an array of arrays, for example, the delimiters for the outer array are Control-B characters, as expected, but for the inner array they are Control-C characters, the next delimiter in the list.
If you are unsure which delimiters Hive uses for a particular nested structure, you can run a command like:
Notice that the octal form of the delimiter characters can be used—001 for Control-A, for instance.
Internally, Hive uses a SerDe called LazySimpleSerDe for this delimited format, along with the line-oriented MapReduce text input and output formats we saw in Chapter 7
However, it is not a compact format because fields are stored in a verbose textual format, so a Boolean value, for instance, is written as the literal string true or false.
The simplicity of the format has a lot going for it, such as making it easy to process with other tools, including MapReduce programs or Streaming, but there are more compact and performant binary SerDes that you might consider using.
Binary SerDes should not be used with the default TEXTFILE format (nor explicitly using a STORED AS TEXTFILE clause)
There is always the possibility that a binary row will contain a newline character, which would cause Hive to truncate the row and fail at deserialization time.
A binary SerDe like LazyBinarySerDe, but optimized for sorting at the expense of compactness (although it is still significantly more compact than LazySimpleSerDe)
A SerDe for reading textual data where columns are specified by a regular expression.
Useful for reading logfiles, but inefficient, so not suitable for general-purpose storage.
HBase storage uses a Hive storage handler, which unifies (and generalizes) the roles of row format and file format.
Storage handlers are specified using a STORED BY clause, which replaces the ROW FORMAT and STORED AS clauses.
You can use sequence files in Hive by using the declaration STORED AS SEQUENCEFILE in the CREATE TABLE statement.
One of the main benefits of using sequence files is their support for splittable compression.
If you have a collection of sequence files that were created outside Hive, then Hive will read them with no extra configuration.
This means that the fields in each row are stored together as the contents of a single sequence-file record.
Hive provides another binary storage format called RCFile, short for Record Columnar File.
RCFiles are similar to sequence files, except that they store data in a columnoriented fashion.
RCFile breaks up the table into row splits, then within each split stores the values for each row in the first column, followed by the values for each row in the second column, and so on.
A column-oriented layout permits columns that are not accessed in a query to be skipped.
With row-oriented storage, like a sequence file, the whole row (stored in a sequence file record) is loaded into memory, even though only the second column is actually read.
Lazy deserialization saves some processing cycles by deserializing only the column fields that are accessed, but it can’t avoid the cost of reading each row’s bytes from disk.
With column-oriented storage, only the column 2 parts of the file (shaded in the figure) need to be read into memory.
In general, column-oriented formats work well when queries access only a small number of columns in the table.
Conversely, row-oriented formats are appropriate when a large number of columns of a single row are needed for processing at the same time.
Use the following CREATE TABLE clauses to enable column-oriented storage in Hive:
We’ll use a contrib SerDe that uses a regular expression for reading the fixed-width station metadata from a text file:
In previous examples, we have used the DELIMITED keyword to refer to delimited text in the ROW FORMAT clause.
SerDes can be configured with extra properties using the WITH SERDEPROPERTIES clause.
Here we set the input.regex property, which is specific to RegexSerDe.
To populate the table, we use a LOAD DATA statement as before:
Recall that LOAD DATA copies or moves the files to Hive’s warehouse directory (in this case, it’s a copy because the source is the local filesystem)
The table’s SerDe is not used for the load operation.
When we retrieve data from the table, the SerDe is invoked for deserialization, as we can see from this simple query, which correctly parses the fields for each row:
The solution is to use a noncapturing group, which has a ? character after the first parenthesis.
There are various noncapturing group constructs (see the Java documentation), but in this example we could use (?:ab)+ to avoid capturing the group as a Hive column.
Importing Data We’ve already seen how to use the LOAD DATA operation to import data into a Hive table (or partition) by copying or moving files to the table’s directory.
You can also populate a table with data from another Hive table using an INSERT statement, or at creation time using the CTAS construct, which is an abbreviation used to refer to CREATE TABLE...AS SELECT.
For partitioned tables, you can specify the partition to insert into by supplying a PARTITION clause:
The OVERWRITE keyword means that the contents of the target table (for the first example) or the 2001-01-01 partition (for the second example) are replaced by the results of the SELECT statement.
If you want to add records to an already-populated nonpartitioned table or partition, use INSERT INTO TABLE.
You can specify the partition dynamically by determining the partition value from the SELECT statement:
Unlike other databases, Hive does not (currently) support a form of the INSERT statement for inserting a collection of records specified in the query in literal form.
In HiveQL, you can turn the INSERT statement around and start with the FROM clause for the same effect:
The reason for this syntax becomes clear when you see that it’s possible to have multiple INSERT clauses in the same query.
This so-called multitable insert is more efficient than multiple INSERT statements because the source table needs to be scanned only once to produce the multiple, disjoint outputs.
Here’s an example that computes various statistics over the weather dataset:
There is a single source table (records2), but three tables to hold the results from three different queries over the source.
It’s often very convenient to store the output of a Hive query in a new table, perhaps because it is too large to be dumped to the console or because there are further processing steps to carry out on the result.
The new table’s column definitions are derived from the columns retrieved by the SELECT clause.
A CTAS operation is atomic, so if the SELECT query fails for some reason, the table is not created.
Altering Tables Because Hive uses the schema on read approach, it’s flexible in permitting a table’s definition to change after the table has been created.
The general caveat, however, is that in many cases, it is up to you to ensure that the data is changed to reflect the new structure.
You can rename a table using the ALTER TABLE statement:
In addition to updating the table metadata, ALTER TABLE moves the underlying table directory so that it reflects the new name.
An external table’s underlying directory is not moved; only the metadata is updated.) Hive allows you to change the definition for columns, add new columns, or even replace all existing columns in a table with a new set.
The new column col3 is added after the existing (nonpartition) columns.
The datafiles are not updated, so queries will return null for all values of col3 (unless of course there were extra fields already present in the files)
Because Hive does not permit updating existing records, you will need to arrange for the underlying files to be updated by another mechanism.
For this reason, it is more common to create a new table that defines new columns and populates them using a SELECT statement.
Changing a column’s metadata, such as a column’s name or data type, is more straightforward, assuming that the old data type can be interpreted as the new data type.
Dropping Tables The DROP TABLE statement deletes the data and metadata for a table.
In the case of external tables, only the metadata is deleted; the data is left untouched.
If you want to delete all the data in a table but keep the table definition (like DELETE or TRUNCATE in MySQL), you can simply delete the datafiles.
Hive treats a lack of files (or indeed no directory for the table) as an empty table.
Another possibility, which achieves a similar effect, is to create a new, empty table that has the same schema as the first, using the LIKE keyword:
Querying Data This section discusses how to use various forms of the SELECT statement to retrieve data from Hive.
Sorting and Aggregating Sorting data in Hive can be achieved by using a standard ORDER BY clause, but there is a catch.
In some cases, you want to control which reducer a particular row goes to, typically so you can perform some subsequent aggregation.
Here’s an example to sort the weather dataset by year and temperature, in such a way to ensure that all the rows for a given year end up in the same reducer partition:11
If the columns for SORT BY and DISTRIBUTE BY are the same, you can use CLUSTER BY as a shorthand for specifying both.
MapReduce Scripts Using an approach like Hadoop Streaming, the TRANSFORM, MAP, and REDUCE clauses make it possible to invoke an external script or program from Hive.
Suppose we want to use a script to filter out rows that don’t meet some condition, such as the script in Example 12-1, which removes poor-quality readings.
Python script to filter out poor-quality weather records #!/usr/bin/env python.
Before running the query, we need to register the script with Hive.
If we use a nested form for the query, we can specify a map and a reduce function.
This time we use the MAP and REDUCE keywords, but SELECT TRANSFORM in both cases would have the same result.
Joins One of the nice things about using Hive, rather than raw MapReduce, is that Hive makes performing commonly used operations very simple.
The simplest kind of join is the inner join, where each match in the input tables results in a row in the output.
Consider two small demonstration tables: sales, which lists the names of people and the ID of the item they bought; and things, which lists the item ID and its name:
The table in the FROM clause (sales) is joined with the table in the JOIN clause (things), using the predicate in the ON clause.
Hive only supports equijoins, which means that only equality can be used in the join predicate, which here matches on the id column in both tables.
Some databases, such as MySQL and Oracle, allow you to list the join tables in the FROM clause and specify the join condition in the WHERE clause of a SELECT statement.
However, this syntax is not supported in Hive, so the following fails with a parse error:
Hive allows only a single table in the FROM clause, and joins must follow the SQL-92 JOIN clause syntax.
In Hive, you can join on multiple columns in the join predicate by specifying a series of expressions, separated by AND keywords.
Hive is intelligent about trying to minimize the number of MapReduce jobs to perform the joins.
A single join is implemented as a single MapReduce job, but multiple joins can be performed in less than one MapReduce job per join if the same column is used in the join condition.12 You can see how many MapReduce jobs Hive will use for any particular query by prefixing it with the EXPLAIN keyword:
The EXPLAIN output includes many details about the execution plan for the query, including the abstract syntax tree, the dependency graph for the stages that Hive will execute, and information about each stage.
Stages may be MapReduce jobs or operations such as file moves.
For even more detail, prefix the query with EXPLAIN EXTENDED.
Hive currently uses a rule-based query optimizer for determining how to execute a query, but it’s likely that in the future a cost-based optimizer will be added.
Outer joins allow you to find nonmatches in the tables being joined.
In the current example, when we performed an inner join, the row for Ali did not appear in the output, because the ID of the item she purchased was not present in the things table.
If we change the join type to LEFT OUTER JOIN, the query will return a row for every row in the left table (sales), even if there is no corresponding row in the table it is being joined to (things):
The order of the tables in the JOIN clauses is significant.
Notice that the row for Ali is now returned, and the columns from the things table are NULL because there is no match.
Hive supports right outer joins, which reverses the roles of the tables relative to the left join.
In this case, all items from the things table are included, even those that weren’t purchased by anyone (a scarf):
Finally, there is a full outer join, where the output has a row for each row from both tables in the join:
Hive doesn’t support IN subqueries (at the time of this writing), but you can use a LEFT SEMI JOIN to do the same thing.
Consider this IN subquery, which finds all the items in the things table that are in the sales table:
There is a restriction that we must observe for LEFT SEMI JOIN queries: the right table (sales) may appear only in the ON clause.
It cannot be referenced in a SELECT expression, for example.
If one table is small enough to fit in memory, Hive can load the smaller table into memory to perform the join in each of the mappers.
The syntax for specifying a map join is a hint embedded in an SQL C-style comment:
The job to execute this query has no reducers, so this query would not work for a RIGHT or FULL OUTER JOIN, since absence of matching can be detected only in an aggregating (reduce) step across all the inputs.
The syntax for the join is the same as for the in-memory case shown earlier; however, you also need to enable the optimization with the following:
Subqueries A subquery is a SELECT statement that is embedded in another SQL statement.
Hive has limited support for subqueries, permitting a subquery only in the FROM clause of a SELECT statement.
Other databases allow subqueries almost anywhere that an expression is valid, such as in the list of values to retrieve from a SELECT statement or in the WHERE clause.
Many uses of subqueries can be rewritten as joins, so if you find yourself writing a subquery where Hive does not support it, see whether it can be expressed as a join.
The following query finds the mean maximum temperature for every year and weather station:
The subquery is used to find the maximum temperature for each station/date combination, and then the outer query uses the AVG aggregate function to find the average of the maximum temperature readings for each station/date combination.
The outer query accesses the results of the subquery like it does a table, which is why the subquery must be given an alias (mt)
The columns of the subquery have to be given unique names so that the outer query can refer to them.
Views can be used to present data to users in a way that differs from the way it is actually stored on disk.
Often, the data from existing tables is simplified or aggregated in a particular way that makes it convenient for further processing.
Views may also be used to restrict users’ access to particular subsets of tables that they are authorized to see.
In Hive, a view is not materialized to disk when it is created; rather, the view’s SELECT statement is executed when the statement that refers to the view is run.
We can use views to rework the query from the previous section for finding the mean maximum temperature for every year and weather station.
First, let’s create a view for valid records, that is, records that have a particular quality value:
When we create a view, the query is not run; it is simply stored in the metastore.
Views are included in the output of the SHOW TABLES command, and you can see more details about a particular view, including the query used to define it, by issuing the DESCRIBE EXTENDED view_name command.
Next, let’s create a second view of maximum temperatures for each station and year.
In this view definition, we list the column names explicitly.
We do this because the maximum temperature column is an aggregate expression, and otherwise Hive would.
We could equally well have used an AS clause in the SELECT to name the column.
With the views in place, we can now use them by running a query:
The result of the query is the same as running the one that uses a subquery, and in particular, Hive creates the same number of MapReduce jobs for both: two in each case, one for each GROUP BY.
This example shows that Hive can combine a query on a view into a sequence of jobs that is equivalent to writing the query without using a view.
In other words, Hive won’t needlessly materialize a view, even at execution time.
Views in Hive are read-only, so there is no way to load or insert data into an underlying base table via a view.
User-Defined Functions Sometimes the query you want to write can’t be expressed easily (or at all) using the built-in functions that Hive provides.
By writing a user-defined function (UDF), Hive makes it easy to plug in your own processing code and invoke it from a Hive query.
UDFs have to be written in Java, the language that Hive itself is written in.
There are three types of UDF in Hive: (regular) UDFs, user-defined aggregate functions (UDAFs), and user-defined table-generating functions (UDTFs)
They differ in the numbers of rows that they accept as input and produce as output:
A UDF operates on a single row and produces a single row as its output.
Most functions, such as mathematical functions and string functions, are of this type.
A UDAF works on multiple input rows and creates a single output row.
Table-generating functions are less well known than the other two types, so let’s look at an example.
Consider a table with a single column, x, which contains arrays of strings.
It’s instructive to take a slight detour to see how the table is defined and populated:
Notice that the ROW FORMAT clause specifies that the entries in the array are delimited by Control-B characters.
The example file that we are going to load has the following contents, where ^ B is a representation of the Control-B character to make it suitable for printing:
After running a LOAD DATA command, the following query confirms that the data was loaded correctly:
Next, we can use the explode UDTF to transform this table.
This function emits a row for each entry in the array, so in this case the type of the output column y is STRING.
The result is that the table is flattened into five rows:
For this reason, Hive supports LATERAL VIEW queries, which are more powerful.
Writing a UDF To illustrate the process of writing and using a UDF, we’ll write a simple UDF to trim characters from the ends of strings.
Hive already has a built-in function called trim, so we’ll call ours strip.
The code for the Strip Java class is shown in Example 12-3
The evaluate() method is not defined by an interface, since it may take an arbitrary number of arguments, of arbitrary types, and it may return a value of arbitrary type.
Hive introspects the UDF to find the evaluate() method that matches the Hive function that was invoked.
The first strips leading and trailing whitespace from the input, and the second can strip any of a set of supplied characters from the ends of the string.
The actual string processing is delegated to the StringUtils class from the Apache Commons project, which makes the only noteworthy part of the code the use of Text from the Hadoop Writable library.
However, by using Text, we can take advantage of object reuse, which can bring efficiency savings, and so is preferred in general.
To use the UDF in Hive, we need to package the compiled Java class in a JAR file (you can do this by typing ant hive with the book’s example code) and register the file with Hive:
The TEMPORARY keyword here highlights the fact that UDFs are defined only for the duration of the Hive session (they are not persisted in the metastore)
In practice, this means you need to add the JAR file and either define the function at the beginning of each script, or create a .hiverc file in your home directory containing these commands, and the UDF will be running automatically at the beginning of each Hive session.
As an alternative to calling ADD JAR at launch time, you can specify a path where Hive looks for auxiliary JAR files to put on its classpath (including the MapReduce classpath)
This technique is useful for automatically adding your own library of UDFs every time you run Hive.
The auxiliary path may be a comma-separated list of JAR file paths or a directory containing JAR files.
Notice that the UDF’s name is not case-sensitive: hive> SELECT STRIP('  bee  ') FROM dummy; bee.
Writing a UDAF An aggregate function is more difficult to write than a regular UDF.
Values are aggregated in chunks (potentially across many map or reduce tasks), so the implementation has to be capable of combining partial aggregations into a final result.
The code to achieve this is best explained by example, so let’s look at the implementation of a simple UDAF for calculating the maximum of a collection of integers (Example 12-4)
The class structure is slightly different from the one for UDFs.
An evaluator must implement five methods, described in turn here (the flow is illustrated in Figure 12-4): init()
The init() method initializes the evaluator and resets its internal state.
We use null to indicate that no values have been aggregated yet, which has the desirable effect of making the maximum value of an empty set NULL.
The evaluator should update its internal state with the result of performing the aggregation.
The arguments that iterate() takes correspond to those in the Hive function from which it was called.
The value is first checked to see whether it is null, and if it is, it is ignored.
Otherwise, the result instance variable is set to value’s integer value (if this is the first value that has been seen) or set to the larger of the current result and value (if one.
We return true to indicate that the input value was valid.
The method must return an object that encapsulates the state of the aggregation.
In this case, an IntWritable suffices because it encapsulates either the maximum value seen or null if no values have been processed.
The method takes a single object whose type must correspond to the return type of the terminatePartial() method.
In this example, the merge() method can simply delegate to the iterate() method because the partial aggregation is represented in the same way as a value being aggregated.
This is not generally the case (and we’ll see a more general example later), and the method should implement the logic to combine the evaluator’s state with the state of the partial aggregation.
The previous example is unusual in that a partial aggregation can be represented using the same type (IntWritable) as the final result.
This is not generally the case for more complex aggregate functions, as can be seen by considering a UDAF for calculating the mean (average) of a collection of double values.
Instead, we can represent the partial aggregation as a pair of numbers: the cumulative sum of the double values processed so far, and the number of values.
This idea is implemented in the UDAF shown in Example 12-5
Notice that the partial aggregation is implemented as a “struct” nested static class, called PartialResult, which Hive is intelligent enough to serialize and deserialize, since we are using field types that Hive can handle (Java primitives in this case)
In this example, the merge() method is different from iterate() because it combines the partial sums and partial counts by pairwise addition.
HBasics HBase is a distributed column-oriented database built on top of HDFS.
HBase is the Hadoop application to use when you require real-time read/write random access to very large datasets.
Many vendors offer replication and partitioning solutions to grow the database beyond the confines of a single node, but these add-ons are generally an afterthought and are complicated to install and maintain.
Joins, complex queries, triggers, views, and foreign-key constraints become prohibitively expensive to run on a scaled RDBMS or do not work at all.
HBase comes at the scaling problem from the opposite direction.
It is built from the ground up to scale linearly just by adding nodes.
HBase is not relational and does not support SQL, but given the proper problem space, it is able to do what an RDBMS cannot: host very large, sparsely populated tables on clusters made from commodity hardware.
The canonical HBase use case is the webtable, a table of crawled web pages and their attributes (such as language and MIME type) keyed by the web page URL.
The webtable is large, with row counts that run into the billions.
Batch analytic and parsing MapReduce jobs are continuously run against the webtable, deriving statistics and adding new columns of verified MIME-type and parsed-text content for later indexing by a search engine.
Concurrently, the table is randomly accessed by crawlers running at various rates and updating random rows while random web pages are served in real time as users click on a website’s cached-page feature.
In this chapter we give an introduction to using HBase.
In February 2007, Mike Cafarella made a code drop of a mostly working system that Jim Kellerman then carried forward.
Concepts In this section, we provide a quick overview of core HBase concepts.
At a minimum, a passing familiarity will ease the digestion of all that follows.1
By default, their version is a timestamp auto-assigned by HBase at the time of cell insertion.
Table row keys are also byte arrays, so theoretically anything can serve as a row key, from strings to binary representations of long or even serialized data structures.
Table rows are sorted by row key, the table’s primary key.
All table accesses are via the table primary key.2 Row columns are grouped into column families.
The qualifying tail, the column family qualifier, can be made of any arbitrary bytes.
For more detail than is provided here, see the HBase Architecture page on the HBase wiki.
As of this writing, there are at least two projects up on github that add secondary indices to HBase.
A table’s column families must be specified up front as part of the table schema definition, but new column family members can be added on demand.
For example, a new column station:address can be offered by a client as part of an update, and its value persisted, as long as the column family station is already in existence on the targeted table.
Physically, all column family members are stored together on the filesystem.
Because tunings and storage specifications are done at the column-family level, it is advised that all column family members have the same general access pattern and size characteristics.
In synopsis, HBase tables are like those in an RDBMS, only cells are versioned, rows are sorted, and columns can be added on the fly by the client as long as the column family they belong to preexists.
A region is denoted by the table it belongs to, its first row, inclusive, and last row, exclusive.
Initially, a table comprises a single region, but as the size of the region grows, after it crosses a configurable size threshold, it splits at a row boundary into two new regions of approximately equal size.
Until this first split happens, all loading will be against the single server hosting the original region.
As the table grows, the number of its regions grows.
Regions are the units that get distributed over an HBase cluster.
In this way, a table that is too big for any one server can be carried by a cluster of servers, with each node hosting a subset of the table’s total regions.
This is also the means by which the loading on a table gets distributed.
The online set of sorted regions comprises the table’s total content.
Row updates are atomic, no matter how many row columns constitute the row-level transaction.
The HBase master is responsible for bootstrapping a virgin install, for assigning regions to registered regionservers, and for recovering regionserver failures.
The regionservers carry zero or more regions and field client read/write requests.
They also manage region splits, informing the HBase master about the new daughter regions for it to manage the offlining of parent region and assignment of the replacement daughters.
HBase depends on ZooKeeper (Chapter 14), and by default it manages a ZooKeeper instance as the authority on cluster state.
HBase hosts vitals such as the location of the root catalog table and the address of the current cluster master.
Assignment of regions is mediated via ZooKeeper in case participating servers crash mid-assignment.
Hosting the assignment transaction state in ZooKeeper makes it so recovery can pick up on the assignment where the crashed server left off.
At a minimum, when bootstrapping a client connection to an HBase cluster, the client must be passed the location of the ZooKeeper ensemble.
Thereafter, the client navigates the ZooKeeper hierarchy to learn cluster attributes such as server locations.4 Regionserver slave nodes are listed in the HBase conf/regionservers file as you would list datanodes and tasktrackers in the Hadoop conf/slaves file.
Start and stop scripts are like those in Hadoop and use the same SSH-based mechanism for running remote commands.
HBase can be configured to use an existing ZooKeeper cluster instead.
Where there is commonality to be found, HBase directly uses or subclasses the parent Hadoop implementation, whether this is a service or type.
When this is not possible, HBase will follow the Hadoop model where it can.
For example, HBase uses the Hadoop Configuration system so configuration files have the same format.
What this means for you, the user, is that you can leverage any Hadoop familiarity in your exploration of HBase.
HBase deviates from this rule only when adding its specializations.
Most people using HBase run it on HDFS for storage, though by default, and unless told otherwise, HBase writes to the local filesystem.
The local filesystem is fine for experimenting with your initial HBase install, but thereafter, the first configuration made in an HBase cluster usually involves pointing HBase at the HDFS cluster it should use.
Internally, HBase keeps special catalog tables named -ROOT- and .META., within which it maintains the current list, state, and location of all regions afloat on the cluster.
Entries in these tables are keyed by region name, where a region name is made of the table name the region belongs to, the region’s start row, its time of creation, and finally, an MD5 hash of all of the former (i.e., a hash of tablename, start row, and creation timestamp).5 As noted previously, row keys are sorted, so finding the region that hosts a particular row is a matter of a lookup to find the first entry whose key is greater than or equal to that of the requested row key.
Fresh clients connect to the ZooKeeper cluster first to learn the location of -ROOT-
Clients consult -ROOT- to elicit the location of the .META.
The client then does a lookup against the found .META.
To save on having to make three round-trips per row operation, clients cache all they learn while traversing -ROOT- and .META.
A comma delimits the table name, start row, and timestamp.
Clients continue to use the cached entry as they work until there is a fault.
Writes arriving at a regionserver are first appended to a commit log and then are added to an in-memory memstore.
When a memstore fills, its content is flushed to the filesystem.
The commit log is hosted on HDFS, so it remains available through a regionserver crash.
When the master notices that a regionserver is no longer reachable, usually because the servers’s znode has expired in ZooKeeper, it splits the dead regionserver’s commit log by region.
On reassignment and before they were open for business, regions that were on the dead regionserver will pick up their just-split file of not-yet-persisted edits and replay them to bring themselves up-to-date with the state they had just before the failure.
If sufficient versions are found reading memstore alone, the query completes there.
Otherwise, flush files are consulted in order, from newest to oldest, either until versions sufficient to satisfy the query are found or until we run out of flush files.
A background process compacts flush files once their number has broached a threshold, rewriting many files as one, because the fewer files a read consults, the more performant it will be.
On compaction, the process cleans out versions beyond the schema-configured maximum, deletes, and expired cells.
A separate process running in the regionserver monitors flush file sizes, splitting the region when they grow in excess of the configured maximum.
Installation Download a stable release from an Apache Download Mirror and unpack it on your local filesystem.
As with Hadoop, you first need to tell HBase where Java is located on your system.
If you have the JAVA_HOME environment variable set to point to a suitable Java installation, then that will be used, and you don’t have to configure anything further.
For convenience, add the HBase binary directory to your command-line path.
Test Drive To start a temporary instance of HBase that uses the /tmp directory on the local filesystem for persistence, type:
This will bring up a JRuby IRB interpreter that has had some HBase-specific commands added to it.
Type help and then press RETURN to see the list of shell commands grouped into categories.
Type help COMMAND_GROUP for help by category or help COMMAND for help on a specific command and example usage.
See the end of the main help screen for a quick tutorial.
Now let us create a simple table, add some data, and then clean up.
To create a table, you must name your table and define its schema.
A table’s schema comprises table attributes and the list of table column families.
Column families themselves have attributes that you in turn set at schema definition time.
Examples of column family attributes include whether the family content should be compressed on the filesystem and how many versions of a cell to keep.
Schemas can be edited later by offlining the table using the shell disable command, making the necessary alterations using alter, then putting the table back online with enable.
To create a table named test with a single column family named data using defaults for table and column family attributes, enter:
See the help output for examples of adding table and column family attributes when specifying a schema.
To prove the new table was created successfully, run the list command.
To insert data into three different rows and columns in the data column family, and then list the table content, do the following:
Notice how we added three new columns without changing the schema.
To remove the table, you must first disable it before dropping it:
To learn how to set up a distributed HBase and point it at a running HDFS, see the Getting Started section of the HBase documentation.
Clients There are a number of client options for interacting with an HBase cluster.
For the sake of brevity, we do not include the package name nor imports.
HBaseAdmin is used for administering your HBase cluster, specifically for adding and dropping tables.
The Configura tion instance points these classes at the cluster the code is to work against.
To create a table, we first need to create an instance of HBaseAdmin and then ask it to create the table named test with a single column family named data.
Next, the code asserts the table was actually created, and then it moves to run operations against the just-created table.
Finally, we clean up by first disabling the table and then deleting it.
A table must be disabled before it can be dropped.
This filter instructs the server to short-circuit when running serverside, doing no more than verifying a row has an entry before returning.
The map is simple and just checks for empty values.
These are useful when the interacting application is written in a language other than Java.
In all cases, a Java server hosts an instance of the HBase client brokering Avro, REST, and Thrift application.
This extra work to proxy requests and responses means these interfaces are slower than using the Java client directly.
To put up a stargate instance (stargate is the name for the HBase REST service), start it using the following command:
This will start a server instance (by default on port 8080), background it, and catch any emissions by the server in logfiles under the HBase logs directory.
Clients can ask for the response to be formatted as JSON, Google’s protobufs, or as XML, depending on how the client HTTP Accept header is set.
See the REST wiki page for documentation and examples of making REST client requests.
Similarly, start a Thrift service by putting up a server to field Thrift clients by running the following:
This will start the server instance (by default on port 9090), background it, and catch any emissions by the server in logfiles under the HBase logs directory.
The HBase Thrift documentation7 notes that the Thrift version uses generating classes.
The Avro server is started and stopped in the same manner as you’d start and stop the Thrift or REST services.
The Avro server uses port 9090 by default (the same as the Thrift server, although you wouldn’t normally run both)
Example Although HDFS and MapReduce are powerful tools for processing batch operations over large datasets, they do not provide ways to read or write individual records efficiently.
In this example, we’ll explore using HBase as the tool to fill this gap.
The existing weather dataset described in previous chapters contains observations for tens of thousands of stations over 100 years, and this data is growing without bound.
In this example, we will build a simple web interface that allows a user to navigate the different stations and page through their historical temperature observations in time order.
For the sake of this example, let us allow that the dataset is massive, that the observations run to the billions, and that the rate at which temperature updates arrive is significant—say, hundreds to thousands of updates per second from around the world and across the whole range of weather stations.
Also, let us allow that it is a requirement that the web application must display the most up-to-date observation within a second or so of receipt.
The first size requirement should preclude our use of a simple RDBMS instance and make HBase a candidate store.
A MapReduce job could build initial indices that allowed random access over all of the observation data, but keeping up this index as the updates arrived is not what HDFS and MapReduce are good at.
Schemas In our example, there will be two tables: stations.
Let this table have a column family info that acts as a key-value dictionary for station information.
Let the dictionary keys be the column names info:name, info:location, and info:description.
This table is static, and in this case, the info family closely mirrors a typical RDBMS table design.
Let the row key be a composite key of stationid plus a reverse-order timestamp.
Give this table a column family data that will contain one column, airtemp, with the observed temperature as the column value.
Our choice of schema is derived from knowing the most efficient way we can read from HBase.
Though there are facilities for secondary indexing and regular expression matching, they come at a performance penalty.
It is vital that you understand the most efficient way to query your data in order to choose the most effective setup for storing and accessing.
For the stations table, the choice of stationid as the key is obvious because we will always access information for a particular station by its ID.
The observations table, however, uses a composite key that adds the observation timestamp at the end.
This will group all observations for a particular station together, and by using a reverse-order timestamp (Long.MAX_VALUE - epoch) and storing it as binary, observations for each station will be ordered with most recent observation first.
In the shell, you would define your tables as follows:
Finally, we call close() on our HTable instance to flush out any write buffers not yet cleared.
The conversion takes advantage of the fact that the station ID is a fixed-length string.
It includes methods for converting between byte arrays and common Java and Hadoop types.
Watch for the phenomenon where an import walks in lockstep through the table, with all clients in concert pounding one of the table’s regions (and thus, a single node), then moving on to the next, and so on, rather than evenly distributing the load over all regions.
This is usually brought on by some interaction between sorted input and how the splitter works.
Randomizing the ordering of your row keys prior to insertion may help.
In our example, given the distribution of stationid values and how TextInputFormat makes splits, the upload should be sufficiently distributed.8
If a table is new, it will have only one region, and initially all updates will be to this single region until it splits.
This will happen even if row keys are randomly distributed.
This startup phenomenon means uploads run slowly at first until there are sufficient regions distributed so all cluster members are able to participate in the upload.
Do not confuse this phenomenon with that noted in the main text.
There is a cost to instantiating an HTable, so if you do this for each insert, you may have a negative impact on performance, hence our setup of HTable in the configure() step.
By default, each HTable.put(put) actually performs the insert without any buffering.
You can disable the HTable auto-flush feature using HTable.setAuto Flush(false) and then set the size of configurable write buffer.
When the inserts committed fill the write buffer, it is then flushed.
You could do this in an override of the mapper’s close() method.
HBase includes TableInputFormat and TableOutputFormat to help with MapReduce jobs that source and sink HBase (see Example 13-2)
Web Queries To implement the web application, we will use the HBase Java API directly.
Here it becomes clear how important your choice of schema and storage format is.
The simplest query will be to get the static station information.
This type of query is simple in a traditional database, but HBase gives you additional control and flexibility.
Using the info family as a key-value dictionary (column names as keys, column values as values), the code would look like this:
Note that the NavigableMap that is returned is actually now in descending time order.
You can ask for the next row’s results or a number of rows.
Each invocation of next() involves a trip back to the regionserver, so grabbing a bunch of rows at once can make for significant performance savings.10
You can also set how much to cache/prefetch on the Scan instance itself.
Scanners will, under the covers, fetch this many results at a time, bringing them client-side, and returning to the server to fetch the next batch only after the current batch has been exhausted.
Higher caching values will enable faster scanning but will eat up more memory in the client.
Also, avoid setting the caching so high that the time spent processing the batch client-side exceeds the scanner lease period.
If a client fails to check back with the server before the scanner lease expires, the server will go ahead and garbage-collect resources consumed by the scanner server-side.
The advantage of storing things as Long.MAX_VALUE - stamp may not be clear in the previous example.
It is more useful when you want to get the newest observations for a given offset and limit, which is often the case in web applications.
If the observations were stored with the actual stamps, we would be able to get only the oldest observations for a given offset and limit efficiently.
Getting the newest would mean getting all of them and then grabbing them off the end.
One of the prime reasons for moving from RDBMS to HBase is to allow for these types of “early-out” scenarios.
HBase Versus RDBMS HBase and other column-oriented databases are often compared to more traditional and popular relational databases or RDBMSs.
Although they differ dramatically in their implementations and in what they set out to accomplish, the fact that they are potential solutions to the same problems means that despite their enormous differences, the comparison is a fair one to make.
As described previously, HBase is a distributed, column-oriented data storage system.
It picks up where Hadoop left off by providing random reads and writes on top of HDFS.
It has been designed from the ground up with a focus on scale in every direction: tall in numbers of rows (billions), wide in numbers of columns (millions), and to be horizontally partitioned and replicated across thousands of commodity nodes automatically.
The table schemas mirror the physical storage, creating a system for efficient data structure serialization, storage, and retrieval.
The burden is on the application developer to make use of this storage and retrieval in the right way.
Strictly speaking, an RDBMS is a database that follows Codd’s 12 Rules.
Typical RDBMSs are fixed-schema, row-oriented databases with ACID properties and a sophisticated SQL query engine.
The emphasis is on strong consistency, referential integrity, abstraction from the physical layer, and complex queries through the SQL language.
You can easily create secondary indexes, perform complex inner and outer joins, and count, sum, sort, group, and page your data across a number of tables, rows, and columns.
For a majority of small- to medium-volume applications, there is no substitute for the ease of use, flexibility, maturity, and powerful feature set of available open source RDBMS solutions such as MySQL and PostgreSQL.
However, if you need to scale up in terms of dataset size, read/write concurrency, or both, you’ll soon find that the conveniences of an RDBMS come at an enormous performance penalty and make distribution inherently difficult.
The scaling of an RDBMS usually involves breaking Codd’s rules, loosening ACID restrictions, forgetting conventional DBA wisdom, and on the way losing most of the desirable properties that made relational databases so convenient in the first place.
Successful Service Here is a synopsis of how the typical RDBMS scaling story runs.
The following list presumes a successful growing service: Initial public launch.
Move from local workstation to shared, remotely hosted MySQL instance with a well-defined schema.
Service becomes more popular; too many reads hitting the database Add memcached to cache common queries.
Reads are now no longer strictly ACID; cached data must expire.
New features increases query complexity; now we have too many joins Denormalize your data to reduce joins.
Rising popularity swamps the server; things are too slow Stop doing any server-side computations.
Some queries are still too slow Periodically prematerialize the most complex queries, and try to stop joining in most cases.
Reads are OK, but writes are getting slower and slower Drop secondary indexes and triggers (no indexes?)
At this point, there are no clear solutions for how to solve your scaling problems.
In any case, you’ll need to begin to scale horizontally.
You can attempt to build some type of partitioning on your largest tables, or look into some of the commercial solutions that provide multiple master capabilities.
Countless applications, businesses, and websites have successfully achieved scalable, fault-tolerant, and distributed data systems built on top of RDBMSs and are likely using many of the previous strategies.
But what you end up with is something that is no longer a true RDBMS, sacrificing features and conveniences for compromises and complexities.
Any form of slave replication or external caching introduces weak consistency into your now denormalized data.
The inefficiency of joins and secondary indexes means almost all queries become primary key lookups.
A multiwriter setup likely means no real joins at all, and distributed transactions are a nightmare.
There’s now an incredibly complex network topology to manage with an entirely separate cluster for caching.
HBase Enter HBase, which has the following characteristics: No real indexes.
Rows are stored sequentially, as are the columns within each row.
Therefore, no issues with index bloat, and insert performance is independent of table size.
Automatic partitioning As your tables grow, they will automatically be split into regions and distributed across all available nodes.
Scale linearly and automatically with new nodes Add a node, point it to the existing cluster, and run the regionserver.
Fault tolerance Lots of nodes means each is relatively insignificant.
Batch processing MapReduce integration allows fully parallel, distributed jobs against your data with locality awareness.
If you stay up at night worrying about your database (uptime, scale, or speed), you should seriously consider making a jump from the RDBMS world to HBase.
Utilize a solution that was intended to scale rather than a solution based on stripping down and throwing money at what used to work.
With HBase, the software is free, the hardware is cheap, and the distribution is intrinsic.
Use Case: HBase at Streamy.com Streamy.com is a real-time news aggregator and social sharing platform.
With a broad feature set, we started out with a complex implementation on top of PostgreSQL.
It’s a terrific product with a great community and a beautiful codebase.
We tried every trick in the book to keep things fast as we scaled, going so far as to modify the PostgreSQL code directly to suit our needs.
Originally taking advantage of all RDBMS goodies, we found that eventually, one by one, we had to let them all go.
We did manage to solve many of the issues that we ran into, but there were two that eventually led to the decision to find another solution from outside the world of RDBMS.
Streamy crawls thousands of RSS feeds and aggregates hundreds of millions of items from them.
In addition to having to store these items, one of our more complex queries reads a time-ordered list of all items from a set of sources.
At the high end, this can run to several thousand sources and all of their items all in a single query.
At first, this was a single items table, but the high number of secondary indexes made inserts and updates very slow.
We started to divide items up into several one-to-one link tables to store other information, separating static fields from dynamic ones, grouping fields based on how they were queried, and denormalizing everything along the way.
Even with these changes, single updates required rewriting the entire record, so tracking statistics on items was difficult to scale.
The rewriting of records and having to update indexes along the way are intrinsic properties of the RDBMS we were using.
We partitioned our tables, which was not too difficult because of the natural partition of time, but the complexity got out of hand fast.
Performing sorted merges of time-ordered lists is common in many Web 2.0 applications.
Assuming id is a primary key on streams, and that stamp and type have secondary indexes, an RDBMS query planner treats this query as follows:
The problem here is that we are after only the top 10 IDs, but the query planner actually materializes an entire merge and then limits at the end.
In our case, each type could have tens of thousands of IDs in it, so materializing the entire list and sorting it was extremely slow and unnecessary.
We actually went so far as to write a custom PL/Python script that performed a heapsort using a series of queries like the following:
If we ended up taking from typeN (it was the next most recent in the heap), we would run another query:
In nearly all cases, this outperformed the native SQL implementation and the query planner’s strategy.
In the worst cases for SQL, we were more than an order of magnitude faster using the Python procedure.
We found ourselves continually trying to outsmart the query planner.
Our RDBMS-based system was always capable of correctly implementing our requirements; the issue was scaling.
When you start to focus on scale and performance rather than correctness, you end up shortcutting and optimizing for your domain-specific use cases everywhere possible.
Once you start implementing your own solutions to your data problems, the overhead and complexity of an RDBMS gets in your way.
The abstraction from the storage layer and ACID requirements are an enormous barrier and luxury that you cannot always afford when building for scale.
HBase is a distributed, column-oriented, sorted map store and not much else.
The only major part that is abstracted from the user is the distribution, and that’s exactly what we don’t want to deal with.
Business logic, on the other hand, is very specialized and optimized.
With HBase not trying to solve all of our problems, we’ve been able to solve them better ourselves and rely on HBase for scaling our storage, not our logic.
It was an extremely liberating experience to be able to focus on our applications and logic rather than the scaling of the data itself.
We currently have tables with hundreds of millions of rows and tens of thousands of columns; the thought of storing billions of rows and millions of columns is exciting, not scary.
Praxis In this section, we discuss some of the common issues users run into when running an HBase cluster under load.
Versions Up until HBase 0.20, HBase aligned its versioning with that of Hadoop.
A particular HBase version would run on any Hadoop that had a matching minor version, where minor version in this context is considered the number between the periods (e.g., 20 is.
The Hadoop release cycle has slowed and no longer aligns with that of HBase developments.
Also, the intent is that now a particular HBase version can run on multiple versions of Hadoop.
This said, ensure you are running compatible versions of Hadoop and HBase.
Incompatible versions will throw an exception complaining about the version mismatch, if you are lucky.
If they cannot talk to each sufficiently to pass versions, you may see your HBase cluster hang indefinitely, soon after startup.
The mismatch exception or HBase hang can also happen on upgrade if older versions of either HBase or Hadoop can still be found on the classpath because of imperfect cleanup of the old software.
In MapReduce, generally, HDFS files are opened with their content streamed through a map task and then closed.
In HBase, datafiles are opened on cluster startup and kept open so that we avoid paying the costs associated with opening files on each access.
Because of this, HBase tends to see issues not normally encountered by MapReduce clients: Running out of file descriptors.
Why 0.90? We wanted there to be no confusion that a break had been made, so we put a large gap between our new versioning and that of Hadoop’s.
See the HBase FAQ for how to up the ulimit on your cluster.
Running out of datanode threads Similarly, the Hadoop datanode has an upper bound of 256 on the number of threads it can run at any one time.
Given the same table statistics quoted in the preceding description, it’s easy to see how we can exceed this upper bound relatively early, given that as of this writing, in the datanode each open connection to a file block consumes a thread.
Sync You must run HBase on an HDFS that has a working sync.
The master UI displays a list of basic attributes such as software versions, cluster load, request rates, lists of cluster tables, and participating regionservers.
Click on a regionserver in the master UI, and you are taken to the web server running on the individual regionserver.
It lists the regions this server is carrying and basic metrics such as resources consumed and request rates.
Enabling Hadoop metrics, and in particular tying them to Ganglia or emitting them via JMX, will give you views on what is happening on your cluster, both currently and in the recent past.
Schema Design HBase tables are like those in an RDBMS, except that cells are versioned, rows are sorted, and columns can be added on the fly by the client as long as the column family.
See the HBase troubleshooting guide for more detail on this issue.
Yes, this file is named for Hadoop, though it’s for setting up HBase metrics.
These factors should be considered when designing schemas for HBase, but far and away the most important concern when designing schemas is considering how the data will be accessed.
All access is via primary key, so the key design should lend itself to how the data is going to be queried.
There is no native database join facility in HBase, but wide tables can make it so that there is no need for database joins to pull from secondary or tertiary tables.
A wide row can sometimes be made to hold all data that pertains to a particular primary key.
In the weather data example in this chapter, the compound row key has a station prefix that served to group temperatures by station.
The reversed timestamp suffix made it so temperatures could be scanned and ordered from most recent to oldest.
A smart compound key can be used to cluster data in ways amenable to how it will be accessed.
Designing compound keys, you may have to zero-pad number components so row keys sort properly.
If your keys are integers, use a binary representation rather than persist the string version of a number.
Counters At StumbleUpon, the first production feature deployed on HBase was keeping counters for the stumbleupon.com frontend.
Counters used to be kept in MySQL, but the rate of change was such that drops were frequent, and the load imposed by the counter writes was such that web designers self-imposed limits on what was counted.
Bulk Load HBase has an efficient facility for bulk loading HBase by writing its internal data format directly into the filesystem from MapReduce.
Going this route, it’s possible to load an HBase instance at rates that are an order of magnitude or more beyond those attainable.
It’s also possible to bulk load into a live table.
So far in this book, we have been studying large-scale data processing.
This chapter is different: it is about building general distributed applications using Hadoop’s distributed coordination service, called ZooKeeper.
When a message is sent across the network between two nodes and the network fails, the sender does not know whether the receiver got the message.
It may have gotten through before the network failed, or it may not have.
The only way that the sender can find out what happened is to reconnect to the receiver and ask it.
This is partial failure: when we don’t even know if an operation failed.
ZooKeeper can’t make partial failures go away, since they are intrinsic to distributed systems.
It certainly does not hide partial failures, either.1 But what ZooKeeper does do is give you a set of tools to build distributed applications that can safely handle partial failures.
ZooKeeper is, at its core, a stripped-down filesystem that exposes a few simple operations, and some extra abstractions such as ordering and notifications.
ZooKeeper is expressive The ZooKeeper primitives are a rich set of building blocks that can be used to build a large class of coordination data structures and protocols.
Examples include: distributed queues, distributed locks, and leader election among a group of peers.
That is, distributed programming is fundamentally different from local programming, and the differences cannot simply be papered over.
ZooKeeper is highly available ZooKeeper runs on a collection of machines and is designed to be highly available, so applications can depend on it.
ZooKeeper can help you avoid introducing single points of failure into your system, so you can build a reliable application.
ZooKeeper facilitates loosely coupled interactions ZooKeeper interactions support participants that do not need to know about one another.
For example, ZooKeeper can be used as a rendezvous mechanism so that processes that otherwise don’t know of each other’s existence (or network details) can discover and interact with each other.
Coordinating parties may not even be contemporaneous, since one process may leave a message in ZooKeeper that is read by another after the first has shut down.
ZooKeeper is a library ZooKeeper provides an open source, shared repository of implementations and recipes of common coordination patterns.
Individual programmers are spared the burden of writing common protocols themselves (which are often difficult to get right)
Over time, the community can add to and improve the libraries, which is to everyone’s benefit.
For workloads where reads dominate, which is the norm, the throughput is several times higher.2
Installing and Running ZooKeeper When trying out ZooKeeper for the first time, it’s simplest to run it in standalone mode with a single ZooKeeper server.
You can do this on a development machine, for example.
ZooKeeper requires Java 6 to run, so make sure you have it installed first.
You don’t need Cygwin to run ZooKeeper on Windows, since there are Windows versions of the ZooKeeper scripts.
Windows is supported only as a development platform, not as a production platform.) Download a stable release of ZooKeeper from the Apache ZooKeeper releases page at http://zookeeper.apache.org/releases.html, and unpack the tarball in a suitable location:
ZooKeeper provides a few binaries to run and interact with the service, and it’s convenient to put the directory containing the binaries on your command-line path:
Before running the ZooKeeper service, we need to set up a configuration file.
The configuration file is conventionally called zoo.cfg and placed in the conf subdirectory (although you can also place it in /etc/zookeeper, or in the directory defined by the ZOOCFGDIR environment variable, if set)
This is a standard Java properties file, and the three properties defined in this example are the minimum required for running ZooKeeper in standalone mode.
Briefly, tickTime is the basic time unit in ZooKeeper (specified in milliseconds), dataDir is the local filesystem location where ZooKeeper stores persistent data, and clientPort is the port the ZooKeeper listens on for client connections (2181 is a common choice)
You should change dataDir to an appropriate setting for your system.
With a suitable configuration defined, we are now ready to start a local ZooKeeper server:
Server status ruok Prints imok if the server is running and not in an error state.
Client connections dump Lists all the sessions and ephemeral znodes for the ensemble.
You must connect to the leader (see srvr) for this command.
Caution: may impact server performance for large number of watches.
Caution: may impact server performance for large number of watches.
Monitoring mntr Lists server statistics in Java Properties format, suitable as a source for monitoring systems such as Ganglia and Nagios.
In addition to the mntr command, ZooKeeper exposes statistics via JMX.
There are also monitoring tools and recipes in the src/contrib directory of the distribution.
An Example Imagine a group of servers that provide some service to clients.
We want clients to be able to locate one of the servers so they can use the service.
One of the challenges is maintaining the list of servers in the group.
The membership list clearly cannot be stored on a single node in the network, as the failure of that node would mean the failure of the whole system (we would like the list to be highly available)
Suppose for a moment that we had a robust way of storing the list.
We would still have the problem of how to remove a server from the list if it failed.
Some process needs to be responsible for removing failed servers, but note that it can’t be the servers themselves because they are no longer running! What we are describing is not a passive distributed data structure, but an active one, and one that can change the state of an entry when some external event occurs.
ZooKeeper provides this service, so let’s see how to build this group membership application (as it is known) with it.
Group Membership in ZooKeeper One way of understanding ZooKeeper is to think of it as providing a high-availability filesystem.
It doesn’t have files and directories, but a unified concept of a node, called a znode, which acts both as a container of data (like a file) and a container of other znodes (like a directory)
Znodes form a hierarchical namespace, and a natural way to build a membership list is to create a parent znode with the name of the group and child znodes with the name of the group members (servers)
In this example, we won’t store data in any of the znodes, but in a real application, you could imagine storing data about the members, such as hostnames, in their znodes.
When the main() method is run, it creates a CreateGroup instance and then calls its connect() method.
This method instantiates a new ZooKeeper object, which is the central class of the client API and the one that maintains the connection between the client and the ZooKeeper service.
The Watcher object receives callbacks from ZooKeeper to inform it of various events.
In this scenario, CreateGroup is a Watcher, so we pass this to the ZooKeeper constructor.
When a ZooKeeper instance is created, it starts a thread to connect to the ZooKeeper service.
The call to the constructor returns immediately, so it is important to wait for the connection to be established before using the ZooKeeper object.
When the client has connected to ZooKeeper, the Watcher receives a call to its process() method with an event indicating that it has connected.
The latch was created with a count of one, representing the number of events that need to occur before it releases all waiting threads.
After calling count Down() once, the counter reaches zero and the await() method returns.
The connect() method has now returned, and the next method to be invoked on the CreateGroup is the create() method.
For a replicated ZooKeeper service, this parameter is the comma-separated list of servers (host and optional port) in the ensemble.
The arguments it takes are the path (represented by a string), the contents of the znode (a byte array, null here), an access control list (or ACL for short, which here is a completely open ACL, allowing any client to read from or write to the znode), and the nature of the znode to be created.
An ephemeral znode will be deleted by the ZooKeeper service when the client that created it disconnects, either by explicitly disconnecting or if the client terminates for whatever reason.
A persistent znode, on the other hand, is not deleted when the client disconnects.
We want the znode representing a group to live longer than the lifetime of the program that creates it, so we create a persistent znode.
The return value of the create() method is the path that was created by ZooKeeper.
We use it to print a message that the path was successfully created.
We will see how the path returned by create() may differ from the one passed into the method when we look at sequential znodes.
To see the program in action, we need to have ZooKeeper running on the local machine, and then we can type:
It creates an ephemeral znode as a child of the group znode in its join() method, then simulates doing work of some kind by sleeping until the process is forcibly terminated.
Later, you will see that upon termination, the ephemeral znode is removed by ZooKeeper.
In the list() method, we call getChildren() with a znode path and a watch flag to retrieve a list of child paths for the znode, which we print out.
Placing a watch on a znode causes the registered Watcher to be triggered if the znode changes state.
Although we’re not using it here, watching a znode’s children would permit a program to get notifications of members joining or leaving the group, or of the group being deleted.
As expected, the zoo group is empty, since we haven’t added any members yet:
We launch them as background processes, since they don’t terminate on their own (due to the sleep statement):
The last line saves the process ID of the Java process running the program that adds goat as a member.
We need to remember the ID so that we can kill the process in a moment, after checking the members:
And a few seconds later, it has disappeared from the group because the process’s ZooKeeper session has terminated (the timeout was set to 5 seconds) and its associated ephemeral node has been removed:
We have a way of building up a list of a group of nodes that are participating in a distributed system.
A client that wants to use the nodes in the list to perform some work, for example, can discover the nodes without them being aware of the client’s existence.
Finally, note that group membership is not a substitution for handling network errors when communicating with a node.
Even if a node is a group member, communications with it may fail, and such failures must be handled in the usual ways (retrying, trying a different member of the group, and so on)
ZooKeeper comes with a command-line tool for interacting with the ZooKeeper namespace.
We can use it to list the znodes under the /zoo znode as follows:
You can run the command without arguments to display usage instructions.
The ZooKeeper Service ZooKeeper is a highly available, high-performance coordination service.
In this section, we look at the nature of the service it provides: its model, operations, and implementation.
Data Model ZooKeeper maintains a hierarchical tree of nodes called znodes.
ZooKeeper is designed for coordination (which typically uses small datafiles), not high-volume data storage, so there is a limit of 1 MB on the amount of data that may be stored in any znode.
A client reading the data stored at a znode will never receive only some of the data; either the data will be delivered in its entirety or the read will fail.
Similarly, a write will replace all the data associated with a znode.
ZooKeeper guarantees that the write will either succeed or fail; there is no such thing as a partial write, where only some of the data written by the client is stored.
These characteristics contrast with HDFS, which is designed for high-volume data storage with streaming data access and provides an append operation.
Znodes are referenced by paths, which in ZooKeeper are represented as slash-delimited Unicode character strings, like filesystem paths in Unix.
Paths must be absolute, so they must begin with a slash character.
Furthermore, they are canonical, which means that each path has a single representation, and so paths do not undergo resolution.
For example, in Unix, a file with the path /a/b can equivalently be referred to by the path /a/./b because “.” refers to the current directory at the point it is encountered in the path.
Path components are composed of Unicode characters, with a few restrictions (these are spelled out in the ZooKeeper reference documentation)
The string “zookeeper” is a reserved word and may not be used as a path component.
In particular, ZooKeeper uses the /zookeeper subtree to store management information, such as information on quotas.
Znodes have some properties that are very useful for building distributed applications, which we discuss in the following sections.
Znodes can be one of two types: ephemeral or persistent.
A znode’s type is set at creation time and may not be changed later.
An ephemeral znode is deleted by ZooKeeper when the creating client’s session ends.
By contrast, a persistent znode is not tied to the client’s session and is deleted only when explicitly deleted by a client (not necessarily the one that created it)
An ephemeral znode may not have children, not even ephemeral ones.
Even though ephemeral nodes are tied to a client session, they are visible to all clients (subject to their ACL policy, of course)
Ephemeral znodes are ideal for building applications that need to know when certain distributed resources are available.
The example earlier in this chapter uses ephemeral znodes to implement a group membership service, so any process can discover the members of the group at any particular time.
A sequential znode is given a sequence number by ZooKeeper as a part of its name.
If a znode is created with the sequential flag set, then the value of a monotonically increasing counter (maintained by the parent znode) is appended to its name.
In the Java API, the actual path given to sequential znodes is communicated back to the client as the return value of the create() call.
Sequence numbers can be used to impose a global ordering on events in a distributed system and may be used by the client to infer the ordering.
Watches allow clients to get notifications when a znode changes in some way.
Watches are set by operations on the ZooKeeper service and are triggered by other operations on the service.
For example, a client might call the exists operation on a znode, placing a watch on it at the same time.
If the znode doesn’t exist, the exists operation will return false.
If, some time later, the znode is created by a second client, the watch is triggered, notifying the first client of the znode’s creation.
You will see precisely which operations trigger others in the next section.
Watchers are triggered only once.5 To receive multiple notifications, a client needs to reregister the watch.
If the client in the previous example wishes to receive further notifications for the znode’s existence (to be notified when it is deleted, for example), it needs to call the exists operation again to set a new watch.
Operations There are nine basic operations in ZooKeeper, listed in Table 14-2
It is conventional (but not required) to have a trailing dash on pathnames for sequential nodes, to make their sequence numbers easy to read and parse (by the application)
Except for callbacks for connection events, which do not need reregistration.
A delete or setData operation has to specify the version number of the znode that is being updated (which is found from a previous exists call)
If the version number does not match, the update will fail.
Updates are a nonblocking operation, so a client that loses an update (because another process updated the znode in the meantime) can decide whether to try again or take some other action, and it can do so without blocking the progress of any other process.
Although ZooKeeper can be viewed as a filesystem, there are some filesystem primitives that it does away with in the name of simplicity.
Because files are small and are written and read in their entirety, there is no need to provide open, close, or seek operations.
The sync operation is not like fsync() in POSIX filesystems.
As mentioned earlier, writes in ZooKeeper are atomic, and a successful write operation is guaranteed to have been written to persistent storage on a majority of ZooKeeper servers.
However, it is permissible for reads to lag the latest state of the ZooKeeper service, and the sync operation exists to allow a client to bring itself up-to-date.
There is another ZooKeeper operation, called multi, which batches together multiple primitive operations into a single unit that either succeeds or fails in its entirety.
The situation where some of the primitive operations succeed and some fail can never arise.
Multiupdate is very useful for building structures in ZooKeeper that maintain some global invariant.
Each vertex in the graph is naturally represented as a znode in ZooKeeper, and to add or remove an edge we need to update the two znodes corresponding to its vertices because each has a reference to the other.
If we used only primitive ZooKeeper operations, it would be possible for another client to observe the graph in an inconsistent state where one vertex is connected to another but the reverse connection is absent.
Batching the updates on the two znodes into one multi operation ensures that the update is atomic, so a pair of vertices can never have a dangling connection.
There are two core language bindings for ZooKeeper clients, one for Java and one for C; there are also contrib bindings for Perl, Python, and REST clients.
For each binding, there is a choice between performing operations synchronously or asynchronously.
Here’s the signature for the exists operation, which returns either a Stat object that encapsulates the znode’s metadata or null if the znode doesn’t exist:
The asynchronous equivalent, which is also found in the ZooKeeper class, looks like this: public void exists(String path, Watcher watcher, StatCallback cb, Object ctx)
In the Java API, all the asynchronous methods have void return types, since the result of the operation is conveyed via a callback.
The caller passes a callback implementation whose method is invoked when a response is received from ZooKeeper.
In this case, the callback is the StatCallback interface, which has the following method:
The rc argument is the return code, corresponding to the codes defined by KeeperEx ception.
A nonzero code represents an exception, in which case the stat parameter will be null.
The path and ctx arguments correspond to the equivalent arguments passed by the client to the exists() method, and can be used to identify the request for which this callback is a response.
The ctx parameter can be an arbitrary object that may be used by the client when the path does not give enough context to disambiguate the request.
The single-threaded library, zookeeper_st, supports only the asynchronous API and is intended for platforms where the pthread library is not available or stable.
Most developers will use the multithreaded library, zookeeper_mt, as it supports both the synchronous and asynchronous APIs.
For details on how to build and use the C API, refer to the README file in the src/c directory of the ZooKeeper distribution.
Should I Use the Synchronous or Asynchronous API? Both APIs offer the same functionality, so the one you use is largely a matter of style.
The asynchronous API is appropriate if you have an event-driven programming model, for example.
The asynchronous API allows you to pipeline requests, which in some scenarios can offer better throughput.
Imagine that you want to read a large batch of znodes and process them independently.
Using the synchronous API, each read would block until it returned, whereas with the asynchronous API, you can fire off all the asynchronous reads very quickly and process the responses in a separate thread as they come back.
The read operations exists, getChildren, and getData may have watches set on them, and the watches are triggered by write operations: create, delete, and setData.
When a watch is triggered, a watch event is generated, and the watch event’s type depends both on the watch and the operation that triggered it:
A watch set on an exists operation will be triggered when the znode being watched is created, deleted, or has its data updated.
A watch set on a getData operation will be triggered when the znode being watched is deleted or has its data updated.
No trigger can occur on creation because the znode must already exist for the getData operation to succeed.
A watch set on a getChildren operation will be triggered when a child of the znode being watched is created or deleted, or when the znode itself is deleted.
A watch event includes the path of the znode that was involved in the event, so for NodeCreated and NodeDeleted events, you can tell which node was created or deleted simply by inspecting the path.
To discover which children have changed after a Node ChildrenChanged event, you need to call getChildren again to retrieve the new list of children.
Similarly, to discover the new data for a NodeDataChanged event, you need to call getData.
In both of these cases, the state of the znodes may have changed between receiving the watch event and performing the read operation, so you should bear this in mind when writing applications.
A znode is created with a list of ACLs, which determines who can perform certain operations on it.
ACLs depend on authentication, the process by which the client identifies itself to ZooKeeper.
There are a few authentication schemes that ZooKeeper provides: digest.
Authentication is optional, although a znode’s ACL may require an authenticated client, in which case the client must authenticate itself to access the znode.
Here is an example of using the digest scheme to authenticate with a username and password:
An ACL is the combination of an authentication scheme, an identity for that scheme, and a set of permissions.
In Java, we would create the ACL object as follows:
The full set of permissions are listed in Table 14-4
Note that the exists operation is not governed by an ACL permission, so any client may call exists to find the Stat for a znode or to discover that a znode does not in fact exist.
There are a number of predefined ACLs defined in the ZooDefs.Ids class, including OPEN_ACL_UNSAFE, which gives all permissions (except ADMIN permission) to everyone.
In addition, ZooKeeper has a pluggable authentication mechanism, which makes it possible to integrate third-party authentication systems if needed.
In standalone mode, there is a single ZooKeeper server, which is useful for testing due to its simplicity (it can even be embedded in unit tests), but provides no guarantees of high-availability or resilience.
In production, ZooKeeper runs in replicated mode on a cluster of machines called an ensemble.
ZooKeeper achieves high-availability through replication, and can provide a service as long as a majority of the machines in the ensemble are up.
For example, in a five-node ensemble, any two machines can fail and the service will still work because a majority of three remain.
Note that a six-node ensemble can also tolerate only two machines failing, since with three failures the remaining three do not constitute a majority of the six.
For this reason, it is usual to have an odd number of machines in an ensemble.
Conceptually, ZooKeeper is very simple: all it has to do is ensure that every modification to the tree of znodes is replicated to a majority of the ensemble.
If a minority of the machines fail, then a minimum of one machine will survive with the latest state.
The other remaining replicas will eventually catch up with this state.
ZooKeeper uses a protocol called Zab that runs in two phases, which may be repeated indefinitely: Phase 1: Leader election.
The machines in an ensemble go through a process of electing a distinguished member, called the leader.
This phase is finished once a majority (or quorum) of followers have synchronized their state with the leader.
Phase 2: Atomic broadcast All write requests are forwarded to the leader, which broadcasts the update to the followers.
When a majority have persisted the change, the leader commits the update, and the client gets a response saying the update succeeded.
The protocol for achieving consensus is designed to be atomic, so a change either succeeds or fails.
Zab is similar, but it differs in several aspects of its operation, such as relying on TCP for its message ordering guarantees.
If the leader fails, the remaining machines hold another leader election and continue as before with the new leader.
If the old leader later recovers, it then starts as a follower.
All machines in the ensemble write updates to disk before updating their in-memory copy of the znode tree.
Read requests may be serviced from any machine, and because they involve only a lookup from memory, they are very fast.
Consistency Understanding the basis of ZooKeeper’s implementation helps in understanding the consistency guarantees that the service makes.
This is a consequence of the fact that only a majority and not all of the ensemble needs to have persisted a change before it is committed.
A good mental model for ZooKeeper is of clients connected to ZooKeeper servers that are following the leader.
It is possible to configure ZooKeeper so that the leader does not accept client connections.
This is recommended for ensembles of more than three servers.
Reads are satisfied by followers, whereas writes are committed by the leader.
The following guarantees for data consistency flow from ZooKeeper’s design: Sequential consistency.
Updates from any particular client are applied in the order that they are sent.
This means that if a client updates the znode z to the value a, and in a later operation, it updates z to the value b, then no client will ever see z with value a after it has seen it with value b (if no other updates are made to z)
This means that if an update fails, no client will ever see it.
Single system image A client will see the same view of the system, regardless of the server it connects to.
This means that if a client connects to a new server during the same session, it will not see an older state of the system than the one it saw with the previous server.
When a server fails and a client tries to connect to another in the ensemble, a server that is behind the one that failed will not accept connections from the client until it has caught up with the failed server.
Durability Once an update has succeeded, it will persist and will not be undone.
Timeliness The lag in any client’s view of the system is bounded, so it will not be out of date by more than some multiple of tens of seconds.
For performance reasons, reads are satisfied from a ZooKeeper server’s memory and do not participate in the global ordering of writes.
This property can lead to the appearance of inconsistent ZooKeeper states from clients that communicate through a mechanism outside ZooKeeper.
For example, client A updates znode z from a to a’, A tells B to read z, and B reads the value of z as a, not a’
To prevent this condition from happening, B should call sync on z before reading z’s value.
Slightly confusingly, the sync operation is available only as an asynchronous call.
This is because you don’t need to wait for it to return, since ZooKeeper guarantees that any subsequent operation will happen after the sync completes on the server, even if the operation is issued before the sync completes.
Sessions A ZooKeeper client is configured with the list of servers in the ensemble.
On startup, it tries to connect to one of the servers in the list.
If the connection fails, it tries another server in the list, and so on, until it either successfully connects to one of them or fails if all ZooKeeper servers are unavailable.
Once a connection has been made with a ZooKeeper server, the server creates a new session for the client.
A session has a timeout period that is decided on by the application that creates it.
If the server hasn’t received a request within the timeout period, it may expire the session.
Once a session has expired, it may not be reopened, and any ephemeral nodes associated with the session will be lost.
Sessions are kept alive by the client sending ping requests (also known as heartbeats) whenever the session is idle for longer than a certain period.
Pings are automatically sent by the ZooKeeper client library, so your code doesn’t need to worry about maintaining the session.) The period is chosen to be low enough to detect server failure (manifested by a read timeout) and reconnect to another server within the session timeout period.
Failover to another ZooKeeper server is handled automatically by the ZooKeeper client, and, crucially, sessions (and associated ephemeral znodes) are still valid after another server takes over from the failed one.
During failover, the application will receive notifications of disconnections and connections to the service.
Watch notifications will not be delivered while the client is disconnected, but they will be delivered when the client successfully reconnects.
Also, if the application tries to perform an operation while the client is reconnecting to another server, the operation will fail.
The tick time is the fundamental period of time in ZooKeeper and is used by servers in the ensemble to define the schedule on which their interactions run.
Other settings are defined in terms of tick time, or are at least constrained by it.
If you attempt to set a session timeout outside this range, it will be modified to fall within the range.
There are a few considerations in selecting a session timeout.
A low session timeout leads to faster detection of machine failure.
In the group membership example, the session timeout is the time it takes for a failed machine to be removed from the group.
Beware of setting the session timeout too low, however, because a busy network can cause packets to be delayed and may cause inadvertent session expiry.
In such an event, a machine would appear to “flap”: leaving and then rejoining the group repeatedly in a short space of time.
Applications that create more complex ephemeral state should favor longer session timeouts, as the cost of reconstruction is higher.
In some cases, it is possible to design the application so it can restart within the session timeout period and avoid session expiry.
This might be desirable to perform maintenance or upgrades.) Every session is given a unique identity and password by the server, and if these are passed to ZooKeeper while a connection is being made, it is possible to recover a session (as long as it hasn’t expired)
An application can therefore arrange a graceful shutdown, whereby it stores the session identity and password to stable storage before restarting the process, retrieving the stored session identity and password and recovering the session.
You should view this feature as an optimization that can help avoid expire sessions.
It does not remove the need to handle session expiry, which can still occur if a machine fails unexpectedly, or even if an application is shut down gracefully but does not restart before its session expires—for whatever reason.
As a general rule, the larger the ZooKeeper ensemble, the larger the session timeout should be.
Connection timeouts, read timeouts, and ping periods are all defined internally as a function of the number of servers in the ensemble, so as the ensemble grows, these periods decrease.
Consider increasing the timeout if you experience frequent connection loss.
States The ZooKeeper object transitions through different states in its lifecycle (see Figure 14-3)
You can query its state at any time by using the getState() method:
States is an enum representing the different states that a ZooKeeper object may be in.
Despite the enum’s name, an instance of ZooKeeper may be in only one state at a time.) A newly constructed ZooKeeper instance is in the CONNECTING state while it tries to establish a connection with the ZooKeeper service.
Once a connection is established, it goes into the CONNECTED state.
A client using the ZooKeeper object can receive notifications of the state transitions by registering a Watcher object.
On entering the CONNECTED state, the watcher receives a WatchedEvent whose KeeperState value is SyncConnected.
The (default) watcher passed into the ZooKeeper object constructor is used for state changes, but znode changes may either use a dedicated instance of Watcher (by passing one in to the appropriate read operation) or share the default one if using the form of the read operation that takes a Boolean flag to specify whether to use a watcher.
The ZooKeeper instance may disconnect and reconnect to the ZooKeeper service, moving between the CONNECTED and CONNECTING states.
Note that these state transitions are initiated by the ZooKeeper instance itself, and it will automatically try to reconnect if the connection is lost.
The ZooKeeper instance may transition to a third state, CLOSED, if either the close() method is called or the session times out as indicated by a KeeperState of type Expired.
Once in the CLOSED state, the ZooKeeper object is no longer considered to be alive (this can be tested using the isAlive() method on States) and cannot be reused.
To reconnect to the ZooKeeper service, the client must construct a new ZooKeeper instance.
Building Applications with ZooKeeper Having covered ZooKeeper in some depth, let’s turn back to writing some useful applications with it.
A Configuration Service One of the most basic services that a distributed application needs is a configuration service so that common pieces of configuration information can be shared by machines in a cluster.
At the simplest level, ZooKeeper can act as a highly available store for configuration, allowing application participants to retrieve or update configuration files.
Using ZooKeeper watches, it is possible to create an active configuration service, where interested clients are notified of changes in configuration.
We make a couple of assumptions that simplify the implementation (they could be removed with a little more work)
First, the only configuration values we need to store are strings, and keys are just znode paths, so we use a znode to store each key-value pair.
Second, there is a single client that performs updates at any one time.
Among other things, this model fits with the idea of a master (such as the namenode in HDFS) that wishes to update information that its workers need to follow.
The run() method loops forever, updating the /config znode at random times with random values.
Next, let’s look at how to read the /config configuration property.
ConfigWatcher acts on this event in its process() method by reading and displaying the latest version of the config.
Furthermore, we are not guaranteed to receive every update, since between the receipt of the watch event and the next read, the znode may have been updated, possibly many times, and as the client has no watch registered during that period, it is not notified.
For the configuration service, this is not a problem, because clients care only about the latest value of a property, as it takes precedence over previous values.
However, in general you should be aware of this potential limitation.
Let’s examine possible failure modes and what we can do to correct them so that our programs are resilient in the face of failure.
There is a standard Java mechanism for canceling blocking methods, which is to call interrupt() on the thread from which the blocking method was called.
ZooKeeper adheres to this standard, so you can cancel a ZooKeeper operation in this way.
A KeeperException is thrown if the ZooKeeper server signals an error or if there is a communication problem with the server.
For different error cases, there are various subclasses of KeeperException.
Every subclass of KeeperException has a corresponding code with information about the type of error.
There are two ways then to handle KeeperException: either catch KeeperException and test its code to determine what remedying action to take, or catch the equivalent KeeperException subclasses and perform the appropriate action in each catch block.
A state exception occurs when the operation fails because it cannot be applied to the znode tree.
State exceptions usually happen because another process is mutating a znode at the same time.
The programmer is usually aware that this kind of conflict is possible and will code to deal with it.
Recoverable exceptions are those from which the application can recover within the same ZooKeeper session.
ZooKeeper will try to reconnect, and in most cases the reconnection will succeed and ensure that the session is intact.
This is an example of partial failure (which we introduced at the beginning of the chapter)
The onus is therefore on the programmer to deal with the uncertainty, and the action that should be taken depends on the application.
At this point, it is useful to make a distinction between idempotent and nonidempotent operations.
An idempotent operation is one that may be applied one or more times with the same result, such as a read request or an unconditional setData.
A nonidempotent operation cannot be retried indiscriminately, as the effect of applying it multiple times is not the same as applying it once.
The program needs a way of detecting whether its update was applied by encoding information in the znode’s pathname or its data.
In any case, all ephemeral nodes associated with the session will be lost, so the application needs to rebuild its state before reconnecting to ZooKeeper.
Taken as a whole, the write() method is idempotent, so we can afford to unconditionally retry it.
Here’s a modified version of the write() method that retries in a loop.
We simply rethrow the exception10 and let the caller create a new ZooKeeper instance, so that the whole write() method can be retried.
Which method you use is a matter of style, since they both behave in the same way.
An alternative way of dealing with session expiry would be to look for a KeeperState of type Expired in the watcher (that would be the ConnectionWatcher in the example here), and create a new connection when this is detected.
Regardless of the precise mechanics of how we recover from an expired session, the important point is that it is a different kind of failure from connection loss and needs to be handled differently.
When the ZooKeeper object is created, it tries to connect to a ZooKeeper server.
If the connection fails or times out, then it tries another server in the ensemble.
If, after trying all of the servers in the ensemble, it can’t connect, then it throws an IOException.
The likelihood of all ZooKeeper servers being unavailable is low; nevertheless, some applications may choose to retry the operation in a loop until ZooKeeper is available.
There are many others, such as using exponential backoff where the period between retries is multiplied by a constant each time.
A Lock Service A distributed lock is a mechanism for providing mutual exclusion between a collection of processes.
At any one time, only a single process may hold the lock.
Distributed locks can be used for leader election in a large distributed system, where the leader is the process that holds the lock at any point in time.
Do not confuse ZooKeeper’s own leader election with a general leader election service, which can be built using ZooKeeper primitives (and in fact, one implementation is included with ZooKeeper)
ZooKeeper’s own leader election is not exposed publicly, unlike the type of general leader election service we are describing here, which is designed to be used by distributed systems that need to agree upon a master process.
To implement a distributed lock using ZooKeeper, we use sequential znodes to impose an order on the processes vying for the lock.
The idea is simple: first, designate a lock znode, typically describing the entity being locked on, say, /leader; then, clients that want to acquire the lock create sequential ephemeral znodes as children of the lock znode.
At any point in time, the client with the lowest sequence number holds the lock.
The ZooKeeper service is the arbiter of order because it assigns the sequence numbers.
The lock may be released simply by deleting the znode /leader/lock-1; alternatively, if the client process dies, it will be deleted by virtue of being an ephemeral znode.
The client that created /leader/lock-2 will then hold the lock because it has the next lowest sequence number.
It will be notified that it has the lock by creating a watch that fires when znodes go away.
Create an ephemeral sequential znode named lock- under the lock znode, and remember its actual pathname (the return value of the create operation)
Get the children of the lock znode and set a watch.
Although this algorithm is correct, there are some problems with it.
The first problem is that this implementation suffers from the herd effect.
Consider hundreds or thousands of clients, all trying to acquire the lock.
Each client places a watch on the lock znode for changes in its set of children.
Every time the lock is released or another process starts the lock acquisition process, the watch fires, and every client receives a notification.
In this case, only one client will successfully acquire the lock, and the process of maintaining and sending watch events to all clients causes traffic spikes, which put pressure on the ZooKeeper servers.
To avoid the herd effect, the condition for notification needs to be refined.
The key observation for implementing locks is that a client needs to be notified only when the child znode with the previous sequence number goes away, not when any child znode is deleted (or created)
Another problem with the lock algorithm as it stands is that it doesn’t handle the case when the create operation fails due to connection loss.
Recall that in this case we do not know whether the operation succeeded or failed.
Creating a sequential znode is a nonidempotent operation, so we can’t simply retry, because if the first create had.
The problem is that after reconnecting, the client can’t tell whether it created any of the child znodes.
By embedding an identifier in the znode name, if it suffers a connection loss, it can check to see whether any of the children of the lock node have its identifier in their name.
If a child contains its identifier, it knows that the create operation succeeded, and it shouldn’t create another child znode.
If no child has the identifier in its name, the client can safely create a new sequential child znode.
The client’s session identifier is a long integer that is unique for the ZooKeeper service and therefore ideal for the purpose of identifying a client across connection loss events.
The session identifier can be obtained by calling the getSessionId() method on the ZooKeeper Java class.
The sequence numbers are unique to the parent, not to the name of the child, so this technique allows the child znodes to identify their creators as well as impose an order of creation.
If a client’s ZooKeeper session expires, the ephemeral znode created by the client will be deleted, effectively relinquishing the lock or at least forfeiting the client’s turn to acquire the lock.
The application using the lock should realize that it no longer holds the lock, clean up its state, and then start again by creating a new lock object and trying to acquire it.
Notice that it is the application that controls this process, not the lock implementation, since it cannot second-guess how the application needs to clean up its state.
Implementing a distributed lock correctly is a delicate matter, since accounting for all of the failure modes is nontrivial.
ZooKeeper comes with a production-quality lock implementation in Java called WriteLock that is very easy for clients to use.
More Distributed Data Structures and Protocols There are many distributed data structures and protocols that can be built with ZooKeeper, such as barriers, queues, and two-phase commit.
One interesting thing to note is that these are synchronous protocols, even though we use asynchronous ZooKeeper primitives (such as notifications) to build them.
The ZooKeeper website describes several such data structures and protocols in pseudocode.
ZooKeeper comes with implementations of some of these standard recipes.
It can be used to provide write-ahead logging, which is a common technique for ensuring data integrity in storage systems.
In a system using write-ahead logging, every write operation is written to the transaction log before it is applied.
Using this procedure, we don’t have to write the data to permanent storage after every write operation, because in the event of a system failure, the latest state may be recovered by replaying the transaction log for any writes that were not applied.
BookKeeper clients create logs called ledgers, and each record appended to a ledger is called a ledger entry, which is simply a byte array.
Ledgers are managed by bookies, which are servers that replicate the ledger data.
Note that ledger data is not stored in ZooKeeper, only metadata is.
Traditionally, the challenge has been to make systems that use write-ahead logging robust in the face of failure of the node writing the transaction log.
This is usually done by replicating the transaction log in some manner.
Hadoop’s HDFS namenode, for instance, writes its edit log to multiple disks, one of which is typically an NFS-mounted disk.
However, in the event of failure of the primary, failover is still manual.
By providing logging as a highly available service, BookKeeper promises to make failover transparent, since it can tolerate the loss of bookie servers.
In the case of HDFS HighAvailability, described on page 48, a BookKeeper-based edit log will remove the requirement for using NFS for shared storage.) Hedwig is a topic-based publish-subscribe system built on BookKeeper.
Thanks to its ZooKeeper underpinnings, Hedwig is a highly available service and guarantees message delivery even if subscribers are offline for extended periods of time.
BookKeeper is a ZooKeeper subproject, and you can find more information on how to use it, as well as Hedwig, at http://zookeeper.apache.org/bookkeeper/
ZooKeeper in Production In production, you should run ZooKeeper in replicated mode.
Here, we will cover some of the considerations for running an ensemble of ZooKeeper servers.
However, this section is not exhaustive, so you should consult the ZooKeeper Administrator’s Guide for detailed, up-to-date instructions, including supported platforms, recommended hardware, maintenance procedures, and configuration properties.
Resilience and Performance ZooKeeper machines should be located to minimize the impact of machine and network failure.
In practice, this means that servers should be spread across racks, power supplies, and switches, so that the failure of any one of these does not cause the ensemble to lose a majority of its servers.
For applications that require low-latency service (on the order of a few milliseconds), it is important to run all the servers in an ensemble in a single data center.
Some use cases don’t require low-latency responses, however, which makes it feasible to spread servers across data centers (at least two per data center) for extra resilience.
Example applications in this category are leader election and distributed coarse-grained locking, both of which have relatively infrequent state changes, so the overhead of a few tens of milliseconds incurred by inter-data-center messages is not significant relative to the overall functioning of the service.
ZooKeeper has the concept of an observer node, which is like a nonvoting follower.
Because they do not participate in the vote for consensus during write requests, observers allow a ZooKeeper cluster to improve read performance without hurting write performance.11 Observers can be used to good advantage to allow a ZooKeeper cluster to span data centers without impacting latency as much as regular voting followers.
This is achieved by placing the voting members in one data center and observers in the other.
ZooKeeper is a highly available system, and it is critical that it can perform its functions in a timely manner.
Therefore, ZooKeeper should run on machines that are dedicated to ZooKeeper alone.
Having other applications contend for resources can cause ZooKeeper’s performance to degrade significantly.
Configure ZooKeeper to keep its transaction log on a different disk drive from its snapshots.
By default, both go in the directory specified by the dataDir property, but by specifying a location for dataLogDir, the transaction log will be written there.
By having its own dedicated device (not just a partition), a ZooKeeper server can maximize the rate at which it writes log entries to disk, which it does sequentially without seeking.
Because all writes go through the leader, write throughput does not scale by adding servers, so it is crucial that writes are as fast as possible.
If the process swaps to disk, performance will be adversely affected.
This can be avoided by setting the Java heap size to less than the amount of unused physical memory on the machine.
From its configuration directory, the ZooKeeper scripts will source a file.
The server number is specified in plain text in a file named myid in the directory specified by the dataDir property.
Setting each server number is only half of the job.
We also need to give every server all the identities and network locations of the others in the ensemble.
The ZooKeeper configuration file must include a line for each server, of the form:
The value of n is replaced by the server number.
There are two port settings: the first is the port that followers use to connect to the leader, and the second is used for leader election.
Here is a sample configuration for a three-machine replicated ZooKeeper ensemble:
When a ZooKeeper server starts up, it reads the myid file to determine which server it is, and then reads the configuration file to determine the ports it should listen on and to discover the network addresses of the other servers in the ensemble.
In replicated mode, there are two extra mandatory properties: initLimit and syncLimit, both measured in multiples of tickTime.
If a majority of followers fail to sync within this period, the leader renounces its leadership status and another leader election takes place.
If this happens often (and you can discover if this is the case because it is logged), it is a sign that the setting is too low.
If a follower fails to sync within this period, it will restart itself.
Clients that were attached to this follower will connect to another one.
These are the minimum settings needed to get up and running with a cluster of ZooKeeper servers.
There are, however, more configuration options, particularly for tuning performance, which are documented in the ZooKeeper Administrator’s Guide.
A great strength of the Hadoop platform is its ability to work with data in several different forms.
But to interact with data in storage repositories outside of HDFS, MapReduce programs need to use external APIs to get to this data.
Often, valuable data in an organization is stored in structured data stores such as relational database systems (RDBMS)
Apache Sqoop is an open source tool that allows users to extract data from a structured data store into Hadoop for further processing.
This processing can be done with MapReduce programs or other higher-level tools such as Hive.
It’s even possible to use Sqoop to move data from a database into HBase.) When the final results of an analytic pipeline are available, Sqoop can export these results back to the data store for consumption by other clients.
In this chapter, we’ll take a look at how Sqoop works and how you can use it in your data processing pipeline.
This repository contains all the Sqoop source code and documentation.
Official releases are available at this site, as well as the source code for the version currently under development.
Alternatively, Cloudera’s Distribution Including Apache Hadoop (CDH) contains an installation package for Sqoop alongside compatible editions of Hadoop and other tools such as Hive.
If you’ve installed a release from Cloudera, the package will have placed Sqoop’s scripts in a standard locations such as /usr/bin/sqoop.
You can run Sqoop by simply typing sqoop at the command line.
Regardless of how you install Sqoop, we’ll refer to this script as just sqoop from here on.) Running Sqoop with no arguments does not do much of interest:
Sqoop is organized as a set of tools or commands.
Without selecting a tool, Sqoop does not know what to do.
Available commands: codegen            Generate code to interact with database records create-hive-table  Import a table definition into Hive eval               Evaluate a SQL statement and display the results export             Export an HDFS directory to a database table help               List available commands import             Import a table from a database to HDFS import-all-tables  Import tables from a database to HDFS job                Work with saved jobs list-databases     List available databases on a server list-tables        List available tables in a database merge              Merge results of incremental imports metastore          Run a standalone Sqoop metastore version            Display version information.
See 'sqoop help COMMAND' for information on a specific command.
As it explains, the help tool can also provide specific usage instructions on a particular tool when you provide that tool’s name as an argument:
An alternate way of running a Sqoop tool is to use a tool-specific script.
This script will be named sqoop-toolname—for example, sqoop-help, sqoop-import, etc.
These commands are identical to running sqoop help or sqoop import.
A Sqoop connector is a modular component that uses this framework to enable Sqoop imports and exports.
Sqoop ships with connectors for working with a range of popular relational databases, including MySQL, PostgreSQL, Oracle, SQL Server, and DB2
There is also a generic JDBC connector for connecting to any database that supports Java’s JDBC protocol.
As well as the built-in Sqoop connectors, various third-party connectors are available for data stores, ranging from enterprise data warehouses (including Netezza, Teradata, and Oracle) to NoSQL stores (such as Couchbase)
These connectors must be downloaded separately and can be added to an existing Sqoop installation by following the instructions that come with the connector.
A Sample Import After you install Sqoop, you can use it to import data to Hadoop.
For the examples in this chapter we’ll use MySQL, which is easy to use and available for a large number of platforms.
Users of Debian-based Linux systems (e.g., Ubuntu) can type sudo aptget install mysql-client mysql-server.
Now that MySQL is installed, let’s log in and create a database (Example 15-1)
Creating a new MySQL database schema % mysql -u root -p Enter password: Welcome to the MySQL monitor.
The password prompt shown in this example asks for your root user password.
This is likely the same as the password for the root shell login.
If you are running Ubuntu or another variant of Linux where root cannot log in directly, enter the password you picked at MySQL installation time.
In this session, we created a new database schema called hadoopguide, which we’ll use throughout this chapter.
Populating the database % mysql hadoopguide Welcome to the MySQL monitor.
Of course, in a production deployment, we’d need to be much more careful about access control, but this serves for demonstration purposes.
The grant privilege shown in the example also assumes you’re running a pseudo-distributed Hadoop instance.
If you’re working with a distributed Hadoop cluster, you’d need to enable remote access by at least one user, whose account would be used to perform imports and exports via Sqoop.
In this listing, we created a new table called widgets.
We’ll be using this fictional product database in further examples in this chapter.
The widgets table contains several fields representing a variety of data types.
Now let’s use Sqoop to import this table into HDFS:
Sqoop’s import tool will run a MapReduce job that connects to the MySQL database and reads the table.
By default, this will use four map tasks in parallel to speed up the import process.
Each task will write its imported results to a different file, but all in a common directory.
Because we knew that we had only three rows to import in this example, we specified that Sqoop should use a single map task (-m 1) so we get a single file in HDFS.
If a distributed Hadoop cluster is being used, localhost should not be specified in the connect string, because map tasks not running on the same machine as the database will fail to connect.
Even if Sqoop is run from the same host as the database sever, the full hostname should be specified.
By default, Sqoop will generate comma-delimited text files for our imported data.
Delimiters can be specified explicitly, as well as field enclosing and escape characters, to allow the presence of delimiters in the field contents.
The command-line arguments that specify delimiter characters, file formats, compression, and more fine-grained.
Text and Binary File Formats Sqoop is capable of importing into a few different file formats.
Text files (the default) offer a human-readable representation of data, platform independence, and the simplest structure.
However, they cannot hold binary fields (such as database columns of type VARBINARY) and distinguishing between null values and String-based fields containing the value "null" can be problematic (although using the --null-string import option allows you to control the representation of null values)
To handle these conditions, you can either use Sqoop’s SequenceFile-based format or its Avro-based format.
Both Avro datafiles and SequenceFiles provide the most precise representation possible of the imported data.
They also allow data to be compressed while retaining MapReduce’s ability to process different sections of the same file in parallel.
However, current versions of Sqoop cannot load either Avro or SequenceFiles into Hive (although you can load Avro datafiles into Hive manually)
A final disadvantage of SequenceFiles is that they are Java-specific, whereas Avro datafiles can be processed by a wide range of languages.
Generated Code In addition to writing the contents of the database table to HDFS, Sqoop has also provided you with a generated Java source file (widgets.java) written to the current local directory.
The generated class (widgets) is capable of holding a single record retrieved from the imported table.
It can manipulate such a record in MapReduce or store it in a SequenceFile in HDFS.
SequenceFiles written by Sqoop during the import process will store each imported row in the “value” element of the SequenceFile’s key-value pair format, using the generated class.) It is likely that you don’t want to name your generated class widgets since each instance of the class refers to only a single record.
We can use a different Sqoop tool to generate source code without performing an import; this generated code will still examine the database table to determine the appropriate data types for each field:
The codegen tool simply generates code; it does not perform the full import.
We specified that we’d like it to generate a class named Widget; this will be written to Widget.java.
We also could have specified --class-name and other code-generation arguments during the import process we performed earlier.
This tool can be used to regenerate code if you accidentally remove the source file, or generate code with different settings than were used during the import.
If you’re working with records imported to SequenceFiles, it is inevitable that you’ll need to use the generated classes (to deserialize data from the SequenceFile storage)
Imports: A Deeper Look As mentioned earlier, Sqoop imports a table from a database by running a MapReduce job that extracts rows from the table, and writes the records to HDFS.
How does MapReduce read the rows? This section explains how Sqoop works under the hood.
At a high level, Figure 15-1 demonstrates how Sqoop interacts with both the database source and Hadoop.
Java provides an API called Java Database Connectivity, or JDBC, that allows applications to access data stored in an RDBMS as well as inspect the nature of this data.
Most database vendors provide a JDBC driver that implements the JDBC API and contains the necessary code to connect to their database server.
Based on the URL in the connect string used to access the database, Sqoop attempts to predict which driver it should load.
You may still need to download the JDBC driver itself and install it on your Sqoop client.
For cases where Sqoop does not know which JDBC driver is appropriate, users can specify exactly how to load the JDBC driver into Sqoop.
This capability allows Sqoop to work with a wide variety of database platforms.
Before the import can start, Sqoop uses JDBC to examine the table it is to import.
It retrieves a list of all the columns and their SQL data types.
These SQL types (VARCHAR, INTEGER, and so on) can then be mapped to Java data types (String, Integer, etc.), which will hold the field values in MapReduce applications.
Sqoop’s code generator will use this information to create a table-specific class to hold a record extracted from the table.
The Widget class from earlier, for example, contains the following methods that retrieve each column from an extracted record:
More critical to the import system’s operation, though, are the serialization methods that form the DBWritable interface, which allow the Widget class to interact with JDBC:
JDBC’s ResultSet interface provides a cursor that retrieves records from a query; the readFields() method here will populate the fields of the Widget object with the columns from one row of the ResultSet’s data.
The write() method shown here allows Sqoop to insert new Widget rows into a table, a process called exporting.
The MapReduce job launched by Sqoop uses an InputFormat that can read sections of a table from a database via JDBC.
Reading a table is typically done with a simple query such as:
But often, better import performance can be gained by dividing this query across multiple nodes.
Using metadata about the table, Sqoop will guess a good column to use for splitting the table (typically the primary key for the table, if one exists)
The minimum and maximum values for the primary key column are retrieved, and then these are used in conjunction with a target number of tasks to determine the queries that each map task should issue.
When importing this table, Sqoop would determine that id is the primary key column for the table.
These values would then be used to interpolate over the entire range of data.
The choice of splitting column is essential to parallelizing work efficiently.
Users can specify a particular splitting column when running an import job, to tune the job to the data’s actual distribution.
If an import job is run as a single (sequential) task with -m 1, this split process is not performed.
After generating the deserialization code and configuring the InputFormat, Sqoop sends the job to the MapReduce cluster.
Map tasks execute the queries and deserialize rows from the ResultSet into instances of the generated class, which are either stored directly in SequenceFiles or transformed into delimited text before being written to HDFS.
Controlling the Import Sqoop does not need to import an entire table at a time.
For example, a subset of the table’s columns can be specified for import.
Users can also specify a WHERE clause to include in queries, which bound the rows of the table to import.
Imports and Consistency When importing data to HDFS, it is important that you ensure access to a consistent snapshot of the source data.
Map tasks reading from a database in parallel are running in separate processes.
The best way to do this is to ensure that any processes that update existing rows of a table are disabled during the import.
Direct-mode Imports Sqoop’s architecture allows it to choose from multiple available strategies for performing an import.
Some databases offer specific tools designed to extract data quickly.
For example, MySQL’s mysqldump application can read from a table with greater throughput than a JDBC channel.
The use of these external tools is referred to as direct mode in Sqoop’s documentation.
Direct mode must be specifically enabled by the user (via the --direct argument), as it is not as general-purpose as the JDBC approach.
For example, MySQL’s direct mode cannot handle large objects, such as CLOB or BLOB columns, because Sqoop needs to use a JDBC-specific API to load these columns into HDFS.) For databases that provide such tools, Sqoop can use these to great effect.
A directmode import from MySQL is usually much more efficient (in terms of map tasks and time required) than a comparable JDBC-based import.
These tasks will then spawn instances of the mysqldump program and read its output.
The effect is similar to a distributed implementation of mkparallel-dump from the Maatkit tool set.
Even when direct mode is used to access the contents of a database, the metadata is still queried through JDBC.
Working with Imported Data Once data has been imported to HDFS, it is now ready for processing by custom MapReduce programs.
Text-based imports can be easily used in scripts run with Hadoop Streaming or in MapReduce jobs run with the default TextInputFormat.
To use individual fields of an imported record, though, the field delimiters (and any escape/enclosing characters) must be parsed and the field values extracted and converted to the appropriate data types.
The generated table class provided by Sqoop can automate this process, allowing you to focus on the actual MapReduce job to run.
Each autogenerated class has several overloaded methods named parse() that operate on the data represented as Text, CharSequence, char[], or other common types.
The MapReduce application called MaxWidgetId (available in the example code) will find the widget with the highest ID.
The class can be compiled into a JAR file along with Widget.java.
The class files can then be combined into a JAR file and executed like so:
When run, the maxwidgets path in HDFS will contain a file named part-r-00000 with the following expected result:
It is worth noting that in this example MapReduce program, a Widget object was emitted from the mapper to the reducer; the autogenerated Widget class implements the Writable interface provided by Hadoop, which allows the object to be sent via Hadoop’s serialization mechanism, as well as written to and read from SequenceFiles.
The MaxWidgetId example is built on the new MapReduce API.
MapReduce applications that rely on Sqoop-generated code can be built on the new or old APIs, though some advanced features (such as working with large objects) are more convenient to use in the new API.
With the generic Avro mapping, the MapReduce program does not need to use schema-specific generated code (although this is an option too, by using Avro’s specific compiler; Sqoop does not do the code generation in this case)
Imported Data and Hive As noted in Chapter 12, for many types of analysis, using a system such as Hive to handle relational operations can dramatically ease the development of the analytic pipeline.
Especially for data originally from a relational data source, using Hive makes.
Hive and Sqoop together form a powerful toolchain for performing analysis.
Suppose we had another log of data in our system, coming from a web-based widget purchasing system.
This may return logfiles containing a widget ID, a quantity, a shipping address, and an order date.
Here is a snippet from an example log of this type:
By using Hadoop to analyze this purchase log, we can gain insight into our sales operation.
By combining this data with the data extracted from our relational data source (the widgets table), we can do better.
In this example session, we will compute which zip code is responsible for the most sales dollars, so we can better focus our sales team’s operations.
Doing this requires data from both the sales log and the widgets table.
The table shown in the previous code snippet should be in a local file named sales.log for this to work.
Sqoop can generate a Hive table based on a table from an existing relational data source.
Since we’ve already imported the widgets data to HDFS, we can generate the Hive table definition and then load in the HDFS-resident data:
When creating a Hive table definition with a specific already-imported dataset in mind, we need to specify the delimiters used in that dataset.
Otherwise, Sqoop will allow Hive to use its default delimiters (which are different from Sqoop’s default delimiters)
Hive’s type system is less rich than that of most SQL systems.
Many SQL types do not have direct analogues in Hive.
When Sqoop generates a Hive table definition for an import, it uses the best Hive type available to hold a column’s values.
When this occurs, Sqoop will provide you with a warning message such as this one:
This three-step process of importing data to HDFS, creating the Hive table, and then loading the HDFS-resident data into Hive can be shortened to one step if you know that you want to import straight from a database directly into Hive.
During an import, Sqoop can generate the Hive table definition and then load in the data.
Had we not already performed the import, we could have executed this command, which re-creates the widgets table in Hive, based on the copy in MySQL:
The sqoop import tool run with the --hive-import argument will load the data directly from the source database into Hive; it infers a Hive schema automatically based on the schema for the table in the source database.
Using this, you can get started working with your data in Hive with only one command.
Regardless of which data import route we chose, we can now use the widgets dataset and the sales data set together to calculate the most profitable zip code.
Let’s do so, and also save the result of this query in another table for later:
Importing Large Objects Most databases provide the capability to store large amounts of data in a single field.
Depending on whether this data is textual or binary in nature, it is usually represented as a CLOB or BLOB column in the table.
In particular, most tables are physically laid out on disk as in Figure 15-2
When scanning through rows to determine which rows match the criteria for a particular query, this typically involves reading all columns of each row from disk.
If large objects were stored “inline” in this fashion, they would adversely affect the performance of such scans.
Therefore, large objects are often stored externally from their rows, as in Figure 15-3
Accessing a large object often requires “opening” it through the reference contained in the row.
Database tables are typically physically represented as an array of rows, with all the columns in a row stored adjacent to one another.
Large objects are usually held in a separate area of storage; the main row storage contains indirect references to the large objects.
The difficulty of working with large objects in a database suggests that a system such as Hadoop, which is much better suited to storing and processing large, complex data objects, is an ideal repository for such information.
Sqoop can extract large objects from tables and store them in HDFS for further processing.
As in a database, MapReduce typically materializes every record before passing it along to the mapper.
If individual records are truly large, this can be very inefficient.
As shown earlier, records imported by Sqoop are laid out on disk in a fashion very similar to a database’s internal structure: an array of records with all fields of a record concatenated together.
When running a MapReduce program over imported records, each map task must fully materialize all fields of each record in its input split.
If the contents of a large object field are relevant only for a small subset of the total number of records used as input to a MapReduce program, it would be inefficient to fully materialize all these records.
Furthermore, depending on the size of the large object, full materialization in memory may be impossible.
To overcome these difficulties, Sqoop will store imported large objects in a separate file called a LobFile.
The LobFile format can store individual records of very large size (a 64-bit address space is used)
Each record in a LobFile holds a single large object.
The LobFile format allows clients to hold a reference to a record without accessing the record contents.
When records are accessed, this is done through a java.io.Input Stream (for binary objects) or java.io.Reader (for character-based objects)
When a record is imported, the “normal” fields will be materialized together in a text file, along with a reference to the LobFile where a CLOB or BLOB column is stored.
For example, suppose our widgets table contained a BLOB field named schematic holding the actual schematic diagram for each widget.
When running a MapReduce job processing many Widget records, you might need to access the schematic field of only a handful of records.
This system allows you to incur the I/O costs of accessing only the required large object entries, as individual schematics may be several megabytes or more of data.
The BlobRef and ClobRef classes cache references to underlying LobFiles within a map task.
If you do access the schematic field of several sequentially ordered records, they will take advantage of the existing file pointer’s alignment on the next record body.
Performing an Export In Sqoop, an import refers to the movement of data from a database system into HDFS.
By contrast, an export uses HDFS as the source of data and a remote database as the destination.
In the previous sections, we imported some data and then performed some analysis using Hive.
We can export the results of this analysis to a database for consumption by other tools.
Before exporting a table from HDFS to a database, we must prepare the database to receive the data by creating the target table.
We are going to export the zip_profits table from Hive.
We need to create a table in MySQL that has target columns in the same order, with the appropriate SQL types:
When we created the zip_profits table in Hive, we did not specify any delimiters.
So Hive used its default delimiters: a Ctrl-A character (Unicode 0x0001) between fields and a newline at the end of each record.
When we used Hive to access the contents of this table (in a SELECT statement), Hive converted this to a tab-delimited representation for.
But when reading the tables directly from files, we need to tell Sqoop which delimiters to use.
Sqoop assumes records are newline-delimited by default, but needs to be told about the Ctrl-A field delimiters.
The --input-fields-ter minated-by argument to sqoop export specified this information.
Sqoop supports several escape sequences, which start with a backslash (\) character, when specifying delimiters.
In the example syntax, the escape sequence is enclosed in single quotes to ensure that the shell processes it literally.
The escape sequences supported by Sqoop are listed in Table 15-1
Escape sequences can be used to specify nonprintable characters as field and record delimiters in Sqoop.
The actual character is specified by the octal value ooo.
This should be of the form \0xhhh, where hhh is the hex value.
Exports: A Deeper Look The architecture of Sqoop’s export capability is very similar in nature to how Sqoop performs imports.
See Figure 15-4.) Before performing the export, Sqoop picks a strategy based on the database connect string.
Sqoop then generates a Java class based on the target table definition.
This generated class has the ability to parse records from text files and insert values of the appropriate types into a table (in addition to the ability to read the columns from a ResultSet)
A MapReduce job is then launched that reads the source datafiles from HDFS, parses the records using the generated class, and executes the chosen export strategy.
The JDBC-based export strategy builds up batch INSERT statements that will each add multiple records to the target table.
Inserting many records per statement performs much better than executing many single-row INSERT statements on most database systems.
Separate threads are used to read from HDFS and communicate with the database, to ensure that I/O operations involving different systems are overlapped as much as possible.
For MySQL, Sqoop can employ a direct-mode strategy using mysqlimport.
Each map task spawns a mysqlimport process that it communicates with via a named FIFO on the local filesystem.
Data is then streamed into mysqlimport via the FIFO channel, and from there into the database.
Whereas most MapReduce jobs reading from HDFS pick the degree of parallelism (number of map tasks) based on the number and size of the files to process, Sqoop’s export system allows users explicit control over the number of tasks.
Exports and Transactionality Due to the parallel nature of the process, often an export is not an atomic operation.
Sqoop will spawn multiple tasks to export slices of the data in parallel.
These tasks can complete at different times, meaning that even though transactions are used inside tasks, results from one task may be visible before the results of another task.
As a result, one transaction cannot necessarily contain the entire set of operations performed by a task.
Sqoop commits results every few thousand rows, to ensure that it does not run out of memory.
Applications that will use the results of an export should not be started until the export process is complete, or they may see partial results.
You can specify a staging table with the --staging-table option.
The staging table must already exist and have the same schema as the destination.
Exports and SequenceFiles The example export reads source data from a Hive table, which is stored in HDFS as a delimited text file.
Sqoop can also export delimited text files that were not Hive tables.
For example, it can export text files that are the output of a MapReduce job.
Sqoop can also export records stored in SequenceFiles to an output table, although some restrictions apply.
Sqoop’s export tool will read objects from SequenceFiles and send them directly to the Output Collector, which passes the objects to the database export OutputFormat.
If you use the codegen tool (sqoop-codegen) to generate a SqoopRecord implementation for a record based on your export target table, you can then write a MapReduce program, which populates instances of this class and writes them to SequenceFiles.
Another means by which data may be in SqoopRecord instances in SequenceFiles is if data is imported from a database table to HDFS and modified in some fashion, and then the results are stored in SequenceFiles holding records of the same data type.
In this case, Sqoop should reuse the existing class definition to read data from SequenceFiles, rather than generate a new (temporary) record container class to perform the export, as is done when converting text-based records to database rows.
You can suppress code generation and instead use an existing record class and JAR by providing.
Sqoop will use the specified class, loaded from the specified JAR, when exporting records.
In the following example, we reimport the widgets table as SequenceFiles, and then export it back to the database in a different table:
During the import, we specified the SequenceFile format and indicated that we wanted the JAR file to be placed in the current directory (with --bindir) so we can reuse it.
We then created a destination table for the export, which had a slightly different schema, albeit one that is compatible with the original data.
We then ran an export that used the existing generated code to read the records from the SequenceFile and write them to the database.
Last.fm: The Social Music Revolution Founded in 2002, Last.fm is an Internet radio and music community website that offers many services to its users, such as free music streams and downloads, music and event recommendations, personalized charts, and much more.
There are about 25 million people who use Last.fm every month, generating huge amounts of data that need to be processed.
One example of this is users transmitting information indicating which songs they are listening to (this is known as “scrobbling”)
This data is processed and stored by Last.fm, so the user can access it directly (in the form of charts), and it is also used to make decisions about users’ musical tastes and compatibility, and artist and track similarity.
Hadoop at Last.fm As Last.fm’s service developed and the number of users grew from thousands to millions, storing, processing, and managing all the incoming data became increasingly challenging.
Fortunately, Hadoop was quickly becoming stable enough and was enthusiastically adopted as it became clear how many problems it solved.
It was first used at Last.fm in early 2006 and was put into production a few months later.
The distributed filesystem provided redundant backups for the data stored on it (e.g., web logs, user listening data) at no extra cost.
Scalability was simplified through the ability to add cheap commodity hardware when required.
The cost was right (free) at a time when Last.fm had limited financial resources.
The open source code and active community meant that Last.fm could freely modify Hadoop to add custom features and patches.
Hadoop provided a flexible framework for running distributed computing algorithms with a relatively easy learning curve.
Hundreds of daily jobs are run on the clusters performing operations, such as logfile analysis, evaluation of A/B tests, ad hoc processing, and charts generation.
This case study will focus on the process of generating charts, as this was the first usage of Hadoop at Last.fm and illustrates the power and flexibility that Hadoop provides over other approaches when working with very large datasets.
Generating Charts with Hadoop Last.fm uses user-generated track listening data to produce many different types of charts, such as weekly charts for tracks, per country and per user.
A number of Hadoop programs are used to process the listening data and generate these charts, and these run on a daily, weekly, or monthly basis.
Figure 16-1 shows an example of how this data is displayed on the site, in this case, the weekly top tracks.
A user tunes into one of Last.fm’s Internet radio stations and streams a song to his computer.
The Last.fm player or website can be used to access these streams and extra functionality is made available to the user, allowing him to love, skip, or ban each track that he listens to.
When processing the received data, we distinguish between a track listen submitted by a user (the first source in the previous list, referred to as a scrobble from here on) and a track listened to on the Last.fm radio (the second source, mentioned earlier, referred to as a radio listen from here on)
This distinction is very important in order to prevent a feedback loop in the Last.fm recommendation system, which is based only on scrobbles.
One of the most fundamental Hadoop jobs at Last.fm takes the incoming listening data and summarizes it into a format that can be used for display purposes on the Last.fm website as well as for input to other Hadoop programs.
This is achieved by the Track Statistics program, which is the example described in the following sections.
The Track Statistics Program When track listening data is submitted to Last.fm, it undergoes a validation and conversion phase, the end result of which is a number of space-delimited text files containing the user ID, the track ID, the number of times the track was scrobbled, the number of times the track was listened to on the radio, and the number of times it was skipped.
Table 16-1 contains sample listening data, which is used in the following examples as input to the Track Statistics program (the real data is gigabytes in size and includes many more fields that have been omitted here for simplicity’s sake)
These text files are the initial input provided to the Track Statistics program, which consists of two jobs that calculate various values from this data and a third job that merges the results (see Figure 16-2)
The Unique Listeners job calculates the total number of unique listeners for a track by counting the first listen by a user and ignoring all other listens by the same user.
The Sum job accumulates the total listens, scrobbles, radio listens, and skips for each track.
Although the input format of these two jobs is identical, two separate jobs are needed, as the Unique Listeners job is responsible for emitting values per track per user, and the Sum job emits values per track.
The final Merge job is responsible for merging the intermediate output of the two other jobs into the final result.
The end results of running the program are the following values per track:
Each job and its MapReduce phases are described in more detail next.
Please note that the provided code snippets have been simplified due to space constraints; for download details for the full code listings, refer to the Preface.
The size of this set is then emitted (i.e., the number of unique listeners) for each track ID.
Storing all the reduce values in a Set runs the risk of running out of memory if there are many values for a certain key.
Table 16-2 shows the sample input data for the job.
The Sum job is relatively simple; it just adds up the values we are interested in for each track.
The input data is again the raw text files, but in this case, it is handled quite differently.
The desired end result is a number of totals (unique listener count, play count, scrobble count, radio listen count, skip count) associated with each track.
To simplify things, we use an intermediate TrackStats object generated using Hadoop Record I/O, which implements WritableComparable (so it can be used as output) to hold these values.
The Mapper creates a TrackStats object and sets the values on it for each line in the file, except for the unique listener count, which is left empty (it will be filled in by the final merge job):
In this case, the reducer performs a very similar function to the Mapper, summing the statistics per track and returning an overall total:
Table 16-5 shows the input data for the job (the same as for the Unique Listeners job)
The final job needs to merge the output from the two previous jobs: the number of unique listeners per track and the statistics per track.
In order to merge these different inputs, two different mappers (one for each type of input) are used.
The two intermediate jobs are configured to write their results to different paths, and the MultipleInputs class is used to specify which Mapper will process which files.
The following code shows how the JobConf for the job is set up to do this:
It is possible to use a single Mapper to handle different inputs, but the example solution is more convenient and elegant.
It creates a TrackStats object in a similar manner to the SumMapper, but this time, it fills in only the unique listener count per track and leaves the other values empty:
The IdentityMapper is configured to process the SumJob’s output of TrackStats objects and, as no additional processing is required, directly emits the input data (see Table 16-10)
The two Mappers emit values of the same type: a TrackStats object per track, with different values filled in.
The final reduce phase can reuse the SumReducer described earlier to create a TrackStats object per track, sum up all the values, and emit it (see Table 16-11)
The final output files are then accumulated and copied to a server, where a web service makes the data available to the Last.fm website for display.
An example of this is shown in Figure 16-3, where the total number of listeners and plays are displayed for a track.
Summary Hadoop has become an essential part of Last.fm’s infrastructure and is used to generate and process a wide variety of datasets ranging from web logs to user listening data.
The example covered here has been simplified considerably in order to get the key concepts across; in real-world usage, the input data has a more complicated structure and the code that processes it is more complex.
Hadoop itself, although mature enough for production use, is still in active development, and new features and improvements are added by the Hadoop community every week.
We at Last.fm are happy to be part of this community as a contributor of code and ideas, and as end users of a great piece of open source technology.
Hadoop and Hive at Facebook Hadoop can be used to form core backend batch and near real-time computing infrastructures.
It can also be used to store and archive massive datasets.
In this case study, we will explore backend data architectures and the role Hadoop can play in them.
The amount of log and dimension data in Facebook that needs to be processed and stored has exploded as the usage of the site has increased.
A key requirement for any data processing platform for this environment is the ability to scale rapidly.
Further, with limited engineering resources, the system should be very reliable and easy to use and maintain.
Initially, data warehousing at Facebook was performed entirely on an Oracle instance.
After we started hitting scalability and performance problems, we investigated whether there were open source technologies that could be used in our environment.
As part of this investigation, we deployed a relatively small Hadoop instance and started publishing some of our core datasets into this instance.
Hadoop was attractive because Yahoo! was using it internally for its batch processing needs and because we were familiar with the simplicity and scalability of the MapReduce model as popularized by Google.
Our initial prototype was very successful: the engineers loved the ability to process massive amounts of data in reasonable timeframes, an ability that we just did not have.
They also loved being able to use their favorite programming language for processing (using Hadoop streaming)
Having our core datasets published in one centralized data store was also very convenient.
This made it even easier for users to process data in the Hadoop cluster by being able to express common computations in the form of SQL, a language with which most engineers and analysts are familiar.
As a result, the cluster size and usage grew by leaps and bounds, and today Facebook is running the second-largest Hadoop cluster in the world.
We are able to scale out this cluster rapidly in response to our growth, and we have been able to take advantage of open source by modifying Hadoop where required to suit our needs.
We have contributed back to open source, both in the form of contributions to some core components of Hadoop as well as by open-sourcing Hive, which is now a Hadoop top-level project.
There are at least four interrelated but distinct classes of uses for Hadoop at Facebook: • Producing daily and hourly summaries over large amounts of data.
These summaries are used for a number of different purposes within the company: — Reports based on these summaries are used by engineering and nonengineering.
These summaries include reports on growth of the users, page views, and average time spent on the site by the users.
Providing performance numbers about advertisement campaigns that are run on Facebook.
Backend processing for site features such as people you may like and applications you may like.
These analyses help answer questions from our product groups and executive team.
As a de facto long-term archival store for our log datasets.
To look up log events by specific attributes (where logs are indexed by such.
Figure 16-4 shows the basic components of our architecture and the data flow within these components.
As shown in Figure 16-4, the following components are used in processing data: Scribe.
Log data is generated by web servers as well as internal services such as the Search backend.
We use Scribe, an open source log collection service developed in Facebook that deposits hundreds of log datasets with a daily volume in the tens of terabytes into a handful of NFS servers.
Dimension data is also scraped from our internal MySQL databases and copied over into HDFS daily.
Hive/Hadoop We use Hive, a Hadoop subproject developed in Facebook, to build a data warehouse over all the data collected in HDFS.
Files in HDFS, including log data from Scribe and dimension data from the MySQL tier, are made available as tables with logical partitions.
A SQL-like query language provided by Hive is used in conjunction with MapReduce to create/publish a variety of summaries and reports, as well as to perform historical analysis over these tables.
Tools Browser-based interfaces built on top of Hive allow users to compose and launch Hive queries (which in turn launch MapReduce jobs) using just a few mouse clicks.
The volume of data here is relatively small, but the query rate is high and needs real-time response.
DataBee An in-house Extract, Transform, Load (ETL) workflow software that is used to provide a common framework for reliable batch processing across all data processing jobs.
Data from the NFS tier storing Scribe data is continuously replicated to the HDFS cluster by copier jobs.
The NFS devices are mounted on the Hadoop tier, and the copier processes run as Map-only jobs on the Hadoop cluster.
This makes it easy to scale the copier processes and makes them fault-resilient.
Currently, we copy over 6 TB per day from Scribe to HDFS in this manner.
We also download up to 4 TB of dimension data from our MySQL tier to HDFS every day.
These are also conveniently arranged on the Hadoop cluster as Map-only jobs that copy data out of MySQL boxes.
We use a single HDFS instance, and a vast majority of processing is done in a single MapReduce cluster (running a single jobtracker)
We can minimize the administrative overheads by operating a single cluster.
All data is available in a single place for all.
By using the same compute cluster across all departments, we get tremendous.
Our users work in a collaborative environment, so requirements in terms of quality.
We also have a single shared Hive metastore (using a MySQL database) that holds metadata about all the Hive tables stored in HDFS.
Hypothetical Use Case Studies In this section, we will describe some typical problems that are common for large websites, which are difficult to solve through traditional warehousing technologies, simply because the costs and scales involved are prohibitively high.
Hadoop and Hive can provide a more scalable and more cost-effective solution in such situations.
One of the most common uses of Hadoop is to produce summaries from large volumes of data.
It is very typical of large ad networks, such as the Facebook ad network, Google AdSense, and many others, to provide advertisers with standard aggregated statistics.
Computing advertisement performance numbers on large datasets is a very data-intensive operation, and the scalability and cost advantages of Hadoop and Hive can really help in computing these numbers in a reasonable time frame and at a reasonable cost.
Many ad networks provide standardized CPC- and CPM-based ad units to the advertisers.
The CPC ads are cost-per-click ads: the advertiser pays the ad network amounts that are dependent on the number of clicks that the particular ad gets from the users visiting the site.
The CPM ads (short for cost per mille, that is, the cost per thousand impressions), on the other hand, bill the advertisers amounts that are proportional to the number of users who see the ad on the site.
Apart from these standardized ad units, in the last few years ads with more dynamic content that is tailored to each individual user have also become common in the online advertisement industry.
Yahoo! does this through SmartAds, whereas Facebook provides its advertisers with Social Ads.
The latter allows the advertisers to embed information from a user’s network of friends; for example, a Nike ad may refer to a friend of the user who recently fanned Nike and shared that information with her friends on Facebook.
In addition, Facebook also provides Engagement Ad units to the advertisers, wherein the users can more effectively interact with the ad, be it by commenting on it or by playing embedded videos.
In general, a wide variety of ads are provided to the advertisers by the online ad networks, and this variety adds yet another dimension to the various kinds of performance numbers that the advertisers are interested in getting about their campaigns.
At the most basic level, advertisers are interested in knowing the total number of users, as well as the number of unique users, who have seen the ad or have clicked on it.
For more dynamic ads, they may even be interested in getting the breakdown of these aggregated numbers by the kind of dynamic information shown in the ad unit or the kind of engagement action undertaken by the users on the ad.
Similarly, a video embedded inside an Engagement Ad may have been watched by 100,000 unique users.
In addition, these performance numbers are typically reported for each ad, campaign, and account.
An account may have multiple campaigns, with each campaign running multiple ads on the network.
Finally, these numbers are typically reported for different time durations by the ad networks.
Typical durations are daily, rolling week, month to date, rolling month, and sometimes even for the entire lifetime of the campaign.
Moreover, advertisers also look at the geographic breakdown of these numbers, among other ways of slicing and dicing this data, such as what percentage of the total viewers or clickers of a particular ad are in the Asia Pacific region.
As is evident, there are four predominant dimension hierarchies: the account, campaign, and ad dimension; the time period; the type of interaction; and the user dimension.
The last of these is used to report unique numbers, whereas the other three are the reporting dimensions.
The user dimension is also used to create aggregated geographic profiles for the viewers and clickers of ads.
All this information in totality allows the advertisers to tune their campaigns to improve their effectiveness on any given ad.
Aside from the multidimensional nature of this set of pipelines, the volumes of data processed and the rate at which this data is growing on a daily basis make this difficult to scale without a technology such as Hadoop for large ad networks.
As of this writing, for example, the ad log volume that is processed for ad performance numbers at Facebook is approximately 1 TB per day of (uncompressed) logs.
Hadoop’s ability to scale with hardware has been a major factor behind the ability of these pipelines to keep up with this data growth with only minor tweaking of job configurations.
Typically, these configuration changes involve increasing the number of reducers for the Hadoop jobs that are processing the intensive portions of these pipelines.
Apart from regular reports, another primary use case for a data warehousing solution is to support ad hoc analysis and product feedback solutions.
Any typical website, for example, makes product changes, and product managers or engineers often need to understand the impact of a new feature, based on user engagement as well as on the click-through rate on that feature.
The product team may even wish to perform a deeper analysis on the impact of the change based on various regions and countries, such as whether this change increases the click-through rate of the users in the US or whether it reduces the engagement of users in India.
A lot of this type of analysis could be done with Hadoop by using Hive and regular SQL.
The measurement of click-through rate can be easily expressed as a join of the impressions and clicks for the particular link related to the feature.
This information can be joined with geographic information to compute the effect of product changes on different regions.
Subsequently, one can compute average click-through rate for different geographic regions by performing aggregations over them.
All of these are easily expressible in Hive using a couple of SQL queries (that would, in turn, generate multiple Hadoop jobs)
If only an estimate were required, the same queries can be run for a sample set of the users using the sampling functionality natively supported by Hive.
Some of this analysis needs the use of custom map and reduce scripts in conjunction with the Hive SQL, and that is also easy to plug into a Hive query.
A good example of a more complex analysis is estimating the peak number of users logging into the site per minute for the entire past year.
This would involve sampling page view logs (because the total page view data for a popular website is huge), grouping it by time, and then finding the number of new users at different time points via a custom reduce script.
This is a good example where both SQL and MapReduce are required for solving the end user problem, and something that is possible to achieve easily with Hive.
Hive and Hadoop are easy to use for training and scoring for data analysis applications.
These data analysis applications can span multiple domains, such as popular websites, bioinformatics companies, and oil exploration companies.
A typical example of such an application in the online ad network industry would be the prediction of what features of an ad make it more likely to be noticed by the user.
The training phase typically would involve identifying the response metric and the predictive features.
In this case, a good metric to measure the effectiveness of an ad could be its click-through rate.
Some interesting features of the ad could be the industry vertical that it belongs to, the content of the ad, the placement of the ad on the page, and so on.
Hive is useful for assembling training data and then feeding the same into a data analysis engine (typically R or user programs written in MapReduce)
In this particular case, different ad performance numbers and features can be structured as tables in Hive.
One can easily sample this data (sampling is required, as R can handle only limited data volume) and perform the appropriate aggregations and joins using Hive queries to assemble a response table containing the most important ad features that determine the effectiveness of an advertisement.
However, because sampling loses information, some of the more important data analysis applications use parallel implementations of popular data analysis kernels using the MapReduce framework.
Once the model has been trained, it may be deployed for scoring on a daily basis.
The bulk of the data analysis tasks do not perform daily scoring, though.
Many of them are ad hoc in nature and require one-time analysis that can be used as input into the product design process.
Hive When we started using Hadoop, we very quickly became impressed by its scalability and availability.
However, we were worried about widespread adoption, primarily because of the complexity involved in writing MapReduce programs in Java (as well as the cost of training users to write them)
We were aware that a lot of engineers and analysts in the company understood SQL as a tool to query and analyze data, and that many of them were proficient in a number of scripting languages, such as PHP and Python.
As a result, it was imperative for us to develop software that could bridge this gap between the languages that the users were proficient in and the languages required to program Hadoop.
It was also evident that a lot of our datasets were structured and could be easily partitioned.
The natural consequence of these requirements was a system that could model data as tables and partitions and that could also provide a SQL-like language for query and analysis.
Also essential was the ability to plug in customized MapReduce programs written in the programming language of the user’s choice into the query.
Hive is a data warehouse infrastructure built on top of Hadoop and.
In the following sections, we describe this system in more detail.
Data is organized consistently across all datasets and is compressed, partitioned, and sorted: Compression.
Almost all datasets are stored as sequence files using the gzip codec.
Older datasets are recompressed to use the bzip codec, which affords substantially more compression than gzip.
Bzip is slower than gzip, but older data is accessed much less frequently, and this performance hit is well worth the savings in terms of disk space.
Individual partitions are loaded into Hive, which loads each partition into a separate HDFS directory.
In most cases, this partitioning is based simply on datestamps associated with Scribe logfiles.
However, in some cases, we scan data and collate them based on the timestamp available inside a log entry.
Going forward, we are also going to be partitioning data on multiple attributes (for example, country and date)
Sorting Each partition within a table is often sorted (and hash-partitioned) by unique ID (if one is present)
It is easy to run sampled queries on such datasets.
Aggregates and joins involving unique IDs can be done very efficiently on such.
Loading data into this long-term format is done by daily MapReduce jobs (and is distinct from the near real-time data import processes)
It has traditional SQL constructs such as joins, group bys, where, select, from clauses, and from clause subqueries.
It tries to convert SQL commands into a set of MapReduce jobs.
Apart from the normal SQL clauses, it has a bunch of other extensions, such as the ability to specify custom Mapper and reducer scripts in the query itself; the ability to insert into multiple tables, partitions, HDFS, or local files while doing a single scan of the data; and the ability to run the query on data samples rather than the full dataset (this ability is fairly useful when testing queries)
The Hive metastore stores the metadata for a table and provides this metadata to the Hive compiler for converting SQL commands to MapReduce jobs.
Through partition pruning, map-side aggregations, and other features, the compiler tries to create plans that can optimize the runtime for the query.
Additionally, the ability provided by Hive in terms of expressing data pipelines in SQL can and has provided the much-needed flexibility in putting these pipelines together in an easy and expedient manner.
This is especially useful for organizations and products that are still evolving and growing.
Many of the operations needed in processing data pipelines are the well-understood SQL operations such as join, group by, and distinct aggregations.
With Hive’s ability to convert SQL into a series of Hadoop MapReduce jobs, it becomes fairly easy to create and maintain these pipelines.
We illustrate these facets of Hive in this section by using an example of a hypothetical ad network and showing how some typical aggregated reports needed by the advertisers can be computed using Hive.
This would also be the typical SQL statement that one could use in other RDBMSs, such as Oracle, DB2, and so on.
In order to compute the daily impression numbers by ad and account from the same joined data as earlier, Hive provides the ability to do multiple group bys simultaneously, as shown in the following query (which is SQL-like, but not strictly SQL):
In one of the optimizations that is being added to Hive, the query can be converted into a sequence of Hadoop MapReduce jobs that are able to scale with data skew.
Essentially, the join is converted into one MapReduce job ,and the three group bys are converted into four MapReduce jobs, with the first one generating a partial aggregate on unique_id.
As a result, computing the partial aggregation by unique_id allows the pipeline to distribute the work more uniformly to the reducers.
Computing the lifetime numbers can be more tricky, though, because using the strategy described previously, one would have to scan all the partitions of the impres sion_logs table.
The data in this table combined with the next day’s impression_logs can be used to incrementally generate the lifetime ad performance numbers.
The Hive queries that can be used to achieve this are as follows:
The SQL is converted to a single Hadoop MapReduce job that essentially computes the group by statement on the combined stream of inputs.
This SQL can be followed by the next Hive query, which computes the actual numbers for different groupings (similar to the one in the daily pipelines):
Hive and Hadoop are batch processing systems that cannot serve the computed data with the same latency as a standard RDBMS, such as Oracle or MySQL.
Therefore, on many occasions, it is still useful to load the summaries generated through Hive and Hadoop to a more traditional RDBMS for serving this data to users through different business intelligence (BI) tools, or even though a web portal.
Hadoop clusters typically run a mix of daily production jobs that need to finish computation within a reasonable time frame as well as ad hoc jobs that may be of different priorities and sizes.
In typical installations, these production jobs tend to run overnight, when interference from ad hoc jobs run by users is minimal.
However, overlap between large ad hoc and production jobs is often unavoidable and, without adequate safeguards, can impact the latency of production jobs.
It also means that a single rogue job can bring down the entire cluster and put production processes at risk.
The fair-sharing Hadoop job scheduler, developed at Facebook and contributed back to Hadoop, provides a solution to many of these issues.
It reserves guaranteed compute resources for specific pools of jobs while at the same time letting idle resources be used by everyone.
It also prevents large jobs from hogging cluster resources by allocating compute resources in a fair manner across these pools.
Memory can become one of the most contended resources in the cluster.
We have made some changes to Hadoop so that if the job tracker is low on memory, Hadoop job submissions are throttled.
This can allow the user processes to run with reasonable per-process memory limits, and it is possible to put in place some monitoring scripts in order to prevent MapReduce jobs from impacting HDFS daemons (due primarily to high memory consumption) running on the same node.
Log directories are stored in separate disk partitions and cleaned regularly, and we think it can also be useful to put MapReduce intermediate storage in separate disk partitions as well.
Utilization is increasing at a fast rate with the growth of data, and many growing companies with expanding datasets have the same pain.
In many situations, much of this data is temporary in nature.
In such cases, one can use retention settings in Hive and also recompress older data in bzip format to save on space.
This will make it cheaper to store archival data in Hadoop.
We are currently working on a data archival layer to make this possible and to unify all the aspects of dealing with older data.
Currently, Scribe writes to a handful of NFS filers from where data is picked up and delivered to HDFS by custom copier jobs, as described earlier.
We are working on making Scribe write directly to another HDFS instance.
This will make it very easy to scale and administer Scribe.
Due to the high uptime requirements for Scribe, its target HDFS instance is likely to be different from the production HDFS instance (so that it is isolated from any load/downtime issues due to user jobs)
We are working on a number of key features, such as support for order by and having clause supports, more aggregate functions, more built-in functions, a datetime data type, and so on.
At the same time, a number of performance optimizations are being worked on, such as predicate pushdown and common subexpression elimination.
On the integration side, JDBC and ODBC drivers are being developed in order to integrate with OLAP and BI tools.
With all these optimizations, we hope that we can unlock the power of MapReduce and Hadoop, and bring it closer to nonengineering communities as well as within Facebook.
Nutch Search Engine Nutch is a framework for building scalable Internet crawlers and search engines.
It’s an Apache Software Foundation project and a subproject of Lucene, and it’s available under the Apache 2.0 license.
We won’t go deeply into the anatomy of a web crawler as such; instead, the purpose of this case study is to show how Hadoop can be used to implement various complex processing tasks typical for a search engine.
Interested readers can find plenty of Nutchspecific information on the official site of the project (http://lucene.apache.org/nutch)
Suffice it to say that in order to create and maintain a search engine, one needs the following subsystems: Database of pages.
This database keeps track of all pages known to the crawler and their status, such as the last time it visited the page, its fetching status, refresh interval, content checksum, etc.
List of pages to fetch As crawlers periodically refresh their view of the Web, they download new pages (previously unseen) or refresh pages that they think have already expired.
Nutch calls such a list of candidate pages prepared for fetching a fetchlist.
Raw page data Page content is downloaded from remote sites and stored locally in the original uninterpreted format, as a byte array.
Parsed page data Page content is then parsed using a suitable parser.
Nutch provides parsers for documents in many popular formats, such as HTML, PDF, Open Office and Microsoft Office, RSS, and others.
Link graph database This database is necessary to compute link-based page ranking scores, such as PageRank.
Full-text search index This is a classical inverted index, built from the collected page metadata and from the extracted plain-text content.
We briefly mentioned before that Hadoop began its life as a component in Nutch, intended to improve its scalability and to address clear performance bottlenecks caused by a centralized data processing model.
Nutch was also the first public proof-of-concept application ported to the framework that would later become Hadoop, and the effort required to port Nutch algorithms and data structures to Hadoop proved to be surprisingly small.
This probably encouraged the following development of Hadoop as a separate subproject with the aim of providing a reusable framework for applications other than Nutch.
Currently, nearly all Nutch tools process data by running one or more MapReduce jobs.
Data Structures There are several major data structures maintained in Nutch, and they all make use of Hadoop I/O classes and formats.
Depending on the purpose of the data and the way it’s accessed once it’s created, the data is kept using either Hadoop map files or sequence files.
So to be precise, we should say that data is kept in several partial map files or sequence files, with as many parts as there were reduce tasks in the job that created the data.
For simplicity, we omit this distinction in the following sections.
In order to provide quick random access to the records (sometimes useful for diagnostic reasons, when users want to examine individual records in CrawlDb), this data is stored in map files and not in sequence files.
CrawlDb is initially created using the Injector tool, which simply converts a plain-text representation of the initial list of URLs (called the seed list) to a map file in the format described earlier.
Subsequently, it is updated with the information from the fetched and parsed pages—more on that shortly.
This database stores the incoming link information for every URL known to Nutch.
It’s worth noting that this information is not immediately available during page collection, but the reverse information is available, namely that of outgoing links from a page.
The process of inverting this relationship is implemented as a MapReduce job, described shortly.
Segments in Nutch parlance correspond to fetching and parsing a batch of URLs.
Nutch uses a map file here because it needs fast random access in order to present a cached view of a page.
This data uses the sequence file format, first because it’s processed sequentially, and second because we couldn’t satisfy the map file invariants of sorted keys.
We need to spread URLs that belong to the same host as far apart as possible to minimize the load per target host, and this means that records are sorted more or less randomly.
This information is crucial later on to build an inverted graph (of incoming links, or inlinks)
As this list is processed in several steps, the segment collects output data from the processing tools in a set of subdirectories.
For example, the content part is populated by a tool called Fetcher, which downloads raw data from URLs on the fetchlist (2)
This tool also saves the status information in crawl_fetch so that this data can be used later on for updating the status of the page in CrawlDb.
Selected Examples of Hadoop Data Processing in Nutch The following sections present relevant details of some Nutch tools to illustrate how the MapReduce paradigm is applied to a concrete data processing task in Nutch.
However, most algorithms for calculating a page’s importance (or quality) need the opposite information, that is, what pages contain outlinks that point to the current page.
Also, the indexing process benefits from taking into account the anchor text on inlinks so that this text may semantically enrich the text of the current page.
As mentioned earlier, Nutch collects the outlink information and then uses this data to build a LinkDb, which contains this reversed link data in the form of inlinks and anchor text.
This section presents a rough outline of the implementation of the LinkDb tool.
Many details have been omitted (such as URL normalization and filtering) in order to present a clear picture of the process.
What’s left gives a classical example of why the MapReduce paradigm fits so well with the key data transformation processes required to run a search engine.
Large search engines need to deal with massive web graph data (many pages with a lot of outlinks/inlinks), and the parallelism and fault tolerance offered by Hadoop make this possible.
Additionally, it’s easy to express the link inversion using the Map-sort-Reduce primitives, as illustrated next.
The snippet here presents the job initialization of the LinkDb tool:
As we can see, the source data for this job is the list of fetched URLs (keys) and the corresponding ParseData records that contain, among others, the outlink information for each page as an array of outlinks.
An outlink contains both the target URL and the anchor text.
The output from the job is again a list of URLs (keys), but the values are instances of inlinks, which is simply a specialized set of inlinks that contain target URLs and anchor text.
Perhaps surprisingly, URLs are typically stored and processed as plain text and not as java.net.URL or java.net.URI instances.
There are several reasons for this: URLs extracted from downloaded content usually need normalization (e.g., converting hostnames to lowercase, resolving relative paths), are often broken or invalid, or refer to unsupported protocols.
Many normalization and filtering operations are better expressed as text patterns that span several parts of a URL.
Also, for the purpose of link analysis, we may still want to process and count invalid URLs.
Let’s take a closer look now at the map() and reduce() implementations.
In this case, they are simple enough to be implemented in the body of the same class:
Subsequently, these one-element-long Inlinks are aggregated in the reduce() method:
From this code, it’s obvious that we got exactly what we wanted—that is, a list of all fromUrls that point to our toUrl, together with their anchor text.
Let’s take a look now at a more complicated use case.
Earlier, we mentioned briefly that fetchlists need to be generated in a special way so that the data in each part of the fetchlist (and consequently processed in each map task) meets certain requirements:
All URLs from the same host need to end up in the same partition.
This is required so that Nutch can easily implement in-JVM host-level blocking to avoid overwhelming target hosts.
URLs from other hosts) in order to minimize the host-level blocking.
There should be no more than x URLs from any single host so that large sites with many URLs don’t dominate smaller sites (and URLs from smaller sites still have a chance to be scheduled for fetching)
URLs with high scores should be preferred over URLs with low scores.
There should be, at most, y URLs in total in the fetchlist.
In this case, two MapReduce jobs are needed to satisfy all these requirements, as illustrated in Figure 16-7
Again, in the following listings, we are going to skip some details of these steps for the sake of brevity.
In this step, Nutch runs a MapReduce job to select URLs that are considered eligible for fetching and to sort them by their score (a floating-point value assigned to each URL, e.g., a PageRank score)
The Selector class implements three functions: mapper, reducer, and partitioner.
The last function is especially interesting: Selector uses a custom Partitioner to assign URLs from the same host to the same reduce task so that we can satisfy criteria 3–5 from the previous list.
If we didn’t override the default partitioner, URLs from the same host would end up in different partitions of the output, and we wouldn’t be able to track and limit the total counts, because MapReduce tasks don’t communicate between themselves.
As it is now, all URLs that belong to the same host will end up being.
Step 1: Select, sort by score, limit by URL count per host.
It’s easy to implement a custom partitioner so that data that needs to be processed in the same task ends up in the same partition.
Let’s take a look first at how the Selector class implements the Partitioner interface (which consists of a single method):
As you can see from the code snippet, the partition number is a function of only the host part of the URL, which means that all URLs that belong to the same host will end up in the same partition.
The output from this job is sorted in decreasing order by score.
Observant readers will notice that we had to use something other than the original keys, but we still want to preserve the original key-value pairs.
We use here a SelectorEn try class to pass the original key-value pairs to the next step of processing.
Please note that the enforcement of the total count limit is necessarily approximate.
We calculate the limit for the current task as the total limit divided by the number of reduce tasks.
But we don’t know for sure from within the task that it is going to get an equal share of URLs;
The input data for this step is the output data produced in step 1
The following is a snippet showing the setup of this job:
Careful readers may wonder why we don’t go one step further, extracting also the original CrawlDatum and using it as the value—more on this shortly.
Otherwise, Hadoop assumes that we use the same classes as declared for the reduce output (this conflict usually would cause a job to fail)
The output from the map phase is partitioned using the PartitionUrlByHost class so that it again assigns URLs from the same host to the same partition.
Once the data is shuffled from map to reduce tasks, it’s sorted by Hadoop according to the output key comparator, in this case the HashComparator.
This class uses a simple hashing scheme to mix URLs in a way that is least likely to put URLs from the same host close to each other.
In order to meet requirement 6, we set the number of reduce tasks to the desired number of Fetcher map tasks (the numParts mentioned earlier), keeping in mind that each reduce partition will be used later on to create a single Fetcher map task.
A surprising side effect of using HashCompara tor is that several URLs may be hashed to the same hash value, and Hadoop will call.
Now it becomes clear why we had to preserve all URLs in SelectorEntry records, because now we can extract them from the iterated values.
The Fetcher application in Nutch is responsible for downloading the page content from remote sites.
As such, it is important that the process uses every opportunity for parallelism, in order to minimize the time it takes to crawl a fetchlist.
There is already one level of parallelism present in Fetcher; multiple parts of the input fetchlists are assigned to multiple map tasks.
However, in practice this is not sufficient: sequential download of URLs, from different hosts (see the earlier section on HashCom parator), would be a tremendous waste of time.
For this reason, the Fetcher map tasks process this data using multiple worker threads.
Hadoop uses the MapRunner class to implement the sequential processing of input data records.
The Fetcher class implements its own MapRunner that uses multiple threads to process input records in parallel.
We can’t run several map tasks to download content from the same hosts, because it would violate the host-level load limits (such as the number of concurrent requests and the number of requests per second)
Next, we use a custom InputFormat implementation that prevents Hadoop from splitting partitions of input data into smaller chunks (splits), thus creating more map tasks.
Output data is stored using a custom OutputFormat implementation, which uses data contained in NutchWritable values to create several output map files and sequence files.
The NutchWritable class is a subclass of GenericWritable, able to pass instances of several different Writable classes declared in advance.
The Fetcher class implements the MapRunner interface, and we set this class as the job’s MapRunner implementation.
Fetcher reads many input records in advance, using the QueueFeeder thread that puts input records into a set of per-host queues.
Then several FetcherThread instances are started, which consume items from per-host queues, while QueueFeeder keeps reading input data to keep the queues filled.
In the meantime, the main thread of the map task spins around, waiting for all threads to finish their job.
Periodically, it reports the status to the framework to ensure that Hadoop doesn’t consider this task to be dead and kill it.
Once all items are processed, the loop is finished and the control is returned to Hadoop, which considers this map task to be completed.
This is an example of a MapReduce application that doesn’t produce sequence file or map file output; instead, the output from this application is a Lucene index.
Again, as MapReduce applications may consist of several reduce tasks, the output from this application may consist of several partial Lucene indexes.
The Nutch Indexer tool uses information from CrawlDb, LinkDb, and Nutch segments (fetch status, parsing status, page metadata, and plain-text data), so the job setup section involves adding several input paths:
All corresponding records for a URL dispersed among these input locations need to be combined to create Lucene documents for addition to the index.
The Mapper implementation in Indexer simply wraps input data, whatever its source and implementation class, in a NutchWritable so that the reduce phase may receive data from different sources, using different classes, and still be able to consistently declare a single output value class (as NutchWritable) from both map and reduce steps.
In addition to all textual content (coming either from the plain-text data or from metadata), this document also contains PageRank-like score information (obtained from CrawlDb data)
Nutch uses this score to set the boost value of the Lucene document.
The OutputFormat implementation is the most interesting part of this tool:
When an instance of RecordWriter is requested, the OutputFormat creates a new Lucene index by opening an IndexWriter.
When a reduce task is finished, Hadoop will try to close the RecordWriter.
In this case, the process of closing may take a long time because we would like to optimize the index before closing.
During this time, Hadoop may conclude that the task is hung, since there are no progress updates, and it may attempt to kill it.
For this reason, we first start a background thread to give reassuring progress updates, and then proceed to perform the index optimization.
Once the optimization is completed, we stop the progress updater thread.
The output index is now created, optimized, and closed, after which it is ready for use in a searcher application.
Summary This short overview of Nutch necessarily omits many details, such as error handling, logging, URL filtering and normalization, dealing with redirects or other forms of “aliased” pages (such as mirrors), removing duplicate content, calculating PageRank scoring, etc.
You can find this and much more information on the official page of the project and on the wiki (http://wiki.apache.org/nutch)
Today, Nutch is used by many organizations and individual users.
Still, operating a search engine requires nontrivial investments in hardware, integration, customization, and the maintenance of the index, so in most cases, Nutch is used to build commercial vertical- or field-specific search engines.
Nutch is under active development, and the project closely follows new releases of Hadoop.
As such, it will continue to be a practical example of a real-life application that uses Hadoop at its core, with excellent results.
Log Processing at Rackspace Rackspace Hosting has always provided managed systems for enterprises, and in that vein, Mailtrust became Rackspace’s mail division in Fall 2007
Rackspace currently hosts email for over 1 million users and thousands of companies on hundreds of servers.
It is extremely helpful to aggregate that data for growth planning purposes and to understand how customers use our applications, and the records are also a boon for troubleshooting problems in the system.
If an email fails to be delivered, or a customer is unable to log in, it is vital that our customer support team is able to find enough information about the problem to begin the debugging process.
To make it possible to find that information quickly, we cannot leave the logs on the machines that generated them or in their original format.
Instead, we use Hadoop to do a considerable amount of processing, with the end result being Lucene indexes that customer support can query.
Two of our highest-volume log formats are produced by the Postfix mail transfer agent and Microsoft Exchange Server.
All mail that travels through our systems touches Postfix at some point, and the majority of messages travel through multiple Postfix servers.
The Exchange environment is independent by necessity, but one class of Postfix machines acts as an added layer of protection and uses SMTP to transfer messages between mailboxes hosted in each environment.
The messages travel through many machines, but each server knows only enough about the destination of the mail to transfer it to the next responsible server.
Thus, in order to build the complete history of a message, our log processing system needs to have a global view of the system.
This is where Hadoop helps us immensely: as our system grows, so does the volume of logs.
For our log processing logic to stay viable, we had to ensure that it would scale, and MapReduce was the perfect framework for that growth.
Brief History Earlier versions of our log processing system were based on MySQL, but as we gained more and more logging machines, we reached the limits of what a single MySQL server could process.
The database schema was already reasonably denormalized, which would have made it less difficult to shard, but MySQL’s partitioning support was still very weak at that point in time.
Rather than implementing our own sharding and processing solution around MySQL, we chose to use Hadoop.
Choosing Hadoop As soon as you shard the data in a RDBMS system, you lose a lot of the advantages of SQL for performing analysis of your dataset.
Hadoop gives us the ability to easily process all of our data in parallel using the same algorithms we would for smaller datasets.
The servers generating the logs we process are distributed across multiple data centers, but we currently have a single Hadoop cluster, located in one of those data centers (see Figure 16-8)
In order to aggregate the logs and place them into the cluster, we use the Unix syslog replacement syslog-ng and some simple scripts to control the creation of files in Hadoop.
Within a data center, syslog-ng is used to transfer logs from a source machine to a loadbalanced set of collector machines.
On the collectors, each type of log is aggregated into a single stream and lightly compressed with gzip (step A in Figure 16-8)
From remote collectors, logs can be transferred through an SSH tunnel cross-data center to collectors that are local to the Hadoop cluster (step B)
Once the compressed log stream reaches a local collector, it can be written to Hadoop (step C)
We currently use a simple Python script that buffers input data to disk and periodically pushes the data into the Hadoop cluster using the Hadoop command-line interface.
The script copies the log buffers to input folders in Hadoop when they reach a multiple of the Hadoop block size or when enough time has passed.
By using SOCKS support and the HDFS API directly from remote collectors, we could eliminate one disk write and a lot of complexity from the system.
We plan to implement a replacement using these features in future development sprints.
Once the raw logs have been placed in Hadoop, they are ready for processing by our MapReduce jobs.
We use a default replication factor of three for files that need to survive for our archive period of six months and a factor of two for anything else.
To provide reasonably high availability, we use two secondary namenodes and a virtual IP that can easily be pointed at any of the three machines with snapshots of the HDFS.
This means that in a failover situation, there is potential for us to lose up to 30 minutes of data, depending on the ages of the snapshots on the secondary namenodes.
This is acceptable for our log processing application, but other Hadoop applications may require lossless failover by using shared storage for the namenode’s image.
In distributed systems, the sad truth of unique identifiers is that they are rarely truly unique.
All email messages have a (supposedly) unique identifier called a message-id that is generated by the host where they originated, but a bad client could easily send out duplicates.
In addition, since the designers of Postfix could not trust the messageid to uniquely identify the message, they were forced to come up with a separate ID called a queue-id, which is guaranteed to be unique only for the lifetime of the message on a local machine.
Although the message-id tends to be the definitive identifier for a message, in Postfix logs, it is necessary to use queue-ids to find the message-id.
Looking at the second line in Example 16-1 (which is formatted to better fit the page), you will see the hex string.
From a MapReduce perspective, each line of the log is a single key-value pair.
In phase 1, we need to map all lines with a single queue-id key together, and then reduce them to determine whether the log message values indicate that the queue-id is complete.
Similarly, once we have a completed the queue-id phase for a message, we need to group by the message-id in phase 2
We map each completed queue-id with its message-id as key and a list of its log lines as the value.
In the reduce, we determine whether all of the queue-ids for the message-id indicate that the message left our system.
Together, the two phases of the mail log MapReduce job and their InputFormat and OutputFormat form a type of staged event-driven architecture (SEDA)
In SEDA, an application is broken up into multiple “stages,” which are separated by queues.
In a Hadoop context, a queue could be either an input folder in HDFS that a MapReduce job consumes from or the implicit queue that MapReduce forms between the map and reduce steps.
In Figure 16-9, the arrows between stages represent the queues, with a dashed arrow being the implicit MapReduce queue.
Each stage can send a key-value pair (SEDA calls them events or messages) to another stage via these queues.
During the first phase of our Mail log processing job, the inputs to the map stage are either a line number key and log message value or a queue-id key to an array of log-message values.
The first type of input is generated when we process a raw logfile from the queue of input files, and the second type is an intermediate format that represents the state of a queue-id we have already attempted to process but that was requeued because it was incomplete.
The two input formats come from different input folders (queues) in HDFS.
During this phase, the reduce stage determines whether the queue-id has enough lines to be considered completed.
If the queue-id is completed, we output the message-id as key and a HopWritable object as value.
Otherwise, the queue-id is set as the key, and the array of log lines is requeued to be Mapped with the next set of raw logs.
This will continue until we complete the queue-id or until it times out.
It completely describes a message from the viewpoint of a single server, including the sending address and IP, attempts to deliver the message to other servers, and typical message header information.
This split output is accomplished with an OutputFormat implementation that is somewhat symmetrical with our dual InputFormat.
In the next stage of the mail log processing job, the input is a message-id key, with a HopWritable value from the previous phase.
In the final reduce stage, we want to see whether all of the HopWrita bles we have collected for the message-id represent a complete message path through our system.
A message path is essentially a directed graph (which is typically acyclic, but it may contain loops if servers are misconfigured)
In this graph, a vertex is a server, which can be labeled with multiple queue-ids, and attempts to deliver the message from one server to another are edges.
If the reducer decides that all of the queue-ids for a message-id create a complete message path, then the message is serialized and queued for the SolrOutputFormat.
Otherwise, the HopWritables for the message are queued for the phase 2: map stage to be reprocessed with the next batch of queue-ids.
Closing the OutputFormat then involves compressing the disk index to the final destination for the output file.
This approach has a few advantages over using Solr’s HTTP interface or using Lucene directly:
We currently use the default HashPartitioner class to decide which reduce task will receive particular keys, which means that the keys are semirandomly distributed.
In a future iteration of the system, we’d like to implement a new Partitioner to split by the sending address instead (our most common search term)
Once the indexes are split by sender, we can use the hash of the address to determine where to merge or query for an index, and our search API will need to communicate only with the relevant nodes.
After a set of MapReduce phases have completed, a different set of machines are notified of the new indexes and can pull them for merging.
These search nodes are running Apache Tomcat and Solr instances to host completed indexes, along with a service to pull and merge the indexes to local disk (step D in Figure 16-8)
Our MergeAgent service decompresses each new index into a Lucene RAMDirectory or FSDirectory (depending on size), merges them to local disk, and sends a <commit/> request to the Solr instance hosting the index to make the changed index visible to queries.
The Query/Management API is a thin layer of PHP code that handles sharding the output indexes across all of the search nodes.
We use a simple implementation of consistent hashing to decide which search nodes are responsible for each index file.
Because HDFS already handles replication of the Lucene indexes, there is no need to keep multiple copies available in Solr.
Instead, in a failover situation, the search node is completely removed, and other nodes become responsible for merging the indexes.
With this system, we’ve achieved a 15-minute turnaround time from log generation to availability of a search result for our Customer Support team.
Our search API supports the full Lucene query syntax, so we commonly see complex queries like:
Each result returned by a query is a complete serialized message path that indicates whether individual servers and recipients received the message.
In addition to providing short-term search for Customer Support, we are also interested in performing analyses of our log data.
Every night, we run a series of MapReduce jobs with the day’s indexes as input.
We implemented a SolrInputFormat that can pull and decompress an index, and emit each document as a key-value pair.
With this InputFormat, we can iterate over all message paths for a day and answer almost any question about our mail system, including:
Because we have months of compressed indexes archived in Hadoop, we are also able to retrospectively answer questions that our nightly log summaries leave out.
For instance, we recently wanted to determine the top sending IP addresses per month, which we accomplished with a simple one-off MapReduce job.
Cascading Cascading is an open source Java library and API that provides an abstraction layer for MapReduce.
It allows developers to build complex, mission-critical data processing applications that run on Hadoop clusters.
Binaries, source code, and add-on modules can be downloaded from the project website, http://www.cascading.org/
However, they tend to be at the wrong level of granularity for creating sophisticated, highly composable code that can be shared among different developers.
Moreover, many developers find it difficult to “think” in terms of MapReduce when faced with real-world problems.
To address the first issue, Cascading substitutes the keys and values used in MapReduce with simple field names and a data tuple model, where a tuple is simply a list of values.
For the second issue, Cascading departs from map and reduce operations directly by introducing higher-level abstractions as alternatives: Functions, Filters, Aggregators, and Buffers.
Other alternatives began to emerge at about the same time as the project’s initial public release, but Cascading was designed to complement them.
Consider that most of these alternative frameworks impose pre- and post-conditions, or other expectations.
For example, in several other MapReduce tools, you must preformat, filter, or import your data into HDFS prior to running the application.
That step of preparing the data must be performed outside of the programming abstraction.
In contrast, Cascading provides the means to prepare and manage your data as integral parts of the programming abstraction.
This case study begins with an introduction to the main concepts of Cascading, then finishes with an overview of how ShareThis uses Cascading in its infrastructure.
See the Cascading User Guide on the project website for a more in-depth presentation of the Cascading processing model.
Fields, Tuples, and Pipes The MapReduce model uses keys and values to link input data to the map function, the map function to the reduce function, and the reduce function to the output data.
But as we know, real-world Hadoop applications usually consist of more than one MapReduce job chained together.
If you needed to sort the numeric counts in descending order, which is not an unlikely requirement, it would need to be done in a second MapReduce job.
So, in the abstract, keys and values not only bind map to reduce, but reduce to the next map, and then to the next reduce, and so on (Figure 16-11)
That is, key-value pairs are sourced from input files and stream through chains of map and reduce operations, and finally rest in an output file.
When you implement enough of these chained MapReduce applications, you start to see a well-defined set of key-value manipulations used over and over again to modify the key-value data stream.
Cascading simplifies this by abstracting away keys and values and replacing them with tuples that have corresponding field names, similar in concept to tables and column names in a relational database.
And during processing, streams of these fields and tuples are then manipulated as they pass through user-defined operations linked together by pipes (Figure 16-12)
So fields are used to declare the names of values in a tuple and to select values by name from a tuple.
A tuple is very much like a database row or record.
And the map and reduce operations are abstracted behind one or more pipe instances (Figure 16-13): Each.
The Each pipe processes a single input tuple at a time.
It may apply either a Func tion or a Filter operation (described shortly) to the input tuple.
It can also merge multiple input tuple streams into a single stream if they all share the same field names.
CoGroup The CoGroup pipe joins multiple tuple streams together by common field names, and it also groups the tuples by the common grouping fields.
All standard join types (inner, outer, etc.) and custom joins can be used across two or more tuple streams.
Every The Every pipe processes a single grouping of tuples at a time, where the group was grouped by a GroupBy or CoGroup pipe.
The Every pipe may apply either an Aggregator or a Buffer operation to the grouping.
SubAssembly The SubAssembly pipe allows for nesting of assemblies inside a single pipe, which can, in turn, be nested in more complex assemblies.
On the surface, this might seem more complex than the traditional MapReduce model.
And admittedly there are more concepts here than Map, Reduce, Key, and Value.
But in practice, there are many more concepts that must all work in tandem to provide different behaviors.
Operations As mentioned earlier, Cascading departs from MapReduce by introducing alternative operations that are applied either to individual tuples or groups of tuples (Figure 16-15): Function.
A Function operates on individual input tuples and may return zero or more output tuples for every one input.
Filter A Filter is a special kind of function that returns a Boolean value indicating whether the current input tuple should be removed from the tuple stream.
A function could serve this purpose, but the Filter is optimized for this case, and many filters can be grouped by “logical” filters such as And, Or, Xor, and Not, rapidly creating more complex filtering operations.
Aggregator An Aggregator performs some operation against a group of tuples, where the grouped tuples are grouped by a common set of field values.
This is useful when the developer needs to efficiently insert missing values in an ordered set of tuples (such as a missing date or duration) or create a running average.
Usually Aggregator is the operation of choice when working with groups of tuples, since many Aggregators can be chained together very efficiently, but sometimes a Buffer is the best tool for the job.
Operations are bound to pipes when the pipe assembly is created (Figure 16-16)
The Each and Every pipes provide a simple mechanism for selecting some or all values out of an input tuple before being passed to its child operation.
And there is a simple mechanism for merging the operation results with the original input tuple to create the output tuple.
Without going into great detail, this allows for each operation only to care about argument tuple values and fields, not the whole set of fields in the current input tuple.
Subsequently, operations can be reusable across applications in the same way that Java methods can be reusable.
For example, in Java, a method declared as concatenate(String first, String second) is more abstract than concatenate(Person person)
In the second case, the concatenate() function must “know” about the Person object; in the first case, it is agnostic to where the data came from.
Scheme A Scheme is responsible for reading raw data and converting it to a tuple and/or writing a tuple out into raw data, where this “raw” data can be lines of text, Hadoop binary sequence files, or some proprietary format.
Note that Taps are not part of a pipe assembly, and so they are not a type of Pipe.
But they are connected with pipe assemblies when they are made cluster-executable.
When a pipe assembly is connected with the necessary number of source and sink Tap instances, we get a Flow.
A Flow is created when a pipe assembly is connected with its required number of source and sink Taps, and the Taps either emit or capture the field names the pipe assembly expects.
Otherwise, the process that connects the pipe assembly with the Taps will immediately fail with an error.
So pipe assemblies are really data process definitions, and are not “executable” on their own.
They must be connected to source and sink Tap instances before they can run on a cluster.
This separation between Taps and pipe assemblies is part of what makes Cascading so powerful.
If you think of pipe assemblies like a Java class, then a Flow is like a Java Object instance (Figure 16-17)
That is, the same pipe assembly can be “instantiated” many times into new Flow, in the same application, without fear of any interference between them.
This allows pipe assemblies to be created and shared like standard Java libraries.
We create a new Scheme that reads simple text files and emits a new Tuple for each line in a field named “line,” as declared by the Fields instance.
We create a new Scheme that writes simple text files and expects a Tuple with any number of fields/values.
If more than one value, they will be tab-delimited in the output file.
We create source and sink Tap instances that reference the input file and output directory, respectively.
The sink Tap will overwrite any file that may already exist.
We construct the head of our pipe assembly and name it “wordcount.” This name is used to bind the source and sink Taps to the assembly.
We construct an Each pipe with a function that will parse the “line” field into a new Tuple for each word encountered.
We construct a GroupBy pipe that will create a new Tuple grouping for each unique value in the field “word.” We construct an Every pipe with an Aggregator that will count the number of Tuples in every unique word group.
We connect the pipe assembly to its sources and sinks into a Flow, and then execute the Flow on the cluster.
We subclass the SubAssembly class, which is itself a kind of Pipe.
We replace the Each from the previous example with our ParseWordsAssembly pipe.
Finally, we just substitute in our new SubAssembly right where the previous Every and word parser function was used in the previous example.
Flexibility Take a step back and see what this new model has given us or, better yet, what it has taken away.
You see, we no longer think in terms of MapReduce jobs, or Mapper and Reducer interface implementations and how to bind or link subsequent MapReduce jobs to the ones that precede them.
Because of this, developers can build applications of arbitrary granularity.
They can start with a small application that just filters a logfile, but then can iteratively build up more features into the application as needed.
Since Cascading is an API and not a syntax like strings of SQL, it is more flexible.
First off, developers can create domain-specific languages (DSLs) using their favorite language, such as Groovy, JRuby, Jython, Scala, and others (see the project site for examples)
Second, developers can extend various parts of Cascading, such as allowing custom Thrift or JSON objects to be read and written to and allowing them to be passed through the tuple stream.
Hadoop and Cascading at ShareThis ShareThis is a sharing network that makes it simple to share any online content.
With the click of a button on a web page or browser plug-in, ShareThis allows users to seamlessly access their contacts and networks from anywhere online and share the content through email, IM, Facebook, Digg, mobile SMS, etc., without ever leaving the current page.
Publishers can deploy the ShareThis button to tap the service’s universal sharing capabilities to drive traffic, stimulate viral activity, and track the sharing of online content.
ShareThis also simplifies social media services by reducing clutter on web pages and providing instant distribution of content across social networks, affiliate groups, and communities.
As ShareThis users share pages and information through the online widgets, a continuous stream of events enter the ShareThis network.
These events are first filtered and processed, and then handed to various backend systems, including AsterData, Hypertable, and Katta.
The volume of these events can be huge, too large to process with traditional systems.
For this reason, ShareThis chose to deploy Hadoop as the preprocessing and orchestration frontend to their backend systems.
This pipeline simply takes data stored in an S3 bucket, processes it (described shortly), and stores the results back into another bucket.
Simple Queue Service (SQS) is used to coordinate the events that mark the start and completion of data processing runs.
Downstream, other processes pull data that load AsterData, pull URL lists from Hypertable to source a web crawl, or pull crawled page data to create Lucene indexes for use by Katta.
It is used to coordinate the processing and movement of data between architectural components.
With Hadoop as the frontend, all the event logs can be parsed, filtered, cleaned, and organized by a set of rules before ever being loaded into the AsterData cluster or used by any other component.
AsterData is a clustered data warehouse that can support large datasets and allow for complex ad hoc queries using a standard SQL syntax.
ShareThis chose to clean and prepare the incoming datasets on the Hadoop cluster and then to load that data into the AsterData cluster for ad hoc analysis and reporting.
Though possible with AsterData, it made a lot of sense to use Hadoop as the first stage in the processing pipeline to offset load on the main data warehouse.
Cascading was chosen as the primary data processing API to simplify the development process, codify how data is coordinated between architectural components, and provide the developer-facing interface to those components.
This represents a departure from more “traditional” Hadoop use cases, which essentially just query stored data.
Instead, Cascading and Hadoop together provide a better and simpler structure to the complete solution, end-to-end, and thus provide more value to the users.
First, standalone operations (Functions, Filters, etc.) could be written and tested independently.
Second, the application was segmented into stages: one for parsing, one for rules, and a final stage for binning/collating the data, all via the SubAssembly base class described earlier.
For integration and deployment, many of the features built into Cascading allowed for easier integration with external systems and for greater process tolerance.
In production, all the subassemblies are joined and planned into a Flow, but instead of just source and sink Taps, trap Taps were planned in (Figure 16-20)
Normally, when an operation throws an exception from a remote mapper or reducer task, the Flow will fail and kill all its managed MapReduce jobs.
When a Flow has traps, any exceptions are caught and the data causing the exception is saved to the Tap associated with the current trap.
Then the next Tuple is processed without stopping the Flow.
Sometimes you want your Flows to fail on errors, but in this case, the ShareThis developers knew they could go back and look at the “failed” data and update their unit tests while the production system kept running.
Losing a few hours of processing time was worse than losing a couple of bad records.
When a Flow finishes, a message is sent to notify other systems that there is data ready to be picked up from Amazon S3
On failure, a different message is sent, alerting other processes.
The remaining downstream processes pick up where the log processing pipeline leaves off on different independent clusters.
Independently, other clusters are booting and shutting down at different intervals based on the needs of the business unit responsible for that component.
For example, the web crawler component (using Bixo, a Cascading-based web-crawler toolkit developed by EMI and ShareThis) may run continuously on a small cluster with a companion Hypertable cluster.
This on-demand model works very well with Hadoop, where each cluster can be tuned for the kind of workload it is expected to handle.
Summary Hadoop is a very powerful platform for processing and coordinating the movement of data across various architectural components.
Its only drawback is that the primary computing model is MapReduce.
Cascading aims to help developers build powerful applications quickly and simply, through a well-reasoned API, without needing to think in MapReduce, and while leaving the heavy lifting of data distribution, replication, distributed process management, and liveness to Hadoop.
Read more about Cascading, join the online community, and download sample applications by visiting the project website.
TeraByte Sort on Apache Hadoop This article is reproduced from http://sortbenchmark.org/YahooHadoop.pdf, which was written in May 2008
Jim Gray and his successors define a family of benchmarks to find the fastest sort programs every year.
TeraByte Sort and other sort benchmarks are listed with winners over the years at http://sortbenchmark.org/
We also sorted a terabyte in 62 seconds on the same cluster.
Additionally, we used LZO compression on the intermediate data between the nodes.
Apache Hadoop is an open source software framework that dramatically simplifies writing distributed data-intensive applications.
Since the primary primitive of MapReduce is a distributed sort, most of the custom code is glue to get the desired behavior.
I wrote three Hadoop applications to run the terabyte sort:
TeraValidate is a MapReduce program that validates the output is sorted.
You can see the distribution of running tasks over the job run in Figure 16-22
The number of things they model are extremely general: if you have a collection of things (that we’ll call nodes), they are related (edges), and if the nodes and edges tell a story (node/edge metadata), you have a network graph.
I started the Infochimps project, a site to find, share, or sell any dataset in the world.
At Infochimps, we have a whole bag of tricks ready to apply to any interesting network graph that comes into the collection.
We chiefly use Pig (described in Chapter 11) and Wukong, a toolkit we’ve developed for Hadoop streaming in the Ruby programming language.
The number of messages a user has sent and the bag of words from all those messages are each important pieces of node metadata.
A linked document collection such as Wikipedia or the entire Web.4 Each page is a node (carrying its title, view count, and categories as node metadata)
Each hyperlink is an edge, and the frequency at which people click from one page to the next is edge metadata.
The connections of neurons (nodes) and synapses (edges) in the C.
A highway map, with exits as nodes and highway segments as edges.
The Open Street Map project’s dataset has global coverage of place names (node metadata), street number ranges (edge metadata), and more.6
What’s amazing about these organic network graphs is that given enough data, a collection of powerful tools are able to generically use this network structure to expose insight.
For example, we’ve used variants of the same algorithm7 to do each of:
Rank the most important pages in the Wikipedia linked-document collection.
Google uses a vastly more refined version of this approach to identify top search hits.
Users who have many more followers than their “trstrank” would imply are often spammers.
Predict a school’s impact on student education, using millions of anonymized exam scores gathered over five years.
Measuring Community The most interesting network in the Infochimps collection is a massive crawl of the Twitter social graph.
Everybody’s Talkin’ at Me: The Twitter Reply Graph Twitter lets you reply to another user’s message and thus engage in conversation.
Since it’s an expressly public activity, a reply is a strong social token: it shows interest in what the other is saying and demonstrates that interest is worth rebroadcasting.
The first step in our processing is done in Wukong, a Ruby language library for Hadoop.
It lets us write small, agile programs capable of handling multiterabyte data streams.
Here is a snippet from the class that represents a twitter message (or tweet):9
A flowing crowd of websurfers wandering the linkeddocument collection will visit the most interesting pages the most often.
The transfer of social capital implied by social network interactions highlights the most central actors within each community.
The year-to-year progress of students to higher or lower test scores implies what each school’s effect on a generic class would be.
Chosen without apology, in keeping with the ego-centered ethos of social networks.
You can find full working source code on this book’s website.
Twitter’s Stream API lets anyone easily pull gigabytes of messages.10 They arrive in a raw JSON format:
Let’s find each reply and emit the respective user IDs as an edge:11
The Mapper derives from LineStreamer, a class that feeds each line as a single record to its process method.
We only have to define that process method; Wukong and Hadoop take care of the rest.
In this case, we use the raw JSON record to create a tweet object.
Where user A replies to user B, emit the edge as A and B separated by a tab.
Refer to the Twitter developer site, or use a tool like Hayes Davis’ Flamingo.
In order to keep the graph-theory discussion general, I’m going to play loose with some details and leave out various janitorial details of loading and running.
It’s simple, and it gives an equal jumping-off point for in- or out-edges, but there’s some duplication of data.
You can tell the same story from the node’s point of view (and save some disk space) by rolling up on the source node.
We call this the adjacency list, and it can be generated in Pig by a simple GROUP BY.
A simple, useful measure of influence is the number of replies a user receives.
In graph terms, this is the degree (specifically the in-degree, since this is a directed graph)
Pig’s nested FOREACH syntax lets us count the distinct incoming repliers (neighbor nodes) and the total incoming replies in one pass:12
Due to the small size of the edge pair records and a pesky Hadoop implementation detail, the Mapper may spill data to disk early.
This large variation in degree is typical for social networks.
Symmetric Links Although millions of people have given @THE_REAL_SHAQ a shout-out on twitter, he has understandably not reciprocated with millions of replies.
As the graph shows, I frequently converse with @mndoci,14 making ours a symmetric link.
One way to find symmetric links is to take the edges in A Replied To B that are also in A Replied By B.
We can do that set intersection with an inner self-join:15
However, this sends two full copies of the edge-pairs list to the reduce phase, doubling the memory required.
We can do better by noticing that from a node’s point of view, a symmetric link is equivalent to a paired edge: one out and one in.
Make the graph undirected by putting the node with lowest sort order in the first slot—but preserve the direction as a piece of edge metadata:
Deepak Singh, open data advocate and bizdev manager of the Amazon AWS cloud.
A symmetric edge has at least one reply in each direction:
Community Extraction So far, we’ve generated a node measure (in-degree) and an edge measure (symmetric link identification)
Let’s move out one step and look at a neighborhood measure: how many of a given person’s friends are friends with each other? Along the way, we’ll produce the edge set for a visualization like the one discussed earlier.
Now intersect the set of neighbors with the set of starting nodes to find all edges originating in n1_nodes:
Our copy of the graph (with more than 1 billion edges) is far too large to fit in memory.
On the other hand, the neighbor count for a single user rarely exceeds a couple of million, which fits easily in memory.
Including USING 'replicated' in the JOIN command instructs Pig to do a map-side join (also called a fragment replicate join)
Pig holds the n1_nodes relation in memory as a lookup table and streams the full edge list past.
No reduce step means an enormous speedup! To leave only edges where both source and destination are neighbors of the seed node, repeat the join:
As you can see, the big data community is very interconnected.
The link neighborhood of a celebrity such as @THE_REAL_SHAQ is far more sparse.
It ranges from zero (no neighbor links to any other neighbor) to one (every neighbor links to every other neighbor)
A low clustering coefficient could indicate widely dispersed interest (as it does with @THE_REAL_SHAQ), or it could indicate the kind of inorganic community that a spam account would engender.
We’ve calculated community metrics at the scale of a node, an edge, and a neighborhood.
How about the whole globe? There’s not enough space here to cover it, but you can simultaneously determine the clustering coefficient for every node by generating.
For each user, comparing the number of triangles they belong to with their degree leads to the clustering coefficient.
Weight each edge (by number of replies, whether it’s symmetric, and so on), and set limits on the number of links from any node.
This sharply reduces the intermediate data size, yet still does a reasonable job of estimating cohesiveness.
It’s easy to install Hadoop on a single machine to try it out.
For installation on a cluster, please refer to Chapter 9.) The quickest way is to download and run a binary release from an Apache Software Foundation Mirror.
In this appendix, we cover how to install Hadoop Common, HDFS, and MapReduce.
Instructions for installing the other projects covered in this book are included at the start of the relevant chapters.
Prerequisites Hadoop is written in Java, so you will need to have Java installed on your machine, version 6 or later.
Sun’s JDK is the one most widely used with Hadoop, although others have been reported to work.
Linux is the only supported production platform, but other flavors of Unix (including Mac OS X) can be used to run Hadoop for development.
Windows is supported only as a development platform, and additionally requires Cygwin to run.
During the Cygwin installation process, you should include the openssh package if you plan to run Hadoop in pseudodistributed mode (see the following explanation)
Installation Start by deciding which user you’d like to run Hadoop as.
For trying out Hadoop or developing Hadoop programs, it is simplest to run Hadoop on a single machine using your own user account.
Download a stable release, which is packaged as a gzipped tar file, from the Apache Hadoop releases page, and unpack it somewhere on your filesystem:
Before you can run Hadoop, you need to tell it where Java is located on your system.
If you have the JAVA_HOME environment variable set to point to a suitable Java installation, that will be used, and you don’t have to configure anything further.
For example, on my Mac, I changed the line to read:
It’s very convenient to create an environment variable that points to the Hadoop installation directory (HADOOP_INSTALL, say) and to put the Hadoop binary directory on your command-line path.
In Hadoop 2.0, you may need to put the sbin directory on the path too.) For example:
Configuration Each component in Hadoop is configured using an XML file.
Common properties go in core-site.xml, HDFS properties go in hdfs-site.xml, and MapReduce properties go in mapred-site.xml.
In earlier versions of Hadoop, there was a single site configuration file for the Common, HDFS, and MapReduce components, called hadoopsite.xml.
From release 0.20.0 onward, this file has been split into three: one for each component.
The property names have not changed, just the configuration file they have to go in.
In Hadoop 2.0 and later, MapReduce runs on YARN and there is an additional configuration file called yarn-site.xml.
All the configuration files should go in the etc/ hadoop subdirectory.
Standalone (or local) mode There are no daemons running and everything runs in a single JVM.
Standalone mode is suitable for running MapReduce programs during development, since it is easy to test and debug them.
Pseudodistributed mode The Hadoop daemons run on the local machine, thus simulating a cluster on a small scale.
Fully distributed mode The Hadoop daemons run on a cluster of machines.
To run Hadoop in a particular mode, you need to do two things: set the appropriate properties, and start the Hadoop daemons.
Table A-1 shows the minimal set of properties to configure each mode.
In standalone mode, the local filesystem and the local MapReduce job runner are used, whereas in the distributed modes the HDFS and MapReduce (or YARN) daemons are started.
Standalone Mode In standalone mode, there is no further action to take, since the default properties are set for standalone mode and there are no daemons to run.
Pseudodistributed Mode The configuration files should be created with the following contents and placed in the conf directory (although you can place configuration files in any directory as long as you start the daemons with the --config option):
In pseudodistributed mode, we have to start daemons, and to do that, we need to have SSH installed.
Hadoop doesn’t actually distinguish between pseudodistributed and fully distributed modes; it merely starts daemons on the set of hosts in the cluster (defined by the slaves file) by SSH-ing to each host and starting a daemon process.
Pseudodistributed mode is just a special case of fully distributed mode in which the (single) host is localhost, so we need to make sure that we can SSH to localhost and log in without having to enter a password.
First, make sure that SSH is installed and a server is running.
If successful, you should not have to type in a password.
Before it can be used, a brand-new HDFS installation needs to be formatted.
The formatting process creates an empty filesystem by creating the storage directories and the initial versions of the namenode’s persistent data structures.
Datanodes are not involved in the initial formatting process, since the namenode manages all of the filesystem’s metadata, and datanodes can join or leave the cluster dynamically.
For the same reason, you don’t need to say how large a filesystem to create, since this is determined by the number of datanodes in the cluster, which can be increased as needed, long after the filesystem is formatted.
If you have placed configuration files outside the default conf directory, start the daemons with the --config option, which takes an absolute path to the configuration directory:
The following daemons will be started on your local machine: a namenode, a secondary namenode, a datanode, a jobtracker, and a tasktracker.
You can check whether the daemons started successfully by looking at the logfiles in the logs directory (in the Hadoop installation directory) or by looking at the web UIs, at http://localhost:
You can also use Java’s jps command to see whether they are running.
These commands will start the HDFS daemons, and for YARN, a resource manager and a node manager.
Fully Distributed Mode Setting up a cluster of machines brings many additional considerations, so this mode is covered in Chapter 9
Cloudera’s Distribution Including Apache Hadoop (hereafter CDH) is an integrated Apache Hadoop-based stack containing all the components needed for production use, tested and packaged to work together.
Cloudera makes the distribution available in a number of different formats: Linux packages, virtual machine images, tarballs, and scripts for running CDH in the cloud.
As of CDH3, the following components are included, many of which are covered elsewhere in this book:
Cloudera also provides Cloudera Manager for deploying and operating Hadoop clusters running CDH.
This appendix gives a runthrough of the steps taken to prepare the raw weather datafiles so they are in a form that is amenable for analysis using Hadoop.
If you want to get a copy of the data to process using Hadoop, you can do so by following the instructions given at the website that accompanies this book at http://www.hadoopbook.com/
The rest of this appendix explains how the raw weather datafiles were processed.
The raw data is provided as a collection of tar files, compressed with bzip2
Each tar file contains a file for each weather station’s readings for the year, compressed with gzip.
The fact that the files in the archive are compressed makes the bzip2 compression on the archive itself redundant.) For example:
Because there are tens of thousands of weather stations, the whole dataset is made up of a large number of relatively small files.
I did this using a MapReduce program, to take advantage of its parallel processing capabilities.
No reduce function is needed because the map does all the file processing in parallel with no combine stage.
The processing can be done with a Unix script, so the Streaming interface to MapReduce is appropriate in this case; see Example C-1
Bash script to process raw NCDC datafiles and store in HDFS #!/usr/bin/env bash.
By specifying the input format to be NLineInputFormat, each mapper receives one line of input, which contains the file it has to process.
The processing is explained in the script, but briefly, it unpacks the bzip2 file and then concatenates each station file into a single file for the whole year.
Note the use of hadoop fs -put - to consume from standard input.
Status messages are echoed to standard error with a reporter:status prefix so that they get interpreted as a MapReduce status update.
This tells Hadoop that the script is making progress and is not hanging.
I set the number of reduce tasks to zero, since this is a map-only job.
The task timeout was set to a high value so that Hadoop doesn’t kill tasks that are taking a long time (for example, when unarchiving files or copying to HDFS, when no progress is reported)
Finally, the files were archived on S3 by copying them from HDFS using distcp.
We’d like to hear your suggestions for improving our indexes.
About the Author Tom White is one of the foremost experts on Hadoop.
Tom is a software engineer at Cloudera, where he has worked since its foundation, on the core distributions from Apache and Cloudera.
Previously he was an independent Hadoop consultant, working with companies to set up, use, and extend Hadoop.
He has written numerous articles for O’Reilly, java.net and IBM’s developerWorks, and has spoken at many conferences, including ApacheCon and OSCON.
These members of the genus Loxodonta are the largest land animals on earth (slightly larger than their cousin, the Asian elephant) and can be identified by their ears, which have been said to look somewhat like the continent of Asia.
Most of the continent’s elephants live on savannas and in dry woodlands.
In some regions, they can be found in desert areas; in others, they are found in mountains.
The species plays an important role in the forest and savanna ecosystems in which they live.
Many plant species are dependent on passing through an elephant’s digestive tract before they can germinate; it is estimated that at least a third of tree species in west African forests rely on elephants in this way.
Elephants grazing on vegetation also affect the structure of habitats and influence bush fire patterns.
For example, under natural conditions, elephants make gaps through the rainforest, enabling the sunlight to enter, which allows the growth of various plant species.
This, in turn, facilitates more abundance and more diversity of smaller animals.
As a result of the influence elephants have over many plants and animals, they are often referred to as a keystone species because they are vital to the long-term survival of the ecosystems in which they live.
Audit Logging Tools dfsadmin Filesystem check (fsck) Datanode block scanner Finding the blocks for a file.
Upgrades HDFS data and metadata upgrades Start the upgrade Wait until the upgrade is complete Check the upgrade Roll back the upgrade (optional) Finalize the upgrade (optional)
Merging for near-term search Archiving for analysis Sharding Search results.
