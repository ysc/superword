O’Reilly books may be purchased for educational, business, or sales promotional use.
HBase: The Definitive Guide, the image of a Clydesdale horse, and related trade dress are trademarks of O’Reilly Media, Inc.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a trademark claim, the designations have been printed in caps or initial caps.
While every precaution has been taken in the preparation of this book, the publisher and author assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
For my wife Katja, my daughter Laura, and son Leon.
The HBase story begins in 2006, when the San Francisco-based startup Powerset was trying to build a natural language search engine for the Web.
Their indexing pipeline was an involved multistep process that produced an index about two orders of magnitude larger, on average, than your standard term-based index.
The datastore that they’d built on top of the then nascent Amazon Web Services to hold the index intermediaries and the webcrawl was buckling under the load (Ring.
Chad Walters, Powerset’s head of engineering at the time, reflects back on the experience as follows:
After the publication of the Google BigTable paper, there were on-again, off-again discussions around what a BigTable-like system on top of Hadoop might look.
It’s not perfect, but it’s ready for other people to play with and examine.” Mike had been working with Doug Cutting on Nutch, an open source search engine.
He’d done similar drive-by code dumps there to add features such as a Google File System clone so the Nutch indexing process was not bounded by the amount of disk you attach to a single machine.
This Nutch distributed filesystem would later grow up to be HDFS.) Jim Kellerman of Powerset took Mike’s dump and started filling in the gaps, adding tests and getting it into shape so that it could be committed as part of Hadoop.
Not long after, Lars, the author of the book you are now reading, showed up on the #hbase IRC channel.
He had a big-data problem of his own, and was game to try HBase.
After some back and forth, Lars became one of the first users to run HBase in production outside of the Powerset home base.
I distinctly remember a directory listing Lars made for me a while back on his production cluster at WorldLingo, where he was employed as CTO, sysadmin, and grunt.
Of all those who have contributed to HBase over the years, it is poetic justice that Lars is the one to write this book.
Lars was always dogging HBase contributors that the documentation needed to be better if we hoped to gain broader adoption.
Everyone agreed, nodded their heads in ascent, amen’d, and went back to coding.
So Lars started writing critical how-tos and architectural descriptions inbetween jobs and his intraEuropean travels as unofficial HBase European ambassador.
His luscious diagrams were poached by one and all wherever an HBase presentation was given.
HBase has seen some interesting times, including a period of sponsorship by Microsoft, of all things.
Powerset was acquired in July 2008, and after a couple of months during which Powerset employees were disallowed from contributing while Microsoft’s legal department vetted the HBase codebase to see if it impinged on SQLServer patents, we were allowed to resume contributing (I was a Microsoft employee working near full time on an Apache open source project)
Other developments include HBase running on filesystems other than Apache HDFS, such as MapR.
But plain to me though is that none of these developments would have been possible were it not for the hard work put in by our awesome HBase community driven by a core of HBase committers.
These include Jonathan Gray, who gambled his startup streamy.com on HBase; Andrew Purtell, who built an HBase team at Trend Micro long before such a thing was fashionable; Ryan Rawson, who got StumbleUpon—which became the main sponsor after HBase moved on from Powerset/Microsoft—on board, and who had the sense to hire John-Daniel Cryans, now a power contributor but just a bushy-tailed student at the time.
Of those of us who know HBase, there is no better man qualified to write this first, critical HBase book.
It could be because you heard all about Hadoop and what it can do to crunch petabytes of data in a reasonable amount of time.
While reading into Hadoop you found that, for random access to the accumulated data, there is something called HBase.
Or it was the hype that is prevalent these days addressing a new kind of data storage architecture.
It strives to solve large-scale data problems where traditional solutions may be either too involved or cost-prohibitive.
You also heard that HBase can scale without much effort, and that alone is reason enough to look at it since you are building the next webscale system.
I was at that point in late 2007 when I was facing the task of storing millions of documents in a system that needed to be fault-tolerant and scalable while still being maintainable by just me.
I had decent skills in managing a MySQL database system, and was using the database to store data that would ultimately be served to our website users.
This database was running on a single server, with another as a backup.
The issue was that it would not be able to hold the amount of data I needed to store for this new project.
I would have to either invest in serious RDBMS scalability skills, or find something else instead.
These days, I try not to think about how difficult my first experience with Hadoop and HBase was.
Looking back, I realize that I would have wished for this customer project to start today.
Mine was one of the very first clusters in production (and is still in use today!) and my use case triggered a few very interesting issues (let me refrain from saying more)
But that was to be expected, betting on a 0.1x version of a community project.
And I had the opportunity over the years to contribute back and stay close to the development team so that eventually I was humbled by being asked to become a full-time committer as well.
I learned a lot over the past few years from my fellow HBase developers and am still learning more every day.
My belief is that we are nowhere near the peak of this technology and it will evolve further over the years to come.
Let me pay my respect to the entire HBase community with this book, which strives to cover not just the internal workings of HBase or how to get it going, but more specifically, how to apply it to your use case.
In fact, I strongly assume that this is why you are here right now.
You want to learn how HBase can solve your problem.
General Information Before we get started, here a few general notes.
Since it was not possible to follow the frantic development pace of HBase, and because the book had a deadline before 0.92.0 was released, the book could not document anything after a specific revision: 1130916 (http://svn.apache.org/viewvc/hbase/trunk/ what is written here and what HBase offers, you can use the aforementioned revision number to compare all changes that have been applied after this book went into print.
I have made every effort to update the JDiff (a tool to compare different revisions of a software project) documentation on the book’s website at http://www.hbasebook .com.
You can use it to quickly see what is different.
Building the Examples The examples you will see throughout this book can be found in full detail in the the sake of brevity, they are usually printed only in parts here, to focus on the important bits, and to avoid repeating the same boilerplate code over and over again.
The name of an example matches the filename in the repository, so it should be easy to find your way through.
Each chapter has its own subdirectory to make the separation more intuitive.
If you are reading, for instance, an example in Chapter 3, you can go to the matching directory in the source repository and find the full source code there.
Many examples use internal helpers for convenience, such as the HBaseHelper class, to set up a test environment for reproducible results.
You can modify the code to create different scenarios, or introduce faulty data and see how the feature showcased in the example behaves.
Consider the code a petri dish for your own experiments.
Building the code requires a few auxiliary command-line tools: Java.
HBase is written in Java, so you do need to have Java set up for it to work.
For the examples, you also need Java on the workstation you are using to run them.
Alternatively, you can download a static snapshot of the entire archive using the GitHub download link.
Maven The build system for the book’s repository is Apache Maven.‡ It uses the so-called Project Object Model (POM) to describe what is needed to build a software project.
You can download Maven from its website and also find installation instructions there.
Once you have gathered the basic tools required for the example code, you can build the project like so:
You are left with a Java archive file (also called a JAR file) in the target directory in each of the subdirectories, that is, one for each chapter of the book that has source code examples:
Assuming you have a running installation of HBase, you can then run each of the included classes using the supplied command-line script:
The supplied bin/run.sh helps to assemble the required Java classpath, adding the dependent JAR files to it.
Hush: The HBase URL Shortener Looking at each feature HBase offers separately is a good way to understand what it does.
The book uses code examples that set up a very specific set of tables, which contain an equally specific set of data.
This makes it easy to understand what is given and how a certain operation changes the data from the before to the after state.
You can execute every example yourself to replicate the outcome, and it should match exactly with what is described in the accompanying book section.
You can also modify the examples to explore the discussed feature even further—and you can use the supplied helper classes to create your own set of proof-of-concept examples.
Yet, sometimes it is important to see all the features working in concert to make the final leap of understanding their full potential.
For this, the book uses a single, realworld example to showcase most of the features HBase has to offer.
The book also uses the example to explain advanced concepts that come with this different storage territory—compared to more traditional RDBMS-based systems.
Many services on the Internet offer this kind of service.
This link can then be used in places where real estate is at a premium: Twitter only allows you to send messages with a maximum length of 140 characters.
Running this through a URL shortener like Hush results in the following URL: http://hush.li/1337
Obviously, this is much shorter, and easier to copy into an email or send through a restricted medium, like Twitter or SMS.
But this service is not simply a large lookup table.
Granted, popular services in this area have hundreds of millions of entries mapping short to long URLs.
Users want to shorten specific URLs and also track their usage: how often has a short URL been used? A shortener service should retain counters for every shortened URL to report how often they have been clicked.
More advanced features are vanity URLs that can use specific domain names, and/or custom short URL IDs, as opposed to auto-generated ones, as in the preceding example.
Users must be able to log in to create their own short URLs, track their existing ones, and see reports for the daily, weekly, or monthly usage.
All of this is realized in Hush, and you can easily compile and run it on your own server.
It uses a wide variety of HBase features, and it is mentioned, where appropriate, throughout this book, showing how a newly discussed topic is used in a productiontype application.
While you could create your own user account and get started with Hush, it is also a great example of how to import legacy data from, for example, a previous system.
To emulate this use case, the book makes use of a freely available data set on the Internet: the Delicious RSS feed.
There are a few sets that were made available by individuals, and can be downloaded by anyone.
Use Case: Hush Be on the lookout for boxes like this throughout the book.
Whenever possible, such boxes support the explained features with examples from Hush.
Many will also include example code, but often such code is kept very simple to showcase the feature at hand.
The data is also set up so that you can repeatedly make sense of the functionality (even though the examples may be a bit academic)
Using Hush as a use case more closely mimics what you would implement in a production system.
Hush is actually built to scale out of the box.
It might not have the prettiest interface, but that is not what it should prove.
You can run many Hush servers behind a load balancer and serve thousands of requests with no difficulties.
The snippets extracted from Hush show you how the feature is used in context, and since it is part of the publicly available repository accompanying the book, you have the full source available as well.
Run it yourself, tweak it, and learn all about it!
Running Hush Building and running Hush is as easy as building the example code.
After the last log message is output on the console, you can navigate your browser to http://localhost:8080 to access your local Hush server.
Stopping the server requires a Ctrl-C to abort the start script.
As all data is saved on the HBase cluster accessed remotely by Hush, this is safe to do.
Conventions Used in This Book The following typographical conventions are used in this book: Italic.
Indicates new terms, URLs, email addresses, filenames, file extensions, and Unix commands.
Constant width Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.
Constant width bold Shows commands or other text that should be typed literally by the user.
Constant width italic Shows text that should be replaced with user-supplied values or by values determined by context.
Using Code Examples This book is here to help you get your job done.
In general, you may use the code in this book in your programs and documentation.
You do not need to contact us for permission unless you’re reproducing a significant portion of the code.
For example, writing a program that uses several chunks of code from this book does not require permission.
Selling or distributing a CD-ROM of examples from O’Reilly books does require permission.
Answering a question by citing this book and quoting example code does not require permission.
Incorporating a significant amount of example code from this book into your product’s documentation does require permission.
An attribution usually includes the title, author, publisher, and ISBN.
With a subscription, you can read any page and watch any video from our library online.
Access new titles before they are available for print, and get exclusive access to manuscripts in development and post feedback for the authors.
Copy and paste code samples, organize your favorites, download chapters, bookmark key sections, create notes, print out pages, and benefit from tons of other time-saving features.
To have full digital access to this book and others on similar topics from O’Reilly and other publishers, sign up for free at http://my.safaribooksonline.com.
How to Contact Us Please address comments and questions concerning this book to the publisher:
We have a web page for this book, where we list errata, examples, and any additional information.
Find us on Facebook: http://facebook.com/oreilly Follow us on Twitter: http://twitter.com/oreillymedia Watch us on YouTube: http://www.youtube.com/oreillymedia.
Acknowledgments I first want to thank my late dad, Reiner, and my mother, Ingrid, who supported me and my aspirations all my life.
You were the ones to make me a better person.
Writing this book was only possible with the support of the entire HBase community.
Without that support, there would be no HBase, nor would it be as successful as it is today in production at companies all around the world.
The relentless and seemingly tireless support given by the core committers as well as contributors and the community at large on IRC, the Mailing List, and in blog posts is the essence of what open source stands for.
I would like to extend a heartfelt thank you to all the contributors to HBase; you know who you are.
Please keep contributing! Finally, I would like to thank Cloudera, my employer, which generously granted me time away from customers so that I could write this book.
Before we start looking into all the moving parts of HBase, let us pause to think about why there was a need to come up with yet another storage architecture.
Relational database management systems (RDBMSes) have been around since the early 1970s, and have helped countless companies and organizations to implement their solution to given problems.
There are many use cases for which the relational model makes perfect sense.
Yet there also seem to be specific problems that do not fit this model very well.*
The Dawn of Big Data We live in an era in which we are all connected over the Internet and expect to find results instantaneously, whether the question concerns the best turkey recipe or what to buy mom for her birthday.
We also expect the results to be useful and tailored to our needs.
Because of this, companies have become focused on delivering more targeted information, such as recommendations or online ads, and their ability to do so directly influences their success as a business.
Systems like Hadoop† now enable them to gather and process petabytes of data, and the need to collect even more data continues to increase with, for example, the development of new machine learning algorithms.
Where previously companies had the liberty to ignore certain data sources because there was no cost-effective way to store all that information, they now are likely to lose out to the competition.
There is an increasing need to store and analyze every data point they generate.
The results then feed directly back into their e-commerce platforms and may generate even more data.
Please also see the excellent Hadoop: The Definitive Guide (Second Edition) by Tom White (O’Reilly) for everything you want to know about Hadoop.
In the past, the only option to retain all the collected data was to prune it to, for example, retain the last N days.
While this is a viable approach in the short term, it lacks the opportunities that having all the data, which may have been collected for months or years, offers: you can build mathematical models that span the entire time range, or amend an algorithm to perform better and rerun it with all the previous data.
Google and Amazon are prominent examples of companies that realized the value of data and started developing solutions to fit their needs.
For instance, in a series of technical publications, Google described a scalable storage and processing system based on commodity hardware.
These ideas were then implemented outside of Google as part of the open source Hadoop project: HDFS and MapReduce.
Hadoop excels at storing data of arbitrary, semi-, or even unstructured formats, since it lets you decide how to interpret the data at analysis time, allowing you to change the way you classify the data at any time: once you have updated the algorithms, you simply run the analysis again.
Hadoop also complements existing database systems of almost any kind.
It offers a limitless pool into which one can sink data and still pull out what is needed when the time is right.
It is optimized for large file storage and batch-oriented, streaming access.
This makes analysis easy and fast, but users also need access to the final data, not in batch mode but using random access—this is akin to a full table scan versus using indexes in a database system.
We are used to querying databases when it comes to random access for structured data.
RDBMSes are the most prominent, but there are also quite a few specialized variations and implementations, like object-oriented databases.
The architecture used underneath is well researched and has not changed significantly in quite some time.
Ralph Kimball, of the Kimball Group, available at http://www.informatica.com/ campaigns/rethink_edw_kimball.pdf.
It discusses the changing needs of an evolving enterprise data warehouse market.
While HBase does fulfill the more generic rules, it fails on others, most importantly, on rule 5: the comprehensive data sublanguage rule, defining the support for at least one relational language.
Column-Oriented Databases Column-oriented databases save their data grouped by columns.
The reason to store values on a per-column basis instead is based on the assumption that, for specific queries, not all of the values are needed.
This is often the case in analytical databases in particular, and therefore they are good candidates for this different storage schema.
Reduced I/O is one of the primary reasons for this new layout, but it offers additional advantages playing into the same category: since the values of one column are often very similar in nature or even vary only slightly between logical rows, they are often much better suited for compression than the heterogeneous values of a row-oriented record structure; most compression algorithms only look at a finite window.
Note, though, that HBase is not a column-oriented database in the typical RDBMS sense, but utilizes an on-disk column storage format.
This is also where the majority of similarities end, because although HBase stores data on disk in a column-oriented format, it is distinctly different from traditional columnar databases: whereas columnar databases excel at providing real-time analytical access to data, HBase excels at providing key-based access to a specific cell of data, or a sequential range of cells.
The speed at which data is created today is already greatly increased, compared to only just a few years back.
We can take for granted that this is only going to increase further, and with the rapid pace of globalization the problem is only exacerbated.
Websites like Google, Amazon, eBay, and Facebook now reach the majority of people on this planet.
The term planet-size web application comes to mind, and in this case it is fitting.
One source of this data is click-stream logging, saving every step a user performs on its website, or on sites that use the social plug-ins offered by Facebook.
This is an ideal case in which batch processing to build machine learning models for predictions and recommendations is appropriate.
Facebook also has a real-time component, which is its messaging system, including chat, wall posts, and email.
If we were to take 140 bytes per message, as used by Twitter, it would.
See this blog post, as well as this one, by the Facebook engineering team.
Then they also add SMS and others to create an even larger number.
Facebook uses Haystack, which provides an optimized storage infrastructure for large binary objects, such as photos.
Such as the data generated by point-of-sale (POS) or stock/inventory systems Genomics.
Cellular services, military, environmental Which all collect a tremendous amount of data as well.
Storing petabytes of data efficiently so that updates and retrieval are still performed well is no easy feat.
We will now look deeper into some of the challenges.
The Problem with Relational Database Systems RDBMSes have typically played (and, for the foreseeable future at least, will play) an integral role when designing and implementing business applications.
As soon as you have to retain information about your users, products, sessions, orders, and so on, you are typically going to use some storage backend providing a persistence layer for the frontend application server.
This works well for a limited number of records, but with the dramatic increase of data being retained, some of the architectural implementation details of common database systems show signs of weakness.
Let us use Hush, the HBase URL Shortener mentioned earlier, as an example.
Assume that you are building this system so that it initially handles a few thousand users, and that your task is to do so with a reasonable budget—in other words, use free software.
The typical scenario here is to use the open source LAMP‡ stack to quickly build out a prototype for the business idea.
The relational database model normalizes the data into a user table, which is accompanied by a url, shorturl, and click table that link to the former by means of a foreign.
The tables also have indexes so that you can look up URLs by their short ID, or the users by their username.
If you need to find all the shortened URLs for a particular list of customers, you could run an SQL JOIN over both tables to get a comprehensive list of URLs for each customer that contains not just the shortened URL but also the customer details you need.
In addition, you are making use of built-in features of the database: for example, stored procedures, which allow you to consistently update data from multiple clients while the database system guarantees that there is always coherent data stored in the various tables.
Transactions make it possible to update multiple tables in an atomic fashion so that either all modifications are visible or none are visible.
Referential integrity takes care of enforcing relationships between various table schemas, and you get a domainspecific language, namely SQL, that lets you form complex queries over everything.
Finally, you do not have to deal with how data is actually stored, but only with higherlevel concepts such as table schemas, which define a fixed layout your application code can reference.
This usually works very well and will serve its purpose for quite some time.
If you are lucky, you may be the next hot topic on the Internet, with more and more users joining your site every day.
As your user numbers grow, you start to experience an increasing amount of pressure on your shared database server.
Adding more application servers is relatively easy, as they share their state only with the central database.
Your CPU and I/O load goes up and you start to wonder how long you can sustain this growth rate.
The first step to ease the pressure is to add slave database servers that are used to being read from in parallel.
You still have a single master, but that is now only taking writes, and those are much fewer compared to the many reads your website users generate.
While this may help you with the amount of reads, you have not yet addressed the writes.
This is going to double or triple the cost, if not more.
With more site popularity, you are asked to add more features to your application, which translates into more queries to your database.
The SQL JOINs you were happy to run in the past are suddenly slowing down and are simply not performing well enough at scale.
If things get even worse, you will also have to cease your use of stored procedures, as they are also simply becoming too slow to complete.
Essentially, you reduce the database to just storing your data in a way that is optimized for your access patterns.
Your load continues to increase as more and more users join your site, so another logical step is to prematerialize the most costly queries from time to time so that you can serve the data to your customers faster.
Finally, you start dropping secondary indexes as their maintenance becomes too much of a burden and slows down the database too much.
You end up with queries that can only use the primary key and nothing else.
You essentially make do with the RDBMS for lack of an alternative.
Sharding The term sharding describes the logical separation of records into horizontal partitions.
The separation of values into those partitions is performed on fixed boundaries: you have to set fixed rules ahead of time to route values to their appropriate store.
With it comes the inherent difficulty of having to reshard the data when one of the horizontal partitions exceeds its capacity.
Resharding is a very costly operation, since the storage layout has to be rewritten.
This entails defining new boundaries and then horizontally splitting the rows across them.
Massive copy operations can take a huge toll on I/O performance as well as temporarily elevated storage requirements.
And you may still take on updates from the client applications and need to negotiate updates during the resharding process.
This can be mitigated by using virtual shards, which define a much larger key partitioning range, with each server assigned an equal number of these shards.
When you add more servers, you can reassign shards to the new server.
This still requires that the data be moved over to the added server.
Sharding is often a simple afterthought or is completely left to the operator.
Without proper support from the database system, this can wreak havoc on production systems.
Let us stop here, though, and, to be fair, mention that a lot of companies are using RDBMSes successfully as part of their technology stack.
This database farm suits the given business goal and may not be replaced anytime soon.
The question here is if you were to start working on implementing a new product and knew that it needed to scale very fast, wouldn’t you want to have all the options available instead of using something you know has certain constraints?
Nonrelational Database Systems, Not-Only SQL or NoSQL? Over the past four or five years, the pace of innovation to fill that exact problem space has gone from slow to insanely fast.
It seems that every week another framework or project is announced to fit a related need.
We saw the advent of the so-called NoSQL solutions, a term coined by Eric Evans in response to a question from Johan Oskarsson, who was trying to find a name for an event in that very emerging, new data storage system space.# The term quickly rose to fame as there was simply no other name for this new class of products.
It was (and is) discussed heavily, as it was also deemed the nemesis of “SQL”—or was meant to bring the plague to anyone still considering using traditional RDBMSes...
The actual idea of different data store architectures for specific problem sets is not new at all.
Systems like Berkeley DB, Coherence, GT.M, and object-oriented database systems have been around for years, with some dating back to the early 1980s, and they fall into the NoSQL group by definition as well.
The tagword is actually a good fit: it is true that most new storage systems do not provide SQL as a means to query data, but rather a different, often simpler, API-like interface to the data.
On the other hand, tools are available that provide SQL dialects to NoSQL data stores, and they can be used to form the same complex queries you know from relational databases.
So, limitations in querying no longer differentiate RDBMSes from their nonrelational kin.
The difference is actually on a lower level, especially when it comes to schemas or ACIDlike transactional features, but also regarding the actual storage architecture.
For example, they often have no support for transactions or secondary indexes.
Consistency Models It seems fitting to talk about consistency a bit more since it is mentioned often throughout this book.
On the outset, consistency is about guaranteeing that a database always appears truthful to its clients.
Every operation on the database must carry its state from one consistent state to the next.
How this is achieved or implemented is not specified explicitly so that a system has multiple choices.
In the end, it has to get to the next consistent state, or return to the previous consistent state, to fulfill its obligation.
Consistency can be classified in, for example, decreasing order of its properties, or guarantees offered to clients.
The changes to the data are atomic and appear to take effect instantaneously.
Sequential Every client sees all changes in the same order they were applied.
Causal All changes that are causally related are observed in the same order by all clients.
Eventual When no updates occur for a period of time, eventually all updates will propagate through the system and all replicas will be consistent.
Weak No guarantee is made that all updates will propagate and changes may appear out of order to various clients.
The class of system adhering to eventual consistency can be even further divided into subtler sets, where those sets can also coexist.
The article also picks up on the topic of the CAP theorem,* which states that a distributed system can only achieve two out of the following three properties: consistency, availability, and partition tolerance.
The CAP theorem is a highly discussed topic, and is certainly not the only way to classify, but it does point out that distributed systems are not easy to develop given certain requirements.
An important observation is that in larger distributed scale systems, network partitions are a given and as such consistency and availability cannot be achieved at the same time.
This means that one has two choices on what to drop; relaxing consistency will allow the system to remain highly available [...] and prioritizing consistency means that under certain conditions the system will not be available.
See Eric Brewer’s original paper on this topic and the follow-up post by Coda Hale, as well as this PDF by Gilbert and Lynch.
Relaxing consistency, while at the same time gaining availability, is a powerful proposition.
However, it can force handling inconsistencies into the application layer and may increase complexity.
There are many overlapping features within the group of nonrelational databases, but some of these features also overlap with traditional storage solutions.
So the new systems are not really revolutionary, but rather, from an engineering perspective, are more evolutionary.
Even projects like memcached are lumped into the NoSQL category, as if anything that is not an RDBMS is automatically NoSQL.
This creates a kind of false dichotomy that obscures the exciting technical possibilities these systems have to offer.
And there are many; within the NoSQL category, there are numerous dimensions you could use to classify where the strong points of a particular system lie.
Dimensions Let us take a look at a handful of those dimensions here.
Note that this is not a comprehensive list, or the only way to classify them.
There are many variations in how the data is stored, which include key/value stores (compare to a HashMap), semistructured, column-oriented stores, and documentoriented stores.
Storage model In-memory or persistent? This is fairly easy to decide since we are comparing with RDBMSes, which usually persist their data to permanent storage, such as physical disks.
But you may explicitly need a purely in-memory solution, and there are choices for that too.
As far as persistent storage is concerned, does this affect your access pattern in any way?
It may especially affect latency, that is, how fast the system can respond to read and write requests.
And if it does offer scalability, does it imply specific steps to do so? The easiest solution would be to add one machine at a time, while sharded setups (especially those not supporting virtual shards) sometimes require for each shard to be increased simultaneously because each partition needs to be equally powerful.
Read/write performance You have to understand what your application’s access patterns look like.
Secondary indexes Secondary indexes allow you to sort and access tables based on different fields and sorting orders.
The options here range from systems that have absolutely no secondary indexes and no guaranteed sorting order (like a HashMap, i.e., you need to know the keys) to some that weakly support them, all the way to those that offer them out of the box.
Can your application cope, or emulate, if this feature is missing?
Compression When you have to store terabytes of data, especially of the kind that consists of prose or human-readable text, it is advantageous to be able to compress the data to gain substantial savings in required raw storage.
Some compression algorithms can achieve a 10:1 reduction in storage space needed.
Load balancing Given that you have a high read or write rate, you may want to invest in a storage system that transparently balances itself while the load shifts over time.
It may not be the full answer to your problems, but it may help you to ease into a highthroughput application design.
Atomic read-modify-write While RDBMSes offer you a lot of these operations directly (because you are talking to a central, single server), they can be more difficult to achieve in distributed systems.
They allow you to prevent race conditions in multithreaded or sharednothing application server design.
Having these compare and swap (CAS) or check and set operations available can reduce client-side complexity.
Locking, waits, and deadlocks It is a known fact that complex transactional processing, like two-phase commits, can increase the possibility of multiple clients waiting for a resource to become available.
In a worst-case scenario, this can lead to deadlocks, which are hard to resolve.
We will look back at these dimensions later on to see where HBase fits and where its strengths lie.
For now, let us say that you need to carefully select the dimensions that are best suited to the issues at hand.
Be pragmatic about the solution, and be aware that there is no hard and fast rule, in cases where an RDBMS is not working ideally, that a NoSQL system is the perfect match.
Evaluate your options, choose wisely, and mix and match if needed.
An interesting term to describe this issue is impedance match, which describes the need to find the ideal solution for a given problem.
Instead of using a “one-size-fits-all” approach, you should know what else is available.
Try to use the system that solves your problem best.
Scalability While the performance of RDBMSes is well suited for transactional processing, it is less so for very large-scale analytical processing.
This refers to very large queries that scan wide ranges of records or entire tables.
Analytical databases may contain hundreds or thousands of terabytes, causing queries to exceed what can be done on a single server in a reasonable amount of time.
What is even worse is that with RDBMSes, waits and deadlocks are increasing nonlinearly with the size of the transactions and concurrency—that is, the square of concurrency and the third or even fifth power of the transaction size.‡ Sharding is often an impractical solution, as it has to be done within the application layer, and may involve complex and costly (re)partitioning procedures.
Commercial RDBMSes are available that solve many of these issues, but they are often specialized and only cover certain aspects.
Looking at open source alternatives in the RDBMS space, you will likely have to give up many or all relational features, such as secondary indexes, to gain some level of performance.
The question is, wouldn’t it be good to trade relational features permanently for performance? You could denormalize (see the next section) the data model and avoid waits and deadlocks by minimizing necessary locking.
Database (De-)Normalization At scale, it is often a requirement that we design schema differently, and a good term to describe this principle is Denormalization, Duplication, and Intelligent Keys (DDI).§ It is about rethinking how data is stored in Bigtable-like storage systems, and how to make use of it in an appropriate way.
Part of the principle is to denormalize schemas by, for example, duplicating data in more than one table so that, at read time, no further aggregation is required.
Or the related prematerialization of required views, once again optimizing for fast reads without any further processing.
There is much more on this topic in Chapter 9, where you will find many ideas on how to design solutions that make the best use of the features HBase provides.
Let us look at an example to understand the basic principles of converting a classic relational database model to one that fits the columnar nature of HBase much better.
The entity relationship diagram (ERD) can be seen in Figure 1-2
The full SQL schema can be found in Appendix E.‖ The shortened URL, stored in the shorturl table, can then be given to others that subsequently click on it to open the linked full URL.
Each click is tracked, recording the number of times it was used, and, for example, the country the click came from.
This is stored in the click table, which aggregates the usage on a daily basis, similar to a counter.
Users, stored in the user table, can sign up with Hush to create their own list of shortened URLs, which can be edited to add a description.
This links the user and short url tables with a foreign key relationship.
Note, though, that this is provided purely for demonstration purposes, so the schema is deliberately kept simple.
The system also downloads the linked page in the background, and extracts, for instance, the TITLE tag from the HTML, if present.
The entire page is saved for later processing with asynchronous batch jobs, for analysis purposes.
Every linked page is only stored once, but since many users may link to the same long URL, yet want to maintain their own details, such as the usage statistics, a separate entry in the shorturl is created.
This also allows you to aggregate statistics to the original short ID, refShortId, so that you can see the overall usage of any short URL to map to the same long URL.
The shortId and refShortId are the hashed IDs assigned uniquely to each shortened URL.
Figure 1-3 shows how the same schema could be represented in HBase.
Every shortened URL is stored in a separate table, shorturl, which also contains the usage statistics, storing various time ranges in separate column families, with distinct time-to-live settings.
The columns form the actual counters, and their name is a combination of the date, plus an optional dimensional postfix—for example, the country code.
The downloaded page, and the extracted details, are stored in the url table.
This table uses compression to minimize the storage requirements, because the pages are mostly HTML, which is inherently verbose and contains a lot of text.
The user-shorturl table acts as a lookup so that you can quickly find all short IDs for a given user.
This is used on the user’s home page, once she has logged in.
The additional user-shorturl table is replacing the foreign key relationship, making user-related lookups faster.
There are various approaches to converting one-to-one, one-to-many, and many-tomany relationships to fit the underlying architecture of HBase.
You could implement even this simple example in different ways.
You need to understand the full potential of HBase storage design to make an educated decision regarding which approach to take.
The support for sparse, wide tables and column-oriented design often eliminates the need to normalize data and, in the process, the costly JOIN operations needed to aggregate the data at query time.
Building Blocks This section provides you with an overview of the architecture behind HBase.
After giving you some background information on its lineage, the section will introduce the general concepts of the data model and the available storage API, and presents a highlevel overview on implementation.
This scalable distributed file system, abbreviated as GFS, uses a cluster of commodity hardware to store huge amounts of data.
The filesystem handled data replication between nodes so that losing a storage server would have no effect on data availability.
It was also optimized for streaming reads so that data could be read for processing later on.
MapReduce was the missing piece to the GFS architecture, as it made use of the vast number of CPUs each commodity server in the GFS cluster provides.
MapReduce plus GFS forms the backbone for processing massive amounts of data, including the entire search index Google owns.
What is missing, though, is the ability to access data randomly and in close to real-time (meaning good enough to drive a web service, for example)
Another drawback of the GFS design is that it is good with a few very, very large files, but not as good with millions of tiny files, because the data retained in memory by the master node is ultimately bound to the number of files.
The more files, the higher the pressure on the memory of the master.
So, Google was trying to find a solution that could drive interactive applications, such as Mail or Analytics, while making use of the same infrastructure and relying on GFS for replication and data availability.
The data stored should be composed of much smaller entities, and the system would transparently take care of aggregating the small records into very large storage files and offer some sort of indexing that allows the user to retrieve data with a minimal number of disk seeks.
Finally, it should be able to store the entire web crawl and work with MapReduce to build the entire search index in a timely manner.
Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers.
It is highly recommended that everyone interested in HBase read that paper.
It describes a lot of reasoning behind the design of Bigtable and, ultimately, HBase.
We will, however, go through the basic concepts, since they apply directly to the rest of this book.
HBase is implementing the Bigtable storage architecture very faithfully so that we can explain everything using HBase.
Appendix F provides an overview of where the two systems differ.
Tables, Rows, Columns, and Cells First, a quick summary: the most basic unit is a column.
One or more columns form a row that is addressed uniquely by a row key.
A number of rows, in turn, form a table, and there can be many of them.
Each column may have multiple versions, with each distinct value contained in a separate cell.
This sounds like a reasonable description for a typical database, but with the extra dimension of allowing multiple versions of each cells.
All rows are always sorted lexicographically by their row key.
Example 1-1 shows how this will look when adding a few rows with different keys.
Note how the numbering is not in sequence as you may have expected it.
You may have to pad keys to get a proper sorting order.
In lexicographical sorting, each key is compared on a binary level, byte by byte, from left to right.
Having the row keys always sorted can give you something like a primary key index known from RDBMSes.
It is also always unique, that is, you can have each row key.
The row keys can be any arbitrary array of bytes and are not necessarily human-readable.
Rows are composed of columns, and those, in turn, are grouped into column families.
All columns in a column family are stored together in the same lowlevel storage file, called an HFile.
Column families need to be defined when the table is created and should not be changed too often, nor should there be too many of them.
There are a few known shortcomings in the current implementation that force the count to be limited to the low tens, but in practice it is often a much smaller number (see Chapter 9 for details)
The name of the column family must be composed of printable characters, a notable difference from all other names or values.
Columns are often referenced as family:qualifier with the qualifier being any arbitrary array of bytes.# As opposed to the limit on column families, there is no such thing for the number of columns: you could have millions of columns in a particular column family.
There is also no type nor length boundary on the column values.
Figure 1-4 helps to visualize how different rows are in a normal database as opposed to the column-oriented design of HBase.
You should think about rows and columns not being arranged like the classic spreadsheet model, but rather use a tag metaphor, that is, information is available under a specific tag.
All rows and columns are defined in the context of a table, adding a few more concepts across all included column families, which we will discuss shortly.
Every column value, or cell, either is timestamped implicitly by the system or can be set explicitly by the user.
This can be used, for example, to save multiple versions of a value as it changes over time.
Different versions of a cell are stored in decreasing timestamp order, allowing you to read the newest value first.
This is an optimization aimed at read patterns that favor more current values over historical ones.
The user can specify how many versions of a value should be kept.
The values (or cells) are also just uninterpreted arrays of bytes, that the client needs to know how to handle.
If you recall from the quote earlier, the Bigtable model, as implemented by HBase, is a sparse, distributed, persistent, multidimensional map, which is indexed by row key, column key, and a timestamp.
Putting this together, we can express the access to data like so:
The first SortedMap is the table, containing a List of column families.
The families contain another SortedMap, which represents the columns, and their associated values.
These values are in the final List that holds the value and the timestamp it was set.
An interesting feature of the model is that cells may exist in multiple versions, and different columns have been written at different times.
The API, by default, provides you with a coherent view of all columns wherein it automatically picks the most current value of each cell.
Figure 1-5 shows a piece of one specific row in an example table.
The diagram visualizes the time component using tn as the timestamp when the cell was written.
The ascending index shows that the values have been added at different times.
Figure 1-6 is another way to look at the data, this time in a more spreadsheetlike layout wherein the timestamp was added to its own column.
The same parts of the row rendered as a spreadsheet.
Although they have been added at different times and exist in multiple versions, you would still see the row as the combination of all columns and their most current versions—in other words, the highest tn from each column.
There is a way to ask for values at (or before) a specific timestamp, or more than one version at a time, which we will see a little bit later in Chapter 3
The Webtable The canonical use case of Bigtable and HBase is the webtable, that is, the web pages stored while crawling the Internet.
The row key is the reversed URL of the page—for example, org.hbase.www.
There is a column family storing the actual HTML code, the contents family, as well as others like anchor, which is used to store outgoing links, another one to store inbound links, and yet another for metadata like language.
Using multiple versions for the contents family allows you to store a few older copies of the HTML, and is helpful when you want to analyze how often a page changes, for example.
The timestamps used are the actual times when they were fetched from the crawled website.
Access to row data is atomic and includes any number of columns being read or written to.
There is no further guarantee or transactional feature that spans multiple rows or across tables.
The atomic access is also a contributing factor to this architecture being strictly consistent, as each concurrent reader and writer can make safe assumptions about the state of a row.
Using multiversioning and timestamping can help with application layer consistency issues as well.
Auto-Sharding The basic unit of scalability and load balancing in HBase is called a region.
They are dynamically split by the system when they become too large.
Alternatively, they may also be merged to reduce their number and required storage files.*
The HBase regions are equivalent to range partitions as used in database sharding.
They can be spread across many physical servers, thus distributing the load, and therefore providing scalability.
Initially there is only one region for a table, and as you start adding data to it, the system is monitoring it to ensure that you do not exceed a configured maximum size.
Each region is served by exactly one region server, and each of these servers can serve many regions at any time.
Figure 1-7 shows how the logical view of a table is actually a set of regions hosted by many region servers.
Although HBase does not support online region merging, there are tools to do this offline.
This refers to the hardware in use in 2006 (and earlier)
But, while the numbers have increased, the basic principle is the same: the number of regions per server, and their respective sizes, depend on what can be handled sufficiently by a single server.
Splitting and serving regions can be thought of as autosharding, as offered by other systems.
The regions allow for fast recovery when a server fails, and fine-grained load balancing since they can be moved between servers when the load of the server currently serving the region is under pressure, or if that server becomes unavailable because of a failure or because it is being decommissioned.
Storage API Bigtable does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format [...]
The API offers operations to create and delete tables and column families.
In addition, it has functions to change the table and column family metadata, such as compression or block sizes.
Furthermore, there are the usual operations for clients to create or delete values as well as retrieving them with a given row key.
A scan API allows you to efficiently iterate over ranges of rows and be able to limit which columns are returned or the number of versions of each cell.
You can match columns using filters and select versions using time ranges, specifying start and end times.
On top of this basic functionality are more advanced features.
The system has support for single-row transactions, and with this support it implements atomic read-modifywrite sequences on data stored under a single row key.
Although there are no cross-row or cross-table transactions, the client can batch operations for performance reasons.
Cell values can be interpreted as counters and updated atomically.
These counters can be read and modified in one operation so that, despite the distributed nature of the architecture, clients can use this mechanism to implement global, strictly consistent, sequential counters.
There is also the option to run client-supplied code in the address space of the server.
The code has access to the server local data and can be used to implement lightweight batch jobs, or use expressions to analyze or summarize data based on a variety of operators.
Finally, the system is integrated with the MapReduce framework by supplying wrappers that convert tables into input source and output targets for MapReduce jobs.
Unlike in the RDBMS landscape, there is no domain-specific language, such as SQL, to query data.
Access is not done declaratively, but purely imperatively through the client-side API.
For HBase, this is mostly Java code, but there are many other choices to access the data from other programming languages.
Implementation Bigtable [...] allows clients to reason about the locality properties of the data represented in the underlying storage.
The data is stored in store files, called HFiles, which are persistent and ordered immutable maps from keys to values.
Internally, the files are sequences of blocks with a block index stored at the end.
The index is loaded when the HFile is opened and kept in.
The default block size is 64 KB but can be configured differently if required.
The store files provide an API to access specific values as well as to scan ranges of values given a start and end key.
The text here is an introduction only, while the full details are discussed in the referenced chapter(s)
Since every HFile has a block index, lookups can be performed with a single disk seek.
First, the block possibly containing the given key is determined by doing a binary search in the in-memory block index, followed by a block read from disk to find the actual key.
The store files are typically saved in the Hadoop Distributed File System (HDFS), which provides a scalable, persistent, replicated storage layer for HBase.
It guarantees that data is never lost by writing the changes across a configurable number of physical servers.
When data is updated it is first written to a commit log, called a write-ahead log (WAL) in HBase, and then stored in the in-memory memstore.
Once the data in memory has exceeded a given maximum value, it is flushed as an HFile to disk.
After the flush, the commit logs can be discarded up to the last unflushed modification.
While the system is flushing the memstore to disk, it can continue to serve readers and writers without having to block them.
This is achieved by rolling the memstore in memory where the new/empty one is taking the updates, while the old/full one is converted into a file.
Note that the data in the memstores is already sorted by keys matching exactly what HFiles represent on disk, so no sorting or other special processing has to be performed.
We can now start to make sense of what the locality properties are, mentioned in the Bigtable quote at the beginning of this section.
Since all files contain sorted key/value pairs, ordered by the key, and are optimized for block operations such as reading these pairs sequentially, you should specify keys to keep related data together.
Referring back to the webtable example earlier, you may have noted that the key used is the reversed FQDN (the domain name part of the URL), such as org.hbase.www.
The reason is to store all pages from hbase.org close to one another, and reversing the URL puts the most important part of the URL first, that is, the top-level domain (TLD)
Pages under blog.hbase.org would then be sorted with those from www.hbase.org— or in the actual key format, org.hbase.blog sorts next to org.hbase.www.
Because store files are immutable, you cannot simply delete values by removing the key/value pair from them.
Instead, a delete marker (also known as a tombstone marker) is written to indicate the fact that the given key has been deleted.
Reading data back involves a merge of what is stored in the memstores, that is, the data that has not been written to disk, and the on-disk store files.
Note that the WAL is never used during data retrieval, but solely for recovery purposes when a server has crashed before writing the in-memory data to disk.
Since flushing memstores to disk causes more and more HFiles to be created, HBase has a housekeeping mechanism that merges the files into larger ones using compaction.
There are two types of compaction: minor compactions and major compactions.
The former reduce the number of storage files by rewriting smaller files into fewer but larger ones, performing an n-way merge.
Since all the data is already sorted in each HFile, that merge is fast and bound only by disk I/O performance.
The major compactions rewrite all files within a column family for a region into a single new one.
They also have another distinct feature compared to the minor compactions: based on the fact that they scan all key/value pairs, they can drop deleted entries including their deletion marker.
Predicate deletes are handled here as well—for example, removing values that have expired according to the configured time-to-live or when there are too many versions.
The only difference is that LSM-trees are storing data in multipage blocks that are arranged in a B-tree-like structure on disk.
They are updated, or merged, in a rotating fashion, while in Bigtable the update is more course-grained and the whole memstore is saved as a new store file and not merged right away.
There are three major components to HBase: the client library, one master server, and many region servers.
The region servers can be added or removed while the system is up and running to accommodate changing workloads.
The master is responsible for assigning regions to region servers and uses Apache ZooKeeper, a reliable, highly available, persistent and distributed coordination service, to facilitate that task.
Apache ZooKeeper ZooKeeper† is a separate open source project, and is also part of the Apache Software Foundation.
ZooKeeper is the comparable system to Google’s use of Chubby for Bigtable.
It offers filesystem-like access with directories and files (called znodes) that.
For more information on Apache ZooKeeper, please refer to the official project website.
Every region server creates its own ephemeral node in ZooKeeper, which the master, in turn, uses to discover available servers.
They are also used to track server failures or network partitions.
Ephemeral nodes are bound to the session between ZooKeeper and the client which created it.
The session has a heartbeat keepalive mechanism that, once it fails to report, is declared lost by ZooKeeper and the associated ephemeral nodes are deleted.
HBase uses ZooKeeper also to ensure that there is only one master running, to store the bootstrap location for region discovery, as a registry for region servers, as well as for other purposes.
ZooKeeper is a critical component, and without it HBase is not operational.
This is mitigated by ZooKeeper’s distributed design using an assemble of servers and the Zab protocol to keep its state consistent.
Figure 1-8 shows how the various components of HBase are orchestrated to make use of existing system, like HDFS and ZooKeeper, but also adding its own layers to form a complete platform.
The master server is also responsible for handling load balancing of regions across region servers, to unload busy servers and move regions to less occupied ones.
The master is not part of the actual data storage or retrieval path.
It negotiates load balancing and maintains the state of the cluster, but never provides any data services to either the region servers or the clients, and is therefore lightly loaded in practice.
In addition, it takes care of schema changes and other metadata operations, such as creation of tables and column families.
Region servers are responsible for all read and write requests for all regions they serve, and also split regions that have exceeded the configured region size thresholds.
Clients communicate directly with them to handle all data-related operations.
We have seen how the Bigtable storage architecture is using many servers to distribute ranges of rows sorted by their key for load-balancing purposes, and can scale to petabytes of data on thousands of machines.
The storage format used is ideal for reading adjacent key/value pairs and is optimized for block I/O operations that can saturate disk transfer channels.
Table scans run in linear time and row key lookups or mutations are performed in logarithmic order—or, in extreme cases, even constant order (using Bloom filters)
Designing the schema in a way to completely avoid explicit locking, combined with row-level atomicity, gives you the ability to scale your system without any notable effect on read or write performance.
The column-oriented architecture allows for huge, wide, sparse tables as storing NULLs is free.
Because each row is served by exactly one server, HBase is strongly consistent, and using its multiversioning can help you to avoid edit conflicts caused by concurrent decoupled processes or retain a history of changes.
The actual Bigtable has been in production at Google since at least 2005, and it has been in use for a variety of different use cases, from batch-oriented processing to realtime data-serving.
The stored data varies from very small (like URLs) to quite large (e.g., web pages and satellite imagery) and yet successfully provides a flexible, highperformance solution for many well-known Google products, such as Google Earth, Google Reader, Google Finance, and Google Analytics.
HBase: The Hadoop Database Having looked at the Bigtable architecture, we could simply state that HBase is a faithful, open source implementation of Google’s Bigtable.
But that would be a bit too simplistic, and there are a few (mostly subtle) differences worth addressing.
Since then, it has become its own top-level project under the Apache Software Foundation umbrella.
It is available under the Apache Software License, version 2.0
Powerset is a company based in San Francisco that was developing a natural language search engine for the Internet.
The project home page is http://hbase.apache.org/, where you can find links to the documentation, wiki, and source repository, as well as download sites for the binary and source releases.
Here is a short overview of how HBase has evolved over time: November 2006
Hadoop becomes an Apache top-level project, HBase becomes subproject October 2008
Around May 2010, the developers decided to break with the version numbering that was used to be in lockstep with the Hadoop releases.
The rationale was that HBase had a much faster release cycle and was also approaching a version 1.0 level sooner than what was expected from Hadoop.
In addition, a decision was made to title 0.89.x the early access version for developers and bleeding-edge integrators.
For an interesting flash back in time, see HBASE-287 on the Apache JIRA, the issue tracking system.
You can see how Mike Cafarella did a code drop that was then quickly picked up by Jim Kellerman, who was with Powerset back then.
Nomenclature One of the biggest differences between HBase and Bigtable concerns naming, as you can see in Table 1-1, which lists the various terms and what they correspond to in each system.
HBase extends the Bigtable model, which only considers a single index, similar to a primary key in the RDBMS world, offering the server-side hooks to implement flexible secondary index solutions.
In addition, it provides push-down predicates, that is, filters, reducing data transferred over the network.
There is no declarative query language as part of the core implementation, and it has limited support for transactions.
Row atomicity and read-modify-write operations make up for this in practice, as they cover most use cases and remove the wait or deadlock-related pauses experienced with other systems.
HBase handles shifting load and failures gracefully and transparently to the clients.
Changing the cluster does not involve any complicated rebalancing or resharding procedure, but is completely automated.
In this chapter, we will look at how HBase is installed and initially configured.
We will see how HBase can be used from the command line for basic operations, such as adding, retrieving, and deleting data.
All of the following assumes you have the Java Runtime Environment (JRE) installed.
Quick-Start Guide Let us get started with the “tl;dr” section of this book: you want to know how to run HBase and you want to know it now! Nothing is easier than that because all you have to do is download the most recent release of HBase from the Apache HBase release page and unpack the contents into a suitable directory, such as /usr/local or /opt, like so:
Setting the Data Directory At this point, you are ready to start HBase.
But before you do so, it is advisable to set the data directory to a proper location.
You need to edit the configuration file conf/ hbase-site.xml and set the directory you want HBase to write to by assigning a value to the property key named hbase.rootdir:
Replace <PATH> in the preceding example configuration file with a path to a directory where you want HBase to store its data.
By default, hbase.rootdir is set to /tmp/hbase${user.name}, which could mean you lose all your data whenever your server reboots because a lot of operating systems (OSes) clear out /tmp during a restart.
With that in place, we can start HBase and try our first interaction with it.
We will use the interactive shell to enter the status command at the prompt (complete the command by pressing the Return key):
This confirms that HBase is up and running, so we will now issue a few commands to show that we can put data into it and retrieve the same data subsequently.
It may not be clear, but what we are doing right now is similar to sitting in a car with its brakes engaged and in neutral while turning the ignition key.
There is much more that you need to configure and understand before you can use HBase in a production-like environment.
But it lets you get started with some basic HBase commands and become familiar with top-level concepts.
Many people have lost their test data during a reboot, only to learn that they kept the default path.
Once it is deleted by the OS, there is no going back!
After we create the table with one column family, we verify that it actually exists by issuing a list command.
You can see how it outputs the testtable name as the only table currently known.
Subsequently, we are putting data into a number of rows.
Next we want to check if the data we added can be retrieved.
You can observe how HBase is printing the data in a cell-oriented way by outputting each column separately.
It prints out myrow-2 twice, as expected, and shows the actual value for each column next to it.
If we want to get exactly one row back, we can also use the get command.
It has many more options, which we will look at later, but for now simply try the following:
What is missing in our basic set of operations is to delete a value.
Again, delete offers many options, but for now we just delete one specific cell and check that it is gone:
Before we conclude this simple exercise, we have to clean up by first disabling and then dropping the test table:
Finally, we close the shell by means of the exit command and return to our commandline prompt:
The last thing to do is stop HBase on our local system.
We have successfully created a table, added, retrieved, and deleted data, and eventually dropped the table using the HBase Shell.
Requirements Not all of the following requirements are needed for specific run modes HBase supports.
Hardware It is difficult to specify a particular server type that is recommended for HBase.
In fact, the opposite is more appropriate, as HBase runs on many, very different hardware configurations.
But what does that mean? For starters, we are not talking about desktop PCs, but server-grade machines.
Given that HBase is written in Java, you at least need support for a current Java Runtime, and.
In practice, a lot of HBase setups are collocated with Hadoop, to make use of locality using HDFS as well as MapReduce.
This can significantly reduce the required network I/O and boost processing speeds.
Running Hadoop and HBase on the same server results in at least three Java processes running (data node, task tracker, and region server) and may spike to much higher numbers when executing MapReduce jobs.
All of these processes need a minimum amount of memory, disk, and CPU resources to run sufficiently.
It is assumed that you have a reasonably good understanding of Hadoop, since it is used as the backing store for HBase in all known production systems (as of this writing)
If you are completely new to HBase and Hadoop, it is recommended that you get familiar with Hadoop first, even on a very basic level.
Giving all the available memory to the Java processes is also not a good idea, as most operating systems need some spare resources to work more effectively—for example, disk I/O buffers maintained by Linux kernels.
HBase indirectly takes advantage of this because the already local disk I/O, given that you collocate the systems on the same server, will perform even better when the OS can keep its own block cache.
We can separate the requirements into two categories: servers and networking.
We will look at the server hardware first and then into the requirements for the networking setup subsequently.
They do benefit from slightly different hardware specifications when possible.
It is also quite common to use exactly the same hardware for both (out of convenience), but the master does not need that much storage, so it makes sense to not add too many disks.
And since the masters are also more important than the slaves, you could beef them up with redundant hardware components.
We will address the differences between the two where necessary.
It allows you to select from a wide variety of vendors, or even build your own hardware.
It comes down to more generic requirements like the following:
For production use, it is typical that you use multicore processors.* Quad-core are state of the art and affordable, while hexa-core processors are also becoming more popular.
Most server hardware supports more than one CPU so that you can use two quad-core CPUs for a total of eight cores.
This allows for each basic Java process to run on its own core while the background tasks like Java garbage collection can be executed in parallel.
In addition, there is hyperthreading, which adds to their overall performance.
As far as CPU is concerned, you should spec the master and slave machines the same.
Memory The question really is: is there too much memory? In theory, no, but in practice, it has been empirically determined that when using Java you should not set the amount of memory given to a single process too high.
Memory (called heap in Java terms) can start to get fragmented, and in a worst-case scenario, the entire heap would need rewriting—this is similar to the well-known disk fragmentation, but it cannot run in the background.
The Java Runtime pauses all processing to clean up the mess, which can lead to quite a few problems (more on this later)
The larger you have set the heap, the longer this process will take.
Processes that do not need a lot of memory should only be given their required amount to avoid this scenario, but with the region servers and their block cache there is, in theory, no upper limit.
You need to find a sweet spot depending on your access pattern.
At the time of this writing, setting the heap of the region servers to larger than 16 GB is considered dangerous.
Once a stop-the-world garbage collection is required, it simply takes too long to rewrite the fragmented heap.
Your server could be considered dead by the master and be removed from the working set.
This may change sometime as this is ultimately bound to the Java Runtime Environment used, and there is development going on to implement JREs that do not stop the running Java processes when performing garbage collections.
Table 2-1 shows a very basic distribution of memory to specific processes.
Please note that this is an example only and highly depends on the size of your cluster and how much data you put in, but also on your access pattern, such as interactive access only or a combination of interactive and batch use (using MapReduce)
Exemplary memory allocation per Java process for a cluster with 800 TB of raw disk storage space.
SecondaryNameNode 8 GB Applies the edits in memory, and therefore needs about the same amount as the NameNode.
HBase Master 4 GB Usually lightly loaded, moderate requirements only.
HBase RegionServer 12 GB Majority of available memory, while leaving enough room for the operating system (for the buffer cache), and for the Task Attempt processes.
Task Attempts 1 GB (ea.) Multiply by the maximum number you allow for each.
It is recommended that you optimize your RAM for the memory channel width of your server.
For example, when using dualchannel memory, each machine should be configured with pairs of DIMMs.
With triple-channel memory, each server should have triplets of DIMMs.
Also make sure that not just the server’s motherboard supports this feature, but also your CPU: some CPUs only support dual-channel memory, and therefore, even if you put in triple-channel DIIMMs, they will only be used in dual-channel mode.
Disks The data is stored on the slave machines, and therefore it is those servers that need plenty of capacity.
Depending on whether you are more read/write- or processingoriented, you need to balance the number of disks with the number of CPU cores available.
Typically, you should have at least one core per disk, so in an eight-core server, adding six disks is good, but adding more might not be giving you optimal performance.
Here is where we can draw a line between the master server and the slaves.
For the master nodes, on the other hand, it does make sense to use a RAID disk setup to protect the crucial filesystem data.
For both servers, though, make sure to use disks with RAID firmware.
The difference between these and consumer-grade disks is that the RAID firmware will fail fast if there is a hardware error, and therefore will not freeze the DataNode in disk wait for a long time.
In general, SATA drives are recommended over SAS since they are more cost-effective, and since the nodes are all redundantly storing replicas of the data across multiple servers, you can safely use the more affordable disks.
Depending on your requirements, you need to make sure to combine the right number of disks to achieve your goals.
Chassis The actual server chassis is not that crucial, as most servers in a specific price bracket provide very similar features.
It is often better to shy away from special hardware that offers proprietary functionality and opt for generic servers so that they can be easily combined over time as you extend the capacity of the cluster.
As far as networking is concerned, it is recommended that you use a two-port Gigabit Ethernet card—or two channel-bonded cards.
If you already have support for 10 Gigabit Ethernet or InfiniBand, you should use it.
For the slave servers, a single power supply unit (PSU) is sufficient, but for the master node you should use redundant PSUs, such as the optional dual PSUs available for many servers.
In terms of density, it is advisable to select server hardware that fits into a low number of rack units (abbreviated as “U”)
A consideration while choosing the size is how many disks they can hold and their power consumption.
Given the Gigabit speed per server, you need to ensure that the ToR switch is fast enough to handle the throughput these servers can create.
Often the backplane of a switch cannot handle all ports at line.
This results in a two-tier architecture, where the distribution is handled by the ToR switch and the aggregation by the CaS.
While we cannot address all the considerations for large-scale setups, we can still notice that this is a common design pattern.
Given that the operations team is part of the planning, and it is known how much data is going to be stored and how many clients are expected to read and write concurrently, this involves basic math to compute the number of servers needed—which also drives the networking considerations.
When users have reported issues with HBase on the public mailing list or on other channels, especially regarding slower-than-expected I/O performance bulk inserting huge amounts of data, it became clear that networking was either the main or a contributing issue.
This ranges from misconfigured or faulty network interface cards (NICs) to completely oversubscribed switches in the I/O path.
Please make sure that you verify every component in the cluster to avoid sudden operational problems—the kind that could have been avoided by sizing the hardware appropriately.
Finally, given the current status of built-in security in Hadoop and HBase, it is common for the entire cluster to be located in its own network, possibly protected by a firewall to control access to the few required, client-facing ports.
Software After considering the hardware and purchasing the server machines, it’s time to consider software.
This can range from the operating system itself to filesystem choices and configuration of various auxiliary services.
Most of the requirements listed are independent of HBase and have to be applied on a very low, operational level.
You may have to advise with your administrator to get everything applied and verified.
Recommending an operating system (OS) is a tough call, especially in the open source realm.
In terms of the past two to three years, it seems there is a preference for using Linux with HBase.
In fact, Hadoop and HBase are inherently designed to work with Linux, or any other Unix-like system, or with Unix.
The supplied start and stop scripts, for example, expect a command-line shell as provided by Linux or Unix.
Within the Unix and Unix-like group you can also differentiate between those that are free (as in they cost no money) and those you have to pay for.
Again, both will work and your choice is often limited by company-wide regulations.
Here is a short list of operating systems that are commonly found as a basis for HBase clusters: CentOS.
CentOS is a community-supported, free software operating system, based on Red Hat Enterprise Linux (as RHEL)
It mirrors RHEL in terms of functionality, features, and package release levels as it is using the source code packages Red Hat provides for its own enterprise product to create CentOS-branded counterparts.
It is also focused on enterprise usage, and therefore does not adopt new features or newer versions of existing packages too quickly.
The goal is to provide an OS that can be rolled out across a large-scale infrastructure while not having to deal with short-term gains of small, incremental package updates.
Fedora Fedora is also a community-supported, free and open source operating system, and is sponsored by Red Hat.
But compared to RHEL and CentOS, it is more a playground for new technologies and strives to advance new ideas and features.
Because of that, it has a much shorter life cycle compared to enterprise-oriented products.
An average maintenance period for a Fedora release is around 13 months.
The fact that it is aimed at workstations and has been enhanced with many new features has made Fedora a quite popular choice, only beaten by more desktoporiented operating systems.‖ For production use, you may want to take into account the reduced life cycle that counteracts the freshness of this distribution.
You may also want to consider not using the latest Fedora release, but trailing by one version to be able to rely on some feedback from the community as far as stability and other issues are concerned.
Debian Debian is another Linux-kernel-based OS that has software packages released as free and open source software.
It can be used for desktop and server systems and has a conservative approach when it comes to package updates.
Releases are only published after all included packages have been sufficiently tested and deemed stable.
As opposed to other distributions, Debian is not backed by a commercial entity, but rather is solely governed by its own project rules.
DistroWatch has a list of popular Linux and Unix-like operating systems and maintains a ranking by popularity.
Debian is known to run on many hardware platforms as well as having a very large repository of packages.
It is distributed as free and open source software, and backed by Canonical Ltd., which is not charging for the OS but is selling technical support for Ubuntu.
The life cycle is split into a longer- and a shorter-term release.
The long-term support (LTS) releases are supported for three years on the desktop and five years on the server.
The packages are also DEB format and are based on the unstable branch of Debian: Ubuntu, in a sense, is for Debian what Fedora is for Red Hat Linux.
Using Ubuntu as a server operating system is made more difficult as the update cycle for critical components is very frequent.
Solaris Solaris is offered by Oracle, and is available for a limited number of architecture platforms.
It is a descendant of Unix System V Release 4, and therefore, the most different OS in this list.
Some of the source code is available as open source while the rest is closed source.
Solaris is a commercial product and needs to be purchased.
The OS is available as a server and a desktop version.
The license comes with offerings for official support, training, and a certification program.
The package format for RHEL is called RPM (the Red Hat Package Manager), and it consists of the software packaged in the .rpm file format, and the package manager itself.
You have a choice when it comes to the operating system you are going to use on your servers.
A sensible approach is to choose one you feel comfortable with and that fits into your existing infrastructure.
As for a recommendation, many production systems running HBase are on top of CentOS, or RHEL.
With the operating system selected, you will have a few choices of filesystems to use with your disks.
There is not a lot of publicly available empirical data in regard to comparing different filesystems and their effect on HBase, though.
For some there are HBase users reporting on their findings, while for more exotic ones you would need to run enough tests before using it on your production cluster.
Note that the selection of filesystems is for the HDFS data nodes.
HBase is directly impacted when using HDFS as its backing store.
Here are some notes on the more commonly used filesystems: ext3
It has been proven stable and reliable, meaning it is a safe bet in terms of setting up your cluster with it.
Being part of Linux since 2001, it has been steadily improved over time and has been the default filesystem for years.
There are a few optimizations you should keep in mind when using ext3
First, you should set the noatime option when mounting the filesystem to reduce the administrative overhead required for the kernel to keep the access time for each file.
It is not needed or even used by HBase, and disabling it speeds up the disk’s read performance.
Disabling the last access time gives you a performance boost and is a recommended optimization.
Mount options are typically specified in a configuration file called /etc/fstab.
Here is a Linux example line where the noatime option is specified:
Another optimization is to make better use of the disk space provided by ext3
By default, it reserves a specific number of bytes in blocks for situations where a disk fills up but crucial system processes need this space to continue to function.
Do this for all disks on which you want to store data.
A final word of caution: only do this for your data disk, NOT for the disk hosting the OS nor for any drive on the master node!
This shows that, although it is by far not the most current or modern filesystem, it does very well in large clusters.
In fact, you are more likely to saturate your I/O on other levels of the stack before reaching the limits of ext3
The biggest drawback of ext3 is that during the bootstrap process of the servers it requires the largest amount of time.
It has been officially part of the Linux kernel since the end of 2008
To that extent, it has had only a few years to prove its stability and reliability.
Choosing an entirely different filesystem like XFS would have made this impossible.
A more critical feature is the so-called delayed allocation, and it is recommended that you turn it off for Hadoop and HBase use.
Delayed allocation keeps the data in memory and reserves the required number of blocks until the data is finally flushed to disk.
It helps in keeping blocks for files together and can at times write the entire file into a contiguous set of blocks.
This reduces fragmentation and im#See this post on the Ars Technica website.
On the other hand, it increases the possibility of data loss in case of a server crash.
Its features are similar to those of ext4; for example, both have extents (grouping contiguous blocks together, reducing the number of blocks required to maintain per file) and the aforementioned delayed allocation.
A great advantage of XFS during bootstrapping a server is the fact that it formats the entire drive in virtually no time.
This can significantly reduce the time required to provision new servers with many storage disks.
On the other hand, there are some drawbacks to using XFS.
There is a known shortcoming in the design that impacts metadata operations, such as deleting a large number of files.
The developers have picked up on the issue and applied various fixes to improve the situation.
You will have to check how you use HBase to determine if this might affect you.
For normal use, you should not have a problem with this limitation of XFS, as HBase operates on fewer but larger files.
It has built-in compression support that could be used as a replacement for the pluggable compression codecs in HBase.
It seems that choosing a filesystem is analogous to choosing an operating system: pick one that you feel comfortable with and that fits into your existing infrastructure.
Simply picking one over the other based on plain numbers is difficult without proper testing and comparison.
Installing different filesystems on a single server is not recommended.
This can have adverse effects on performance as the kernel may have to split buffer caches to support the different filesystems.
It has been reported that, for certain operating systems, this can have a devastating performance impact.
Make sure you test this issue carefully if you have to mix filesystems.
It was mentioned in the note  on page 31 that you do need Java for HBase.
Not just any version of Java, but version 6, a.k.a.
The recommended choice is the one provided by Oracle (formerly by Sun), which can be found at http://www.java.com/ download/
You also should make sure the java binary is executable and can be found on your path.
You usually want the latest update level, but sometimes you may find unexpected problems (version 1.6.0_18, for example, is known to cause random JVM crashes) and it may be worth trying an older release to verify.
The supplied scripts try many default locations for Java, so there is a good chance HBase will find it automatically.
If it does not, you most likely have no Java Runtime installed at all.
Start with the download link provided at the beginning of this subsection and read the manuals of your operating system to find out how to install it.
Currently, HBase is bound to work only with the specific version of Hadoop it was built against.
One of the reasons for this behavior concerns the remote procedure call (RPC) API between HBase and Hadoop.
The wire protocol is versioned and needs to match up; even small differences can cause a broken communication between them.
The current version of HBase will only run on Hadoop 0.20.x.
HBase may lose data in a catastrophic event unless it is running on an HDFS that has durable sync support.
Because HBase depends on Hadoop, it bundles an instance of the Hadoop JAR under its lib directory.
The bundled Hadoop was made from the Apache branch-0.20-append branch at the time of HBase’s release.
It is critical that the version of Hadoop that is in use on your cluster matches what is used by HBase.
Replace the Hadoop JAR found in the HBase lib directory with the hadoop-xyz.jar you are running on your cluster to avoid version mismatch issues.
Make sure you replace the JAR on all servers in your cluster that run HBase.
Version mismatch issues have various manifestations, but often the result is the same: HBase does not throw an error, but simply blocks indefinitely.
The bundled JAR that ships with HBase is considered only for use in standalone mode.
A different approach is to install a vanilla Hadoop 0.20.2 and then replace the vanilla Hadoop JAR with the one supplied by HBase.
This is very likely to change after this book is printed.
Consult with the online configuration guide for the latest details; especially the section on Hadoop.
Note that ssh must be installed and sshd must be running if you want to use the supplied scripts to manage remote Hadoop and HBase daemons.
Check with your operating system manuals first, as many OSes have mechanisms to install an already compiled binary release package as opposed to having to build it yourself.
On the servers, you would install the matching server package: $ sudo apt-get install openssh-server.
You must be able to ssh to all nodes, including your local node, using passwordless login.
The supplied shell scripts make use of SSH to send commands to each server in the cluster.
It is strongly advised that you not use simple password authentication.
Instead, you should use public key authentication—only! When you create your key pair, also add a passphrase to protect your every single command sent to a remote server, it is recommended that you use ssh-agent, a helper that comes with SSH.
It lets you enter the passphrase only once and then takes care of all subsequent requests to provide it.
Ideally, you would also use the agent forwarding that is built in to log in to other remote servers from your cluster nodes.
HBase uses the local hostname to self-report its IP address.
You can verify if the setup is correct for forward DNS lookups by running the following command:
You need to make sure that it reports the public IP address of the server and not the loopback address 127.0.0.1
A typical reason for this not to work concerns an incorrect /etc/hosts file, containing a mapping of the machine name to the loopback address.
If your machine has multiple interfaces, HBase will use the interface that the primary hostname resolves to.
This only works if your cluster configuration is consistent and every host has the same network interface configuration.
Another alternative is to set hbase.regionserver.dns.nameserver to choose a different name server than the system-wide default.
The clocks on cluster nodes should be in basic alignment.
Some skew is tolerable, but wild skew can generate odd behaviors.
Even differences of only one minute can cause unexplainable behavior.
Run NTP on your cluster, or an equivalent application, to synchronize the time on all servers.
If you are having problems querying data, or you are seeing weird behavior running cluster operations, check the system time!
HBase is a database, so it uses a lot of files at the same time.
The default ulimit -n of 1024 on most Unix or other Unix-like systems is insufficient.
Any significant amount of loading will lead to I/O errors stating the obvious: java.io.IOException: Too many open files.
You need to change the upper bound on the number of file descriptors.
To be clear, upping the file descriptors for the user who is running the HBase process is an operating system configuration, not an HBase configuration.
Also, a common mistake is that administrators will increase the file descriptors for a particular user but HBase is running with a different user account.
You can estimate the number of required file handles roughly as follows.
Per column family, there is at least one storage file, and possibly up to five or six if a region is under load; on average, though, there are three storage files per column family.
To determine the number of required file handles, you multiply the number of column families by the number of regions per region server.
As the first line in its logs, HBase prints the ulimit it is seeing.
You may also need to edit /etc/sysctl.conf and adjust the fs.file-max value.
Example: Setting File Handles on Ubuntu If you are on Ubuntu, you will need to make the following changes.
Replace hadoop with whatever user is running Hadoop and HBase.
If you have separate users, you will need two entries, one for each user.
In the file /etc/pam.d/common-session add the following as the last line in the file:
Don’t forget to log out and back in again for the changes to take effect!
You should also consider increasing the number of processes allowed by adjusting the nproc value in the same /etc/security/limits.conf file referenced earlier.
With a low limit and a server under duress, you could see OutOfMemoryError exceptions, which will eventually cause the entire Java process to end.
As with the file handles, you need to make sure this value is set for the appropriate user account running the process.
A Hadoop HDFS data node has an upper bound on the number of files that it will serve at any one time.
The upper bound parameter is called xcievers (yes, this is misspelled)
Again, before doing any loading, make sure you have configured Hadoop’s conf/hdfssite.xml file, setting the xcievers value to at least the following:
Be sure to restart your HDFS after making the preceding configuration changes.
Not having this configuration in place makes for strange-looking failures.
Eventually, you will see a complaint in the datanode logs about the xcievers limit being exceeded, but on the run up to this one manifestation is a complaint about missing blocks.
You need to prevent your servers from running out of memory over time.
We already discussed one way to do this: setting the heap sizes small enough that they give the operating system enough room for its own processes.
Once you get close to the physically available memory, the OS starts to use the configured swap space.
This is typically located on disk in its own partition and is used to page out processes and their allocated memory until it is needed again.
Once the server starts swapping, performance is reduced significantly, up to a point where you may not even be able to log in to such a system because the remote access process (e.g., SSHD) is coming to a grinding halt.
HBase needs guaranteed CPU cycles and must obey certain freshness guarantees—for example, to renew the ZooKeeper sessions.
It has been observed over and over again that swapping servers start to miss renewing their leases and are considered lost subsequently by the ZooKeeper ensemble.
The regions on these servers are redeployed on other servers, which now take extra pressure and may fall into the same trap.
Even worse are scenarios where the swapping server wakes up and now needs to realize it is considered dead by the master node.
It will report for duty as if nothing has happened and receive a YouAreDeadException in the process, telling it that it has missed its chance to continue, and therefore terminates itself.
There are quite a few implicit issues with this scenario—for example, pending updates, which we will address later.
You can tune down the swappiness of the server by adding this line to the /etc/ sysctl.conf configuration file on Linux and Unix-like systems:
Choose something you feel comfortable with, but make sure you keep an eye on this problem.
Finally, you may have to reboot the server for the changes to take effect, as a simple.
This obviously is for Unix-like systems and you will have to adjust this for your operating system.
HBase running on Windows has not been tested to a great extent.
Running a production install of HBase on top of Windows is not recommended.
If you are running HBase on Windows, you must install Cygwin to have a Unix-like environment for the shell scripts.
The full details are explained in the Windows Installation guide on the HBase website.
But you are not locked into HDFS because the FileSystem used by HBase has a pluggable architecture and can be used to replace HDFS with any other supported system.
In fact, you could go as far as implementing your own filesystem—maybe even on top of another database.
The possibilities are endless and waiting for the brave at heart.
These are abstractions that define higher-level features and APIs, which are then used by Hadoop to store the data.
The data is eventually stored on a disk, at which point the OS filesystem is used.
Almost all production clusters use it as the underlying storage layer.
It is proven stable and reliable, so deviating from it may impose its own risks and subsequent problems.
The primary reason HDFS is so popular is its built-in replication, fault tolerance, and scalability.
Choosing a different filesystem should provide the same guarantees, as HBase implicitly assumes that data is stored in a reliable manner by the filesystem.
It has no added means to replicate data or even maintain copies of its own storage files.
Figure 2-1 shows how the Hadoop filesystem is different from the low-level OS filesystems for the actual disks.
You can use a filesystem that is already supplied by Hadoop: it ships with a list of filesystems,‖ which you may want to try out first.
Local The local filesystem actually bypasses Hadoop entirely, that is, you do not need to have an HDFS or any other cluster at all.
It is handled all in the FileSystem class used by HBase to connect to the filesystem implementation.
The supplied ChecksumFileSys tem class is loaded by the client and uses local disk paths to store all the data.
The beauty of this approach is that HBase is unaware that it is not talking to a distributed filesystem on a remote or collocated cluster, but actually is using the local filesystem directly.
The standalone mode of HBase uses this feature to run HBase only.
Similar to the URIs used in a web browser, the file: scheme addresses local files.
For HBase, HDFS is the filesystem of choice, as it has all the required features.
As we discussed earlier, HDFS is built to work with MapReduce, taking full advantage of its parallel, streaming access support.
The scalability, fail safety, and automatic replication functionality is ideal for storing files reliably.
HBase adds the random access layer missing from HDFS and ideally complements Hadoop.
Using MapReduce, you can do bulk imports, creating the storage files at disk-transfer speeds.
The S3 FileSystem implementation provided by Hadoop supports two different modes: the raw (or native) mode, and the block-based mode.
You can see all the files in your bucket the same way as you would on your local disk.
The block mode emulates the HDFS filesystem on top of S3
It makes browsing the bucket content more difficult as only the internal block files are visible, and the HBase storage files are stored arbitrarily inside these blocks and strewn across them.
Other Filesystems There are other filesystems, and one that deserves mention is CloudStore (formerly known as the Kosmos filesystem, abbreviated as KFS and the namesake of the URI scheme shown at the end of the next paragraph)
It is an open source, distributed, highperformance filesystem written in C++, with similar features to HDFS.
It is available for Solaris and Linux, originally developed by Kosmix and released as open source in 2007
To select CloudStore as the filesystem for HBase use the following URI format:
Installation Choices Once you have decided on the basic OS-related options, you must somehow get HBase onto your servers.
You have a couple of choices, which we will look into next.
Apache Binary Release The canonical installation process of most Apache projects is to download a release, usually provided as an archive containing all the required files.
Some projects have separate archives for a binary and source release—the former intended to have everything needed to run the release and the latter containing all files needed to build the project yourself.
HBase comes as a single package, containing binary and source files.
For more information on HBase releases, you may also want to check out the Release Notes† page.
Another interesting page is titled Change Log,‡ and it lists everything that was added, fixed, or changed in any form for each release version.
You can download the most recent release of HBase from the Apache HBase release page and unpack the contents into a suitable directory, such as /usr/local or /opt, like so:
Once you have extracted all the files, you can make yourself familiar with what is in the project’s directory.
The root of it only contains a few text files, stating the license terms (LICENSE.txt and NOTICE.txt) and some general information on how to find your way around (README.txt)
The CHANGES.txt file is a static snapshot of the change log page mentioned earlier.
It contains all the changes that went into the current release you downloaded.
You will also find the Java archive, or JAR files, that contain the compiled Java code plus all other necessary resources.
There are two variations of the JAR file, one with just the name and version number and one with a postfix of tests.
This file contains the code required to run the tests provided by HBase.
These are functional unit tests that the developers use to verify a release is fully operational and that there are no regressions.
The last file found is named pom.xml and is the Maven project file needed to build HBase from the sources.
The remainder of the content in the root directory consists of other directories, which are explained in the following list:
Open your web browser of choice and open the docs/index.html file by either dragging it into the browser, double-clicking that file, or using the File→Open (or similarly named) menu.
Most likely you will never have to touch this directory when working with or deploying HBase into production.
All of these libraries are located in the lib directory.
Initially, there may be no logs directory, as it is created when you start HBase for the first time.
The logging framework used by HBase is creating the directory and logfiles dynamically.
Processes that are started and then run in the background to perform their task are often referred to as daemons.
Building from Source HBase uses Maven to build the binary packages.
This section is important only if you want to build HBase from its sources.
This might be necessary if you want to apply patches, which can add new functionality you may be requiring.
Once you have confirmed that both are set up properly, you can build the binary packages using the following command:
Note that the tests for HBase need more than one hour to complete.
If you trust the code to be operational, or you are not willing to wait, you can also skip the test phase, adding a command-line switch like so:
Once the build completes with a Build Successful message, you can find the compiled and packaged tarball archive in the target directory.
Run Modes HBase has two run modes: standalone and distributed.
To set up HBase in distributed mode, you will need to edit files in the HBase conf directory.
Whatever your mode, you may need to edit conf/hbase-env.sh to tell HBase which java to use.
In this file, you set HBase environment variables such as the heap size and other options for the JVM, the preferred location for logfiles, and so on.
Set JAVA_HOME to point at the root of your java installation.
ZooKeeper binds to a well-known port so that clients may talk to HBase.
See the Hadoop requirements and instructions for how to set up an HDFS.
Before proceeding, ensure that you have an appropriate, working HDFS.
A pseudodistributed mode is simply a distributed mode that is run on a single host.
Do not use this configuration for production or for evaluating HBase performance.
Point HBase at the running Hadoop HDFS instance by setting the hbase.rootdir property.
For example, adding the following properties to your hbase-site.xml file says that HBase should use the /hbase directory in the HDFS whose name node is at port 9000 on your local machine, and that it should run with one replica only (recommended for pseudodistributed mode):
Amend accordingly, if you want to connect from a remote location.
See Chapter 12 for information on how to start extra master and region servers when running in pseudodistributed mode.
For running a fully distributed operation on more than one host, you need to use the following configurations.
In hbase-site.xml, add the  hbase.cluster.distributed property and set it to true, and point the HBase hbase.rootdir at the appropriate HDFS name node and location in HDFS where you would like HBase to write data.
For example, if your name node is running at a server with the hostname namenode.foo.com on port 9000 and you want to home your HBase in HDFS at /hbase, use the following configuration:
In addition, a fully distributed mode requires that you modify the conf/regionservers file.
It lists all the hosts on which you want to run HRegionServer daemons.
Specify one host per line (this file in HBase is like the Hadoop slaves file)
All servers listed in this file will be started and stopped when the HBase cluster start or stop scripts are run.
All participating nodes and clients need to be able to access the running ZooKeeper ensemble.
HBase, by default, manages a ZooKeeper cluster (which can be as low as a single node) for you.
It will start and stop the ZooKeeper ensemble as part of the HBase start and stop process.
You can also manage the ZooKeeper ensemble independent of HBase and just point HBase at the cluster it should use.
To toggle HBase management of ZooKeeper, use the HBASE_MANAGES_ZK variable in conf/hbase-env.sh.
When HBase manages the ZooKeeper ensemble, you can specify the ZooKeeper configuration using its native zoo.cfg file, or just specify the ZooKeeper options directly in conf/hbase-site.xml.
You can set a ZooKeeper configuration option as a property in the HBase hbase-site.xml XML configuration file by prefixing the ZooKeeper option name with hbase.zookeeper.property.
For example, you can change the clientPort setting in ZooKeeper by setting the hbase.zookeeper.property.clientPort property.
For all default values used by HBase, including ZooKeeper configuration, see Appendix A.
For starters, if there is a zoo.cfg on the classpath (meaning it can be found by the Java process), it takes precedence over all settings in hbase-site.xml—but only those starting with the hbase.zookeeper.property prefix, plus a few others.
There are some ZooKeeper client settings that are not read from zoo.cfg but must be set in hbase-site.xml.
This includes, for example, the important client session timeout value set with zookeeper.session.timeout.
To avoid any confusion during deployment, it is highly recommended that you not use a zoo.cfg file with HBase, and instead use only the hbase-site.xml file.
Especially in a fully distributed setup where you have your own ZooKeeper servers, it is not practical to copy the configuration from the ZooKeeper nodes to the HBase servers.
If you are using the hbase-site.xml approach to specify all ZooKeeper settings, you must at least set the ensemble servers with the hbase.zookeeper.quorum property.
It otherwise defaults to a single ensemble member at localhost, which is not suitable for a fully.
For the full list of ZooKeeper configurations, see ZooKeeper’s zoo.cfg.
HBase does not ship with that file, so you will need to browse the conf directory in an appropriate ZooKeeper download.
How Many ZooKeepers Should I Run? You can run a ZooKeeper ensemble that comprises one node only, but in production it is recommended that you run a ZooKeeper ensemble of three, five, or seven machines; the more members an ensemble has, the more tolerant the ensemble is of host failures.
Also, run an odd number of machines, since running an even count does not make for an extra server building consensus—you need a majority vote, and if you have three or four servers, for example, both would have a majority with three nodes.
Using an odd number allows you to have two servers fail, as opposed to only one with even numbers.
Give each ZooKeeper server around 1 GB of RAM, and if possible, its own dedicated disk (a dedicated disk is the best thing you can do to ensure a performant ZooKeeper ensemble)
For very heavily loaded clusters, run ZooKeeper servers on separate machines from RegionServers, DataNodes, and TaskTrackers.
You should also set hbase.zookeeper.property.dataDir to something other than the default, as the default has ZooKeeper persist data under /tmp, which is often cleared on system restart.
In the following example, we have ZooKeeper persist to /var/zookeeper:
To point HBase at an existing ZooKeeper cluster, one that is not managed by HBase, set HBASE_MANAGES_ZK in conf/hbase-env.sh to false:
Tell HBase whether it should manage it's own instance of Zookeeper or not.
Next, set the ensemble locations and client port, if nonstandard, in hbase-site.xml, or add a suitably configured zoo.cfg to HBase’s CLASSPATH.
HBase will prefer the configuration found in zoo.cfg over any settings in hbase-site.xml.
When HBase manages ZooKeeper, it will start/stop the ZooKeeper servers as a part of the regular start/stop scripts.
If you would like to run ZooKeeper yourself, independent of HBase start/stop, do the following:
Note that you can use HBase in this manner to spin up a ZooKeeper cluster, unrelated to HBase.
Just make sure to set HBASE_MANAGES_ZK to false if you want it to stay up across HBase restarts so that when HBase shuts down, it doesn’t take ZooKeeper down with it.
For more information about running a distinct ZooKeeper cluster, see the ZooKeeper Getting Started Guide.
Additionally, see the ZooKeeper wiki, or the ZooKeeper documentation for more information on ZooKeeper sizing.
Configuration Now that the basics are out of the way (we’ve looked at all the choices when it comes to selecting the filesystem, discussed the run modes, and fine-tuned the operating system parameters), we can look at how to configure HBase itself.
Similar to Hadoop, all configuration parameters are stored in files located in the conf directory.
These are simple text files either in XML format arranged as a set of properties, or in simple flat files listing one option per line.
You also need to add configuration properties to an XML file* named conf/hbase-site.xml to, for example, override HBase defaults, tell HBase what filesystem to use, and tell HBase the location of the ZooKeeper ensemble.
When running in distributed mode, after you make an edit to an HBase configuration file, make sure you copy the content of the conf directory to all nodes of the cluster.
Check your file using a tool like xmlint, or something similar, to ensure well-formedness of your document after an edit session.
There are many ways to synchronize your configuration files across your cluster.
For the list of configurable properties, see Appendix A, or view the raw hbasedefault.xml source file in the HBase source code at src/main/resources.
The doc directory also has a static HTML page that lists the configuration options.
Configurations that users would rarely change can exist only in code; the only way to turn up such configurations is to read the source code itself.
The servers always read the hbase-default.xml file first and subsequently merge it with the hbase-site.xml file content—if present.
The properties set in hbase-site.xml always take precedence over the default values loaded from hbase-default.xml.
Any modifications in your site file require a cluster restart for HBase to notice the changes.
Add a copy of hdfs-site.xml (or hadoop-site.xml) or, better, symbolic links, under ${HBASE_HOME}/conf.
An example of such an HDFS client property is dfs.replication.
When you add Hadoop configuration files to HBase, they will always take the lowest priority.
In other words, the properties contained in any of the HBase-related configuration files, that is, the default and site files, take precedence over any Hadoop configuration file containing a property with the same name.
This allows you to override Hadoop properties in your HBase configuration file.
Examples include options to pass to the JVM when an HBase daemon starts, such as Java heap size and garbage collector configurations.
You also set options for HBase configuration, log directories, niceness, SSH options, where to locate process pid files, and so on.
Add your own environment variables here if you want them read when an HBase daemon is started.
Changes here will require a cluster restart for HBase to notice the change.†
It is a flat text file that has one hostname per line.
The list is used by the HBase maintenance script to be able to iterate over all the servers to start the region server process.
If you used previous versions of HBase, you may miss the masters file, available in the 0.20.x line.
It has been removed as it is no longer needed.
The list of masters is now dynamically maintained in ZooKeeper and each master registers itself when started.
Changes here will require a cluster restart for HBase to notice the change, though log levels can be changed for particular daemons via the HBase UI.
Example Configuration Here is an example configuration for a distributed 10-node cluster.
The HBase Master and the HDFS name node are running on the node master.foo.com.
As of this writing, you have to restart the server.
However, work is being done to enable online schema and configuration changes, so this will change over time.
The hbase-site.xml file contains the essential configuration properties, defining the HBase cluster setup.
In this file, you list the nodes that will run region servers.
In our example, we run region servers on all but the head node master.foo.com, which is carrying the HBase Master and the HDFS name node.
Here are the lines that were changed from the default in the supplied hbase-env.sh file.
Once you have edited the configuration files, you need to distribute them across all servers in the cluster.
One option to copy the content of the conf directory to all servers.
For that reason, clients require the ZooKeeper quorum information in an hbase-site.xml file that is on their Java CLASSPATH.
You can also set the hbase.zookeeper.quorum configuration key in your code.
Doing so would lead to clients that need no external configuration files.
If you are configuring an IDE to run an HBase client, you could include the conf/ directory on your classpath.
That would make the configuration files discoverable by the client code.
Minimally, a Java client needs the following JAR files specified in its CLASSPATH, when connecting to HBase: hbase, hadoop-core, zookeeper, log4j, commons-logging, and commons-lang.
All of these JAR files come with HBase and are usually postfixed with the a version number of the required release.
Ideally, you use the supplied JARs and do not acquire them somewhere else because even minor release changes could cause problems when running the client against a remote HBase cluster.
A basic example hbase-site.xml file for client applications might contain the following properties:
Deployment After you have configured HBase, the next thing you need to do is to think about deploying it on your cluster.
There are many ways to do that, and since Hadoop and HBase are written in Java, there are only a few necessary requirements to look out for.
You can simply copy all the files from server to server, since they usually share the same configuration.
Script-Based Using a script-based approach seems archaic compared to the more advanced approaches listed shortly.
But they serve their purpose and do a good job for small to even medium-size clusters.
It is not so much the size of the cluster but the number of people maintaining it.
In a larger operations group, you want to have repeatable deployment procedures, and not deal with someone having to run scripts to update the cluster.
The scripts make use of the fact that the regionservers configuration file has a list of all servers in the cluster.
Example 2-2 shows a very simple script that could be used to copy a new release of HBase from the master node to all slave nodes.
Another simple script is shown in Example 2-3; it can be used to copy the configuration files of HBase from the master node to all slave nodes.
It assumes you are editing the configuration files on the master in such a way that the master can be copied across to all region servers.
The second script uses rsync just like the first script, but adds the --delete option to make sure the region servers do not have any older files remaining but have an exact copy of what is on the originating server.
There are obviously many ways to do this, and the preceding examples are simply for your perusal and to get you started.
Ask your administrator to help you set up mechanisms to synchronize the configuration files appropriately.
Many beginners in HBase have run into a problem that was ultimately caused by inconsistent configurations among the cluster nodes.
Also, do not forget to restart the servers when making changes.
Apache Whirr Recently, we have seen an increase in the number of users who want to run their cluster in dynamic environments, such as the public cloud offerings by Amazon’s EC2, or Rackspace Cloud Servers, as well as in private server farms, using open source tools like Eucalyptus.
The advantage is to be able to quickly provision servers and run analytical workloads and, once the result has been retrieved, to simply shut down the entire cluster, or reuse the servers for other dynamic loads.
Since it is not trivial to program against each of the APIs providing dynamic cluster infrastructures, it would be useful to abstract the provisioning part and, once the cluster is operational, simply launch the MapReduce jobs the same way you would on a local, static cluster.
One of those is HBase, giving you the ability to quickly deploy a fully operational HBase cluster on dynamic setups.
Please note that Whirr is still part of the incubator program of the Apache Software Foundation.
Once it is accepted and promoted to a full member, its URL is going to change to a permanent place.
You can download the latest Whirr release from the aforementioned site and find preconfigured configuration files in the recipes directory.
Use it as a starting point to deploy your own dynamic clusters.
The rest is handled by Whirr using services that represent, for example, Hadoop or HBase.
Each service executes every required step on each remote server to set up the user accounts, download and install the required software packages, write out configuration files for them, and so on.
This is all highly customizable and you can add extra steps as needed.
Puppet and Chef Similar to Whirr, there are other deployment frameworks for dedicated machines.
Both work similar to Whirr in that they have a central provisioning server that stores all the configurations, combined with client software, executed on each server, which communicates with the central server to receive updates and apply them locally.
Also similar to Whirr, both have the notion of recipes, which essentially translate to scripts or commands executed on each node.§ In fact, it is quite possible to replace the scripting employed by Whirr with a Puppet- or Chef-based process.
While Whirr solely handles the bootstrapping, Puppet and Chef have further support for changing running clusters.
Their master process monitors the configuration repository and, upon updates, triggers the appropriate remote action.
This can be used to reconfigure clusters on-the-fly or push out new releases, do rolling restarts, and so on.
It can be summarized as configuration management, rather than just provisioning.
You heard it before: select an approach you like and maybe even are familiar with already.
In the end, they achieve the same goal: installing everything you need on your cluster nodes.
Some of the available recipe packages are an adaption of early EC2 scripts, used to deploy HBase to dynamic, cloud-based server.
For Chef, you can find HBase-related examples at http://cookbooks.opscode.com/ cookbooks/hbase.
For Puppet, please refer to http://hstack.org/hstack-automated-deployment-using-puppet/ and the repository with the recipes at http://github.com/hstack/puppet.
Operating a Cluster Now that you have set up the servers, configured the operating system and filesystem, and edited the configuration files, you are ready to start your HBase cluster for the first time.
Start and stop the Hadoop HDFS daemons by running bin/start-dfs.sh over in the HADOOP_HOME directory.
You can ensure that it started properly by testing the put and get of files into the Hadoop filesystem.
You only need to start them for actual MapReduce jobs, something we will look into in detail in Chapter 7
If you are managing your own ZooKeeper, start it and confirm that it is running: otherwise, HBase will start up ZooKeeper for you as part of its start process.
The HBase logfiles can be found in the logs subdirectory.
Web-based UI Introduction HBase also starts a web-based user interface (UI) listing vital attributes.
If the master is running on a host named master.foo.com on the default port, to see the master’s home page you can point your browser at http://master.foo.com: 60010
Figure 2-2 is an example of how the resultant page should look.
From this page you can access a variety of status information about your HBase cluster.
The top part has the attributes pertaining to the cluster setup.
You can see the currently running tasks—if there are any.
The catalog and user tables list details about the available tables.
For the user table you also see the table schema.
The lower part of the page has the region servers table, giving you access to all the currently registered servers.
Finally, the region in transition list informs you about regions that are currently being maintained by the system.
After you have started the cluster, you should verify that all the region servers have registered themselves with the master and appear in the appropriate table with the expected hostnames (that a client can connect to)
Also verify that you are indeed running the correct version of HBase and Hadoop.
You saw how to create a table, add and retrieve data, and eventually drop the table.
Anything you can do in IRB, you should be able to do in the HBase Shell.
Type help and then press Return to see a listing of shell commands and options.
Browse at least the paragraphs at the end of the help text for the gist of how variables and command arguments are entered into the HBase Shell; in particular, note how table names, rows, and columns, must be quoted.
Since the shell is JRuby-based, you can mix Ruby with HBase commands, which enables you to do things like this:
The second command uses a Ruby loop to create rows with columns in the newly created tables.
It creates row keys starting with row-aa, row-ab, all the way to row-zz.
Stopping the Cluster To stop HBase, enter the following command.
Once you have started the script, you will see a message stating that the cluster is being stopped, followed by “.” (period) characters printed in regular intervals (just to indicate that the process is still running, not to give you any percentage feedback, or some other hidden meaning):
It can take longer if your cluster is composed of many machines.
If you are running a distributed operation, be sure to wait until HBase has shut down completely before stopping the Hadoop daemons.
It also has information on how to analyze and fix problems when the cluster does not start, or shut down.
This chapter will discuss the client APIs provided by HBase.
As noted earlier, HBase is written in Java and so is its native API.
This does not mean, though, that you must use Java to access HBase.
In fact, Chapter 6 will show how you can use other programming languages.
General Notes The primary client interface to HBase is the HTable class in the org.apache.hadoop.
It provides the user with all the functionality needed to store and retrieve data from HBase as well as delete obsolete values and so on.
Before looking at the various methods this class provides, let us address some general aspects of its usage.
All operations that mutate data are guaranteed to be atomic on a per-row basis.
This affects all other concurrent readers and writers of that same row.
Suffice it to say for now that during normal operations and load, a reading client will not be affected by another updating a particular row since their contention is nearly negligible.
There is, however, an issue with many clients trying to update the same row at the same time.
Try to batch updates together to reduce the number of separate operations on the same row as much as possible.
It also does not matter how many columns are written for the particular row; all of them are covered by this guarantee of atomicity.
The region servers use a multiversion concurrency control mechanism, implemented internally by the ReadWriteConsistencyControl (RWCC) class, to guarantee that readers can read without having to wait for writers.
Writers do need to wait for other writers to complete, though, before they can continue.
Here is a summary of the points we just discussed: • Create HTable instances only once, usually when your application.
Create a separate HTable instance for every thread you execute (or.
HBase has a set of those and we will look into each of them subsequently.
They are provided by the HTable class, and the remainder of this chapter will refer directly to the methods without specifically mentioning the containing class again.
Most of the following operations are often seemingly self-explanatory, but the subtle details warrant a close look.
However, this means you will start to see a pattern of repeating functionality so that we do not have to explain them again and again.
The examples you will see in partial source code can be found in full detail in the publicly available GitHub repository at https://github.com/ larsgeorge/hbase-book.
Initially you will see the import statements, but they will be subsequently omitted for the sake of brevity.
Also, specific parts of the code are not listed if they do not immediately help with the topic explained.
Put Method This group of operations can be split into separate types: those that work on single rows and those that work on lists of rows.
Along the way, you will also be introduced to accompanying client API features.
The very first method you may want to know about is one that lets you store data in HBase.
It expects one or a list of Put objects that, in turn, are created with one of these constructors:
You need to supply a row to create a Put instance.
For now, we assume this can be anything, and often it represents a fact from the physical world—for example, a username or an order ID.
These can be simple numbers but also UUIDs† and so on.
HBase is kind enough to provide us with a helper class that has many static methods to convert Java types into byte[] arrays.
Example 3-1 provides a short list of what it offers.
Once you have created the Put instance you can add data to it.
Note that if you do not specify the timestamp with the.
The variant that takes an existing KeyValue instance is for advanced users that have learned how to retrieve, or create, this internal class.
It represents a single, unique cell; like a coordination system used with maps it is addressed by the row key, column family, column qualifier, and timestamp, pointing to one value in a three-dimensional, cube-like system—where time is the third dimension.
One way to come across the internal KeyValue type is by using the reverse methods to.
These two calls retrieve what you have added earlier, while having converted the unique cells into KeyValue instances.
You can retrieve all cells for either an entire column family, which you can then iterate over to check the details contained in each available Key Value.
It is the lowest-level class in HBase with respect to the storage architecture.
Instead of having to iterate to check for the existence of specific cells, you can use the following set of methods:
They increasingly ask for more specific details and return true if a match can be found.
The first method simply checks for the presence of a column.
The others add the option to check for a timestamp, a given value, or both.
There are more methods provided by the Put class, summarized in Table 3-1
Note that the getters listed in Table 3-1 for the Put class only retrieve what you have set beforehand.
They are rarely used, and make sense only when you, for example, prepare a Put instance in a private method in your code, and inspect the values in another place.
Quick overview of additional methods provided by the Put class Method Description.
Example 3-2 shows how all this is put together (no pun intended) into a basic application.
The examples in this chapter use a very limited, but exact, set of data.
When you look at the full source code you will notice that it uses an internal class named HBaseHelper.
It is used to create a test table with a very specific number of rows and columns.
This makes it much easier to compare the before and after.
Feel free to run the code as-is against a standalone HBase instance on your local machine for testing—or against a fully deployed cluster.
Building the Examples” on page xxi explains how to compile the examples.
Also, be adventurous and modify them to get a good feel for the functionality they demonstrate.
The example code usually first removes all data from a previous execution by dropping the table it has created.
If you run the examples against a production cluster, please make sure that you have no name collisions.
Add a column, whose name is “colfam1:qual1”, to the Put.
Add another column, whose name is “colfam1:qual2”, to the Put.
Store the row with the column into the HBase table.
This is a (nearly) full representation of the code used and every line is explained.
The following examples will omit more and more of the boilerplate code so that you can focus on the important parts.
They need access to the hbase-site.xml file to learn where the cluster resides—or you need to specify this location in your code.
Either way, you need to use an HBaseConfiguration class within your code to handle the configuration properties.
This is done using one of the following static methods, provided by that class:
If you specify an existing configuration, using create(Configuration that), it will take the highest precedence over the configuration files loaded from the classpath.
The HBaseConfiguration class actually extends the Hadoop Configuration class, but is still compatible with it: you could hand in a Hadoop configuration instance and it would be merged just fine.
After you have retrieved an HBaseConfiguration instance, you will have a merged configuration composed of the default values and anything that was overridden in the hbase-site.xml configuration file—and optionally the existing configuration you have handed in.
You are then free to modify this configuration in any way you like, before you use it with your HTable instances.
For example, you could override the ZooKeeper quorum address, to point to a different cluster:
In other words, you could simply omit any external, client-side configuration file by setting the quorum property in code.
That way, you create a client that needs no extra configuration.
Another optional parameter while creating a Put instance is called ts, or timestamp.
It allows you to store a value at a particular version in the HBase table.
Versioning of Data A special feature of HBase is the possibility to store multiple versions of each cell (the value of a particular column)
This is achieved by using timestamps for each of the versions and storing them in descending order.
Each timestamp is a long integer value measured in milliseconds.
Most operating systems provide a timer that can be read from programming languages.
When you put a value into HBase, you have the choice of either explicitly providing a timestamp or omitting that value, which in turn is then filled in by the RegionServer when the put operation is performed.
Clients might be outside your control, and therefore have a different time, possibly different by hours or sometimes even years.
As long as you do not specify the time in the client API calls, the server time will prevail.
But once you allow or have to deal with explicit timestamps, you need to make sure you are not in for unpleasant surprises.
Clients could insert values at unexpected timestamps and cause seemingly unordered version histories.
While most applications never worry about versioning and rely on the built-in handling of the timestamps by HBase, you should be aware of a few peculiarities when using them explicitly.
Here is a larger example of inserting multiple versions of a cell and how to retrieve them:
The example creates a table named test with one column family named cf1
Then a scan operation is used to see the full content of the table.
By default, it keeps three versions of a value and you can use this fact to slightly modify the scan operation to get all available values (i.e., versions) instead.
The last call in the example lists both versions you have saved.
Note how the row key stays the same in the output; you get all cells as separate lines in the shell’s output.
For both operations, scan and get, you only get the latest (also referred to as the newest) version, because HBase saves versions in time descending order and is set to return only one version by default.
Set it to the aforementioned Integer.MAX_VALUE and you get all available versions.
The term maximum versions stems from the fact that you may have fewer versions in a particular cell.
Another option to retrieve more versions is to use the time range parameter these calls expose.
They let you specify a start and end time and will retrieve all versions matching the time range.
When you do not specify that parameter, it is implicitly set to the current time of the RegionServer responsible for the given row at the moment it is added to the underlying storage.
The constructors of the Put class have another optional parameter, called rowLock.
Suffice it to say for now that you can create your own RowLock instance that can be used to prevent other clients from accessing specific rows while you are modifying it repeatedly.
From your code you may have to deal with KeyValue instances directly.
As you may recall from our discussion earlier in this book, these instances contain the data as well as the coordinates of one specific cell.
The coordinates are the row key, name of the column family, column qualifier, and timestamp.
The class provides a plethora of constructors that allow you to combine all of these in many variations.
Be advised that the KeyValue class, and its accompanying comparators, are designed for internal use.
They are available in a few places in the client API to give you access to the raw data so that extra copy operations can be avoided.
They also allow byte-level comparisons, rather than having to rely on a slower, class-level comparison.
The data as well as the coordinates are stored as a Java byte[], that is, as a byte array.
The design behind this type of low-level storage is to allow for arbitrary data, but also.
This is also the reason that there is an offset and length parameter for each byte array paremeter.
They allow you to pass in existing byte arrays while doing very fast byte-level operations.
For every member of the coordinates, there is a getter that can retrieve the byte arrays and their given offset and length.
This also can be accessed at the topmost level, that is, the underlying byte buffer:
They return the full byte array details backing the current KeyValue instance.
There will be few occasions where you will ever have to go that far.
But it is available and you can make use of it—if need be.
In practice, you The KeyValue class also provides a large list of internal classes implementing the Compa rator interface.
They can be used in your own code to do the same comparisons as done inside HBase.
This is useful when retrieving KeyValue instances using the API and further sorting or processing them in order.
KVComparator Wraps the raw KeyComparator, providing the same functionality based on two given Key Value instances.
MetaComparator Special version of the KVComparator class for the entries in the .META.
RootKeyComparator Compares two keys of -ROOT- entries in their raw, byte array format.
RootComparator Special version of the KVComparator class for the entries in the -ROOT- catalog table.
The KeyValue class exports most of these comparators as a static instance for each class.
For example, there is a public field named KEY_COMPARATOR, giving access to a KeyCompa rator instance.
The COMPARATOR field is pointing to an instance of the more frequently used KVComparator class.
So instead of creating your own instances, you could use a provided one—for example, when creating a set holding KeyValue instances that should be sorted in the same order that HBase is using internally:
There is one more field per KeyValue instance that is representing an additional dimension for its unique coordinates: the type.
Delete This instance of KeyValue represents a Delete operation, also known as a tombstone marker.
DeleteColumn This is the same as Delete, but more broadly deletes an entire column.
DeleteFamily This is the same as Delete, but more broadly deletes an entire column family, including all contained columns.
You can see the type of an existing KeyValue instance by, for example, using another provided call:
This prints out the meta information of the current KeyValue instance, and has the following format:
This is used by some of the example code for this book to check if data has been set or retrieved, and what the meta information is.
The class has many more convenience methods that allow you to compare parts of the stored data, as well as check what type it is, get its computed heap size, clone or copy it, and more.
There are static methods to create special instances of KeyValue that can be used for comparisons, or when manipulating data on that low of a level within HBase.
See the API documentation for the KeyValue class for a complete description.
Each put operation is effectively an RPC‖ that is transferring data from the client to the server and back.
This is OK for a low number of operations, but not for applications that need to store thousands of values per second into a table.
The importance of reducing the number of separate RPC calls is tied to the round-trip time, which is the time it takes for a client to send a request and the server to send a response over the network.
This does not include the time required for the data transfer.
It simply is the overhead of sending packages over the wire.
The other important factor is the message size: if you send large requests over the network, you already need a much lower number of roundtrips, as most of the time is spent transferring data.
But when doing, for example, counter increments, which are small in size, you will see better performance when batching updates into fewer requests.
The HBase API comes with a built-in client-side write buffer that collects put operations so that they are sent in one RPC call to the server(s)
The global switch to control if it is used or not is represented by the following methods:
You activate the buffer by setting autoflush to false, by invoking:
This will enable the client-side buffering mechanism, and you can check the state of set by your code.
You do not cause any RPCs to occur, though, because the Put instances you stored are kept in memory in your client process.
When you want to force the data to be written, you can call another API function:
The client is smart enough to batch these updates accordingly and send them to the appropriate region server(s)
Figure 3-1 shows how the operations are sorted and grouped before they are shipped over the network, with one single RPC per region server.
While you can force a flush of the buffer, this is usually not necessary, as the API tracks how much data you are buffering by counting the required heap size of every instance you have added.
This tracks the entire overhead of your data, also including necessary internal data structures.
Once you go over a specific limit, the client will call the flush command for you implicitly.
You can control the configured maximum allowed clientside write buffer size with these calls:
If you were to store larger data, you may want to consider increasing this value to allow your client to efficiently group together a certain number of records per RPC.
Setting this value for every HTable instance you create may seem cumbersome and can be avoided by adding a higher value to your local hbase-site.xml configuration file—for example, adding:
The buffer is only ever flushed on two occasions: Explicit flush.
Example 3-3 shows how the write buffer is controlled from the client API.
Set the auto flush to false to enable the client-side write buffer.
This example also shows a specific behavior of the buffer that you may not anticipate.
Nothing was sent to the servers yet, and therefore you cannot access it.
If you were ever required to access the write buffer content, you would table.put(put)
I mentioned earlier that it is exactly that list that makes HTable not safe for multithreaded use.
Be very careful with what you do to that list when accessing it directly.
You are bypassing the heap size checks, or you might modify it while a flush is in progress!
Since the client buffer is a simple list retained in the local process memory, you need to be careful not to run into a problem that terminates the process mid-flight.
If that were to happen, any data that has not yet been flushed will be lost! The servers will have never received that data, and therefore there will be no copy of it that can be used to recover from this situation.
Also note that a bigger buffer takes more memory—on both the client and server side since the server instantiates the passed write buffer to process it.
On the other hand, a larger buffer size reduces the number of RPCs made.
Referring to the round-trip time again, if you only store large cells, the local buffer is less useful, since the transfer is then dominated by the transfer time.
In this case, you are better advised to not increase the client buffer size.
The client API has the ability to insert single Put instances as shown earlier, but it also has the advanced feature of batching operations together.
You will have to create a list of Put instances and hand it to this call.
A quick check with the HBase Shell reveals that the rows were stored as expected.
Note that the example actually modified three columns, but in two rows only.
Since you are issuing a list of row mutations to possibly many different rows, there is a chance that not all of them will succeed.
This could be due to a few reasons—for example, when there is an issue with one of the region servers and the client-side retry mechanism needs to give up because the number of retries has exceeded the configured maximum.
If there is problem with any of the put calls on the remote servers, the error is reported back to you subsequently in the form of an IOException.
Example 3-5 uses a bogus column family name to insert a column.
Add a Put with a nonexistent family to the list.
You may wonder what happened to the other, nonfaulty puts in the list.
Using the shell again you should see that the two correct puts have been applied:
The servers iterate over all operations and try to apply them.
The failed ones are returned and the client reports the remote error using the RetriesExhausted WithDetailsException, giving you insight into how many operations have failed, with what error, and how many times it has retried to apply the erroneous modification.
It is interesting to note that, for the bogus column family, the retry is automatically set.
Those Put instances that have failed on the server side are kept in the local write buffer.
They will be retried the next time the buffer is flushed.
You can also access them using Some checks are done on the client side, though—for example, to ensure that the put has a column specified or that it is completely empty.
In that event, the client is throwing an exception that leaves the operations preceding the faulty one in the client buffer.
If it fails, for example, at the third put out of five—the first two are added to the buffer while the last two are not.
It also then does not trigger the flush command at all.
You could catch the exception and flush the write buffer manually to apply those modifications.
Add a put with no content at all to the list.
The example code this time should give you two errors, similar to: Error: java.lang.IllegalArgumentException: No columns to insert Exception in thread "main"
The first Error is the client-side check, while the second is the remote exception that now is caused by calling.
You need to watch out for a peculiarity using the list-based put call: you cannot control the order in which the puts are applied on the server side, which implies that the order in which the servers are called is also not under your control.
Use this call with caution if you have to guarantee a specific order—in the worst case, you need to create smaller batches and explicitly flush the client-side write cache to enforce that they are sent to the remote servers.
There is a special variation of the put calls that warrants its own section: check and put.
This call allows you to issue atomic, server-side mutations that are guarded by an accompanying check.
If the check passes successfully, the put operation is executed; otherwise, it aborts the operation completely.
It can be used to update data based on current, possibly related, values.
Such guarded operations are often used in systems that handle, for example, account balances, state transitions, or data processing.
The basic principle is that you read data at one point in time and process it.
Once you are ready to write back the result, you want to make sure that no other client has done the same already.
You use the atomic check to compare that the value is not modified and therefore apply your value.
This is achieved by setting the value parameter to null.
In that case, the operation would succeed when the specified column is nonexistent.
The call returns a boolean result value, indicating whether the Put has been applied or not, returning true or false, respectively.
Example 3-7 shows the interactions between the client and the server, returning the expected results.
Check if the column does not exist and perform an optional put operation.
Create another Put instance, but using a different column qualifier.
Store new data only if the previous data has been saved.
Create yet another Put instance, but using a different row.
We will not get here, as an exception is thrown beforehand!
The last call in the example will throw the following error: Exception in thread "main" org.apache.hadoop.hbase.DoNotRetryIOException: Action's getRow must match the passed row.
The compare-and-set operations provided by HBase rely on checking and modifying the same row! As with other operations only providing atomicity guarantees on single rows, this also applies for this call.
Trying to check and modify two different rows will return an exception.
Compare-and-set (CAS) operations are very powerful, especially in distributed systems, with even more decoupled client processes.
In providing these calls, HBase sets itself apart from other architectures that give no means to reason about concurrent updates performed by multiple, independent clients.
Get Method The next step in a client API is to retrieve what was just saved.
For that the HTable is providing you with the Get call and matching classes.
The operations are split into those that operate on a single row and those that retrieve multiple rows in one call.
First, the method that is used to retrieve specific values from an HBase table: Result get(Get get) throws IOException.
Each constructor takes a row parameter specifying the row you want to access, while the second constructor adds an optional rowLock parameter, allowing you to hand in your own locks.
And, similar to the put operations, you have methods to specify rather broad criteria to find what you are looking for—or to specify everything down to exact coordinates for a single cell:
Then there are methods that let you set the exact timestamp you are looking for—or a time range to match those cells that fall inside it.
Lastly, there are methods that allow you to specify how many versions you want to retrieve, given that you have not set an exact timestamp.
By default, this is set to 1, maximum number of versions you can configure in the column family descriptor, and therefore tells the API to return every available version of all matching cells (in other words, up to what is set at the column family level)
The Get class provides additional calls, which are listed in Table 3-4 for your perusal.
Quick overview of additional methods provided by the Get class.
Each HBase region server has a block cache that efficiently retains recently accessed data for subsequent reads of contiguous information.
In some events it is better to not engage the cache to avoid too much churn when doing completely random gets.
The getters listed in Table 3-4 for the Get class only retrieve what you have set beforehand.
They are rarely used, and make sense only when you, for example, prepare a Get instance in a private method in your code, and inspect the values in another place.
As mentioned earlier, HBase provides us with a helper class named Bytes that has many static methods to convert Java types into byte[] arrays.
Here is a short list of what it offers, continued from the earlier discussion:
The output is not very spectacular, but it shows that the basic operation works.
It provides you with the means to access everything that was returned from the server for the given row and matching the specified query, such as column family, column qualifier, timestamp, and so on.
If you have, for example, asked the server to return all columns of one specific column family, you can now ask for specific concrete information to be able to process the matching data on the client side.
Since columns are also sorted lexicographically on the server, this would return the value of the column with the column name (including family and qualifier) sorted first.
The created list is backed by the original array of KeyValue instances.
So it is sorted first by column family, then within each family by qualifier, then by timestamp, and finally by type.
Here you ask for multiple values of a specific column, which solves the issue pointed out earlier, that is, how to get multiple versions of a given column.
The number returned other words, the returned list contains zero (in case the column has no value for the given row) or one entry, which is the newest version of the value.
If you have specified a value greater than the default of 1 version to be returned, it could be any number, up to the specified maximum.
This may be useful when you need more than just the returned in the specified column.
Using no qualifier means that there is no label to the column.
When looking at the table from, for example, the HBase Shell, you need to know what it contains.
A rare case where you might want to consider using the empty qualifier is in column families that only ever contain a single column.
There is a third set of methods that provide access to the returned data from the get request.
Use whichever access method of Result matches your access pattern; the data has already been moved across the network from the server to your client process, so it is not incurring any other performance or resource penalties.
Dump the Contents used to convert the data of an instance into a text representation.
This is not for serialization purposes, but is most often used for debugging.
If the Result instance is empty, the output will be:
As shown in Figure 3-1, the request may actually go to more than one server, but for all intents and purposes, it looks like a single call from the client code.
Using this call is straightforward, with the same approach as seen earlier: you need to create a list that holds all instances of the Get class you have prepared.
This list is handed into the call and you will be returned an array of equal size holding the matching Result instances.
Example 3-9 brings this together, showing two different approaches to accessing the data.
Iterate over the results and check what values are available.
Both iterations return the same values, showing that you have a number of choices on how to access them, once you have received the results.
What you have not yet seen is how errors are reported back to you.
This differs from what you learned in “List of as the given list by the gets parameter, or throws an exception.
One way to have more control over how the API handles partial faults is to use the.
There are a few more calls that you can use from your code to retrieve or check your stored data.
Instead of having to retrieve the data from the remote servers, using an RPC, to verify that it actually exists, you can employ this call because it only returns a boolean flag indicating that same fact.
You only avoid shipping the data over the network—but that is very useful if you are checking very large columns, or do so very frequently.
Sometimes it might be necessary to find a specific row, or the one just before the requested row, when retrieving data.
The following call can help you find a row using these semantics:
You need to specify the row you are looking for, and a column family.
The latter is required because, in HBase, which is a column-oriented database, there is no row if there are no columns.
Specifying a family name tells the servers to check if the row searched for has any values in a column contained in the given family.
Be careful to specify an existing column family name when using the back from the server.
This is caused by the server trying to access a nonexistent storage file.
The returned instance of the Result class can be used to retrieve the found row key.
This should be either the exact row you were asking for, or the one preceding it.
If there is no match at all, the call returns null.
Example 3-11 uses the call to find the rows you created using the put examples earlier.
Returns the row that was sorted at the end of the table.
Assuming you ran Example 3-4 just before this code, you should see output similar or equal to the following:
The first call tries to find a matching row and succeeds.
The second call uses a large number postfix to find the last stored row, starting with the prefix row-
Lastly, the example tries to find row abc, which sorts before the rows the put example added, using the row- prefix, and therefore does not exist, nor matches any previous row keys.
The returned result is then null and indicates the missed lookup.
What is interesting is the loop to print out the data that was returned along with the matching row.
You can see from the preceding code that all columns of the specified column family were returned, including their latest values.
Delete Method You are now able to create, read, and update data in HBase tables.
What is left is the ability to delete from it.
And surely you may have guessed by now that the HTable provides you with a method of exactly that name, along with a matching class aptly named Delete.
Delete instance and then add details about the data you want to remove.
You need to provide the row you want to modify, and optionally provide a rowLock, an instance of RowLock to specify your own lock details, in case you want to modify the same row more than once subsequently.
Otherwise, you would be wise to narrow down what you want to remove from the given row, using one of the following methods:
You do have a choice to narrow in on what to remove using four types of calls.
You have the option to specify a timestamp that triggers more specific filtering of cell versions.
If specified, the timestamp matches the same and all older versions of all columns.
But if you do not specify either a family or a column, this call can make the difference between deleting the entire row or just all contained columns, in all column families, that match or have an older timestamp compared to the given one.
Table 3-5 shows the functionality in a matrix to make the semantics more readable.
All versions of all columns in all column families, whose timestamp is equal to or older than the given timestamp.
Only exactly the specified version of the given column, with the matching timestamp.
Versions equal to or older than the given timestamp of all columns of the given family.
The Delete class provides additional calls, which are listed in Table 3-6 for your reference.
Quick overview of additional methods provided by the Delete class.
Delete the given and all older versions in one column.
Delete the given and all older versions in the entire column family, that is, from all columns therein.
Setting the timestamp for the deletes has the effect of only matching the exact cell, that is, the matching column and value with the exact timestamp.
On the other hand, not setting the timestamp forces the server to retrieve the latest timestamp on the server side on your behalf.
This is slower than performing a delete with an explicit timestamp.
If you attempt to delete a cell with a timestamp that does not exist, nothing happens.
Another note to be made about the example is that it showcases custom versioning.
Instead of relying on timestamps, implicit or explicit ones, it uses sequential numbers, starting with 1
This is perfectly valid, although you are forced to always set the version yourself, since the servers do not know about your schema and would use epoch-based timestamps instead.
As of this writing, using custom versioning is not recommended.
It will very likely work, but is not tested very well.
Make sure you carefully evaluate your options before using this technique.
Example 3-13 shows where three different rows are affected during the operation, deleting various details they contain.
When you run this example, you will see a printout of the before and after states of the delete.
Just as with the other list-based operation, you cannot make any assumption regarding the order in which the deletes are applied on the remote servers.
The API is free to reorder them to make efficient use of the single RPC per affected region server.
If you need to enforce specific orders of how operations are applied, you would need to batch those calls into smaller groups and ensure that they contain the operations in the desired order across the batches.
In a worst-case scenario, you would need to send separate delete calls altogether.
Delete the given and all older versions in another column.
Delete the given and all older versions in the entire column family, that is, from all columns therein.
Delete the data from multiple rows in the HBase table.
For easier readability, the related details were broken up into groups using blank lines.
The deleted original data is highlighted in the Before delete call...
All three rows contain the same data, composed of two column families, three columns in each family, and two versions for each column.
The example code first deletes, from the entire row, everything up to version 4
Here you have only one matching cell, which is removed as expected in due course.
During the execution of the example code, you will see the printed KeyValue details, using something like this:
The Here, the example code inserts the column values, and therefore knows that these are short and human-readable; hence it is safe to print them out on the console as shown.
You could use the same mechanism in your own code for debugging purposes.
Please refer to the entire example code in the accompanying source code repository for this book.
You will see how the data is inserted and retrieved to generate the discussed output.
In other words, when everything has succeeded, the list will be empty.
You will have to guard the call using a try/catch, for example, and react accordingly.
Delete the data from multiple rows in the HBase table.
The output is the same as that for Example 3-13, but has some additional details printed out in the middle part:
As expected, the list contains one remaining Delete instance: the one with the bogus when printing an object—reveals the internal details of the failed delete.
The important part is the family name being the obvious reason for the failure.
You can use this technique in your own code to check why an operation has failed.
Finally, note the exception that was caught and printed out in the catch statement of the example.
It reports the number of failed actions plus how often it did retry to apply them, and on which server.
An advanced task that you will learn about in later chapters is how to verify and monitor servers so that the given server address could be useful to find the root cause of the failure.
There is an equivalent call for deletes that gives you access to server-side, read-and-modify functionality:
You need to specify the row key, column family, qualifier, and value to check before the actual delete operation is performed.
Should the test fail, nothing is deleted and the call returns a false.
If the check is successful, the delete is applied and true is returned.
Check if the column does not exist and perform an optional delete operation.
Create yet another Delete instance, but using a different row.
We will not get here, as an exception is thrown beforehand!
The entire output of the example should look like this: Before delete call...
Error: org.apache.hadoop.hbase.DoNotRetryIOException: org.apache.hadoop.hbase.DoNotRetryIOException: Action's getRow must match the passed row ...
Using null as the value parameter triggers the nonexistence test, that is, the check is successful if the column specified does not exist.
Since the example code inserts the checked column before the check is performed, the test will initially fail, returning false and aborting the delete operation.
The column is then deleted by hand and the check-and-modify call is run again.
This time the check succeeds and the delete is applied, returning true as the overall result.
Just as with the put-related CAS call, you can only perform the check-and-modify on the same row.
The example attempts to check on one row key while the supplied instance of Delete points to another.
An exception is thrown accordingly, once the check is performed.
It is allowed, though, to check across column families—for example, to have one set of columns control how the filtering is done for another set of columns.
This example cannot justify the importance of the check-and-delete operation.
In distributed systems, it is inherently difficult to perform such operations reliably, and without incurring performance penalties caused by external locking approaches, that is, where the atomicity is guaranteed by the client taking out exclusive locks on the entire row.
When the client goes away during the locked phase the server has to rely on lease recovery mechanisms ensuring that these rows are eventually unlocked again.
They also cause additional RPCs to occur, which will be slower than a single, serverside operation.
Batch Operations You have seen how you can add, retrieve, and remove data from a table using single or list-based operations.
In this section, we will look at API calls to batch different operations across multiple rows.
The following methods of the client API represent the available batch operations.
You may note the introduction of a new class type named Row, which is the ancestor, or parent class, for Put, Get, and Delete.
Using the same parent class allows for polymorphic list items, representing any of these three operations.
It is equally easy to use these calls, just like the list-based methods you saw earlier.
Example 3-16 shows how you can mix the operations and then send them off as one server call.
Be aware that you should not mix a Delete and Put operation for the same row in one batch call.
The operations will be applied in a different order that guarantees the best performance, but also causes unpredictable results.
In some cases, you may see fluctuating results due to race conditions.
You should see the following output on the console: Before batch call...
Result[3]: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family BOGUS does not exist in ...
As with the previous examples, there is some wiring behind the printed lines of code that inserts a test row before executing the batch calls.
The content is printed first, then you will see the output from the example code, and finally the dump of the rows after everything else.
The deleted column was indeed removed, and the new column was added to the row as expected.
Finding the result of the Get operation requires you to investigate the middle part of the output, that is, the lines printed by the example code.
The first operation in the example is a Put, and the result is an empty Result instance, containing no KeyValue instances.
This is the general contract of the batch calls; they return a best match result per input action, and the possible types are listed in Table 3-7
Result Returned for successful Get operations, but may also be empty when there was no matching row or column.
Throwable In case the servers return an exception for the operation it is returned to the client as-is.
You can use it to check what went wrong and maybe handle the problem automatically in your code.
Looking further through the returned result array in the console output you can see the match, returning the KeyValue instances accordingly.
Finally, the operation with the BOGUS column family has the exception for your perusal.
There are two different batch calls that look very similar.
The difference is that one needs to have the array handed into the call, while the other creates it for you.
The former function fills your given array and then throws the exception.
The code in Example 3-16 makes use of that fact and hands in the results array.
If there is a problem executing any of them, a client-side exception is thrown, reporting the issues.
Object[] batch(actions) Only returns the client-side exception; no access to partial results is possible.
All batch operations are executed before the results are checked: even if you receive an error for one of the actions, all the other ones have been applied.
In a worst-case scenario, all actions might return faults, though.
On the other hand, the batch code is aware of transient errors, such as the NotServingRegionException (indicating, for instance, that a region has been moved), and is trying to apply the action multiple times.
The hbase.client.retries.number configuration property (by default set to 10) can be adjusted to increase, or reduce, the number of retries.
Row Locks exclusively, which means in a serial fashion, for each row, to guarantee row-level atomicity.
The region servers provide a row lock feature ensuring that only a client holding the matching lock can modify a row.
In practice, though, most client applications do not provide an explicit lock, but rather rely on the mechanism in place that guards each operation separately.
Just as with RDBMSes, you can end up in a situation where two clients create a deadlock by waiting on a locked row, with the lock held by the other client.
While the locks wait to time out, these two blocked clients are holding on to a handler, which is a scarce resource.
If this happens on a heavily used row, many other clients will lock the remaining few handlers and block access to the complete server for all other clients: the server will not be able to serve any row of any region it hosts.
To reiterate: do not use row locks if you do not have to.
In fact, from the client API you cannot even retrieve this short-lived, server-side lock instance.
Instead of relying on the implicit, server-side locking to occur, clients can also acquire explicit locks and use them across multiple operations on the same row.
RowLock lockRow(byte[] row) throws IOException void unlockRow(RowLock rl) throws IOException.
Once you no longer Each unique lock, provided by the server for you, or handed in by you through the client API, guards the row it pertains to against any other lock that attempts to access the same row.
The latter case is a safeguard against faulty processes holding a lock for too long—or possibly indefinitely.
The default timeout on locks is one minute, but can be configured system-wide by adding the following property key to the hbasesite.xml file and setting the value to a different, millisecond-based timeout:
Adding the preceding code would double the timeout to 120 seconds, or two minutes, instead.
Be careful not to set this value too high, since every client trying to acquire an already locked row will have to block for up to that timeout for the lock in limbo to be recovered.
Example 3-17 shows how a user-generated lock on a row will block all concurrent readers.
Use an asynchronous thread to update the same row, but without a lock.
When you run the example code, you should see the following output on the console:
Lock ID: 4751274798057238718 Thread trying to put same row now...
You can see how the explicit lock blocks the thread using a different, implicit lock.
An interesting observation is how the puts are applied on the server side.
Notice that the timestamps of the KeyValue instances show the third put having the lowest timestamp, even though the put was seemingly applied last.
This in the main thread, after it had slept for five seconds.
But the example code has already taken out the lock on that row, and therefore the server-side processing stalls until the lock is released, five seconds and a tad more later.
In the preceding output, you can also see that it took seven milliseconds to execute the two put calls in the main thread and to unlock the row.
This is actually legacy and not used at all on the server side.
In fact, the servers do not calls, never return half-written data—for example, what is written by another thread or client.
Think of this like a small-scale transactional system: only after a mutation has been applied to the entire row can clients read the changes.
While a mutation is in progress, all reading clients will be seeing the previous state of all columns.
When you try to use an explicit row lock that you have acquired earlier but failed to use within the lease recovery time range, you will receive an error from the servers, in the form of an UnknownRowLockException.
Drop it in your code and acquire a new one to recover from this state.
Scans Now that we have discussed the basic CRUD-type operations, it is time to take a look at scans, a technique akin to cursors† in database systems, which make use of the underlying sequential, sorted storage layout HBase is providing.
Introduction all the other functions, there is also a supporting class, named Scan.
But since scans are returns the actual scanner instance you need to iterate over.
The latter two are for your convenience, implicitly creating an instance of Scan on your behalf, and subsequently calling the getScanner(Scan scan) method.
The difference between this and the Get class is immediately obvious: instead of specifying a single row key, you now can optionally provide a startRow parameter—defining the row key where the scan begins to read from the HBase table.
The optional stopRow parameter can be used to limit the scan to a specific row key where it should conclude the reading.
The start row is always inclusive, while the end row is exclusive.
This is often expressed as [startRow, stopRow) in the interval notation.
A special feature that scans offer is that you do not need to have an exact match for either of these rows.
Instead, the scan will match the first row key that is equal to or.
You need to declare, open, fetch, and eventually close a database cursor.
While scans do not need the declaration step, they are otherwise used in the same way.
If no start row was specified, it will start at the beginning of the table.
It will also end its work when the current row key is equal to or greater than the optional stop row.
If no stop row was specified, the scan will run to the end of the table.
There is another optional parameter, named filter, referring to a Filter instance.
Often, though, the Scan instance is simply created using the empty constructor, as all of the optional parameters also have matching getter and setter methods that can be used instead.
Once you have created the Scan instance, you may want to add more limiting details to it—but you are also allowed to use the empty scan, which would read the entire table, including all column families and their columns.
You can narrow down the read data using various methods:
There is a lot of similar functionality compared to the Get class: you may limit the data returned by the scan in setting the column families to specific ones using.
If you only need subsets of the data, narrowing the scan’s scope is playing into the strengths of HBase, since data is stored in column families and omitting entire families from the scan results in those storage files not being read at all.
This is the power of column-oriented architecture at its best.
Scan setTimeRange(long minStamp, long maxStamp) throws IOException Scan setTimeStamp(long timestamp) Scan setMaxVersions(int maxVersions)
A further limiting detail you can add is to set the specific timestamp you want, using per column, or return them all.
There are a few more related methods, listed in Table 3-8
Quick overview of additional methods provided by the Scan class.
You can get the currently assigned filter using this method.
Each HBase region server has a block cache that efficiently retains recently accessed data for subsequent reads of contiguous information.
In some events it is better to not engage the cache to avoid too much churn when doing full table scans.
These methods give you access to the column families and specific columns, map is a map where the key is the family name and the value is a list of added an array of all stored families, i.e., containing only the family names (as byte[] arrays)
Once you have configured the Scan instance, you can call the HTable method, named detail in the next section.
The ResultScanner Class Scans do not ship all the matching rows in one RPC to the client, but instead do this on a row basis.
This obviously makes sense as rows could be very large and sending thousands, and most likely more, of them in one call would use up too many resources, and take a long time.
The ResultScanner converts the scan into a get-like operation, wrapping the Result instance for each row into an iterator functionality.
Scanner Leases Make sure you release a scanner instance as quickly as possible.
An open scanner holds quite a few resources on the server side, which could accumulate to a large amount of consider adding this into a try/finally construct to ensure it is called, even if there are exceptions or errors during the iterations.
The example code does not follow this advice for the sake of brevity only.
Like row locks, scanners are protected against stray clients blocking resources for too long, using the same lease-based mechanisms.
You need to set the same configuration property to modify the timeout threshold (in milliseconds):
You need to make sure that the property is set to an appropriate value that makes sense for locks and the scanner leases.
Alternatively, you can fetch a larger number of rows using the next(int nbRows) call, which returns an array of up to nbRows items, each an instance of Result, representing a unique row.
The resultant array may be shorter if there were not enough rows left.
This obviously can happen just before you reach the end of the table, or the stop row.
Example 3-18 brings together the explained functionality to scan a table, while accessing the column data stored in a row.
Add one column family only; this will suppress the retrieval of “colfam2”
Use a builder pattern to add very specific details to the Scan.
The scans performed vary from the full table scan, to one that only scans one column family, and finally to a very restrictive scan, limiting the row range, and only asking for two very specific columns.
Once again, note the actual rows that have been matched.
The lexicographical sorting of the keys makes for interesting results.
You could simply pad the numbers with zeros, which would result in a more human-readable sort order.
This is completely under your control, so choose carefully what you need.
Thus it would make sense to fetch more than one row per RPC if possible.
This is called scanner caching and is disabled by default.
You can enable it at two different levels: on the table level, to be effective for all scan instances, or at the scan level, only affecting the current scan.
You can set the tablewide scanner caching using these HTable calls:
You can also change the default value of 1 for the entire HBase setup.
You do this by adding the following configuration key to the hbasesite.xml configuration file:
This would set the scanner caching to 10 for all instances of Scan.
You can still override the value at the table and scan levels, but you would need to do so explicitly.
Every time you call getScanner(scan) thereafter, the API will assign the set value to the scan instance—unless you use the scan-level settings, which take highest precedence.
This is done with the following methods of the Scan class:
They work the same way as the table-wide settings, giving you control over how many account.
You may need to find a sweet spot between a low number of RPCs and the memory used on the client and server.
Setting the scanner caching higher will improve scanning performance most of the time, but setting it too high can have adverse effects as well: to the client, and once you exceed the maximum heap the client process has available it may terminate with an OutOfMemoryException.
When the time taken to transfer the rows to the client, or to process the data on the client, exceeds the configured scanner lease threshold, you will end up receiving a lease expired error, in the form of a Scan nerTimeoutException being thrown.
The code gets the currently configured lease period value and sleeps a little longer to trigger the lease recovery on the server side.
The console output (abbreviated for the sake of readability) should look similar to this:
The example code prints its progress and, after sleeping for the specified time, attempts to iterate over the rows the scanner should provide.
This triggers the said timeout exception, while reporting the configured values.
But that is not going to work as the value is configured on the remote region servers, not your client application.
Your value is not being sent to the servers, and therefore will have no effect.
The stack trace in the console output also shows how the ScannerTimeoutException is scanner ID that has since expired and been removed in due course.
In other words, the ID your client has memorized is now unknown to the region servers—which is the name of the exception.
So far you have learned to use client-side scanner caching to make better use of bulk transfers between your client application and the remote region’s servers.
There is an issue, though, that was mentioned in passing earlier: very large rows.
HBase and its client API have an answer for that: batching.
As opposed to caching, which operates on a row level, batching works on the column level instead.
It controls how many columns are retrieved for every call to any of the to use setBatch(5) would return five columns per Result instance.
When a row contains more columns than the value you used for the batch, you will get the entire row piece by piece, with each next Result returned by the scanner.
The last Result may include fewer columns, when the total number of columns in that row is not divisible by whatever batch it is set to.
The combination of scanner caching and batch size can be used to control the number of RPCs required to scan the row key range selected.
Example 3-20 uses the two parameters to fine-tune the size of each Result instance in relation to the number of requests needed.
Using caching and batch parameters for scans counters[0]++; return false;
The code prints out the values used for caching and batching, the number of results returned by the servers, and how many RPCs were needed to get them.
You can tweak the two numbers to see how they affect the outcome.
The batch is too large for each row, so all 20 columns are batched.
Caching brings the number of RPCs down to two (plus the check)
This is the same as above, but this time the batch matches the columns available.
This divides the table into smaller Result instances, but larger caching also means only two RPCs are needed.
To compute the number of RPCs required for a scan, you need to first multiply the number of rows with the number of columns per row (at least some approximation)
Then you divide that number by the smaller value of either the batch size or the columns per row.
In addition, RPCs are also required to open and close the scanner.
You would need to add these two calls to get the overall total of remote calls when dealing with scanners.
Figure 3-2 shows how the caching and batching works in tandem.
It has a table with nine rows, each containing a number of columns.
Using a scanner caching of six, and a batch set to three, you can see that three RPCs are necessary to ship the data across the network (the dashed, rounded-corner boxes)
The scanner caching and batching controlling the number of RPCs.
When the batch size is not specified but scanner caching is specified, the result of the call will contain complete rows, because each row will be contained in one Result instance.
Only when you start to use the batch mode are you getting access to the intra-row scanning functionality.
You may not have to worry about the consequences of using scanner caching and batch mode initially, but once you try to squeeze the optimal performance out of your setup, you should keep all of this in mind and find the sweet spot for both values.
Miscellaneous Features Before looking into more involved features that clients can use, let us first wrap up a handful of miscellaneous features and functionality provided by HBase and its client API.
The HTable Utility Methods The client API is represented by an instance of the HTable class and gives you access to an existing HBase table.
Apart from the major features we already discussed, there are a few more notable methods of this class that you should be aware of:
This is a convenience method to retrieve the table name.
This allows you to access the configuration in use by the HTable instance.
Since this is handed out by reference, you can make changes that are effective immediately.
It checks if the table in question is marked as enabled in ZooKeeper.
These calls give you access to the current physical layout of the table—this is likely to change when you are adding more data to it.
The calls give you the start and/or end keys of all the regions of the table.
This set of methods lets you retrieve more details regarding where a row lives, that is, in what region, and the entire map of the region information.
You can also clear out the cache if you wish to do so.
These calls are only for advanced users that wish to make use of this information to, for example, route traffic or perform work close to where the data resides.
Again, this is a group of methods for advanced usage.
Using these calls, you can either warm up the region cache gain access to the list, and then process it—or switch on region prefetching for the entire table.
The Bytes Class You saw how this class was used to convert native Java types, such as String, or long, into the raw, byte array format HBase supports natively.
There are a few more notes that are worth mentioning about the class and its functionality.
You hand in just a byte array, or an array and an offset, or an array, an offset, and a length value.
The usage depends on the originating byte array you have.
The API, and HBase internally, store data in larger arrays, though, using, for example, the following call:
This call allows you to write the long value into a given byte array, at a specific offset.
If you want to access the data in that larger byte array you can make use of the latter The Bytes class has support to convert from and to the following native Java types: String, boolean, short, int, long, double, and float.
Apart from that, there are some noteworthy methods, which are listed in Table 3-10
Overview of additional methods provided by the Bytes class Method Description.
Whenever you are not sure what a byte array contains you should use this method to print its content, for example, to the console, or into a logfile.
There is some overlap of the Bytes class to the Java-provided ByteBuffer.
The difference is that the former does all operations without creating new class instances.
In a way it is an optimization, because the provided methods are called many times within HBase, while avoiding possibly costly garbage collection issues.
For the full documentation, please consult the JavaDoc-based API documentation.‡
Now that you understand the basic client API, we will discuss the advanced features that HBase offers to clients.
Filters HBase filters are a powerful feature that can greatly enhance your effectiveness when working with data stored in tables.
You will find predefined filters, already provided by HBase for your use, as well as a framework you can use to implement your own.
Introduction to Filters either direct access to data or the use of a start and end key, respectively.
You can limit the data retrieved by progressively adding more limiting selectors to the query.
These include column families, column qualifiers, timestamps or ranges, as well as version number.
While this gives you control over what is included, it is missing more fine-grained features, such as selection of keys, or values, based on regular expressions.
Both classes support filters for exactly these reasons: what cannot be solved with the provided API functionality to filter row or column keys, or values, can be achieved with filters.
The base interface is aptly named Filter, and there is a list of concrete classes supplied by HBase that you can use without doing any programming.
You can, on the other hand, extend the Filter classes to implement your own requirements.
All the filters are actually applied on the server side, also called predicate pushdown.
This ensures the most efficient selection of the data that needs to be transported back to the client.
You could implement most of the filter functionality in your client code as well, but you would have to transfer much more data—something you need to avoid at scale.
Figure 4-1 shows how the filters are configured on the client, then serialized over the network, and then applied on the server.
The filters created on the client side, sent through the RPC, and executed on the server side.
The lowest level in the filter hierarchy is the Filter interface, and the abstract Filter Base class that implements an empty shell, or skeleton, that is used by the actual filter classes to avoid having the same boilerplate code in each of them.
Most concrete filter classes are direct descendants of FilterBase, but a few use another, intermediate ancestor class.
They all work the same way: you define a new instance of the filter you want to apply and hand it to the Get or Scan instances, using:
While you initialize the filter instance itself, you often have to supply parameters for whatever the filter is designed for.
There is a special subset of filters, based on CompareFilter, that ask you for at least two specific parameters, since they are used by the base class to perform its task.
You will learn about the two parameter types next so that you can use them in context.
Filters have access to the entire row they are applied to.
This means that they can decide the fate of a row based on any available information.
This includes the row key, column qualifiers, actual value of a column, timestamps, and so on.
When referring to values, or comparisons, as we will discuss shortly, this can be applied to any of these details.
Specific filter implementations are available that consider only one of those criteria each.
As CompareFilter-based filters add one more feature to the base FilterBase class, how the result of the comparison is interpreted.
LESS_OR_EQUAL Match values less than or equal to the provided one.
NOT_EQUAL Include everything that does not match the provided value.
GREATER_OR_EQUAL Match values that are equal to or greater than the provided one.
The comparison operators define what is included, or excluded, when the filter is applied.
This allows you to select the data that you want as either a range, subset, or exact and single match.
The second type that you need to provide to CompareFilter-related classes is a comparator, which is needed to compare various values and keys in different ways.
They are derived from WritableByteArrayComparable, which implements Writable, and Comparable.
You do not have to go into the details if you just want to use an implementation provided by HBase and listed in Table 4-2
The constructors usually take the control value, that is, the one to compare each table value against.
BinaryPrefixComparator Similar to the above, but does a lefthand, prefix-based match using.
NullComparator Does not compare against an actual value but whether a given one is null, or not null.
BitComparator Performs a bitwise comparison, providing a BitwiseOp class with AND, OR, and XOR operators.
RegexStringComparator Given a regular expression at instantiation this comparator does a pattern match on the table data.
Using them in a LESS or GREATER comparison will yield erroneous results.
Each of the comparators usually has a constructor that takes the comparison value.
In other words, you need to define a value you compare each cell against.
The string-based comparators, RegexStringComparator and Substring Comparator, are more expensive in comparison to the purely byte-based version, as they need to convert a given value into a String first.
The subsequent string or regular expression operation also adds to the overall cost.
Comparison Filters The first type of supplied filter implementations are the comparison filters.
They take the comparison operator and comparator instance as described earlier.
The constructor of each of them has the same signature, inherited from CompareFilter:
You need to supply this comparison operator and comparison class for the filters to do their work.
Next you will see the actual filters implementing a specific comparison.
Please keep in mind that the general contract of the HBase filter API means you are filtering out information—filtered data is omitted from the results returned to the client.
The filter is not specifying what you want to have, but rather what you do not want to have returned when reading data.
In contrast, all filters based on CompareFilter are doing the opposite, in that they include the matching values.
In other words, be careful when choosing the comparison operator, as it makes the difference in regard to what the server returns.
For example, instead of using LESS to skip some information, you may need to use GREATER_OR_EQUAL to include the desired data points.
This filter gives you the ability to filter data based on row keys.
Example 4-1 shows how the filter can use different comparator instances to get the desired results.
It also uses various operators to include the row keys, while omitting others.
Feel free to modify the code, changing the operators to see the possible results.
Create a filter, while specifying the comparison operator and comparator.
Another filter is created, this time using a regular expression to match the row keys.
Here is the full printout of the example on the console: Adding rows to table...
You can see how the first filter did an exact match on the row key, including all of those rows that have a key, equal to or less than the given one.
Note once again the lexicographical sorting and comparison, and how it filters the row keys.
The second filter does a regular expression match, while the third uses a substring match approach.
This filter works very similar to the RowFilter, but applies the comparison to the column families available in a row—as opposed to the row key.
Using the available combinations of operators and comparators you can filter what is included in the retrieved data on a column family level.
Create a filter, while specifying the comparison operator and comparator.
Create a filter on one column family while trying to retrieve another.
Get the same row while applying the new filter; this will return “NONE”
The input data has four column families, with two columns each, and 10 rows in total.
Example 4-3 shows how the same logic is applied on the column qualifier level.
This allows you to filter specific columns from the table.
This filter makes it possible to include only columns that have a specific value.
Combined with the RegexStringComparator, for example, this can filter using powerful expression syntax.
Here a substring match is performed and this must be combined with an EQUAL, or NOT_EQUAL, operator.
Create a filter, while specifying the comparison operator and comparator.
Print out the value to check that the filter works.
Here you have a more complex filter that does not simply filter out data based on directly available information.
It uses the timestamp of the reference column and includes all other columns that have the same timestamp.
Since it is based on CompareFilter, it also offers you to further select columns, but for this filter it does so based on their values.
Think of it as a combination of a ValueFilter and a filter selecting on a reference timestamp.
You can optionally hand in your own operator and comparator pair to enable this feature.
The class provides constructors, though, that let you omit the operator and comparator and disable the value filtering, including all columns by default, that is, performing the timestamp filter based on the reference column only.
You can see how the optional values can be handed in as well.
The dropDependentColumn parameter is giving you additional control over how the reference column is handled: it is either included or dropped by the filter, setting this parameter to false or true, respectively.
Using a filter to include only specific column families CompareFilter.CompareOp operator, WritableByteArrayComparable comparator) Filter filter;
If you try to enable the batch mode nevertheless, you will get an error:
Exception org.apache.hadoop.hbase.filter.IncompatibleFilterException: Cannot set batch on a scan using a filter that returns true for filter.hasFilterRow.
The example also proceeds slightly differently compared to the earlier filters, as it sets the version to the column number for a more reproducible result.
The implicit timestamps that the servers use as the version could result in fluctuating results as you cannot guarantee them using the exact time, down to the millisecond.
Dedicated Filters The second type of supplied filters are based directly on FilterBase and implement more specific use cases.
Many of these filters are only really applicable when performing restrictive and would result in a very harsh filter approach: include the whole row or nothing at all.
You can use this filter when you have exactly one column that decides if an entire row should be returned or not.
You need to first specify the column you want to track, and then some value to check against.
The first one is a convenience function as it simply creates a BinaryComparator instance internally on your behalf.
The second takes the same parameters we used for the CompareFilter-based classes.
Although the SingleColumnValueFilter does not inherit from the CompareFilter directly, it still uses the same parameter types.
The filter class also exposes a few auxiliary methods you can use to fine-tune its behavior:
The former controls what happens to rows that do not have the column at all.
By default, they are included in the result, but you can use setFilterIfMissing(true) to reverse that behavior, that is, all rows that do not have the reference column are dropped from the result.
You must include the column you want to filter by, in other words, the for example.
If you fail to do so, the column is considered missing and the result is either empty, or contains all rows, based on the getFilter.
Example 4-6 combines these features to select a specific set of rows only.
The SingleColumnValueFilter we just discussed is extended in this class to provide slightly different semantics: the reference column, as handed into the constructor, is omitted from the result.
In other words, you have the same features, constructors, and methods to control how this filter works.
The only difference is that you will never get the column you are checking against as part of the Result instance(s) on the client side.
Given a prefix, specified when you instantiate the filter instance, all rows that match this prefix are returned to the client.
Example 4-7 has this applied to the usual test data set.
This filter does not make much sense The scan also is actively ended when the filter encounters a row key that is larger than the prefix.
In this way, and combining this with a start row, for example, the filter is improving the overall performance of the scan as it has knowledge of when to skip the rest of the rows altogether.
When you create the instance, you specify a pageSize parameter, which controls how many rows per page should be returned.
There is a fundamental issue with filtering on physically separate servers.
Filters run on different region servers in parallel and cannot retain or communicate their current state across those boundaries.
Thus, each filter is required to scan at least up to pageCount rows before ending the scan.
This means a slight inefficiency is given for the PageFilter as more rows are reported to the client than necessary.
The final consolidation on the client obviously has visibility into all results and can reduce what is accessible through the API accordingly.
The client code would need to remember the last row that was returned, and then, when another iteration is about to start, set the start row of the scan accordingly, while retaining the same filter properties.
Because pagination is setting a strict limit on the number of rows to be returned, it is possible for the filter to early out the entire scan, once the limit is reached or exceeded.
Filters have a facility to indicate that fact and the region servers make use of this hint to stop any further processing.
Example 4-8 puts this together, showing how a client can reset the scan to a new start row on the subsequent iterations.
Because of the lexicographical sorting of the row keys by HBase and the comparison taking care of finding the row keys in order, and the fact that the start key on a scan is.
This will ensure that the last seen row key is skipped and the next, in sorting order, is found.
The zero byte is the smallest increment, and therefore is safe to use when resetting the scan boundaries.
Even if there were a row that would match the previous plus the extra zero byte, the scan would be correctly doing the next iteration—this is because the start key is inclusive.
Some applications need to access just the keys of each KeyValue, while omitting the actual data.
The KeyOnlyFilter provides this functionality by applying the filter’s ability to modify the processed columns and cells, as they pass through.
It does so by applying the KeyValue.convertToKeyOnly(boolean) call that strips out the data part.
The constructor of this filter has a boolean parameter, named lenAsVal.
The default false simply sets the value to zero length, while the opposite true sets the value to the number representing the length of the original value.
The latter may be useful to your application when quickly iterating over columns, where the keys already convey meaning and the length can be used to perform a secondary sort, for example.
Typically this is used by row counter type applications that only need to check if a row exists.
Recall that in column-oriented databases a row really is composed of columns, and if there are none, the row ceases to exist.
Another possible use case is relying on the column sorting in lexicographical order, and setting the column qualifier to an epoch value.
This would sort the column with the oldest timestamp name as the first to be retrieved.
Combined with this filter, it is possible to retrieve the oldest column from every row using a single scan.
This class makes use of another optimization feature provided by the filter framework: it indicates to the region server applying the filter that the current row is done and that it should skip to the next one.
This improves the overall performance of the scan, compared to a full table scan.
The row boundaries of a scan are inclusive for the start row, yet exclusive for the stop row.
You can overcome the stop row semantics using this filter, which includes the specified stop row.
The output on the console, when running the example code, confirms that the filter works as advertised:
When you need fine-grained control over what versions are included in the scan result, this filter provides the means.
As you have seen throughout the book so far, a version is a specific value of a column at a unique point in time, denoted with a timestamp.
When the filter is asking for a list of timestamps, it will attempt to retrieve the column versions with the matching timestamps.
Example 4-10 sets up a filter with three timestamps and adds a time range to the second scan.
Also add a time range to verify how it affects the filter.
Here is the output on the console in an abbreviated form: Adding rows to table...
The first scan, only using the filter, is outputting the column values for all three specified timestamps as expected.
The second scan only returns the timestamp that fell into the time range specified when the scan was set up.
Both time-based restrictions, the filter and the scanner time range, are doing their job and the result is a combination of both.
You can use this filter to only retrieve a specific maximum number of columns per row.
You can set the number using the constructor of the filter:
Since this filter stops the entire scan once a row has been found that matches the maximum number of columns configured, it is not useful for scan operations, and in fact,
Similar to the PageFilter, this one can be used to page through columns in a row.
It skips all columns up to the number given as offset, and then includes limit columns afterward.
Running this example should render the following output: Adding rows to table...
This example slightly changes the way the rows and columns are numbered by adding a padding to the numeric counters.
For example, the first row is padded to be row-01
This also shows how padding can be used to get a more human-readable style of sorting, for example—as known from a dictionary or telephone book.
Analog to the PrefixFilter, which worked by filtering on row key prefixes, this filter does the same for columns.
All columns that have the given prefix are then included in the result.
Finally, there is a filter that shows what is also possible using the API: including random rows into the result.
Decorating Filters While the provided filters are already very powerful, sometimes it can be useful to modify, or extend, the behavior of a filter to gain additional control over the returned data.
Some of this additional control is not dependent on the filter itself, but can be applied to any of them.
This is what the decorating filter group of classes is about.
This filter wraps a given filter and extends it to exclude an entire row, when the wrapped filter hints for a KeyValue to be skipped.
In other words, as soon as a filter indicates that a column in a row is omitted, the entire row is omitted.
Example 4-12 combines the SkipFilter with a ValueFilter to first select all columns that have no zero-valued column, and subsequently drops all other partial rows that do not have a matching value.
Using a filter to skip entire rows based on another filter’s results.
The example code should print roughly the following results when you execute it— note, though, that the values are randomized, so you should get a slightly different result for every invocation:
The first scan returns all columns that are not zero valued.
Since the value is assigned at random, there is a high probability that you will get at least one or more columns of each possible row.
Some rows will miss a column—these are the omitted zero-valued ones.
The second scan, on the other hand, wraps the first filter and forces all partial rows to be dropped.
You can see from the console output how only complete rows are emitted, that is, those with all five columns the example code creates initially.
The total Key Value count for each scan confirms the more restrictive behavior of the SkipFilter variant.
This second decorating filter type works somewhat similarly to the previous one, but aborts the entire scan once a piece of information is filtered.
Using a filter to skip entire rows based on another filter’s results.
Once you run the example code, you should get this output on the console: Adding rows to table...
The first scan used just the RowFilter to skip one out of 10 rows; the rest is returned to the client.
Adding the WhileMatchFilter for the second scan shows its behavior to stop the entire scan operation, once the wrapped filter omits a row or column.
In the example this is row-05, triggering the end of the scan.
Decorating filters implement the same Filter interface, just like any other single-purpose filter.
In doing so, they can be used as a drop-in replacement for those filters, while combining their behavior with the wrapped filter instance.
In practice, though, you may want to have more than one filter being applied to reduce the data returned to your client application.
The FilterList class implements the same Filter interface, just like any other single-purpose filter.
In doing so, it can be used as a drop-in replacement for those filters, while combining the effects of each included instance.
You can create an instance of FilterList while providing various parameters at instantiation time, using one of these constructors:
The rowFilters parameter specifies the list of filters that are assessed together, using an operator to combine their results.
The default is MUST_PASS_ALL, and can therefore be omitted from the constructor when you do not need a different one.
MUST_PASS_ALL A value is only included in the result when all filters agree to do so, i.e., no filter is omitting the value.
MUST_PASS_ONE As soon as a value was allowed to pass one of the filters, it is included in the overall result.
Adding filters, after the FilterList instance has been created, can be done with: void addFilter(Filter filter)
You can only specify one operator per FilterList, but you are free to add other Filter List instances to an existing FilterList, thus creating a hierarchy of filters, combined with the operators you need.
You can further control the execution order of the included filters by carefully choosing the List implementation you require.
For example, using ArrayList would guarantee that the filters are applied in the order they were added to the list.
The first scan filters out a lot of details, as at least one of the filters in the list excludes some information.
Only where they all let the information pass is it returned to the client.
In contrast, the second scan includes all rows and columns in the result.
This is caused by setting the FilterList operator to MUST_PASS_ONE, which includes all the information as soon as a single filter lets it pass.
And in this scenario, all values are passed by at least one of them, including everything.
Custom Filters Eventually, you may exhaust the list of supplied filter types and need to implement your own.
This can be done by either implementing the Filter interface, or extending the provided FilterBase class.
The latter provides default implementations for all methods that are members of the interface.
The interface provides a public enumeration type, named ReturnCode, that is used by next.
Instead of blindly iterating over all values, the filter has the ability to skip a value, the remainder of a column, or the rest of the entire row.
This helps tremendously in terms of improving performance while retrieving data.
The servers may still need to scan the entire row to find matching data, reduce the work required to do so.
NEXT_COL Skip the remainder of the current column, proceeding to the next.
NEXT_ROW Similar to the previous, but skips the remainder of the current row, moving to the next.
The RowFilter makes use of this return code, for example.
SEEK_NEXT_USING_HINT Some filters want to skip a variable number of values and use this return code to indicate that The ColumnPrefixFilter, for example, uses this feature.
Most of the provided methods are called at various stages in the process of retrieving a row for a client—for example, during a scan operation.
Putting them in call order, you can expect them to be executed in the following sequence:
You can use it to skip an entire row from being further processed.
The RowFilter uses it to suppress entire rows being returned to the client.
The ReturnCode indicates what should happen with the current value.
The DependentColumnFilter uses it to drop those columns that do not match the reference column.
After everything else was checked and invoked, the final inspection is performed if the number of rows to be returned for one iteration in the pagination process is reached, returning true afterward.
The default false would include the current row in the result.
This resets the filter for every new row the scan is iterating over.
It is called by the server, after a row is read, implicitly.
This applies to get and scan operations, although obviously it has no effect for the former, as gets only read a single row.
This method can be used to stop the scan, by returning true.
It is used by filters to provide the early out optimizations mentioned earlier.
If a filter returns false, the scan is continued, and the aforementioned methods are called.
Obviously, this also implies that for get operations this call is not useful.
The framework is using this flag to ensure that a given filter is compatible with the selected scan parameters.
In particular, these filter methods collide with the scanner’s batch mode: when the scanner is using batches to ship partial rows to the client, the previous methods are not called for every batch, but only at the actual end of the current row.
Figure 4-2 shows the logical flow of the filter methods for a single row.
There is a more fine-grained process to apply the filters on a column level, which is not relevant in this context.
The logical flow through the filter methods for a single row.
Example 4-15 implements a custom filter, using the methods provided by FilterBase, overriding only those methods that need to be changed.
The filter first assumes all rows should be filtered, that is, removed from the result.
Only when there is a value in any column that matches the given reference does it include the row, so that it is sent back to the client.
Reset the filter flag for each new row being tested.
When there is a matching value, let the row pass.
Always include this, since the final decision is made later.
Here the actual decision is taking place, based on the flag status.
Write the given value out so that it can be sent to the servers.
Used by the servers to establish the filter instance with the correct values.
Deployment of Custom Filters Once you have written your filter, you need to deploy it to your HBase setup.
You need to compile the class, pack it into a Java Archive (JAR) file, and make it available to the region servers.
You can use the build system of your choice to prepare the JAR file for deployment, and a configuration management system to actually provision the file to all servers.
Once you have uploaded the JAR file, you need to add it to the hbase-env.sh configuration file, for example:
This is using the JAR file created by the Maven build as supplied by the source code repository accompanying this book.
It uses an absolute, local path since testing is done on a standalone setup, in other words, with the development environment and HBase running on the same physical machine.
Note that you must restart the HBase daemons so that the changes in the configuration file are taking effect.
Once this is done you can proceed to test the new filter.
Example 4-16 uses the new custom filter to find rows with specific values in it, also using a FilterList.
Just as with the earlier examples, here is what should appear as output on the console when executing this example:
As expected, the entire row that has a column with the value matching one of the references is included in the result.
Filters Summary Table 4-5 summarizes some of the features and compatibilities related to the provided filter implementations.
Counters In addition to the functionality we already discussed, HBase offers another advanced feature: counters.
Using counters offers the potential of switching to live accounting, foregoing the delayed batch processing step completely.
Introduction to Counters In addition to the check-and-modify operations you saw earlier, HBase also has a mechanism to treat columns as counters.
Otherwise, you would have to lock a row, read the value, increment it, write it back, and eventually unlock the row for other writers to be able to access it subsequently.
This can cause a lot of contention, and in the event of a client process, crashing it could leave the row locked until the lease recovery kicks in—which could be disastrous in a heavily loaded system.
The client API provides specialized methods to do the read-and-modify operation atomically in a single client-side call.
While you can update multiple counters, you are still limited to single rows.
Updating counters in multiple rows would require separate API— the Increment instance, though this should change in the near future.
Before we discuss each type separately, you need to have a few more details regarding how counters work on the column level.
Here is an example using the shell that creates a table, increments a counter twice, and then queries the current value:
Every call to incr returns the new value of the counter.
The final check using get_coun ter shows the current value as expected.
Initializing Counters You should not initialize counters, as they are automatically assumed to be zero when you first use a new counter, that is, a column qualifier that does not yet exist.
You can read and write to a counter directly, but you must use.
The latter, in particular, can be tricky, as you need want to consider typecasting the variable or number you are using to a long explicitly, like so:
If you were to try to erroneously initialize a counter using the put method in the HBase Shell, you might be tempted to do this:
This is obviously not very readable, but it shows that a counter is simply a column, like any other.
Accessing the counter directly gives you the byte array representation, with the shell printing the separate bytes as hexadecimal values.
Using the get_counter once again shows the current value in a more human-readable format, and confirms that variable increments are possible and work as expected.
Finally, you can use the increment value of the incr call to not only increase the counter, but also retrieve the current value, and decrease it as well.
In fact, you can omit it completely and the default of 1 is assumed:
The increment value and its effect on counter increments Value Effect.
Obviously, using the shell’s incr command only allows you to increase a single counter.
You can do the same using the client API, described next.
Single Counters The first type of increment call is for single counters only: you need to specify the exact column you want to use.
Given the coordinates of a column, and the increment account, these methods only differ by the optional writeToWAL parameter—which works the same way as the Put.set Omitting writeToWAL uses the default value of true, meaning the write-ahead log is active.
Apart from that, you can use them easily, as shown in Example 4-17
Get the current value of the counter without increasing it.
Just as with the shell commands used earlier, the API calls have the same effect: they increment the counter when using a positive increment value, retrieve the current value when using zero for the increment, and eventually decrease the counter by using a negative increment value.
Multiple Counters works similarly to the CRUD-type operations discussed earlier, using the following method to do the increment:
You must create an instance of the Increment class and fill it with the appropriate details—for example, the counter coordinates.
The optional parameter rowLock specifies a custom row lock instance, allowing you to run the entire operation under your exclusive control—for example, when you want to modify the same row a few times while protecting it against updates from other writers.
While you can guard the increment operation against other writers, you currently cannot do this for readers.
In fact, there is no atomicity guarantee made for readers.
The difference here, as compared to the Put methods, is that there is no option to specify.
It therefore makes no sense to add a column family alone.
A special feature of the Increment class is the ability to take an optional time range:
Setting a time range for a set of counter increments seems odd in light of the fact that versions are handled implicitly.
The time range is actually passed on to the servers to restrict the internal get operation from retrieving the current counter values.
You can use it to expire counters, for example, to partition them by time: when you set the time range to be restrictive enough, you can mask out older counters from the internal get, making them look like they are nonexistent.
An increment would assume they are unset and start at 1 again.
The Increment class provides additional methods, which are summarized in Table 4-7
Quick overview of additional methods provided by the Increment class.
Similar to the shell example shown earlier, Example 4-18 uses various increment values to increment, retrieve, and decrement the given counters.
Call the actual increment method with the earlier counter updates and receive the results.
Use positive, negative, and zero increment values to achieve the desired counter changes.
When you run the example, the following is output on the console:
When you compare the two sets of increment results, you will notice that this works as expected.
Coprocessors Earlier we discussed how you can use filters to reduce the amount of data being sent over the network from the servers to the client.
With the coprocessor feature in HBase, you can even move part of the computation to where the data lives.
Introduction to Coprocessors Using the client API, combined with specific selector mechanisms, such as filters, or column family scoping, it is possible to limit what data is transferred to the client.
It would be good, though, to take this further and, for example, perform certain operations directly on the server side while only returning a small result set.
Think of this as a small MapReduce framework that distributes work across the entire cluster.
A coprocessor enables you to run arbitrary code directly on each region server.
More precisely, it executes the code on a per-region basis, giving you trigger-like functionality—similar to stored procedures in the RDBMS world.
From the client side, you do not have to take specific actions, as the framework handles the distributed nature transparently.
There is a set of implicit events that you can use to hook into, performing auxiliary tasks.
If this is not enough, you can also extend the RPC protocol to introduce your own set of calls, which are invoked from your client and executed on the server on your behalf.
Once they are compiled, you make these classes available to the servers in the form of a JAR file.
The region server process can instantiate these classes and execute them in the correct environment.
In contrast to the filters, though, coprocessors can be loaded dynamically as well.
This allows you to extend the functionality of a running HBase cluster.
Use cases for coprocessors are, for instance, using hooks into row mutation operations to maintain secondary indexes, or implementing some kind of referential integrity.
Filters could be enhanced to become stateful, and therefore make decisions across row SQL, could be moved to the servers to scan the data locally and only returning the single number result across the network.
The authentication, authorization, and auditing features added in HBase version 0.92 are based on coprocessors.
They are loaded at system startup and use the provided trigger-like hooks to check if a user is authenticated, and authorized to access specific values stored in tables.
The framework already provides classes, based on the coprocessor framework, which you can use to extend from when implementing your own functionality.
This type of coprocessor is comparable to triggers: callback functions (also referred to here as hooks) are executed when certain events occur.
You can handle data manipulation events with this kind of observer.
They are closely bound to the regions of a table.
MasterObserver This can be used to react to administrative or DDL-type operations.
Observers provide you with well-defined event callbacks, for every operation a cluster server may handle.
Endpoint Next to event handling there is also a need to add custom operations to a cluster.
User code can be deployed to the servers hosting the data to, for example, perform server-local computations.
Endpoints are dynamic extensions to the RPC protocol, adding callable remote procedures.
Think of them as stored procedures, as known from RDBMSes.
They may be combined with observer implementations to directly interact with the server-side state.
All of these interfaces are based on the Coprocessor interface to gain common features, but then implement their own specific functionality.
Finally, coprocessors can be chained, very similar to what the Java Servlet API does with request filters.
The following section discusses the various types available in the coprocessor framework.
The Coprocessor Class All coprocessor classes must be based on this interface.
It defines the basic contract of a coprocessor and facilitates the management by the framework itself.
The interface provides two enumerations, which are used throughout the framework: Priority and State.
The priority of a coprocessor defines in what order the coprocessors are executed: system-level instances are called before the user-level coprocessors are executed.
Within each priority level, there is also the notion of a sequence number, which keeps track of the order in which the coprocessors were loaded.
The number starts with zero, and is increased by one thereafter.
Coprocessors are managed by the framework in their own life cycle.
These two methods are called when the coprocessor class is started, and eventually when it is decommissioned.
The provided CoprocessorEnvironment instance is used to retain the state across the lifespan of the coprocessor instance.
A coprocessor instance is always contained in a provided environment.
HTableInterface getTable(byte[] tableName) Returns an HTable instance for the given table name.
This allows the coprocessor to access the actual table data.
Coprocessors should only deal with what they have been given by their environment.
There is a good reason for that, mainly to guarantee that there is no back door for malicious code to harm your data.
Note that this class adds certain safety measures to the default HTable class.
For example, coprocessors are not allowed to lock a row.
While there is currently nothing that can stop you from creating your own HTable instances inside your coprocessor code, this is likely to be checked against in the future and possibly denied.
Table 4-10 lists the life-cycle state values as provided by the coprocessor interface.
The final piece of the puzzle is the CoprocessorHost class that maintains all the coprocessor instances and their dedicated environments.
There are specific subclasses, depending on where the host is used, in other words, on the master, region server, and so on.
The trinity of Coprocessor, CoprocessorEnvironment, and CoprocessorHost forms the basis for the classes that implement the advanced functionality of HBase, depending on where they are used.
They provide the life-cycle support for the coprocessors, manage their state, and offer the environment for them to execute as expected.
In addition, these classes provide an abstraction layer that developers can use to easily build their own custom implementation.
Figure 4-3 shows how the calls from a client are flowing through the list of coprocessors.
Note how the order is the same on the incoming and outgoing sides: first are the systemlevel ones, and then the user ones in the order they were loaded.
Coprocessor Loading Coprocessors are loaded in a variety of ways.
Before we discuss the actual coprocessor types and how to implement your own, we will talk about how to deploy them so that you can try the provided examples.
You can either configure coprocessors to be loaded in a static way, or load them dynamically while the cluster is running.
The static method uses the configuration files and table schemas—and is discussed next.
Unfortunately, there is not yet an exposed API to load them dynamically.‡
Coprocessors are a fairly recent addition to HBase, and are therefore still in flux.
Check with the online documentation and issue tracking system to see what is not yet implemented, or planned to be added.
You can configure globally which coprocessors are loaded when HBase starts.
This is done by adding one, or more, of the following to the hbase-site.xml configuration file:
The order of the classes in each configuration property is important, as it defines the execution order.
All of these coprocessors are loaded with the system priority.
You should configure all globally active classes here so that they are executed first and have a chance to take authoritative actions.
The configuration file is the first to be examined as HBase starts.
Although you can define additional system-level coprocessors in other places, the ones here are executed first.
Only one of the three possible configuration keys is read by the matching CoprocessorHost implementation.
For example, the coprocessors defined in hbase.coprocessor.master.classes are loaded by the MasterCoprocessorHost class.
The coprocessors defined with hbase.coprocessor.region.classes are loaded as defaults when a region is opened for a table.
Note that you cannot specify for which table, or region, they are loaded: the default coprocessors are loaded for every table and region.
You need to keep this in mind when designing your own coprocessors.
The other option to define what coprocessors to load is the table descriptor.
As this is per table, the coprocessors defined here are only loaded for regions of that table—and only by the region servers.
In other words, you can only use this approach for regionrelated coprocessors, not for master or WAL-related ones.
Since they are loaded in the context of a table, they are more targeted compared to the configuration loaded ones, which apply to all tables.
You need to add their definition to the table descriptor using the HTableDescriptor.set the following format:
Here is an example that defines two coprocessors, one with system-level priority, the other with user-level priority:
The path-to-jar can either be a fully specified HDFS location, or any other path supported by the Hadoop FileSystem class.
The second coprocessor definition, for example, uses a local path instead.
While the JAR may contain many coprocessor classes, only one can be specified per table attribute.
Use the standard Java package name conventions to specify the class.
This is case-sensitive and must be specified exactly this way.
The parsing is quite strict, and adding leading, trailing, or spacing characters will render the entire entry invalid.
Using the $<number> postfix for the key enforces the order in which the definitions, and therefore the coprocessors, are loaded.
Although only the prefix of COPROCESSOR is checked, using the numbered postfix is the advised way to define them.
Example 4-19 shows how this can be done using the administrative API for HBase.
Get the location of the JAR file containing the coprocessor implementation.
Instantiate an administrative API to the cluster and add the table.
The final check should show you the following result when running this example against a local, standalone HBase cluster:
The coprocessor definition has been successfully applied to the table schema.
Once the table is enabled and the regions are opened, the framework will first load the configuration coprocessors and then the ones defined in the table descriptor.
The RegionObserver Class The first subclass of Coprocessor we will look into is the one used at the region level: the RegionObserver class.
You can learn from its name that it belongs to the group of observer coprocessors: they have hooks that trigger when a specific region-level operation occurs.
These operations can be divided into two groups as well: region life-cycle changes and client API calls.
The coprocessor reacting to life-cycle state changes of a region.
The observers have the opportunity to hook into the pending open, open, and pending close state changes.
For each of them there is a set of hooks that are called implicitly by the framework.
For the sake of brevity, all parameters and exceptions are omitted when referring to the observer calls.
Read the online documentation for the full specification.§ Note, though, that all calls have a special first parameter:
This special CoprocessorEnvironment wrapper gives you additional control over what should happen after the hook execution.
A region is in this state when it is about to be opened.
These methods are called just before the region is opened, and just after it was opened.
After the pending open, but just before the open state, the region server may have to apply records from the write-ahead log (WAL)
This, in turn, invokes the following methods of the observer:
Hooking into these calls gives you fine-grained control over what mutation is applied during the log replay process.
You get access to the edit record, which you can use to inspect what is being applied.
A region is considered open when it is deployed to a region server and fully operational.
At this point, all the operations discussed throughout the book can take place; for example, the region’s in-memory store could be flushed to disk, or the region could be split when it has grown too large.
This should be quite intuitive by now: the pre calls are executed before, while the post calls are executed after the respective operation.
The last group of hooks for the observers is for regions that go into the pending close state.
This occurs when the region transitions from open to closed.
Just before, and after, the region is closed the following hooks are executed:
Usually regions are closed during normal operation, when, for example, the region is moved to a different region server for load-balancing reasons.
But there also is the possibility for a region server to have gone rogue and be aborted to avoid any side effects.
When this happens, all hosted regions are also aborted, and you can see from the given parameter if that was the case.
As opposed to the life-cycle events, all client API calls are explicitly sent from a client application to the region server.
You have the opportunity to hook into these calls just before they are applied, and just thereafter.
Here is the list of the available calls: void preGet(...) / void postGet(...)
The environment instances provided to a coprocessor that is implementing the RegionObserver interface are based on the RegionCoprocessorEnvironment class—which in turn is implementing the CoprocessorEnvironment interface.
On top of the provided methods, the more specific, region-oriented subclass is adding the methods described in Table 4-12
Methods provided by the RegionCoprocessorEnvironment class, in addition to the inherited one.
In addition, your code can access the shared region server services instance, which is explained in Table 4-13
This can be used to initiate compactions from within the coprocessor.
It allows you to check on what the server currently has allocated—for example, the global memstore size.
I will not be discussing all the details on the provided functionality, and instead refer you to the Java API documentation.‖
For the callbacks provided by the RegionObserver class, there is a special context handed in as the first parameter to all calls: the ObserverContext class.
It provides access to the current environment, but also adds the crucial ability to indicate to the coprocessor framework what it should do after a callback is completed.
The context instance is the same for all coprocessors in the execution chain, but with the environment swapped out for each coprocessor.
Table 4-14 lists the methods as provided by the context class.
When the provided context is null, it will create a new instance.
Instead of having to implement your own RegionObserver, based on the interface, you can use the following base class to only implement what is needed.
This class can be used as the basis for all your observer-type coprocessors.
It has placeholders for all methods required by the RegionObserver interface.
They are all left blank, so by default nothing is done when extending this class.
You must override all the callbacks that you are interested in to add the required functionality.
Example 4-20 is an observer that handles specific row key requests.
Check if the request row key matches a well-known one.
Create a special KeyValue instance containing just the current time on the server.
The following was added to the hbase-site.xml file to enable the coprocessor:
Do not forget to restart HBase, though, to make the changes to the static configuration files active.
This requires an existing table, because trying to issue a get call to a nonexistent table will raise an error, before the actual get operation is executed.
Also, the example does not set the bypass flag, in which case something like the following could happen:
A new table is created and a row with the special row key is inserted.
You can see how the artificial column is mixed with the actual one Example 4-21
Region observer checking for special get requests and bypassing further processing.
Once the special KeyValue is inserted, all further processing is skipped.
You need to adjust the hbase-site.xml file to point to the new example:
Just as before, please restart HBase after making these adjustments.
Only the artificial column is returned, and since the default get operation is bypassed, it is the only column retrieved.
You can amend the example by adding a timestamp and see how that would be printed when using the shell (an exercise left to you)
The MasterObserver Class The second subclass of Coprocessor discussed handles all possible callbacks the master server may initiate.
The operations and API calls are explained in Chapter 5, though they can be classified as data-manipulation operations, similar to DDL used in relational database systems.
For that reason, the MasterObserver class provides the following hooks: void preCreateTable(...) / void postCreateTable(...)
Called before and after a column is added to a table.
Called before and after a column is deleted from a table.
Called before and after the flag for the balancer is changed.
There is no post hook, because after the shutdown, there is no longer a cluster to invoke the callback.
There is no post hook, because after the master has stopped, there is no longer a process to invoke the callback.
Similar to how the RegionCoprocessorEnvironment is enclosing a single RegionObserver coprocessor, the MasterCoprocessorEnvironment is wrapping MasterOb server instances.
On top of the provided methods, the more specific, master-oriented subclass adds the one method described in Table 4-15
The method provided by the MasterCoprocessorEnvironment class, in addition to the inherited one.
Your code can access the shared master services instance, the methods of which are listed and described in Table 4-16
It is responsible for all region assignment operations, such as assign, unassign, balance, and so on.
I will not be discussing all the details on the provided functionality, and instead refer you to the Java API documentation once more.#
Either you can base your efforts to implement a MasterObserver on the interface directly, or you can extend the BaseMasterObserver class instead.
It implements the interface while leaving all callback functions empty.
If you were to use this class unchanged, it would not yield any kind of reaction.
Adding functionality is achieved by overriding the appropriate event methods.
You have the choice of hooking your code into the pre and/or post calls.
Example 4-22 uses the post hook after a table was created to perform additional tasks.
Master observer that creates a separate directory on the filesystem when a table is created.
Get the available services and retrieve a reference to the actual filesystem.
Create a new directory that will store binary data from the client application.
You need to add the following to the hbase-site.xml file for the coprocessor to be loaded by the master process:
Once you have activated the coprocessor, it is listening to the said events and will trigger your code automatically.
The example is using the supplied services to create a directory.
A fictitious application, for instance, could use it to store very large binary objects (known as blobs) outside of HBase.
To trigger the event, you can use the shell like so:
The Hadoop command-line tool can be used to verify the results:
There are many things you can implement with the MasterObserver coprocessor.
Since you have access to most of the shared master resources through the MasterServices instance, you should be careful what you do, as it can potentially wreak havoc.
Finally, because the environment is wrapped in an ObserverContext, you have the same them to explicitly disable certain operations or skip subsequent coprocessor execution, respectively.
Endpoints The earlier RegionObserver example used a well-known row key to add a computed column during a get request.
It seems that this could suffice to implement other functionality as well—for example, aggregation functions that return the sum of all values in a specific column.
Unfortunately, this does not work, as the row key defines which region is handling the request, therefore only sending the computation request to a single server.
What we want, though, is a mechanism to send such a request to all regions, and therefore all region servers, so that they can build the sum of the columns they have access to locally.
Once each region has returned its partial result, we can aggregate the total on the client side much more easily.
If you were to scan the entire table using a purely client API approach, in a worst-case scenario you would transfer all 1 million numbers to build the sum.
Moving such computation to the servers where the data resides is a much better option.
HBase, though, does not know what you may need, so to overcome this limitation, the coprocessor framework provides you with a dynamic call implementation, represented by the endpoint concept.
In order to provide a custom RPC protocol to clients, a coprocessor implementation defines an interface that extends CoprocessorProtocol.
The interface can define any methods that the coprocessor wishes to expose.
Using this protocol, you can communicate with the coprocessor instances via the following calls, provided by HTable:
Since CoprocessorProtocol instances are associated with individual regions within the table, the client RPC calls must ultimately identify which regions should be used in the CoprocessorProtocol method invocations.
Though regions are seldom handled directly in client code and the region names may change over time, the coprocessor RPC calls use row keys to identify which regions should be used for the method invocations.
Clients can call CoprocessorProtocol methods against one of the following: Single region.
Range of regions in the table from the one containing the start row key to the one containing the end row key (inclusive) will be used as the RPC endpoints.
The row keys passed as parameters to the HTable methods are not passed to the CoprocessorProtocol implementations.
They are only used to identify the regions for endpoints of the remote calls.
The Batch class defines two interfaces used for CoprocessorProtocol invocations against multiple regions: clients implement Batch.Call to call methods of the actual selected region, passing the CoprocessorProtocol instance for the region as a parameter.
Clients can optionally implement Batch.Callback to be notified of the results from each region invocation as they complete.
This specifies the communication details for the new endpoint: it defines the RPC protocol between the client and the servers.
You need to provide the actual implementation of the endpoint by extending both the abstract BaseEndpointCoprocessor class and the protocol interface provided in step 1, defining your endpoint protocol.
Example 4-23 implements the CoprocessorProtocol to add custom functions to HBase.
A client can invoke these remote calls to retrieve the number of rows and KeyValues in each region where it is running.
Step 2 is to combine this new protocol interface with a class that also extends BaseEnd pointCoprocessor.
Example 4-24 uses the environment provided to access the data using an InternalScanner instance.
Note how the FirstKeyOnlyFilter is used to reduce the number of columns being scanned.
You need to add (or amend from the previous examples) the following to the hbase-site.xml file for the endpoint coprocessor to be loaded by the region server process:
Example 4-25 showcases how a client can use the provided calls of HTable to execute the deployed coprocessor endpoint functions.
Since the calls are sent to each region separately, there is a need to summarize the total number at the end.
Set start and end row keys to “null” to count all rows.
Create an anonymous class to be sent to all region servers.
Iterate over the returned map, containing the result for each region separately.
The code emits the region names, the count for each of them, and eventually the grand total:
The Batch class also offers a more convenient way to access the remote endpoint: using sent to the region servers.
Example 4-26 amends the previous example to make use of this shortcut.
However, if you want to perform additional processing on the results, implementing Batch.Call directly will provide more power and flexibility.
This can be seen in Example 4-27, which combines the row and key-value count for each region.
Extending the batch call to execute multiple endpoint calls RowCountProtocol.class, null, null,
Using the proxy call of HTable to invoke an endpoint on a single region.
With the proxy reference, you can invoke any remote function defined in your CoprocessorProtocol implementation from within client code, and it returns the result.
Coprocessor calls batched and executed in parallel, and addressing a single region only.
HTablePool Instead of creating an HTable instance for every request from your client application, it makes much more sense to create one initially and subsequently reuse them.
The primary reason for doing so is that creating an HTable instance is a fairly expensive operation that takes a few seconds to complete.
In a highly contended environment with thousands of requests per second, you would not be able to use this approach at all—creating the HTable instance would be too slow.
You need to create the instance at startup and use it for the duration of your client’s life cycle.
There is an additional issue with the HTable being reused by multiple threads within the same process.
The HTable class is not thread-safe, that is, the local write buffer is not guarded against concurrent modifications.
Instead, you should use one instance of HTable for each thread you are running in your client application.
It only serves one purpose, namely to pool client API instances to the HBase cluster.
Creating the pool is accomplished using one of these constructors:
HTablePool(Configuration config, int maxSize) HTablePool(Configuration config, int maxSize, HTableInterfaceFactory tableFactory)
Setting the maxSize parameter gives you control over how many HTable instances a pool is allowed to contain.
The optional tableFactory parameter can be used to hand in a custom factory class that creates the actual HTable instances.
The HTableInterfaceFactory Interface You can create your own factory class to, for example, prepare the HTable instances with specific settings.
Or you could use the instance to perform some initial operations, such as adding some rows, or updating counters.
If you want to implement your own HTableInterfaceFactory you need to implement two methods:
The first creates the HTable instance, while the second releases it.
Take any actions you require in these calls to prepare an instance, or clean up afterward.
The client-side write buffer, in particular, is a concern when sharing the table references.
The releaseHTa There is a default implementation of the factory class, called HTableFactory, which does exactly that: it creates HTable instances, when the create method of the factory is If you do not specify your own HTableInterfaceFactory, the default HTableFactory is created and assigned implicitly.
Using the pool is a matter of employing the following calls: HTableInterface getTable(String tableName) HTableInterface getTable(byte[] tableName) void putTable(HTableInterface table)
Both internally defer some of the work to the mentioned HTableInterfaceFactory instance the pool is configured with.
Setting the maxSize parameter during the construction of a pool does not impose an upper limit on the number of HTableInterface instances you like to get a valid table reference.
The maximum size of the pool only sets the number of HTableInter have created 10 HTable instances (assuming you use the default)
Upon More importantly, the release mechanisms of the factory are not invoked.
Finally, there are calls to close the pool for specific tables: void closeTablePool(String tableName) void closeTablePool(byte[] tableName)
Obviously, both do the same thing, with one allowing you to specify a String, and the other a byte array—use whatever is more convenient for you.
The close call of the pool iterates over the list of retained references for a specific table, invoking the release mechanism provided by the factory.
This is useful for freeing all resources for a named table, and starting all over again.
Keep in mind that for all resources to be released, you would need to call these methods for every table name you have used so far.
Example 4-29 uses these methods to create and use a pool.
Get 10 HTable references, which is more than the pool is retaining.
Five will be kept, while the additional five will be dropped.
You should receive the following output on the console: Acquiring tables...
Note that using more than the configured maximum size of the pool works as we discussed earlier: we receive more references than were configured.
Returning the tables to the pool is not yielding any logging or printout, though, doing its work behind the scenes.
Use Case: Hush All of the tables in Hush are acquired through a shared table pool.
The next code block shows how these methods are called in context.
Once the operations are concluded, the table is returned to the pool subsequently.
Connection Handling Every instance of HTable requires a connection to the remote servers.
This is internally represented by the HConnection class, and more importantly managed process-wide by the shared HConnectionManager class.
From a user perspective, there is usually no immediate need to deal with either of these two classes; instead, you simply create a new Configuration instance, and use that with your client API calls.
Internally, the connections are keyed in a map, where the key is the Configuration instance you are using.
In other words, if you create a number of HTable instances while providing the same configuration reference, they all share the same underlying HConnection instance.
There are good reasons for this to happen: Share ZooKeeper connections.
As each client eventually needs a connection to the ZooKeeper ensemble to perform the initial lookup of where user table regions are located, it makes sense to share this connection once it is established, with all subsequent client instances.
Cache common resources Every lookup performed through ZooKeeper, or the -ROOT-, or .META.
The location is then cached on the client side to reduce the amount of network traffic, and to speed up the lookup process.
Since this list is the same for every local client connecting to a remote cluster, it is equally useful to share it among multiple clients running in the same process.
This is then immediately available to all other clients sharing the same connection reference, thus further reducing the number of network round-trips initiated by a client.
Another class that benefits from the same advantages is the HTablePool: all of the pooled HTable instances automatically share the provided configuration instances, and therefore also the shared connection it references to.
This also means you should always create your own configuration, whenever you plan to instantiate more than one HTable instance.
The latter implicitly uses the connection sharing, as provided by the HBase client-side API classes.
There are no known performance implications for sharing a connection, even for heavily multithreaded applications.
The drawback of sharing a connection is the cleanup: when you do not explicitly close a connection, it is kept open until the client process exits.
This can result in many connections that remain open to ZooKeeper, especially for heavily distributed applications, such as MapReduce jobs talking to HBase.
In a worst-case scenario, you can run out of available connections, and receive an IOException instead.
You can avoid this problem by explicitly closing the shared connection, when you are call decreases an internal reference count and eventually closes all shared resources, such as the connection to the ZooKeeper ensemble, and removes the connection reference from the internal list.
There is also an explicit call to clear out a connection, or all open connections:
Since all shared connections are internally keyed by the configuration instance, you need to provide that instance to close the associated connection.
The boolean stop Proxy parameter lets you further enforce the cleanup of the entire RPC stack of the client—which is its umbilical cord to the remote servers.
Only use true when you do not need any further communication with the server to take place.
If you are ever in need of using a connection explicitly, you can make use of the get.
Use the connection to your hearts' delight and then when done...
The advantage is that you are the sole user of that connection, but you must make sure you close it out properly as well.
Apart from the client API used to deal with data manipulation features, HBase also exposes a data definition-like API.
This is similar to the separation into DDL and DML found in RDBMSes.
First we will look at the classes required to define the data schemas and subsequently see the API that makes use of it to, for example, create a new HBase table.
Schema Definition Creating a table in HBase implicitly involves the definition of a table schema, as well as the schemas for all contained column families.
Tables Everything stored in HBase is ultimately grouped into one or more tables.
The primary reason to have tables is to be able to control certain features that all columns in this table share.
The typical things you will want to define for a table are column families.
The constructor of the table descriptor in Java looks like the following:
Writable and the Parameterless Constructor You will find that most classes provided by the API and discussed throughout this chapter do possess a special constructor, one that does not take any parameters.
This is attributed to these classes implementing the Hadoop Writable interface.
It employs the Writable class to denote objects that can be sent over the network.
They are invoked by the framework to write the object’s data into the output stream, and subsequently read it back on the receiving system.
For that the framework calls taking care of noting the class name and other details on their behalf.
On the receiving server the framework reads the metadata, and will create an empty read back the field data and leave you with a fully working and initialized copy of the sending object.
Since the receiver needs to create the class using reflection, it is implied that it must have access to the matching, compiled class.
Usually that is the case, as both the servers and clients are using the same HBase Java archive file, or JAR.
It is available on both sides of the RPC communication channel, that is, the sending and receiving processes.
It has the parameterless constructor, that is, one without any parameters.
Failing to provide the special constructor will result in a runtime error.
And calling the constructor explicitly from your code is also a futile exercise, since it leaves you with an uninitialized instance that most definitely does not behave as expected.
As a client API developer, you should simply acknowledge the underlying dependency on RPC, and how it manifests itself.
As an advanced developer extending HBase, you need to implement and deploy your custom code appropriately.
You either create a table with a name or an existing descriptor.
The constructor without any parameters is only for deserialization purposes and should not be used directly.
You can specify the name of the table as a Java String or byte[], a byte array.
Many functions in the HBase Java API have these two choices.
The string version is plainly for convenience and converts the string internally into the usual byte array representation as HBase treats everything as such.
You can achieve the same using the supplied Bytes class:
There are certain restrictions on the characters you can use to create a table name.
The name is used as part of the path to the actual storage files, and therefore complies with filename rules.
The column-oriented storage format of HBase allows you to store many details into the same table, which, under relational database modeling, would be divided into many separate tables.
The usual database normalization* rules do not apply directly to HBase, and therefore the number of tables is usually very low.
Although conceptually a table is a collection of rows with columns in HBase, physically they are stored in separate partitions called regions.
Figure 5-1 shows the difference between the logical and physical layout of the stored data.
Every region is served by exactly one region server, which in turn serve the stored values directly to clients.
Table Properties The table descriptor offers getters and setters† to set other options of the table.
In practice, a lot are not used very often, but it is important to know them all, as they can be used to fine-tune the table’s performance.
The constructor already had the parameter to specify the table name.
The Java API has additional methods to access the name or change it.
In regular expression syntax, this could be expressed as [a-zAZ_0-9-.]
Column families This is the most important part of defining a table.
You need to specify the column families you want to use with the table you are creating.
You have the option of adding a family, checking if it exists based on its name, getting a list of all known families, and getting or removing a specific one.
Maximum file size This parameter is specifying the maximum size a region within the table can grow to.
The size is specified in bytes and is read and set using the following methods:
Getters and setters in Java are methods of a class that expose internal fields in a controlled manner.
Maximum file size is actually a misnomer, as it really is about the maximum size of each store, that is, all the files belonging to each column family.
If one single column family exceeds this maximum size, the region is split.
Since in practice, this involves multiple files, the better name would be maxStoreSize.
The maximum size is helping the system to split regions when they reach this configured size.
You need to determine what a good number for the size is, though.
By default, it is set to 256 MB, which is good for many use cases, but a larger value may be required when you have a lot of data.
Please note that this is more or less a desired maximum size and that, given certain conditions, this size can be exceeded and actually be completely rendered without effect.
Since a row cannot be split across regions, you end up with a region of at least 20 MB in size, and the system cannot do anything about it.
Read-only By default, all tables are writable, but it may make sense to specify the read-only option for specific tables.
If the flag is set to true, you can only read from the table and not modify it at all.
Memstore flush size We discussed the storage model earlier and identified how HBase uses an inmemory store to buffer values before writing them to disk as a new storage file in an operation called flush.
This parameter of the table controls when this is going to happen and is specified in bytes.
As you do with the aforementioned maximum file size, you need to check your requirements before setting this value to something other than the default 64 MB.
A larger size means you are generating larger store files, which is good.
On the other hand, you might run into the problem of longer blocking periods, if the region server cannot keep up with flushing the added data.
Also, it increases the time needed to replay the write-ahead log (the WAL) if the server crashes and all inmemory updates are lost.
For now, note that HBase uses one of two different approaches to save write-ahead-log entries to disk.
This is a boolean option and is, by default, set to false.
Here is how to access this parameter through the Java API:
Miscellaneous options In addition to those already mentioned, there are methods that let you set arbitrary key/value pairs:
They are stored with the table definition and can be retrieved if necessary.
You have a few choices in terms of how to specify the key and value, either as a String, or as a byte array.
Column Families We just saw how the HTableDescriptor exposes methods to add column families to a table.
Similar to this is a class called HColumnDescriptor that wraps each column family’s settings into a dedicated Java class.
In other programming languages, you may find the same concept or some other means of specifying the column family properties.
A more appropriate name would be HColumnFamilyDescriptor, which would indicate its purpose to define column family parameters as opposed to actual columns.
Column families define shared features that apply to all columns that are created within them.
The client can create an arbitrary number of columns by simply using new column qualifiers on the fly.
Columns are addressed as a combination of the column family name and the column qualifier (or sometimes also called the column key), divided by a colon:
Recall the Bytes class mentioned earlier, which you can use to convert your chosen names to byte arrays.
The reason why the family name must be printable is that because the name is used as part of the.
Figure 5-2 visualizes how the families are mapped to storage files.
The family name is added to the path and must comply with filename standards.
The advantage is that you can easily access families on the filesystem level as you have the name in a human-readable format.
You should also be aware of the empty column qualifier.
You can simply omit the qualifier and specify just the column family name.
HBase then creates a column with the special empty qualifier.
You can write and read that column like any other, but obviously there is only one of those, and you will have to name the other columns to distinguish them.
For simple applications, using no qualifier is an option, but it also carries no meaning when looking at the data—for example, using the HBase Shell.
You should get used to naming your columns and do this from the start, because you cannot simply rename them later.
When you create a column family, you can specify a variety of parameters that control all of its features.
The Java class has many constructors that allow you to specify most parameters while creating an instance.
The first one is only used internally for deserialization again.
The next two simply take a name as a String or byte[], the usual byte array we have seen many times now.
There is another one that takes an existing HColumnDescriptor and then two more that list all available parameters.
Instead of using the constructor, you can also use the getters and setters to specify the various details.
Each column family has a name, and you can use the following methods to retrieve it from an existing HColumnDescriptor instance:
The common approach to rename a family is to create a new family with the desired name and copy the data over, using the API.
You cannot set the name, but you have to use these constructors to hand it in.
Keep in mind the requirement for the name to be printable characters.
Maximum versions Per family, you can specify how many versions of each value you want to keep.
Recall the predicate deletion mentioned earlier where the housekeeping of HBase removes values that exceed the set maximum.
Getting and setting the value is done using the following API calls:
The default value is NONE—in other words, no compression is enabled when you create a column family.
Once you deal with the Java API and a column descriptor, you can use these methods to change the value:
Note how the value is not a String, but rather a Compression.Algorithm enumeration that exposes the same values as listed in Table 5-1
The constructor of HCo lumnDescriptor takes the same values as a string, though.
Another observation is that there are two sets of methods, one for the general compression setting and another for the compaction compression setting.
Block size All stored files in HBase are divided into smaller blocks that are loaded during a get or scan operation, analogous to pages in RDBMSes.
The default is set to 64 KB and can be adjusted with these methods:
After all, this is open source and a redundancy like this is often caused by legacy code being carried forward.
Please feel free to help clean this up and to contribute back to the HBase project.
The value is specified in bytes and can be used to control how much data HBase is required to read from the storage files during retrieval as well as what is cached in memory for subsequent accesses.
There is an important distinction between the column family block size, or HFile block size, and the block size specified on the HDFS level.
The storage files used by HBase are using this much more fine-grained size to efficiently load and cache data in block operations.
It is independent from the HDFS block size and only used internally.
Block cache As HBase reads entire blocks of data for efficient I/O usage, it retains these blocks in an in-memory cache so that subsequent reads do not need any disk operation.
The default of true enables the block cache for every read operation.
But if your use case only ever has sequential reads on a particular column family, it is advisable that you disable it from polluting the block cache by setting the block cacheenabled flag to false.
Here is how the API can be used to change this flag:
There are other options you can use to influence how the block cache is used, for example, during a scan operation.
This is useful during full table scans so that you do not cause a major churn on the cache.
Time-to-live HBase supports predicate deletions on the number of versions kept for each value, but also on specific times.
The time-to-live (or TTL) sets a threshold based on the timestamp of a value and the internal housekeeping is checking automatically if a value exceeds its TTL.
If that is the case, it is dropped during major compactions.
The API provides the following getter and setter to read and write the TTL:
The default value also is treated as the special case of keeping the values forever, that is, any positive value less than the default enables this feature.
In-memory We mentioned the block cache and how HBase is using it to keep entire blocks of data in memory for efficient sequential access to values.
The in-memory flag defaults to false but can be modified with these methods:
Setting it to true is not a guarantee that all blocks of a family are loaded into memory nor that they stay there.
Think of it as a promise, or elevated priority, to keep them in memory as soon as they are loaded during a normal retrieval operation, and until the pressure on the heap (the memory available to the Java-based server processes) is too high, at which time they need to be discarded by force.
In general, this setting is good for small column families with few values, such as the passwords of a user table, so that logins can be processed very fast.
Since they add overhead in terms of storage and memory, they are turned off by default.
Because there are many more columns than rows (unless you only have a single column in each row), the last option, ROWCOL, requires the largest amount of space.
It is more fine-grained, though, since it knows about each row/column combination, as opposed to just rows.
The Bloom filter can be changed and retrieved with these calls:
As with the compression value, these methods take a StoreFile.BloomType type, while the constructor for the column descriptor lets you specify the aforementioned types as a string.
The letter casing is not important, so you can, for example, use “row”
Bloom Filters” has more on the Bloom filters and how to use them best.
Replication scope Another more advanced feature coming with HBase is replication.
It enables you to have multiple clusters that ship local updates across the network so that they are applied to the remote copies.
By default, replication is disabled and the replication scope is set to 0, meaning it is disabled.
The only other supported value (as of this writing) is 1, which enables replication to a remote cluster.
Finally, the Java class has a helper method to check if a family name is valid:
Use it in your program to verify user-provided input conforming to the specifications that are required for the name.
It does not return a boolean flag, but throws an IllegalArgumentException when the name is malformed.
The fully specified constructors shown earlier use this method internally to verify the given name; in this case, you do not need to call the method beforehand.
HBaseAdmin Just as with the client API, you also have an API for administrative tasks at your disposal.
Compare this to the Data Definition Language (DDL) found in RDBMSes—while the client API is more an analog to the Data Manipulation Language (DML)
It provides operations to create tables with specific column families, check for table existence, alter table and column family definitions, drop tables, and much more.
The provided functions can be grouped into related operations; they’re discussed separately on the following pages.
Basic Operations Before you can use the administrative API, you will have to create an instance of the HBaseAdmin class.
This section omits the fact that most methods may throw either an IOException (or an exception that inherits from it), or an InterruptedException.
The former is usually a result of a communication error between your client application and the remote servers.
The latter is caused by an event that interrupts a running operation, for example, when the region server executing the command is shut down before being able to complete it.
Handing in an existing configuration instance gives enough details to the API to find the cluster using the ZooKeeper quorum, just like the client API does.
Use the administrative API instance for the operation required and discard it afterward.
In other words, you should not hold on to the instance for too long.
The HBaseAdmin instances should be short-lived as they do not, for example, handle master failover gracefully right now.
The class implements the Abortable interface, adding the following call to it: void abort(String why, Throwable e)
This method is called by the framework implicitly—for example, when there is a fatal connectivity issue and the API should be stopped.
This will return an RPC proxy instance of HMasterInterface, allowing you to communicate directly with the master server.
This is not required because the HBaseAdmin class provides a convenient wrapper to all calls exposed by the master interface.
HBaseAdmin perform additional work—for example, checking that the input parameters are valid, converting remote exceptions to client exceptions, or adding the ability to run inherently asynchronous operations as if they were synchronous.
In addition, the HBaseAdmin class also exports these basic calls:
You may use it from your client application to verify that you can communicate with the master, before instantiating the HBaseAdmin class.
Gives you access to the configuration that was used to create the current HBaseAdmin instance.
You can use it to modify the configuration for a running administrative API instance.
Table Operations After the first set of basic operations, there is a group of calls related to HBase tables.
These calls help when working with the tables themselves, not the actual schemas inside.
Before you can do anything with HBase, you need to create tables.
It holds the details of the table to be created, including the a table name.
Create a column family descriptor and add it to the table descriptor.
The code in Example 5-2 uses both possible ways to specify your own set of region boundaries.
Retrieve the start and end keys from the newly created table.
Print the key, but guarding against the empty start (and end) key.
The example uses a method of the HTable class that you saw earlier, getStartEnd.
In between the keys are either the computed, or the provided split keys.
Note how the end key of a region is also the start key of the subsequent one—just that it is exclusive for the former, and inclusive for the latter, respectively.
You must provide a start value that is less than the end value, and a numRegions that is at least 3: otherwise, the call will return with an exception.
This is to ensure that you end up with at least a minimum set of regions.
The start and end key values are subtracted and divided by the given number of regions to compute the region boundaries.
In the example, you can see how we end up with the correct number of regions, while the computed keys are filling in the range.
The createTable(HTableDescriptor desc, byte[][] splitKeys) method used in the second part of the example, on the other hand, is expecting an already set array of split keys: they form the start and end keys of the regions created.
It then proceeds to call the createTable(HTableDescriptor desc, byte[][] splitKeys), doing the actual table creation.
Finally, there is the createTableAsync(HTableDescriptor desc, byte[][] splitKeys) method that is taking the table descriptor, and region keys, to asynchronously perform.
Most of the table-related administrative API functions are asynchronous in nature, which is useful, as you can send off a command and not have to deal with waiting for a result.
For a client application, though, it is often necessary to know if a command has succeeded before moving on with other operations.
In fact, the synchronous commands are simply a wrapper around the asynchronous ones, adding a loop at the end of the call to repeatedly adding a loop that waits for the table to be created on the remote servers before yielding control back to the caller.
Once you have created a table, you can use the following helper functions to retrieve the list of tables, retrieve the descriptor for an existing table, or check if a table exists:
Example 5-3 uses both to show what is returned by the administrative API.
The console output is quite long, since every table descriptor is printed, including every possible property.
The interesting part is the exception you should see being printed as well.
The example uses a nonexistent table name to showcase the fact that you must be using existing table names—or wrap the call into a try/catch guard, handling the exception more gracefully.
After creating a table, it is time to also be able to delete them.
Hand in a table name as a String, or a byte array, and the rest is taken care of: the table is removed from the servers, and all data deleted.
But before you can delete a table, you need to ensure that it is first disabled, using the following methods:
Disabling the table first tells every region server to flush any uncommitted changes to disk, close all the regions, and update the .META.
The choices are again between doing this asynchronously, or synchronously, and supplying the table name in various formats for convenience.
Disabling a table can potentially take a very long time, up to several minutes.
This depends on how much data is residual in the server’s memory and not yet persisted to disk.
Undeploying a region requires all the data to be written to disk first, and if you have a large heap value set for the servers this may result in megabytes, if not even gigabytes, of data being saved.
In a heavily loaded system this could contend with other processes writing to disk, and therefore require time to complete.
Finally, there is a set of calls to check on the status of a table:
Example 5-4 uses various combinations of the preceding calls to create, delete, disable, and check the state of a table.
Using the various calls to disable, enable, and check the status of a table.
The output on the console should look like this (the exception printout was abbreviated, for the sake of brevity):
Table is disabled: true Table available: true Deleting disabled table...
The error thrown when trying to delete an enabled table shows that you either disable it first, or handle the exception gracefully in case that is what your client application requires.
You could prompt the user to disable the table explicitly and retry the operation.
In other words, this method checks if the table is physically present, no matter what check for the state of the table.
After creating your tables with the specified schema, you must either delete the newly created table to change the details, or use the following method to alter its structure:
Example 5-5 does create a table, and subsequently modifies it.
Get the schema, and update by adding a new family and changing the maximum file size property.
Check if the table schema matches the new one created locally.
The output shows that both the schema modified in the client code and the final schema retrieved from the server after the modification are consistent:
If you want to make sure that changes have been propagated to all the servers and applied accordingly, you should use the schema you sent matches up with the remote schema.
Schema Operations HBaseAdmin class to modify specific aspects of the current table schema.
As usual, you need to make sure the table to be modified is disabled first.
Adding or modifying a column requires that you first prepare an HColumnDescriptor instance, as described in detail in “Column returned HTableDescriptor instance to retrieve the existing columns.
All of these calls are asynchronous, so as mentioned before, caveat emptor.
Use Case: Hush An interesting use case for the administrative API is to create and alter tables and their schemas based on an external configuration file.
Hush is making use of this idea and defines the table and column descriptors in an XML file, which is read and the contained schema compared with the current table definitions.
The following example has the core of the code that does this task:
Compute the differences between the XML-based schema and what is currently in HBase.
See if there are any differences in the column and table definitions.
Alter the table itself, if there are any differences found.
If the table did not exist yet, create it now.
Cluster Operations The last group of operations the HBaseAdmin class exposes is related to cluster operations.
They allow you to check the status of the cluster, and perform tasks on tables and/or regions.
Many of the following operations are for advanced users, so please handle with care.
If it fails to do so, an exception is thrown—in other words, this method does not return a boolean flag, but either silently succeeds, or throws said error.
Use these calls to close regions that have previously been deployed to region servers.
Any enabled table has all regions enabled, so you could actively close and undeploy a region.
You need to supply the exact regionname as stored in the .META.
Further, you may optionally supply the hostAndPort parameter, that overrides the server assignment as found in the .META.
Using this close call does bypass any master notification, that is, the region is directly closed by the region server, unseen by the master node.
As updates to a region (and the table in general) accumulate the MemStore instances of the region, servers fill with unflushed modifications.
The method takes either a region name, or a table name.
The value provided by your code is tested to see if it matches an existing table; if it does, it is assumed to be a table, otherwise it is treated as a region name.
If you specify neither a proper table nor a region name, an UnknownRegionException is thrown.
Similar to the preceding operations, you must give either a table or a region name.
The call itself is asynchronous, as compactions can potentially take a long time to complete.
In case a table name is given, the administrative API iterates over all regions of the table and invokes the compaction call implicitly for each of them.
Using these calls allows you to split a specific region, or table.
In case a table name is given, it iterates over all regions of that table and implicitly invokes the split command on each of them.
A noted exception to this rule is when the splitPoint parameter is given.
In the case of specifying a table name, all regions are checked and the one containing the splitPoint is split at the given key.
It also must be greater than the region’s start key, since splitting a region at its start key would make no sense.
If you fail to give the correct row key, the split request is ignored without reporting back to the client.
The region server currently hosting the region will log this locally with the following message:
Split row is not inside region key range or is equal to startkey:
When a client requires a region to be deployed or undeployed from the region servers, it can invoke these calls.
The first would assign a region, based on the overall assignment plan, while the second would unassign the given region.
The force parameter set to true has different meanings for each of the calls: first, continuing in its attempt to assign the region to a new region server.
If force were set to false, this would have no effect.
You can move a region from its current region server to a new one.
The destServerName parameter can be set to null to pick a new server at random; otherwise, it must be a valid server name, running a region server process.
If the server name is wrong, or currently not responding, the region is deployed to a different server instead.
In a worst-case scenario, the move could fail and leave the region unassigned.
The first method allows you to switch the region balancer on or off.
When the from the servers, with more deployed to those with less deployed regions.
These calls either shut down the entire cluster, stop the master server, or stop a particular region server only.
Once invoked, the affected servers will be stopped, that is, there is no delay nor a way to revert the process.
Cluster Status Information will be given a ClusterStatus instance, containing all the information the master server has about the current state of the cluster.
Quick overview of the information provided by the ClusterStatus class.
The names in the collection are ServerName instances, which contain the hostname, RPC port, and start code.
The names in the collection are ServerName instances, which contain the hostname, RPC port, and start code.
Gives you access to a map of all regions currently in transition, e.g., being moved, assigned, or unassigned.
The key of the map is the encoded region name (as returned.
Retrieves the status information available for the given server name.
Accessing the overall cluster status gives you a high-level view of what is going on with instances, lets you drill further into each actual live server, and see what it is doing currently.
Quick overview of the information provided by the ServerName class Method Description.
Each server also exposes details about its load, by offering an HServerLoad instance, servers and retrieve their current details.
The HServerLoad class gives you access to not just the load of the server itself, but also for each hosted region.
Quick overview of the information provided by the HServerLoad class.
It is reset at the end of this time frame, and counts all API requests, such as gets, puts, increments, deletes, and so on.
Returns a map containing the load details for each hosted region of the current server.
The key is the region name and the value an instance of the RegionsLoad class, discussed next.
Finally, there is a dedicated class for the region load, aptly named RegionLoad.
Quick overview of the information provided by the RegionLoad class.
On a standalone setup, and having run the earlier examples in the book, you should see something like this:
HBase comes with a variety of clients that can be used from various programming languages.
This chapter will give you an overview of what is available.
Introduction to REST, Thrift, and Avro Access to HBase is possible from virtually every popular programming language and environment.
You either use the client API directly, or access it through some sort of proxy that translates your request into an API call.
These proxies wrap the native Java API into other protocol APIs so that clients can be written in any language the external API provides.
Typically, the external API is implemented in a dedicated Java-based server that can internally use the provided HTable client API.
This simplifies the implementation and maintenance of these gateway servers.
The protocol between the gateways and the clients is then driven by the available choices and requirements of the remote client.
An obvious choice is Representational State Transfer (REST),* which is based on existing web-based technologies.
The actual transport is typically HTTP—which is the standard protocol for web applications.
This makes REST ideal for communicating between heterogeneous systems: the protocol layer takes care of transporting the data in an interoperable format.
By not changing the protocol, REST is compatible with existing technologies, such as web servers, and proxies.
However, both REST and SOAP suffer from the verbosity level of the protocol.
Humanreadable text, be it plain or XML-based, is used to communicate between client and server.
Transparent compression of the data sent over the network can mitigate this problem to a certain extent.
As a result, companies with very large server farms, extensive bandwidth usage, and many disjoint services felt the need to reduce the overhead and implemented their own RPC layers.
All of them have similar feature sets, vary in the number of languages they support, and have (arguably) slightly better or worse levels of encoding efficiencies.
The key difference with Protocol Buffers when compared to Thrift and Avro is that it has no RPC stack of its own; rather, it generates the RPC definitions, which have to be used with other RPC libraries subsequently.
HBase ships with auxiliary servers for REST, Thrift, and Avro.
They are implemented as standalone gateway servers, which can run on shared or dedicated machines.
Since Thrift and Avro have their own RPC implementation, the gateway servers simply provide a wrapper around them.
For REST, HBase has its own implementation, offering access to the stored data.
Instead of implementing a separate RPC server, it leverages the Accept header of HTTP to send and receive the data encoded in Protocol Buffers.
Figure 6-1 shows how dedicated gateway servers are used to provide endpoints for various remote clients.
Internally, these servers use the common HTable-based client API to access the tables.
You can see how they are started on top of the region server processes, sharing the same physical machine.
There is no one true recommendation for how to place the gateway servers.
You may want to collocate them, or have them on dedicated machines.
Another approach is to run them directly on the client nodes.
For example, when you have web servers constructing the resultant HTML pages using PHP, it is advantageous to run the gateway process on the same server.
Check carefully how you access HBase from your client, to place the gateway servers on the appropriate physical machine.
This is influenced by the load on each machine, as well as the amount of data being transferred: make sure you are not starving either process for resources, such as CPU cycles, or network bandwidth.
Short-lived processes would spend more time setting up the connection and preparing the metadata than in the actual operation itself.
The caching of region information in the server, in particular, makes the reuse for every bit of data they want to access.
Selecting one server type over the others is a nontrivial task, as it depends on your use case.
The initial argument over REST in comparison to the more efficient Thrift, or similar serialization formats, shows that for high-throughput scenarios it is.
However, if you have few requests, but they are large in size, REST is interesting.
A rough separation could look like this: REST use case.
Since REST supports existing web-based infrastructure, it will fit nicely into setups with reverse proxies and other caching technologies.
Plan to run many REST servers in parallel, to distribute the load across them.
For example, run a server on every application server you have, building a single-app-to-server relationship.
Thrift/Avro use case Use the compact binary protocols when you need the best performance in terms of throughput.
Interactive Clients The first group of clients consists of the interactive ones, those that send client API calls on demand, such as get, put, or delete, to servers.
Based on your choice of protocol, you can use the supplied gateway servers to gain access from your applications.
There is no need to start any gateway server, as your client using HTable is directly communicating with the HBase servers, via the native RPC calls.
Refer to the aforementioned chapters to implement a native Java client.
It also provides support for different message formats, offering many choices for a client application to communicate with the server.
For REST-based clients to be able to connect to HBase, you need to start the appropriate gateway server.
The following commands show you how to get the command-line help, and then start the REST server in a nondaemonized mode:
To run the REST server as a daemon, execute bin/hbase-daemon.sh start|stop.
The help stated that you need to run the server using a different script to start it as a background process:
Once the server is started you can use curl# on the command line to verify that it is operational:
Retrieving the root URL, that is "/" (slash), returns the list of available tables, here testtable.
Using "/version" retrieves the REST server version, along with details about the machine it is running on.
Stopping the REST server, and running as a daemon, involves the same script, just replacing start with stop:
The REST server gives you all the operations required to work with HBase tables.
The current documentation for the REST server is online at http://hbase .apache.org/apidocs/org/apache/hadoop/hbase/rest/package-summary .html.
Also, be sure to carefully read the XML schemas documentation on that page.
It explains the schemas you need to use when requesting information, as well as those returned by the server.
You can start as many REST servers as you like, and, for example, use a load balancer to route the traffic between them.
Finally, use the -p, or --port, parameter to specify a different port for the server to listen on.
Using the HTTP Content-Type and Accept headers, you can switch between different formats being sent or returned to the caller.
As an example, you can create a table and row in HBase using the shell like so:
For some operations it is permissible to have the data returned as plain text.
On the other hand, using plain text with more complex return values is not going to work as expected:
This is caused by the fact that the server cannot make any assumptions regarding how to format a complex result value in plain text.
You need to use a format that allows you to express nested information natively.
As per the example table created in the previous text, the row key is a binary one, consisting of three bytes.
See the online documentation referred to earlier for the entire syntax.
When storing or retrieving data, XML is considered the default format.
For example, when retrieving the example row with no particular Accept header, you receive:
The column name and the actual value are encoded in Base64,† as explained in the online schema documentation.
Because it uses the percent sign as the prefix, it is also called percent encoding.
All occurrences of base64Binary are where the REST server returns the encoded data.
This is done to safely transport the binary data that can be contained in the keys, or the value.
This is also true for data that is sent to the REST server.
Make sure to read the schema documentation to encode the data appropriately, including the payload, in other words, the actual data, but also the column name, row key, and so on.
This is obviously useful only to verify the details on the command line.
From within your code you can use any available Base64 implementation to decode the returned values.
Similar to XML, requesting (or setting) the data in JSON simply requires setting the Accept header:
The preceding JSON result was reformatted to be easier to read.
Usually the result on the console is returned as a single line, for example:
The encoding of the values is the same as for XML, that is, Base64 is used to encode any value that potentially contains binary data.
An important distinction to XML is that JSON does not have nameless data fields.
Cell tags, but JSON must specify key/value pairs, so there is no immediate counterpart available.
For that reason, JSON has a special field called “$” (the dollar sign)
The value of the dollar field is the cell data.
In the preceding example, you can see it being used:
You need to query the dollar field to get the Base64-encoded data.
An interesting application of REST is to be able to.
Since Protocol Buffers have no native RPC stack, the HBase REST server offers support for its encoding.
Getting the results returned in Protocol Buffer encoding requires the matching Accept header:
The use of hexdump allows you to print out the encoded message in its binary format.
You need a Protocol Buffer decoder to actually access the data in a structured way.
The ASCII printout on the righthand side of the output shows the column name and cell value for the example row.
Finally, you can dump the data in its raw form, while omitting structural data.
In the following console command, only the data is returned, as stored in the cell.
Depending on the format request, the REST server puts structural data into a custom header.
For example, for the raw get request in the preceding paragraph, the headers look like this (adding -D- to the curl command):
The timestamp of the cell has been moved to the header as X-Time stamp.
Since the row and column keys are part of the request URI, they are omitted from the response to prevent unnecessary data from being transferred.
The REST server also comes with a comprehensive Java client API.
Set up a cluster list adding all known REST server hosts.
Create a remote table instance, wrapping the REST access into a familiar interface.
Scan the table; again, this is the same approach as if using the native Java API.
Running the example requires that the REST server has been started and is listening on the specified port.
If you are running the server on a different machine and/or port, you need to first adjust the value added to the Cluster instance.
Here is what is printed on the console when running the example:
Due to the lexicographical sorting of row keys, you will receive the preceding rows.
The RemoteHTable is a convenient way to talk to a number of REST servers, while being able to use the normal Java client API classes, such as Get or Scan.
The current implementation of the Java REST client is using the Protocol Buffer encoding internally to communicate with the remote REST server.
It is the most compact protocol the server supports, and therefore provides the best bandwidth efficiency.
Once you have compiled a schema, you can exchange messages transparently between systems implemented in one or more of those languages.
Before you can use Thrift, you need to install it, which is preferably done using a binary distribution package for your operating system.
If that is not an option, you need to compile it from its sources.
Download the source tarball from the website, and unpack it into a common location:
Once you have Thrift installed, you need to compile a schema into the programming language of your choice.
HBase comes with a schema file for its client and administrative API.
You need to use the Thrift binary to create the wrappers for your development environment.
The supplied schema file exposes the majority of the API functionality, but is lacking in a few areas.
It was created when HBase had a different API and that is noticeable when using it.
Work is being done in HBASE-1744 to port the Thrift schema file to the current API, while adding all missing features.
Once this is complete, it will be added as the thrift2 package so that you can maintain your existing code using the older schema, while working on porting it over to the new schema.
Before you can access HBase using Thrift, though, you also have to start the supplied ThriftServer.
Starting the Thrift server is accomplished by using the supplied scripts.
You can get the command-line help by adding the -h switch, or omitting all options:
To start the Thrift server run 'bin/hbase-daemon.sh start thrift' To shutdown the thrift server run 'bin/hbase-daemon.sh stop thrift' or send a kill signal to the thrift server pid.
The type of server, protocol, and transport used is usually enforced by the client, since not all language implementations have support for them.
From the command-line help you can see that, for example, using the nonblocking server implies the framed transport.
Using the defaults, you can start the Thrift server in nondaemonized mode:
The help stated that you need to run the server using a different script to start it as a background process:
Stopping the Thrift server, running as a daemon, involves the same script, just replacing start with stop:
The Thrift server gives you all the operations required to work with HBase tables.
The current documentation for the Thrift server is online at http://wiki .apache.org/hadoop/Hbase/ThriftApi.
You should refer to it for all the provided operations.
It is also advisable to read the provided $HBASE_HOME/src/main/resources/org/apache/hadoop/hbase/thrift/ Hbase.thrift schema definition file for the full documentation of the available functionality.
You can start as many Thrift servers as you like, and, for example, use a load balancer to route the traffic between them.
Since they are stateless, you can use a round-robin (or similar) approach to distribute the load.
Finally, use the -p, or --port, parameter to specify a different port for the server to listen on.
HBase not only ships with the required Thrift schema file, but also with an example client for many programming languages.
Here we will enable the PHP implementation to demonstrate the required steps.
You need to enable PHP support for your web server! Follow your server documentation to do so.
The first step is to copy the supplied schema file and compile the necessary PHP source files for it:
The call to thrift should complete with no error or other output on the command line.
Inside the thrift_src directory you will now find a directory named gen-php containing the two generated PHP files required to access HBase:
These generated files require the Thrift-supplied PHP harness to be available as well.
They need to be copied into your web server’s document root directory, along with the generated files:
The generated PHP files are copied into a packages subdirectory, as per the Thrift documentation, which needs to be created if it does not exist yet.
HBase ships with a DemoClient.php file that uses the generated files to communicate with the servers.
This file is copied into the same document root directory of the web server:
You need to edit the DemoClient.php file and adjust the following fields at the beginning of the file:
If these compiled libraries # are not present in this directory, move them there from gen-php/
Usually, editing the first line is enough to set the THRIFT_ROOT path.
Since the DemoClient.php file is also located in the document root directory, it is sufficient to set the variable to thrift, that is, the directory copied from the Thrift sources earlier.
The last line in the preceding excerpt has a hardcoded server name and port.
If you set up the example in a distributed environment, you need to adjust this line to match your environment as well.
After everything has been put into place and adjusted appropriately, you can open a browser and point it to the demo page.
This should load the page and output the following details (abbreviated here for the sake of brevity):
Follow the same steps to start the Thrift server, compile the schema definition into the necessary language, and start the client.
Depending on the language, you will need to put the generated code into the appropriate location first.
HBase already ships with the generated Java classes to communicate with the Thrift server.
You can always regenerate them again from the schema file, but for convenience they are already included.
Once you have compiled a schema, you can exchange messages transparently between systems implemented in one or more of those languages.
Before you can use Avro, you need to install it, which is preferably done using a binary distribution package for your operating system.
If that is not an option, you need to compile it from its sources.
Once you have Avro installed, you need to compile a schema into the programming language of your choice.
HBase comes with a schema file for its client and administrative API.
You need to use the Avro tools to create the wrappers for your development environment.
Before you can access HBase using Avro, though, you also have to start the supplied AvroServer.
Starting the Avro server is accomplished by using the supplied scripts.
You can get the command-line help by adding the -h switch, or omitting all options:
You need to run the server using a different script to start it as a background process:
Stopping the Avro server, running as a daemon, involves the same script, just replacing start with stop:
The Avro server gives you all the operations required to work with HBase tables.
The current documentation for the Avro server is available online at http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/avro/package -summary.html.
You are also advised to read the provided $HBASE_HOME/src/main/java/ org/apache/hadoop/hbase/avro/hbase.avpr schema definition file for the full documentation of the available functionality.
You can start as many Avro servers as you like, and, for example, use a load balancer to route the traffic between them.
Since they are stateless, you can use a round-robin (or similar) approach to distribute the load.
Finally, use the -p, or --port, parameter to specify a different port for the server to listen on.
Other Clients There are other client libraries that allow you to access an HBase cluster.
They can roughly be divided into those that run directly on the Java Virtual Machine, and those that use the gateway servers to communicate with an HBase cluster.
The HBase Shell is an example of using a JVM-based language to access the Javabased API.
It comes with the full source code, so you can use it to add the same features to your own JRuby code.
HBql HBql adds an SQL-like syntax on top of HBase, while adding the extensions needed where HBase has unique features.
HBase-DSL This project gives you dedicated classes that help when formulating queries against an HBase cluster.
Using a builder-like style, you can quickly assemble all the options and parameters necessary.
JPA/JPO You can use, for example, DataNucleus to put a JPA/JPO access layer on top of HBase.
PyHBase The PyHBase project (https://github.com/hammer/pyhbase/) offers an HBase client through the Avro gateway server.
AsyncHBase AsyncHBase offers a completely asynchronous, nonblocking, and thread-safe client to access HBase clusters.
It uses the native RPC protocol to talk directly to the various servers.
Note that some of these projects have not seen any activity for quite some time.
They usually were created to fill a need of the authors, and since then have been made public.
You can use them as a starting point for your own projects.
Batch Clients The opposite use case of interactive clients is batch access to data.
The difference is that these clients usually run asynchronously in the background, scanning large amounts of data to build, for example, search indexes, machine-learning-based mathematical models, or statistics needed for reporting.
Access is less user-driven, and therefore, SLAs are geared more toward overall runtime, as opposed to per-request latencies.
The majority of the batch frameworks reading and writing from and to HBase are MapReduce-based.
MapReduce The Hadoop MapReduce framework is built to process petabytes of data, in a reliable, deterministic, yet easy-to-program way.
There are a variety of ways to include HBase as a source and target for MapReduce jobs.
The HBase-Runner project (https://github.com/mudphone/hbase-runner/) offers support for HBase from the functional programming language Clojure.
You can write MapReduce jobs in Clojure while accessing HBase tables.
It was initially developed at Facebook, but is now part of the open source Hadoop ecosystem.
Hive offers an SQL-like query language, called HiveQL, which allows you to query the semistructured data stored in Hadoop.
The query is eventually turned into a MapReduce job, executed either locally or on a distributed MapReduce cluster.
The data is parsed at job execution time and Hive employs a storage handler§ abstraction layer that allows for data not to just reside in HDFS, but other data sources as well.
A storage handler transparently makes arbitrarily stored information available to the HiveQLbased user queries.
The row key can be exposed as another column when needed.
The implication is that you cannot run the HBase integration against a more current version, since the RPC is very sensitive to version changes and will bail out at even minor differences.
The only way currently is to replace the HBase JARs with the newer ones and recompile Hive from source.
You either need to update the Ivy settings to include the version of HBase (and probably Hadoop) you need, or try to build Hive, then copy the newer JARs into the $HIVE_HOME/src/build/dist/lib directory and compile again (YMMV)
The better approach is to let Ivy load the appropriate version from the remote repositories, and then compile Hive normally.
To get started, download the source tarball from the website and unpack it into a common location:
The Hive wiki has a full explanation of the HBase integration into Hive.
You can now build Hive from the sources using ant, but not before you have set the environment variable for the Hadoop version you are building against:
The build process will take awhile, since Ivy needs to download all required libraries, and that depends on your Internet connection speed.
Once the build is complete, you can start using the HBase handler with the new version of HBase.
In some cases, you need to slightly edit all files in src/hbase-handler/src/java/org/apache/ hadoop/hive/hbase/ and replace the way the configuration is created, from:
After you have installed Hive itself, you have to edit its configuration files so that it has access to the HBase JAR file, and the accompanying configuration.
Set HADOOP_HOME to point to a specific hadoop install directory.
Folder containing extra libraries required for hive compilation/execution # can be controlled by:
You may have to copy the supplied $HIVE_HOME/conf/hiveenv.sh.template file, and save it in the same directory, but without the .template extension.
Once you have copied the file, you can edit it as described.
Once Hive is installed and operational, you can start using the new handler.
First start the Hive command-line interface, create a native Hive table, and insert data from the supplied example files:
This is using the pokes table, as described in the Hive guide at http://wiki.apache.org/ hadoop/Hive/GettingStarted.
This DDL statement maps the HBase table, defined using the TBLPROPERTIES, and SERDEPROPERTIES, using the new HBase handler, to a Hive table named hbase_table_1
The hbase.columns.mapping has a special feature, which is mapping the column with the name ":key" to the HBase row key.
You can place this special column to perform row key mapping anywhere in your definition.
Here it is placed as the first column, thus mapping the values in the key column of the Hive table to be the row key in the HBase table.
The hbase.table.name in the table properties is optional and only needed when you want to use different names for the tables in Hive and HBase.
Here it is set to the same value, and therefore could be omitted.
Loading the table from the previously filled pokes Hive table is done next.
According to the mapping, this will save the pokes.foo values in the row key, and the pokes.bar in the column cf1:val:
You can see how the Hive command line prints out the values it is using.
The job copies the values from the internal Hive table into the HBase-backed one.
In certain setups, especially in the local, pseudodistributed mode, the Hive job may fail with an obscure error message.
Before trying to figure out the details, try running the job in Hive local MapReduce mode.
This mode was added in Hive 0.7.0, and may not be available to you.
If it is, try to use it, since it avoids using the Hadoop MapReduce framework—which means you have one less part to worry about when debugging a failed Hive job.
The following counts the rows in the pokes and hbase_table_1 tables (the CLI output of the job details are omitted for the second and all subsequent queries):
What is interesting to note is the difference in the actual count for each table.
They differ by more than 100 rows, where the HBase-backed table is the shorter one.
What could be the reason for this? In HBase, you cannot have duplicate row keys, so every row that was copied over, and which had the same value in the originating pokes.foo column, is saved as the same row.
This is the same as performing a SELECT DISTINCT on the source table:
This is now the same outcome and proves that the previous results are correct.
Finally, drop both tables, which also removes the underlying HBase table:
You can also map an existing HBase table into Hive, or even map the table into multiple Hive tables.
This is useful when you have very distinct column families, and querying them is done separately.
This will improve the performance of the query significantly, since it uses a Scan internally, selecting only the mapped column families.
If you have a sparsely set family, this will only scan the much smaller files on disk, as opposed to running a job that has to scan everything just to filter out the sparse data.
Mapping an existing table requires the Hive EXTERNAL keyword, which is also used in other places to access data stored in unmanaged Hive tables, that is, those that are not under Hive’s control:
External tables are not deleted when the table is dropped within Hive.
You have the option to map any HBase column directly to a Hive column, or you can map an entire column family to a Hive MAP type.
This is useful when you do not know the column qualifiers ahead of time: map the family and iterate over the columns from within the Hive query instead.
HBase columns you do not map into Hive are not accessible for Hive queries.
Since storage handlers work transparently for the higher-level layers in Hive, you can also use any user-defined function (UDF) supplied with Hive—or your own custom functions.
There are a few shortcomings in the current version, though: No custom serialization.
HBase only stores byte[] arrays, so Hive is simply converting every column value to String, and serializes it from there.
No version support There is currently no way to specify any version details when handling HBase tables.
Check with the Hive project site to see if these features have since been added.
Pig The Apache Pig project# provides a platform to analyze large amounts of data.
It has its own high-level query language, called Pig Latin, which uses an imperative programming style to formulate the steps involved in transforming the input data to the final output.
This is the opposite of Hive’s declarative approach to emulate SQL.
The nature of Pig Latin, in comparison to HiveQL, appeals to everyone with a procedural programming background, but also lends itself to significant parallelization.
When it is combined with the power of Hadoop and the MapReduce framework, you can process massive amounts of data in reasonable time frames.
Version 0.7.0 of Pig introduced the LoadFunc/StoreFunc classes and functionality, which allows you to load and store data from sources other than the usual HDFS.
One of those sources is HBase, implemented in the HBaseStorage class.
Pigs’ support for HBase includes reading and writing to existing tables.
You can map table columns as Pig tuples, which optionally include the row key as the first field for read operations.
For writes, the first field is always used as the row key.
Pig Installation You should try to install the prebuilt binary packages for the operating system distribution of your choice.
If this is not possible, you can download the source from the project website and build it locally.
For example, on a Linux-based system you could perform the following steps.† Download the source tarball from the website, and unpack it into a common location:
Add the pig script to the shell’s search path, and set the PIG_HOME environment variable like so:
You can use the supplied tutorial code and data to experiment with Pig and HBase.
You do have to create the table in the HBase Shell first to work with it from within Pig:
Starting the Pig Shell, aptly called Grunt, requires the pig script.
Local mode implies that Pig is not using a separate MapReduce installation, but uses the LocalJobRunner that comes as part of Hadoop.
It runs the resultant MapReduce jobs within the same process.
This is useful for testing and prototyping, but should not be used for larger data sets.
The full details can be found on the Pig setup page.
You have the option to write the script beforehand in an editor of your choice, and subsequently specify it when you invoke the pig script.
Or you can use Grunt, the Pig Shell, to enter the Pig Latin statements interactively.
Ultimately, the statements are translated into one or more MapReduce jobs, but not all statements trigger the execution.
Instead, you first define the steps line by line, and a call to DUMP or STORE will eventually set the job in motion.
The Pig Latin functions are case-insensitive, though commonly they are written in uppercase.
Names and fields you define are case-sensitive, and so are the Pig Latin functions.
The Pig tutorial comes with a small data set that was published by Excite, and contains an anonymous user ID, a timestamp, and the search terms used on its site.
The first step is to load the data into HBase using a slight transformation to generate a compound key.
You can use the DEFINE statement to abbreviate the long Java package reference for the HBaseStorage class.
This is useful if you are going to reuse the specific load or store function.
The STORE statement started a MapReduce job that read the data from the given logfile and copied it into the HBase table.
Accessing the data involves another LOAD statement, this time using the HBaseStorage class:
The parameters in the brackets define the column to field mapping, as well as the extra option to load the row key as the first field in relation R.
The AS part explicitly defines that the row key and the colfam1:query column are converted to chararray, which is Pig’s string type.
By default, they are returned as bytearray, matching the way they are stored in the HBase table.
Converting the data type allows you, for example, to subsequently split the row key.
You can test the statements entered so far by dumping the content of R, which is the result of the previous statement.
The row key, placed as the first field in the tuple, is the concatenated representation created during the initial copying of the data from the file into HBase.
It can now be split back into two fields so that the original layout of the text file is re-created:
With this in place, you can proceed to the remainder of the Pig tutorial, while replacing the LOAD and STORE statements with the preceding code.
Concluding this example, type in QUIT to finally exit the Grunt shell:
Pig’s support for HBase has a few shortcomings in the current version, though: No version support.
There is currently no way to specify any version details when handling HBase cells.
Fixed column mapping The row key must be the first field and cannot be placed anywhere else.
This can be overcome, though, with a subsequent FOREACH...GENERATE statement, reordering the relation layout.
Check with the Pig project site to see if these features have since been added.
Under the covers, it uses MapReduce during execution, but during development, users don’t have to think in MapReduce to create solutions for execution on Hadoop.
The model used is similar to a real-world pipe assembly, where data sources are taps, and outputs are sinks.
These are piped together to form the processing flow, where data passes through the pipe and is transformed in the process.
Pipes can be connected to larger pipe assemblies to form more complex processing pipelines from existing pipes.
Data then streams through the pipeline and can be split, merged, grouped, or joined.
The data is represented as tuples, forming a tuple stream through the assembly.
This very visually oriented model makes building MapReduce jobs more like construction work, while abstracting the complexity of the actual work involved.
Cascading (as of version 1.0.1) has support for reading and writing data to and from an HBase cluster.
Detailed information and access to the source code can be found on the Cascading Modules page (http://www.cascading.org/modules.html)
Example 6-2 shows how to sink data into an HBase cluster.
See the GitHub repository, linked from the modules page, for more up-to-date API information.
Using Cascading to insert data into HBase // read data from the default filesystem // emits two fields: "offset" and "line"
Cascading to Hive and Pig offers a Java API, as opposed to the domain-specific languages (DSLs) provided by the others.
There are add-on projects that provide DSLs on top of Cascading.
You can use it to connect to local or remote servers and interact with them.
The shell provides both client and administrative operations, mirroring the APIs discussed in the earlier chapters of this book.
Basics The first step to experience the shell is to start it:
The shell is based on JRuby, the Java Virtual Machine-based implementation of Ruby.‡ More specifically, it uses the Interactive Ruby Shell (IRB), which is used to enter Ruby commands and get an immediate response.
HBase ships with Ruby scripts that extend the IRB with specific commands, related to the Java-based APIs.
It inherits the built-in support for command history and completion, as well as all Ruby commands.
There is no need to install Ruby on your machines, as HBase ships with the required JAR files to execute the JRuby shell.
You use the supplied script to start the shell on top of Java, which is already a necessary requirement.
Once started, you can type in help, and then press Return, to get the help text (abbreviated in the following code sample):
Dictionaries of configuration used in the creation and alteration of tables are Ruby Hashes.
As stated, you can request help for a specific command by adding the command when invoking help, or print out the help of all commands for a specific group when using the group name with the help command.
The command or group name has the enclosed in quotes.
You can leave the shell by entering exit, or quit:
The shell also has specific command-line options, which you can see when adding the -h, or --help, switch to the command:
Debugging Adding the -d, or --debug switch, to the shell’s start command enables the debug mode, which switches the logging levels to DEBUG, and lets the shell print out any backtrace information—which is similar to stacktraces in Java.
Once you are inside the shell, you can use the debug command to toggle the debug mode:
Without the debug mode, the shell is set to print only ERROR-level messages, and no backtrace details at all, on the console.
There is an option to switch the formatting being used by the shell.
The shell start script automatically uses the configuration directory located in the same $HBASE_HOME directory.
You can override the location to use other settings, but most an hbase-site.xml file, with an hbase.zookeeper.quorum property pointing to another cluster, and start the shell like so:
Note that you have to specify an entire directory, not just the hbase-site.xml file.
Commands The commands are grouped into five different categories, representing their semantic relationships.
When entering commands, you have to follow a few guidelines: Quote names.
Commands that require a table or column name expect the name to be quoted in either single or double quotes.
You must use double quotes or the shell will interpret them as literals.
Note the mixture of quotes: you need to make sure you use the correct ones, or the result might not be what you had expected.
Text in single quotes is treated as a literal, whereas double-quoted text is interpolated, that is, it transforms the octal, or hexadecimal, values into bytes.
Ruby hashes for properties For some commands, you need to hand in a map with key/value properties.
Usually keys are predefined constants such as NAME, VERSIONS, or COMPRESSION, and do not need to be quoted.
Restricting Output The get command has an optional parameter that you can use to restrict the printed values by length.
This is useful if you have many columns with values of varying length.
To get a quick overview of the actual columns, you could suppress any longer value being printed in full—which on the console can get unwieldy very quickly otherwise.
In the following example, a very long value is inserted and subsequently retrieved with a restricted length, using the MAXLENGTH parameter:
The MAXLENGTH is counted from the start of the row (i.e., it includes the column name)
Set it to the width (or slightly less) of your console to fit each column into one line.
For any command, you can get detailed help by typing in help '<command>'
The majority of commands have a direct match with a method provided by either the client or administrative API.
Next is a brief overview of each command and the matching API functionality.
They allow you to retrieve details about the status of the cluster itself, and the version of HBase it is running.
See the help to get the simple, summary, and detailed status information.
Most of them stem from the administrative API, as described in Chapter 5
Same as the get command but converts the raw counter value into a readable number.
This is a special function offered by an internal class.
The web-based UI of the HBase Master exposes the same information.
Scripting Inside the shell, you can execute the provided commands interactively, getting immediate feedback.
Sometimes, though, you just want to send one command, and possibly script this call from the scheduled maintenance system (e.g., cron or at)
You can do this by piping the command into the shell:
Once the command is complete, the shell is closed and control is given back to the caller.
Finally, you can hand in an entire script to be executed by the shell at startup:
Once the script has completed, you can continue to work in the shell or exit it as usual.
There is also an option to execute a script using the raw JRuby interpreter, which involves running it directly as a Java application.
Using the hbase script sets up the classpath to be able to use any Java class necessary.
The following example simply retrieves the list of tables from the remote cluster:
Since the shell is based on JRuby’s IRB, you can use its built-in features, such as command completion and history.
Enabling them is a matter of creating an .irbrc in your home directory, which is read when the shell starts:
This enables the command history to save across shell starts.
The command completion is already enabled by the HBase scripts.
Another advantage of the interactive interpreter is that you can use the HBase classes and functions to perform, for example, something that would otherwise require you to write a Java application.
Here is an example of binary output received from a.
Note how the shell encoded the first three unprintable characters as hexadecimal values, while the fourth, the "[", was printed as a character.
Another example is to convert a date into a Linux epoch number, and back into a human-readable date:
Finally, you can also add many cells in a loop—for example, to populate a table with test data:
But even with a little bit of programming skills in another language, you might be able to use the features of the IRB-based shell to your advantage.
Web-based UI The HBase processes expose a web-based user interface (UI), which you can use to gain insight into the cluster’s state, as well as the tables it hosts.
The majority of the functionality is read-only, but a few selected operations can be triggered through the UI.
Master UI HBase also starts a web-based listing of vital attributes.
If the master is running on a host named master.foo.com on the default port, to see the master’s home page, you can point your browser at http://master.foo.com:60010
The ports used by the servers can be set in the hbase-site.xml configuration file.
The first page you will see when opening the master’s web UI is shown in Figure 6-2
It consists of multiple sections that give you insight into the cluster status itself, the tables it serves, what the region servers are, and so on.
The details can be broken up into the following groups: Master attributes.
You will find cluster-wide details in a table at the top of the page.
It has information on the version of HBase and Hadoop that you are using, where the root directory is located,§ the overall load average, and the ZooKeeper quorum used.
There is also a link in the description for the ZooKeeper quorum allowing you to see the information for your current HBase cluster stored in ZooKeeper.
Running tasks The next group of details on the master’s main page is the list of currently running tasks.
Every internal operation performed by the master is listed here while it is running, and for another minute after its completion.
Entries with a white background are currently running, a green background indicates successful completion of the task, and a yellow background means the task was aborted.
The latter can happen when an operation failed due to an inconsistent state.
Figure 6-3 shows a completed, a running, and a failed task.
Recall that this should better not be starting with /tmp, or you may lose your data during a machine restart.
Catalog tables This section list the two catalog tables, .META.
You can click on the name of the table to see more details on the table regions—for example, on what server they are currently hosted.
User tables Here you will see the list of all tables known to your HBase cluster.
The table names are links to another page with details on the selected table.
Region servers The next section lists the actual region servers the master knows about.
The table lists the address, which you can click on to see more details.
It also states the server start code, a timestamp representing an ID for each server, and finally, the load of the server.
Regions in transition As regions are managed by the master and region servers to, for example, balance the load across servers, they go through short phases of transition.
Before the operation is performed, the region is added to the list, and once the operation is complete, it is removed.
The Regions in Transitions table provided by the master web UI.
When you click on the name of a user table in the master’s web-based user interface, you have access to the information pertaining to the selected table.
Figure 6-5 shows an abbreviated version of a User Table page (it has a shortened list of regions for the sake of space)
The User Table page with details about the selected table.
The following groups of information are available in the User Table page: Table attributes.
As of this writing, this section only The boolean value states whether the table is enabled, so when you see a true in the Value column, this is the case.
On the other hand, a value of false would mean the table is currently disabled.
Table regions This list can be rather large and shows all regions of a table.
The Name column has the region name itself, and the Region Server column has a link to the server hosting the region.
Sometimes you may see the words not deployed where the server name should be.
This happens when a user table region is not currently served by any region server.
The Start Key and End Key columns show the region’s start and end keys as expected.
Finally, the Requests column shows the total number of requests, including all read (e.g., get or scan) and write (e.g., put or delete) operations, since the region was deployed to the server.
Example of a region that has not been assigned to a server and is listed as not deployed.
Regions by region server The last group on the User Table page lists which region server is hosting how many regions of the selected table.
This number is usually distributed evenly across all available servers.
The User Table page also offers a form that can be used to trigger administrative operations on a specific region, or the entire table.
This triggers the compact functionality, which is asynchronously running in the background.
Specify the optional name of a region to run the operation more selectively.
The name of the region can be taken from the table above, that is, the entries in the Name column of the Table Regions table.
If you do not specify a region name, the operation is performed on all regions of the table instead.
Split Similar to the compact action, the split form action triggers the split command, operating on a table or region scope.
Not all regions may be splittable—for example, those that contain no, or very few, cells, or one that has already been split, but which has not been compacted to complete the process.
Once you trigger one of the operations, you will receive a confirmation page; for example, for a split invocation, you will see:
Use the Back button of your web browser to go back to the previous page, showing the user table details.
There is also a link in the description column that lets you dump the content of all the nodes stored in ZooKeeper by HBase.
The page shows the same information as invoking the zk_dump command of the HBase Shell.
It shows you the root directory HBase is using inside the configured filesystem.
You also can see the currently assigned master, which region server is hosting the -ROOT- catalog table, the list of region servers that have registered with the master, as well as ZooKeeper internal details.
Figure 6-7 shows an exemplary output available on the ZooKeeper page.
The ZooKeeper page, listing HBase and ZooKeeper details, which is useful when debugging HBase installations.
Region Server UI The region servers have their own web-based UI, which you usually access through the master UI, by clicking on the server name links provided.
The main page of the region servers has details about the server, the tasks, and regions it is hosting.
Figure 6-8 shows an abbreviated example of this page (the list of tasks and regions is shortened for the sake of space)
The page can be broken up into the following groups of distinct information: Region server attributes.
This group of information contains the version of HBase you are running, when it was compiled, a printout of the server metrics, and the ZooKeeper quorum used.
Running tasks The table lists all currently running tasks, using a white background for running tasks, a yellow one for failed tasks, and a green one for completed tasks.
Online regions Here you can see all the regions hosted by the currently selected region server.
The table has the region name, the start and end keys, as well as the region metrics.
Shared Pages On the top of the master, region server, and table pages there are also a few generic links that lead to subsequent pages, displaying or controlling additional details of your setup: Local logs.
This link provides a quick way to access the logfiles without requiring access to the server itself.
It firsts list the contents of the log directory where you can select the logfile you want to see.
Thread dumps For debugging purposes, you can use this link to dump the Java stacktraces of the running HBase processes.
Log level This link leads you to a small form that allows you to retrieve and set the logging levels used by the HBase processes.
Figure 6-11 shows the form when it is loaded afresh.
When you enter, for example, org.apache.hadoop.hbase into the first input field, and click on the Get Log Level button, you should see a result similar to that shown in Figure 6-12
The web-based UI provided by the HBase servers is a good way to quickly gain insight into the cluster, the hosted tables, the status of regions and tables, and so on.
The majority of the information can also be accessed using the HBase Shell, but that requires console access to the cluster.
You can use the UI to trigger selected administrative operations; therefore, it might not be advisable to give everyone access to it: similar to the shell, the UI should be used by the operators and administrators of the cluster.
If you want your users to create, delete, and display their own tables, you will need an additional layer on top of HBase, possibly using Thrift or REST as the gateway server, to offer this functionality to end users.
One of the great features of HBase is its tight integration with Hadoop’s MapReduce framework.
Here you will see how this can be leveraged and how unique traits of HBase can be used advantageously in the process.
Framework Before going into the application of HBase with MapReduce, we will first have a look at the building blocks.
MapReduce Introduction MapReduce as a process was designed to solve the problem of processing in excess of terabytes of data in a scalable way.
There should be a way to build such a system that increases in performance linearly with the number of physical machines added.
It follows a divide-and-conquer approach by splitting the data located on a distributed filesystem so that the servers (or rather CPUs, or more modern “cores”) available can access these chunks of data and process them as fast as they can.
The problem with this approach is that you will have to consolidate the data at the end.
This (rather simplified) figure of the MapReduce process shows you how the data is processed.
The first thing that happens is the split, which is responsible for dividing the input data into reasonably sized chunks that are then processed by one server at a time.
This splitting has to be done in a somewhat smart way to make best use of available servers and the infrastructure in general.
In this example, the data may be a very large logfile that is divided into pieces of equal size.
Input data may also be binary, though, in which case you may have to write.
Classes Figure 7-1 also shows you the classes that are involved in the Hadoop implementation of MapReduce.
Let us look at them and also at the specific implementations that HBase provides on top of them.
Its classes are located in the package named mapreduce, while the existing classes for the previous API are located in mapred.
The older API was deprecated and should have been dropped in version 0.21.0—but that did not happen.
In fact, the old API was undeprecated since the adoption of the new one was hindered by its incompleteness.
HBase also has these two packages, which only differ slightly.
The new API has more support by the community, and writing jobs against it is not impacted by the Hadoop changes.
The first class to deal with is the InputFormat class (Figure 7-2)
First it splits the input data, and then it returns a RecordReader instance that used to iterate over each input record.
As far as HBase is concerned, there is a special implementation called TableInput FormatBase whose subclass is TableInputFormat.
The former implements the majority of the functionality but remains abstract.
The subclass is a lightweight concrete version of TableInputFormat and is used by many supplied samples and real MapReduce classes.
These classes implement the full turnkey solution to scan an HBase table.
You have to provide a Scan instance that you can prepare in any way you want: specify start and stop keys, add filters, specify the number of versions, and so on.
The TableInputFormat splits the table into proper blocks for you and hands them over to the subsequent classes in the MapReduce process.
The Mapper class(es) is for the next stage of the MapReduce process and one of its type of key/value pair, but emits possibly another type.
This is handy for converting the raw data into something more useful for further processing.
One specific implementation of the TableMapper is the IdentityTableMapper, which is also a good example of how to add your own functionality to the supplied classes.
The TableMapper class itself does not implement anything but only adds the signatures of.
The IdentityTableMapper is simply passing on the keys/values to the next stage of processing.
The Reducer stage and class hierarchy (Figure 7-4) is very similar to the Mapper stage.
This time we get the output of a Mapper class and process it after the data has been shuffled and sorted.
In the implicit shuffle between the Mapper and Reducer stages, the intermediate data is copied from different Map servers to the Reduce servers and the sort combines the shuffled (copied) data so that the Reducer sees the intermediate data as a nicely sorted set where each unique key is now associated with all of the possible values it was found with.
The final stage is the OutputFormat class (Figure 7-5), and its job is to persist the data in various locations.
There are specific implementations that allow output to files, or to HBase tables in the case of the TableOutputFormat class.
It uses a TableRecord Writer to write the data into the specific HBase output table.
Although many Mappers are handing records to many Reducers, only one OutputFormat takes each output record from its Reducer subsequently.
It is the final class that handles the key/value pairs and writes them to their final destination, this being a file or a table.
The TableOutputCommitter class is required for the Hadoop classes to do their job.
In fact, it is a dummy and does not do anything.
The name of the output table is specified when the job is created.
One rather significant thing it does do is to set the table’s autoflush to false and handle the buffer flushing implicitly.
This helps a lot in terms of speeding up the import of large data sets.
Supporting Classes The MapReduce support comes with the TableMapReduceUtil class that helps in setting up MapReduce jobs over HBase.
It has static methods that configure a job so that you can run it with HBase as the source and/or the target.
MapReduce Locality One of the more ambiguous things in Hadoop is block replication: it happens automatically and you should not have to worry about it.
HBase relies on it to provide durability as it stores its files into the distributed filesystem.
Although block replication works completely transparently, users sometimes ask how it affects performance.
This question usually arises when the user starts writing MapReduce jobs against either HBase or Hadoop directly.
Especially when larger amounts of data are being stored in HBase, how does the system take care of placing the data close to where it is needed? This concept is referred to as data locality, and in the case of HBase using the Hadoop filesystem (HDFS), users may have doubts as to whether it is working.
First let us see how Hadoop handles this: the MapReduce documentation states that tasks run close to the data they process.
Each block is assigned to a map task to process the contained data.
This means larger block sizes equal fewer map tasks to run as the number of mappers is driven by the number of blocks that need processing.
Hadoop knows where blocks are located, and runs the map tasks directly on the node that hosts the block.
Since block replication ensures that we have (by default) three copies on three different physical servers, the framework has the choice of executing the code on any of those three, which it uses to balance workloads.
This is how it guarantees data locality during the MapReduce process.
Once you understand that Hadoop can process data locally, you may start to question how this may work with HBase.
It does so for the actual data files (HFile) as well as the log (WAL)
And if you look into the code, it uses the Hadoop API call FileSystem.create(Path path) to create these files.
If you do not co-share your cluster with Hadoop and HBase, but instead employ a separate Hadoop as well as a standalone HBase cluster, there is no data locality—there can’t be.
This is the same as running a separate MapReduce cluster that would not be able to execute tasks directly on the data node.
It is imperative for data locality to have the Hadoop and HBase processes running on the same cluster—end of line.
How does Hadoop figure out where data is located as HBase accesses it? The most housekeeping on a regular basis.
These so-called compactions rewrite files as new data is added over time.
All files in HDFS, once written, are immutable (for all sorts of reasons)
Because of that, data is written into new files, and as their number grows, HBase compacts them into another set of new, consolidated files.
And here is the kicker: HDFS is smart enough to put the data where it is needed! It has a block placement policy in place that enforces all blocks to be written first on a collocated server.
The receiving data node compares the server name of the writer with its own, and if they match, the block is written to the local filesystem.
Then a replica is sent to a server within the same rack, and another to a remote rack—assuming you are using rack awareness in HDFS.
If not, the additional copies get placed on the least loaded data node in the cluster.
If you have configured a higher replication factor, more replicas are stored on distinct machines.
The important factor here, though, is that you now have a local copy of the block available.
The data node that shares the same physical host has a copy of all data the region server requires.
If you are running a scan or get or any other use case, you can be sure to get the best performance.
An issue to be aware of is region movements during load balancing, or server failures.
In that case, the data is no longer local, but over time it will be once again.
The master also takes this into consideration when a cluster is restarted: it assigns all regions to the original region servers.
If one of them is missing, it has to fall back to the random region assignment approach.
Table Splits When running a MapReduce job in which you read from a table, you are typically using the TableInputFormat.
It fits into the framework by overriding the required public.
Since it has no direct knowledge of the effect of the optional filter, it uses the start and stop keys to narrow down the number of regions.
The number of splits, therefore, is equal to all regions between the start and stop keys.
If you do not set the start and/or stop key, all are included.
It iterates over the splits and creates a new TableRecordReader by calling handles exactly one region, reading and mapping every row between the region’s start and end keys.
The split also contains the server name hosting the region.
This is what drives locality for MapReduce jobs over HBase: the framework checks the server name, and if a task tracker is running on the same machine, it will preferably run it on that server.
Because the region server is also collocated with the data node on that same node, the scan of the region will be able to retrieve all data from the local disk.
When running MapReduce over HBase, it is strongly advised that you turn off speculative execution mode.
It will only create more load on the same region and server, and also works against locality: the speculative task is executed on a different machine, and therefore will not have the region server local, which is hosting the region.
This results in all data being sent over the network, adding to the overall I/O load.
MapReduce over HBase The following sections will introduce you to using HBase in combination with MapReduce.
Before you can use HBase as a source or sink, or both, for data processing jobs, you have to first decide how you want to prepare the support by Hadoop.
Preparation To run a MapReduce job that needs classes from libraries not shipped with Hadoop or the MapReduce framework, you'll need to make those libraries available before the job is executed.
You have two choices: static preparation of all task nodes, or supplying everything needed with the job.
For a library that is used often, it is useful to permanently install its JAR file(s) locally on the task tracker machines, that is, those machines that run the MapReduce tasks.
Copy the JAR files into a common location on all nodes.
Restart all task trackers for the changes to be effective.
Obviously this technique is quite static, and every update (e.g., to add new libraries) requires a restart of the task tracker daemons.
Note that this fixes the versions of these globally provided libraries to whatever is specified on the servers and in their configuration files.
The issue of locking into specific versions of required libraries can be circumvented with the dynamic provisioning approach, explained next.
In case you need to provide different libraries to each job you want to run, or you want to update the library versions along with your job classes, then using the dynamic provisioning approach is more useful.
For this, Hadoop has a special feature: it reads all libraries from an optional /lib directory contained in the job JAR.
You can use this feature to generate so-called fat JAR files, as they ship not just with the actual job code, but also with all libraries needed.
This results in considerably larger job JAR files, but on the other hand, represents a complete, self-contained processing job.
Maven allows you to create the JAR files not just with the example code, but also to build the enhanced fat JAR file that can be deployed to the MapReduce framework as-is.
Maven has support for so-called profiles, which can be used to customize the build process.
The pom.xml for this chapter makes use of this feature to add a fatjar profile that creates the required /lib directory inside the final job JAR, and copies all required libraries into it.
For this to work properly, some of the dependencies need to be defined with a scope of provided so that they are not included in the copy operation.
This is done by adding the appropriate tag to all libraries that are already available on the server, for instance, the Hadoop JARs:
This is done in the parent POM file, located in the root directory of the book repository, as well as inside the POM for the chapter, depending on where a dependency is added.
One example is the Apache Commons CLI library, which is also part of Hadoop.
The fatjar profile uses the Maven Assembly plug-in with an accompanying src/main/ assembly/job.xml file that specifies what should, and what should not, be included in the generated target JAR (e.g., it skips the provided libraries)
This will build a JAR that can be used to execute any of the included MapReduce, using the hadoop jar command:
An example program must be given as the first argument.
It makes use of the Hadoop Program Driver class, which is prepared with all known job classes and their names.
Building a fat JAR only requires the addition of the profile name:
The generated JAR file has an added postfix to distinguish it, but that is just a matter of taste (you can simply override the lean JAR if you prefer, although I refrain from explaining it here):
It behaves exactly like the lean JAR, and you can launch the same jobs with the same parameters.
The difference is that it includes the required libraries, avoiding the configuration change on the servers:
Maven is not the only way to generate different job JARs; you can also use Apache Ant, for example.
What matters is not how you build the JARs, but that they contain the necessary information (either just the code, or the code and its required libraries)
Another option to dynamically provide the necessary libraries is the libjars feature of Hadoop’s MapReduce framework.
When you create a MapReduce job using the supplied GenericOptionsParser harness, you get support for the libjar parameter for free.
GenericOptionsParser is a utility to parse command line arguments generic to the Hadoop framework.
GenericOptionsParser recognizes several standarad command line arguments, enabling applications to easily specify a namenode, a jobtracker, additional configuration resources etc.
The reason to carefully read the documentation is that it not only states the libjars parameter, but also how and where to specify it on the command line.
Failing to add the libjars parameter properly will result in the MapReduce job to fail.
This can be seen from the job’s logfiles, for every task attempt.
The errors are also reported when starting the job on the command line, for example:
The leading HADOOP_CLASSPATH assignment is also required to be able to launch the job from the command line.
The Driver class setting up the job needs to have access to the HBase and ZooKeeper classes.
Fixing the above error requires the libjars parameter to be added, like so:
Finally, the HBase helper class TableMapReduceUtil comes with a method that you can use from your own code to dynamically provision additional JAR and configuration files with your job:
The former uses the latter function to add all the necessary HBase, ZooKeeper, and job classes:
You can see in the source code of the ImportTsv class how this is used: ...
The second call adds the Google Guava JAR, which is needed on top of the others already added.
Note how this method does not require you to specify the actual JAR file.
It uses the Java ClassLoader API to determine the name of the JAR containing the class in question.
This might resolve to the same JAR, but that is irrelevant in this context.
It is important that you have access to these classes in your Java CLASSPATH; otherwise, these calls will fail with a ClassNotFoundException error, similar to what you have seen already.
You are still required to at least add the HADOOP_CLASSPATH to the command line for an unprepared Hadoop setup, or else you will not be able to run the job.
The fat JAR has the advantage of containing everything that is needed for the job to run on a generic Hadoop setup.
As far as this book is concerned, we will be using the fat JAR to build and launch MapReduce jobs.
Data Sink Subsequently, we will go through various MapReduce jobs that use HBase to read from, or write to, as part of the process.
The first use case explained is using HBase as a data sink.
This is facilitated by the TableOutputFormat class and demonstrated in Example 7-1
The example data used is based on the public RSS feed offered by Delicious (http://delicious.com)
Arvind Narayanan used the feed to collect a sample data set, which he published on his blog.
There is no inherent need to acquire the data set, or capture the RSS any other source, including JSON records.
On the other hand, the Delicious data set provides records that can be used nicely with Hush: every entry has a link, user name, date, categories, and so on.
The test-data.txt included in the book’s repository is a small subset of the public data set.
For testing, this subset is sufficient, but you can obviously execute the jobs with the full data set just as well.
The code, shown here in nearly complete form, includes some sort of standard template, and the subsequent examples will not show these boilerplate parts.
MapReduce job that reads from a file and writes into a table.
The row key is the MD5 hash of the line to generate a random key.
Store the original data in a column in the given table.
Parse the command line parameters using the Apache Commons CLI classes.
These are already part of HBase and therefore are handy to process the job specific parameters.
Give the command line arguments to the generic parser first to handle "-Dxyz" properties.
This is a map only job; therefore, tell the framework to bypass the reduce step.
This could be hardcoded here as well, but it is good practice to write your code in a configurable way.
The next step is setting up the job instance, assigning the variable details from the command line, as well as all fixed parameters, such as class names.
One of those is the mapper class, set to ImportMapper.
This class is defined in the same source code file, defining what should be done during the map phase of the job.
It is provided by HBase and allows the job to easily write data into a table.
The key and value types needed by this class is implicitly fixed to ImmutableBytesWritable for the key, and Writable for the value.
Before you can execute the job, you first have to create a target table, for example, using the HBase Shell:
The first command, hadoop dfs -put, stores the sample data in the user’s home directory in HDFS.
The second command launches the job itself, which completes in a short amount of time.
The data is read using the default TextInputFormat, as provided by Hadoop and its MapReduce framework.
This input format can read text files that have As shown in Example 7-1, the ImportMapper defines two methods, overriding the ones with the same name from the parent Mapper class.
Override Woes It is highly recommended to add @Override annotations to your methods, so that wrong methods might be called and do an identity function.
Reducer class, but instead defines a new version of the method.
The MapReduce framework will silently ignore this method and execute the default implementation as provided by the Reducer class.
The reason is that the actual signature of the method is this:
This is a common mistake; the Iterable was erroneously replaced by an Iterator class.
This is all it takes to make for a new signature.
The IDE you are using should already display an error, but at a minimum the compiler will report the mistake:
Here it is used to parse the given column into a column family and qualifier.
The code creates an HBase row key by using an MD5 hash of the line content.
It then stores the line content as-is in the provided column, titled data:json.
The example makes use of the implicit write buffer set up by the TableOutputFormat complete—saving the remaining data in the write buffer.
This is also the reason why the output key format of the job is set to Writable, instead of the explicit Put class.
The TableOutputFormat can (currently) only handle Put and Delete instances.
Passing anything else will raise an IOException with the message set to Pass a Delete or a Put.
Finally, note how the job is just using the map phase, and no reduce is needed.
This is fairly typical with MapReduce jobs in combination with HBase: since data is already stored in sorted tables, or the raw data already has unique keys, you can avoid the more costly sort, shuffle, and reduce phases in the process.
Data Source After importing the raw data into the table, we can use the contained data to parse the JSON records and extract information from it.
This is accomplished using the TableInputFormat class, the counterpart to TableOutputFormat.
It sets up a table as an input to the MapReduce process.
Example 7-2 makes use of the provided InputFor mat class.
MapReduce job that reads the imported data and analyzes it.
Extend the supplied TableMapper class, setting your own output key and value types.
Parse the JSON data, extract the author, and count the occurrence.
Set up the table mapper phase using the supplied utility.
This job runs as a full MapReduce process, where the map phase is reading the JSON data from the input table, and the reduce phase is aggregating the counts for every user.
Executing the job on the command line is done like so:
The end result is a list of counts per author, and can be accessed from the command line using, for example, the hadoop dfs -text command:
The example also shows how to use the TableMapReduceUtil class, with its static methods, to quickly configure a job with all the required classes.
Since the job also needs a implicit use of the default value when no other is specified (in this case, the TextOut putFormat class)
Obviously, this is a simple example, and in practice you will have to perform more involved analytical processing.
But even so, the template shown in the example stays the same: you read from a table, extract the required information, and eventually output the results to a specific target.
Data Source and Sink As already shown, the source or target of a MapReduce job can be a HBase table, but it is also possible for a job to use HBase as both input and output.
In other words, a third kind of MapReduce template uses a table for the input and output types.
This involves setting the TableInputFormat and TableOutputFormat classes into the respective fields of the job configuration.
This also implies the various key and value types, as shown before.
MapReduce job that parses the raw data into separate columns static class ParseMapper.
Store the top-level JSON keys as columns, with their value set as the column value.
Store the column family in the configuration for later use in the mapper.
The example uses the utility methods to configure the map and reduce phases, specifying the ParseMapper, which extracts the details from the raw JSON, and an Identity TableReducer to store the data in the target table.
Launching the job from the command line can be done like this:
The percentages show that both the map and reduce phases have been completed, and that the job overall completed subsequently.
Using the IdentityTableReducer to store the extracted data is not necessary, and in fact the same code with one additional line turns the job into a map-only one.
MapReduce job that parses the raw data into separate columns (map phase only) ...
The reduce stays at 0%, even when the job has completed.
You can also use the Hadoop MapReduce UI to confirm that no reduce task have been executed for this job.
The advantage of bypassing the reduce phase is that the job will complete much faster, since no additional processing of the data by the framework is required.
Both variations of the ParseJson job performed the same work.
The result can be seen using the HBase Shell (omitting the repetitive row key output for the sake of space):
The import makes use of the arbitrary column names supported by HBase: the JSON keys are converted into qualifiers, and form new columns on the fly.
Custom Processing You do not have to use any classes supplied by HBase to read and/or write to a table.
In fact, these classes are quite lightweight and only act as helpers to make dealing with tables easier.
Example 7-5 converts the previous example code to split the parsed JSON data into two target tables.
The link key and its value is stored in a separate table, named linktable, while all other fields are stored in the table named infotable.
MapReduce job that parses the raw data into separate tables static class ParseMapper.
Store table names in configuration for later use in the mapper.
Set the output format to be ignored by the framework.
These two new tables will be used as the target tables for the current example.
So far, this is the same as the previous ParseJson examples.
You can use the HBase Shell and the scan command to list the content of each table after the job has completed.
You should see that the link table contains only the links, while the info table contains the remaining fields of the original JSON.
Writing your own MapReduce code allows you to perform whatever is needed during the job execution.
You can, for example, read lookup values from a different table while storing a combined result in yet another table.
There is no limit as to where you read from, or where you write to.
The supplied classes are helpers, nothing more or less, and serve well for a large number of use cases.
If you find yourself limited by their functionality, simply extend them, or implement generic MapReduce code and use the API to access HBase tables in any shape or form.
It is quite useful for advanced users (or those who are just plain adventurous) to fully comprehend how a system of their choice works behind the scenes.
This chapter explains the various moving parts of HBase and how they work together.
Seek Versus Transfer Before we look into the architecture itself, however, we will first address a more fundamental difference between typical RDBMS storage structures and alternative ones.
Note that RDBMSes do not use B-tree-type structures exclusively, nor do all NoSQL solutions use different architectures.
You will find a colorful variety of mix-and-match technologies, but with one common objective: use the best strategy for the problem at hand.
They represent dynamic, multilevel indexes with lower and upper bounds as far as the number of keys in each segment (also called page) is concerned.
Using these segments, they achieve a much higher fanout compared to binary trees, resulting in a much lower number of I/O operations to find a specific key.
In addition, they also enable you to do range scans very efficiently, since the leaf nodes in the tree are linked and represent an in-order list of all keys, avoiding more costly tree traversals.
That is one of the reasons why they are used for indexes in relational database systems.
That is not a problem until the page, which has a fixed size, exceeds its capacity.
Then it has to split the page into two new ones, and update the parent in the tree to point to the two new half-full pages.
See Figure 8-1 for an example of a page that is full and would need to be split when adding another key.
The issue here is that the new pages aren’t necessarily next to each other on disk.
Log-Structured Merge-Trees Log-structured merge-trees, also known as LSM-trees, follow a different approach.
Incoming data is stored in a logfile first, completely sequentially.
Once the log has the modification saved, it then updates an in-memory store that holds the most recent updates for fast lookup.
When the system has accrued enough updates and starts to fill up the in-memory store, it flushes the sorted list of key → record pairs to disk, creating a new store file.
The store files are arranged similar to B-trees, but are optimized for sequential disk access where all nodes are completely filled and stored as either single-page or multipage blocks.
Updating the store files is done in a rolling merge fashion, that is, the system packs existing on-disk multipage blocks together with the flushed in-memory data until the block reaches its full capacity, at which point a new one is started.
Figure 8-2 shows how a multipage block is merged from the in-memory tree into the next on-disk tree.
Merging writes out a new block with the combined result.
As more flushes are taking place over time, creating many store files, a background process aggregates the files into larger ones so that disk seeks are limited to only a few store files.
The on-disk tree can also be split into separate trees to spread updates across multiple store files.
All of the stores are always sorted by key, so no reordering is required to fit new keys in between existing ones.
Lookups are done in a merging fashion in which the in-memory store is searched first, and then the on-disk store files are searched next.
That way, all the stored data, no matter where it currently resides, forms a consistent view from a client’s perspective.
Deletes are a special case of update wherein a delete marker is stored and is used during the lookup to skip “deleted” keys.
When the pages are rewritten asynchronously, the delete markers and the key they mask are eventually dropped.
An additional feature of the background processing for housekeeping is the ability to support predicate deletions.
These are triggered by setting a time-to-live (TTL) value that retires entries, for example, after 20 days.
The merge processes will check the predicate and, if true, drop the record from the rewritten blocks.
The fundamental difference between B-trees and LSM-trees, though, is how their architecture is making use of modern hardware, especially disk drives.
Seek Versus Sort and Merge in Numbers‡ For our large-scale scenarios, computation is dominated by disk transfers.
As discussed at the beginning of this chapter, there are two different database paradigms: one is seek and the other is transfer.
Seek is typically found in RDBMSes and is caused by the B-tree or B+ tree structures used to store the data.
It operates at the disk seek rate, resulting in log(N) seeks per access.
Transfer, on the other hand, as used by LSM-trees, sorts and merges files while operating at transfer rates, and takes log(updates) operations.
We can safely conclude that, at scale seek, is inefficient compared to transfer.
To compare B+ trees and LSM-trees you need to understand their relative strengths and weaknesses.
B+ trees work well until there are too many modifications, because they force you to perform costly optimizations to retain that advantage for a limited amount of time.
The more and faster you add data at random locations, the faster the pages become fragmented again.
Eventually, you may take in data at a higher rate than the optimization process takes to rewrite the existing files.
The updates and deletes are done at disk seek rates, rather than disk transfer rates.
LSM-trees work at disk transfer rates and scale much better to handle large amounts of data.
They also guarantee a very consistent insert rate, as they transform random writes into sequential writes using the logfile plus in-memory store.
The reads are independent from the writes, so you also get no contention between these two operations.
So, you have a predictable and consistent boundary on the number of disk seeks to access a key, and reading any number of records following that key doesn’t incur any extra seeks.
In general, what could be emphasized about an LSM-tree-based system is cost transparency: you know that if.
The next sections will explain the storage architecture, while referring back to earlier sections of the book where appropriate.
Storage One of the least-known aspects of HBase is how data is actually stored.
While the majority of users may never have to bother with this, you may have to get up to speed when you want to learn the meaning of the various advanced configuration options you have at your disposal.
Chapter 11 lists the more common ones and Appendix A has the full reference list.
You may also want to know more about file storage if, for whatever reason, disaster strikes and you have to recover an HBase installation.
At that point, it is important to know where all the data is stored and how to access it on the HDFS level.
Of course, this shall not happen, but who can guarantee that?
Overview The first step in understanding the various moving parts in the storage layer of HBase is to understand the high-level picture.
Figure 8-3 shows an overview of how HBase and Hadoop’s filesystem are combined to store data.
The figure shows that HBase handles basically two kinds of file types: one is used for the write-ahead log and the other for the actual data storage.
In certain cases, the HMaster will also have to perform low-level file operations.
You may also notice that the actual files are divided into blocks when stored within HDFS.
This is also one of the areas where you can configure the system to handle larger or smaller data records better.
The general communication flow is that a new client contacts the ZooKeeper ensemble (a separate cluster of ZooKeeper nodes) first when trying to access a particular row.
It does so by retrieving the server name (i.e., hostname) that hosts the -ROOT- region from ZooKeeper.
With this information it can query that region server to get the server name that hosts the .META.
Both of these details are cached and only looked up once.
Once it has been told in what region the row resides, it caches this information as well and contacts the HRegionServer hosting that region directly.
The HMaster is responsible for assigning the regions to each HRegion Server when you start HBase.
The HRegionServer opens the region and creates a corresponding HRegion object.
When the HRegion is opened it sets up a Store instance for each HColumnFamily for every table as defined by the user beforehand.
Each Store instance can, in turn, have one or more StoreFile instances, which are lightweight wrappers around the actual storage file called HFile.
Write Path The client issues an HTable.put(Put) request to the HRegionServer, which hands the details to the matching HRegion instance.
The first step is to write the data to the writeahead log (the WAL), represented by the HLog class.§ The WAL is a standard Hadoop SequenceFile and it stores HLogKey instances.
Overview of how HBase handles files in the filesystem, which stores them transparently in HDFS.
In extreme cases, you may turn off this step by setting a flag using the Put.setWriteToWAL(boolean) method.
Once the data is written to the WAL, it is placed in the MemStore.
At the same time, it is checked to see if the MemStore is full and, if so, a flush to disk is requested.
The request is served by a separate thread in the HRegionServer, which writes the data to a new HFile located in HDFS.
It also saves the last written sequence number so that the system knows what was persisted so far.
Preflushing on Stop There is a second reason for memstores to be flushed: preflushing.
When a region server is asked to stop it checks the memstores, and any that has more data than what is configured with the hbase.hregion.preclose.flush.size property (set to 5 MB by default) is first flushed to disk before blocking access to the region for a final round of flushing to close the hosted regions.
Once all memstores are flushed, the regions can be closed and no subsequent logfile replaying is needed when the regions are reopened by a different server.
Using the extra round of preflushing extends availability for the regions: during the preflush, the server and its regions are still available.
This is similar to issuing a flush shell command or API call.
Only when the remaining smaller memstores are flushed in the second round do the regions stop taking any further requests.
This round also takes care of all modifications that came in to any memstore that was preflushed already.
Files HBase has a configurable root directory in HDFS, with the default set to "/hbase"
You can use the hadoop dfs -lsr command to look at the various files HBase stores.
Before doing this, let us first create and fill a table with a handful of regions:
The flush command writes the in-memory data to the store files; otherwise, we would have had to wait until more than the configured flush size of data was inserted into the stores.
The last round of looping over the put command is to fill the write-ahead log again.
Here is the content of the HBase root directory afterward:
The output was reduced to include just the file size and name to fit the available space.
When you run the command on your cluster you will see more details.
The files can be divided into those that reside directly under the HBase root directory, and those that are in the per-table directories.
The first set of files are the write-ahead log files handled by the HLog instances, created in a directory called .logs underneath the HBase root directory.
In each subdirectory, there are several HLog files (because of log rotation)
All regions from that region server share the same HLog files.
An interesting observation is that the logfile is reported to have a size of 0
This is fairly typical when the file was created recently, as HDFS is using built-in append support to write to this file, and only complete blocks are made available to readers—including the hadoop dfs -lsr command.
Although the data of the put operations is safely persisted, the size of the logfile that is currently being written to is slightly off.
The new logfile next to it again starts at zero size:
When a logfile is are no longer needed because all of the contained edits have been persisted into store files, it is decommissioned into the .oldlogs directory under the root HBase directory.
This is triggered when the logfile is rolled based on the configured thresholds.
The old logfiles are deleted by the master after 10 minutes (by default), set with the hbase.master.logcleaner.ttl property.
The hbase.id and hbase.version files contain the unique ID of the cluster, and the file format version:
They are used internally and are otherwise not very interesting.
In addition, there are a few more root-level directories that appear over time.
The splitlog and .corrupt folders are used by the log split process to store the intermediate split files and the corrupted logs, respectively.
There are no corrupt logfiles in this example, but there is one staged split file.
Every table in HBase has its own directory, located under the HBase root directory in the filesystem.
This includes the table and column family schemas, and can be read, for example, by tools to gain insight on what the table looks like.
The .tmp directory contains temporary data, and is used, for example, when the .tableinfo file is updated.
Inside each table directory, there is a separate directory for every region comprising the table.
The names of these directories are the MD5 hash portion of a region name.
For example, the following is taken from the master’s web UI, after clicking on the testta ble link in the User Tables section:
The final dot after the hash is part of the complete region name: it indicates that this is a new style name.
In previous versions of HBase, the region names did not include the hash.
The encoding of the region names for the on-disk directories is also different: they use a Jenkins hash to encode the region name.
The hash guarantees that the directory names are always valid, in terms of filesystem rules: they do not contain any special character, such as the slash (“/”), which is used to divide the path.
Their name is just an arbitrary number, based on the Java builtin random generator.
The code is smart enough to check for collisions, that is, where a file with a newly generated number already exists.
It loops until it finds an unused one and uses that instead.
The region directory also has a .regioninfo file, which contains the serialized information of the HRegionInfo instance for the given region.
Similar to the .tableinfo file, it can be used by external tools to gain insight into the metadata of a region.
The hbase hbck tool uses this to generate missing meta table entries, for example.
The optional .tmp directory is created on demand, and is used to hold temporary files— for example, the rewritten files from a compaction.
These are usually moved out into the region directory once the process has completed.
In rare circumstances, you might find leftover files, which are cleaned out when the region is reopened.
During the replay of the write-ahead log, any edit that has not been committed is written into a separate file per region.
When the region is opened the region server will see the recovery file and replay the entries accordingly.
Sometimes it is difficult to distinguish the file and directory names in the filesystem, because both might refer to the term splits.
Make sure you carefully identify their purpose to avoid confusion—or mistakes.
Once the region needs to split because it has exceeded the maximum configured region size, a matching splits directory is created, which is used to stage the two new daughter regions.
In other words, when you see a region directory that has no .tmp directory, no compaction has been performed for it yet.
When it has no recovered.edits file, no writeahead log replay has occurred for it yet.
In HBase versions before 0.90.x there were additional files, which are now obsolete.
One is oldlogfile.log, which contained the replayed writeahead log edits for the given region.
The oldlogfile.log.old file (note the extra .old extension) indicated that there was already an existing oldlogfile.log file when the new one was put into place.
Another noteworthy file is the compaction.dir file in older versions of HBase, which is now replaced by the .tmp directory.
This concludes the list of what is commonly contained in the various directories inside the HBase root folder.
There are more intermediate files, created by the region split process.
This is done initially very quickly because the system simply creates two reference files for the new regions (also called daughters), which each hosting half of the original region (referred to as the parent)
The region server accomplishes this by creating the splits directory in the parent region.
Next, it closes the region so that it does not take on anymore requests.
The region server then prepares the new daughter regions (using multiple threads) by setting up the necessary file structures inside the splits directory.
This includes the new region directories and the reference files.
If this process completes successfully, it moves the two new region directories into the table directory.
Here is an example of how this looks in the .META.
You can see how the original region was split into two regions, separated at row-550
The name of the reference file is another random number, but with the hash of the referenced region as a postfix, for instance:
This reference file represents one-half of the original region with the hash d9ffc3a5cd016ae58e23d7a6cb937949, which is the region shown in the preceding example.
The reference files only hold a little information: the key the original region was split at, and whether it is the top or bottom reference.
Of note is that these references are then used by the HalfHFileReader class (which was omitted from the earlier overview as it is only used temporarily) to read the original region data files, and either the top or the bottom half of the files.
Both daughter regions are now ready and will be opened in parallel by the same server.
After that, the regions are online and start serving requests.
The opening of the daughters also schedules a compaction for both—which rewrites the store files in the background from the parent region into the two halves, while replacing the reference files.
This takes place in the .tmp directory of the daughter regions.
Once the files have been generated, they atomically replace the reference.
The parent is eventually cleaned up when there are no more references to it, which means it is removed as the parent from the .META.
Finally, the master is informed about the split and can schedule for the new regions to be moved off to other servers for load balancing reasons.
All of the steps involved in the split are tracked in ZooKeeper.
This allows for other processes to reason about the state of a region in case of a server failure.
The store files are monitored by a background thread to keep them under control.
The flushes of memstores slowly build up an increasing number of on-disk files.
If there are enough of them, the compaction process will combine them to a few, larger files.
Minor compactions are responsible for rewriting the last few files into one larger one.
The number of files is set with the hbase.hstore.compaction.min property (which was previously called hbase.hstore.compactionThreshold, and although deprecated is still supported)
A number too large would delay minor compactions, but also would require more resources and take longer once the compactions start.
The maximum number of files to include in a minor compaction is set to 10, and is configured with hbase.hstore.compaction.max.
The list is further narrowed down by the hbase.hstore.compaction.min.size (set to the configured memstore flush size for the region), and the hbase.hstore.compaction.max.size (defaults to Long.MAX_VALUE) configuration properties.
Any file larger than the maximum compaction size is always excluded.
The minimum compaction size works slightly differently: it is a threshold rather than a per-file limit.
It includes all files that are under that limit, up to the total number of files per compaction allowed.
All files that fit under the minimum compaction threshold are included in the compaction process.
A set of store files showing the minimum compaction threshold.
The ratio will also select files that are up to that size compared to the sum of the store file sizes of all newer files.
The evaluation always checks the files from the oldest to the newest.
The combination of these properties allows you to fine-tune how many files are included in a minor compaction.
In contrast to minor compactions, major compactions compact all files into a single file.
Which compaction type is run is automatically determined when the compaction check is executed.
The check is triggered either after a memstore has been flushed to disk, after the compact or major_compact shell commands or corresponding API calls have been invoked, or by a background thread.
This background thread is called the CompactionChecker and each region server runs a single instance.
It runs a check on a regular basis, controlled by hbase.server.thread.wakefrequency (and multiplied by hbase.server.thread.wakefrequency.multiplier, set to 1000, to run it less often than the other thread-based tasks)
Otherwise, the server checks first if the major compaction is due, based on hbase.hregion.majorcompaction (set to 24 hours) from the first time it ran.
Without the jitter, all stores would run a major compaction at the same time, every 24 hours.
If no major compaction is due, a minor compaction is assumed.
Based on the aforementioned configuration properties, the server determines if enough files for a minor compaction are available and continues if that is the case.
Minor compactions might be promoted to major compactions when the former would include all store files, and there are less than the configured maximum files per compaction.
HFile Format The actual storage files are implemented by the HFile class, which was specifically created to serve one purpose: store HBase’s data efficiently.
They are based on Hadoop’s TFile class,‖ and mimic the SSTable format used in Google’s Bigtable architecture.
The previous use of Hadoop’s MapFile class in HBase proved to be insufficient in terms of performance.
The files contain a variable number of blocks, where the only fixed ones are the file info and trailer blocks.
As Figure 8-5 shows, the trailer has the pointers to the other blocks.
It is written after the data has been persisted to the file, finalizing the now immutable data store.
The index blocks record the offsets of the data and meta blocks.
Both the data and the meta blocks are actually optional.
But considering how HBase uses the data files, you will almost always find at least data blocks in the store files.
The block size is configured by the HColumnDescriptor, which, in turn, is specified at table creation time by the user, or defaults to reasonable standard values.
Here is an example as shown in the master web-based interface:
Here is what the HFile JavaDoc explains: Minimum block size.
Larger block size is preferred if files are primarily for sequential access.
However, it would lead to inefficient random access (because there are more data to decompress)
Smaller blocks are good for random access, but require more memory to hold the block index, and may be slower to create (because we must flush the compressor stream at the conclusion of each data block, which leads to an FS I/O flush)
Further, due to the internal caching in Compression codec, the smallest possible block size would be around 20KB-30KB.
If you are not using a compression algorithm, each block is about as large as the configured block size.
This is not an exact science, as the writer has to fit whatever you give it: if you store a KeyValue that is larger than the block size, the writer has to accept that.
But even with smaller values, the check for the block size is done after the last value was written, so in practice, the majority of blocks will be slightly larger.
When you are using a compression algorithm you will not have much control over block size.
Compression codecs work best if they can decide how much data is enough to achieve an efficient compression ratio.
Many compression libraries come with a set of configuration properties you can use to specify the buffer size, and other options.
Refer to the source code of the JNI library to find out what is available to you.
The writer does not know if you have a compression algorithm selected or not: it follows the block size limit to write out raw data close to the configured amount.
If you have compression enabled, less data will be saved less.
As such, the HBase storage file blocks do not match the Hadoop blocks.
In fact, there is no correlation between these two block types.
And HDFS also does not know what HBase stores; it only sees binary files.
Figure 8-6 demonstrates how the HFile content is simply spread across HDFS blocks.
HFile content spread across HDFS blocks when many smaller HFile blocks are transparently stored in two HDFS blocks that are much larger.
Sometimes it is necessary to be able to access an HFile directly, bypassing HBase, for.
The first part of the output is the actual data stored as serialized KeyValue instances.
The second part dumps the internal HFile.Reader properties, as well as the trailer block details.
The last part, starting with Fileinfo, is the file info block values.
The provided information is valuable to, for example, confirm whether a file is compressed or not, and with what compression type.
It also shows you how many cells you have stored, as well as the average size of their keys and values.
In the preceding example, the key is much larger than the value.
This is caused by the overhead required by the KeyValue class to store the necessary data, explained next.
KeyValue Format In essence, each KeyValue in the HFile is a low-level byte array that allows for zerocopy access to the data.
The structure starts with two fixed-length numbers indicating the size and value of the key.
With that information, you can offset into the array to, for example, get direct access to the value, ignoring the key.
Otherwise, you can get the required information from the key.
The reason the average key in the preceding example is larger than the value has to do with the fields that make up the key part of a KeyValue.
The key holds the row key, the column family name, the column qualifier, and so on.
For a small payload, this results in quite a considerable overhead.
If you deal with small values, try to keep the key small as well.
Choose a short row and column key (the family name with a single byte, and the qualifier equally short) to keep the ratio in check.
On the other hand, compression should help mitigate the overwhelming key size problem, as it looks at finite windows of data, and all repeating data should compress well.
The sorting of all KeyValues in the store file helps to keep similar keys (and possibly values too, in case you are using versioning) close together.
Write-Ahead Log The region servers keep data in-memory until enough is collected to warrant a flush to disk, avoiding the creation of too many very small files.
While the data resides in memory it is volatile, meaning it could be lost if the server loses power, for example.
The server then has the liberty to batch or aggregate the data in memory as needed.
Overview The WAL is the lifeline that is needed when disaster strikes.
Similar to a binary log in MySQL, the WAL records all changes to the data.
If the server crashes, the WAL can effectively replay the log to get everything up to where the server should have been just before the crash.
It also means that if writing the record to the WAL fails, the whole operation must be considered a failure.
Since it is shared by all regions hosted by the same region server, it acts as a central logging backbone for every modification.
Figure 8-8 shows how the flow of edits is split between the memstores and the WAL.
All modifications saved to the WAL, and then passed on to the memstores.
The process is as follows: first the client initiates an action that modifies data.
This can is wrapped into a KeyValue object instance and sent over the wire using RPC calls.
The calls are (ideally) batched to the HRegionServer that serves the matching regions.
Once the KeyValue instances arrive, they are routed to the HRegion instances that are responsible for the given rows.
The data is written to the WAL, and then put into the MemStore of the actual Store that holds the record.
Eventually, when the memstores get to a certain size, or after a specific time, the data is persisted in the background to the filesystem.
During that time, data is stored in a volatile state in memory.
The WAL guarantees that the data is never lost, even if the server fails.
Keep in mind that the actual log resides on HDFS, which is a replicated filesystem.
Any other server can open the log and start replaying the edits—nothing on the failed physical server is needed to effect a full recovery.
When an HRegion is instantiated, the single HLog instance that runs inside each region server is passed on as a parameter to the constructor of HRegion.
When a region receives an update operation, it can save the data directly to the shared WAL instance.
By default, you certainly want the WAL, no doubt about that.
But say you run a large bulk import MapReduce job that you can rerun at any time.
You gain extra performance when you disable the WAL, but at the cost of having to take extra care that no data was lost during the import.
You are strongly advised not to lightheartedly turn off writing edits to the WAL.
If you do so, you will lose data sooner or later.
And no, HBase cannot recover data that is lost and that has not been written to the log first.
Another important feature of HLog is the ability to keep track of changes.
It uses an AtomicLong internally to be thread-safe and starts at either zero, or the last known number persisted to the filesystem: as the region is opening its storage files, it reads the highest sequence number, which is stored as a meta field in each HFile and sets the HLog sequence number to that value if it is higher than what was recorded before.
So, after it has opened all the storage files, the HLog is initialized to reflect where persisting ended and where to continue.
Figure 8-9 shows three different regions, hosted on the same region server, with each of them covering a different row key range.
This means the data is written to the WAL in the order it arrives.
But since this happens rather seldomly, the WAL is optimized to store data sequentially, giving it the best I/O performance.
The WAL saving edits in the order they arrive, spanning all regions of the same server.
HLogKey Class Currently, the WAL uses a Hadoop SequenceFile, which stores records as sets of key/ values.
For the WAL, the value is simply the modification(s) sent from the client.
The key is represented by an HLogKey instance: since the KeyValue only represents the row key, column family, column qualifier, timestamp, type, and value, there has to be a place to store what the KeyValue belongs to, in other words, the region and table name.
That number is incremented with each edit in order to keep a sequential order of edits.
This class also records the write time, which is a timestamp that denotes when the edit was written to the log.
Finally, it stores the cluster ID, which is needed for replication across clusters.
WALEdit Class Every modification sent by a client is wrapped into a WALEdit instance, which takes care of atomicity at the log level.
Each column, or cell, is represented as a separate KeyValue instance.
If the server writes five of them to the WAL and then fails, you will end up with a half-persisted row mutation.
Atomicity is guaranteed by bundling all updates that comprise multiple cells into a single WALEdit instance.
This group of edits is then written in a single operation, ensuring that the row mutation is applied in full or not at all.
Before version 0.90.x, HBase did save the KeyValue instances separately.
The default is false and it means that every time forces the update to the log to be acknowledged by the filesystem so that you have durability.
Unfortunately, calling this method involves a pipelined write to N servers (where N is of data loss in case of a server failure.
Pipeline Versus n-Way Writes is written, it is sent to the first data node to persist it.
Once that has succeeded, it is sent by that data node to another data node to do the same thing, and so on.
Only when all three have acknowledged the write operation is the client allowed to proceed.
Another approach to saving edits durably is the n-way write, where the write is sent to three machines at the same time.
The difference between pipelined and n-way writes is that a pipelined write needs time to complete, and therefore has a higher latency.
An n-way write has lower latency, as the client only needs to wait for the slowest data node to acknowledge (assuming the others have already reported back success)
However, an n-way write needs to share the network bandwidth of the sending server, which can cause a bottleneck for heavily loaded systems.
There is work in progress to have support for both in HDFS, giving you the choice to use the one that performs best for your application.
Setting the deferred log flush flag to true causes the edits to be buffered on the region second and is configured by the hbase.regionserver.optionallogflushinterval property.
Note that this only applies to user tables: all catalog tables are always synced right away.
LogRoller Class There are size restrictions when it comes to the logs that are written.
The LogRoller class runs as a background thread and takes care of rolling logfiles at certain intervals.
This is controlled by the hbase.regionserver.logroll.period property, set by default to one hour.
Every 60 minutes the log is closed and a new one is started.
Over time, the system accumulates an increasing number of logfiles that need to be managed as well.
The It checks what the highest sequence number written to a storage file is.
This is the edit sequence number of the last edit persisted out to the filesystem.
It then checks if there is a log left that has edits that are all less than that number.
If that is the case, it moves said logs into the .oldlogs directory, and leaves the remaining ones in place.
This message is printed because the configured maximum number of logfiles to keep exceeds the number of logfiles that are required to be kept because they still contain outstanding edits that have not yet been persisted.
This can occur when you stress out the filesystem to such an extent that it cannot persist the data at the rate at which new data is added.
Note, though, that when this message is printed the server goes into a special mode trying to force edits to be flushed out to reduce the number of outstanding WAL files.
So logs are switched out when they are considered full, or when a certain amount of time has passed—whatever comes first.
Replay The master and region servers need to orchestrate the handling of logfiles carefully, especially when it comes to recovering from server failures.
The WAL is responsible for retaining the edits safely; replaying the WAL to restore a consistent state is a much more complex exercise.
If we kept the commit log for each tablet in a separate logfile, a very large number of files would be written concurrently in GFS.
Depending on the underlying file system implementation on each GFS server, these writes could cause a large number of disk seeks to write to the different physical log files.
HBase followed that principle for pretty much the same reasons: writing too many files at the same time, plus the number of rolled logs that need to be kept, does not scale well.
What is the drawback, though? If you have to split a log because of a server crash, you need to divide it into suitable pieces, as described in the next section.
The master cannot redeploy any region from a crashed server until the logs for that very server have been split.
There are two situations in which logfiles have to be replayed: when the cluster starts, or when a server fails.
The logs’ names contain not just the server name, but also the start code of the server.
This number is reset every time a region server restarts, and the master can use this number to verify whether a log has been abandoned—for example, due to a server crash.
The master is responsible for monitoring the servers using ZooKeeper, and if it detects a server failure, it immediately starts the process of recovering its logfiles, before reassigning the regions to new servers.
Before the edits in the log can be replayed, they need to be separated into one logfile per region.
This process is called log splitting: the combined log is read and all entries are grouped by the region they belong to.
These grouped edits are then stored in a file next to the target region for subsequent recovery.
The actual process of splitting the logs is different in nearly every version of HBase: early versions would read the file in a single thread, directly on the master.
This was improved to at least write the grouped edits per region in multiple threads.
Version 0.92.0 finally introduces the concept of distributed log splitting, which removes the burden of doing the actual work from the master to all region servers.
Consider a larger cluster with many region servers and many (rather large) logfiles.
This meant that, for any region that had pending edits, it had to be blocked from opening until the log split and recovery had been completed.
The new distributed mode uses ZooKeeper to hand out each abandoned logfile to a region server.
They monitor ZooKeeper for available work, and if the master indicates that a log is available for processing, they race to accept the task.
The winning region server then proceeds to read and split the logfiles in a single thread (so as not to overload the already busy region server)
You can turn the new distributed log splitting off by means of the hbase.master.distributed.log.splitting configuration property.
Setting this property to false disables distributed splitting, and falls back to doing the work directly on the master only.
In nondistributed mode the writers are multithreaded, controlled by the hbase.regionserver.hlog.splitlog.writer.threads property, which is set to 3 by default.
You need to be careful when increasing this number, as you are likely bound by the performance of the single log reader.
The split process writes the edits first into the splitlog staging directory under the HBase root folder.
They are placed in the same path that is needed for the target region.
The path contains the logfile name itself to distinguish it from other, possibly concurrently executed, log split output.
The path also contains the table name, region name (hash), and recovered.edits directory.
Lastly, the name of the split file is the sequence ID of the first edit for the particular region.
The .corrupt directory contains any logfile that could not be parsed.
This is influenced by the hbase.hlog.split.skip.errors property, which is set to true by default.
It means that any edit that could not be read from a file causes the entire log to be moved to the .corrupt folder.
If you set the flag to false, an IOExecption is thrown and the entire log splitting process is stopped.
Once a log has been split successfully, the per-region files are moved into the actual region directories.
They are now ready to be recovered by the region itself.
This is also why the splitting has to stall opening the affected regions, since it first has to provide the pending edits for replay.
When a region is opened, either because the cluster is started or because it has been moved from one region server to another, it first checks for the presence of the recovered.edits directory.
If it exists, it opens the contained files and starts reading the edits they contain.
The files are sorted by their name, which contains the sequence ID.
This allows the region to recover the edits in order.
Any edit that has a sequence ID that is less than or equal to what has been persisted in the on-disk store files is ignored, because it has already been applied.
All other edits are applied to the matching memstore of the region to recover the previous state.
At the end, a flush of the memstores is forced to write the current state to disk.
The files in the recovered.edits folder are removed once they have been read and their edits persisted to disk.
If a file cannot be read, the hbase.skip.errors property defines what happens next: the default value is false and causes the entire region recovery to fail.
If this property is set to true, the file is renamed to the original filename plus .<currentTimeMillis>
Either way, you need to carefully check your logfiles to determine why the recovery has had issues and fix the problem to continue.
Durability You want to be able to rely on the system to save all your data, no matter what newfangled algorithms are employed behind the scenes.
As far as HBase and the log are concerned, you can set the log flush times to be as low as you want, or sync them for every edit—you are still dependent on the underlying filesystem as mentioned earlier; the stream used to store the data is flushed, but is it written to disk yet? We are talking about fsync style issues.
Now for HBase we are most likely dealing with Hadoop’s HDFS as being the filesystem that is persisted to.
At this point, it should be abundantly clear that the log is what keeps data safe.
It is being kept open for up to an hour (or more if configured to do so), and as data arrives a new key/value pair is written to the SequenceFile.
Eventually, the log is rolled and a new one is created.
But that is not how Hadoop was designed to work.
Hadoop provides an API tailored to MapReduce that allows you to open a file, write data into it (preferably a lot), and close it right away, leaving an immutable file for everyone else to read many times.
Only after a file is closed is it visible and readable to others.
If a process dies while writing the data, the file is considered lost.
For HBase to be able to work properly, what is required is a feature that allows you to read the log up to the point where the crashed server has written it.
This was added to HDFS in later versions and is referred to as append.
Append is the feature needed by HBase to guarantee durability, but previous versions of Hadoop did not offer it.
Support was added over a longer period of time and in a list of patches.
It was committed in Hadoop 0.19.0 and was meant to solve the problem.
But that was not the case: the append in Hadoop 0.19.0 was so badly suited that a hadoop fsck / would report the HDFS as being corrupt because of the open logfiles HBase kept.
Then came HDFS-265, which revisits the append idea in general.
It also introduces a method: it writes a synchronization marker into the file, which helps when reading it later—or recovers data from a corrupted sequence file.
HBase currently detects whether the underlying Hadoop library has support for the abandoned logfile up to the last edits.
Read Path HBase uses multiple store files per column family, which contain the actual cells, or KeyValue instances.
These files are created over time as modifications aggregated in the memstores are eventually flushed as store files to disk.
The background process of compactions keeps the number of files under control by rewriting smaller files into larger ones.
Major compactions eventually compact the entire set of files into a single one, after which the flushes start adding smaller files again.
Since all store files are immutable, there is no way to delete a particular value out of them, nor does it make sense to keep rewriting large store files to remove the deleted cells one by one.
Consider you are writing a column in a given row today.
You keep adding data in other rows over a few more days, then you write a different column in the given row.
But in reality, the data lives as separate KeyValue instances, spread across any number of store files.
If you are deleting the initial column value, and you perform the get again, you expect the value to be gone, when in fact it still exists somewhere, but the tombstone marker is indicating that you have deleted it.
HBase solves the problem by using a QueryMatcher in combination with a ColumnTracker, which comes in a few variations: one for explicit matching, for when you specify a list of columns to retrieve, and another that includes all columns.
Both allow you to set the maximum number of versions to match.
They keep track of what needs to be included in the final result.
Why Gets Are Scans In previous versions of HBase, the Get method was implemented as a separate code path.
This was changed in recent versions and completely replaced internally by the same code that the Scan API uses.
You may wonder why that was done since a straight Get should be faster than a Scan.
A separate code path could take care of some sort of special knowledge to quickly access the data the user is asking for.
That is where the architecture of HBase comes into play.
There are no index files that allow such direct access of a particular row or column.
The smallest unit is a block in an HFile, and to find the requested data the RegionServer code and its underlying Store instances must load a block that could potentially have that data stored and scan through it.
In other words, a Get is nothing but a scan of a single row.
Before all the store files are read to find a matching entry, a quick exclusion check is conducted, which uses the timestamps and optional Bloom filter to skip files that definitely have no KeyValue belonging to the row in question.
The remaining store files, including the memstore, are then scanned to find a matching key.
The scan is implemented by the RegionScanner class, which retrieves a StoreScanner for every Store instance—each representing a column family.
If the read operation excludes certain column families, their stores are omitted as well.
The StoreScanner class combines the store files and memstore that the Store instance contains.
It is also where the exclusion happens, based on the Bloom filter, or the timestamp.
If you are asking for versions that are not more than 30 minutes old, for example, you can skip all storage files that are older than one hour: they will not contain anything of interest.
The StoreScanner class also has the QueryMatcher (here the ScanQueryMatcher class), which will keep track of which KeyValues to include in the final result.
The RegionScanner internally is using a KeyValueHeap class to arrange all store scanners ordered by timestamps.
The StoreScanner is using the same to order the stores the same way.
This guarantees that you are reading KeyValues in their correct order (e.g., descending by timestamp)
When the store scanners are opened, they will position themselves at the requested row now ready to read data.
Figure 8-10 shows an example of what this looks like.
Rows stored and scanned across different stores, on disk or in memory.
Consider a column that has three versions, and you are requesting.
The three KeyValue instances could be spread across any store, next row is reached, or enough versions have been found.
At the same time, it keeps track of delete markers too.
As it scans through the Key Values of the current row, it will come across these delete markers and note that anything with a timestamp that is less than or equal to the marker is considered erased.
Figure 8-10 also shows the logical row as a list of KeyValues, some in the same store file, some on other files, spanning multiple column families.
A store file and a memstore were skipped because of the timestamp and Bloom filter exclusion process.
The delete marker in the last store file is masking out entries, but they are still all part of the same row.
The final result is a list of KeyValue instances that matched the given get or scan operation.
The list is sent back to the client, which can then use the API methods to access the contained columns.
Region Lookups For the clients to be able to find the region server hosting a specific row key range, HBase provides two special catalog tables, called -ROOT- and .META..* The -ROOT- table is used to refer to all regions in the .META.
The second level is the lookup of a matching meta region from the -ROOT- table, and the third is the retrieval of the user table region from the .META.
The row keys in the catalog tables are the region names, which are a concatenation of the region’s table name, its start row, and an ID (usually the current time in milliseconds)
As of HBase 0.90.0 these keys may have another hashed value attached to them.
Subsequently, they are referred to interchangeably as root table and meta table, respectively, since, for example, "-ROOT-" is how the table is actually named in HBase and calling it a root table is stating its purpose.
Avoiding any concerns about the three-level location scheme, the Bigtable paper states that with average limits on the .META.
Since the size of the regions can be increased without any impact on the location scheme, this is a conservative number and can be increased as needed.
Although clients cache region locations, there is an initial need to figure out where to send requests when looking for a specific row key—or when the cache is stale and a region has since been split, merged, or moved.
The client library uses a recursive discovery process moving up in the hierarchy to find the current information.
It asks the corresponding region server hosting the matching .META.
If that information is invalid, it backs out, asking the root table where the .META.
Eventually, if all else fails, it has to do a read of the ZooKeeper node to find the root table region.
In a worst-case scenario, it would need six network round-trips to discover the user region, since stale entries in the cache are only discovered when the lookup fails, because it is assumed that assignments, especially of meta regions, do not change too often.
When the cache is empty, the client needs three network round-trips to update its cache.
One way to mitigate future round-trips is to prefetch location information in a single request, thus updating the client cache ahead of time.
Figure 8-11 shows the mapping of user table regions, through meta, and finally to the root table information.
Once the user table region is known, it can be accessed directly without any further lookups.
However, if the cache were filled with only stale details, the client would fail on all three lookups, requiring a refresh of all three and resulting in the aforementioned six network round-trips.
Mapping of user table regions, starting with an empty cache and then performing three lookups.
The Region Life Cycle The state of a region is tracked by the master, using the AssignmentManager class.
It follows the region from its offline state, all the way through its life cycle.
Pending Open A request to open the region was sent to the server.
Pending Close A request to close the region has been sent to the server.
Closing The server is in the process of closing the region.
The transitions between states are commonly initiated by the master, but may also be initiated by the region server hosting the region.
For example, the master assigns a region to a server, which is then opened by the assignee.
On the other hand, the region server starts the split process, which in itself triggers multiple region close and open events.
Because of the distributed nature of these events, the servers are using ZooKeeper to track specific states in a dedicated znode.
ZooKeeper Since version 0.20.x, HBase has been using ZooKeeper as its distributed coordination service.
This includes tracking of region servers, where the root region is hosted, and more.
Version 0.90.x introduced a new master implementation which has an even tighter integration with ZooKeeper.
It enables HBase to remove critical heartbeat messages that needed to be sent between the master and the region servers.
These are now moved into ZooKeeper, which informs either party of changes whenever they occur, as opposed to the fixed intervals that were used before.
HBase creates a list of znodes under its root node.
The default is /hbase and is configured with the zookeeper.znode.parent property.
Here is the list of the contained znodes and their purposes:
The examples use the ZooKeeper command-line interface (CLI) to issue the commands.
The output of each command was shortened by the ZooKeeper internal details.
Each znode inside is ephemeral and its name is the server name of the region server.
It contains the time when the cluster was started, and is empty when it was shut down.
These examples list various things: you can see how a log to be split was first unassigned, and then owned by a region server.
The RESCAN nodes are signifying that the workers, that is, the region server, is supposed to check for more work, in case a split has failed on another machine.
The name of the table is the newly created znode, and its content is the word DISABLED.
It contains znodes for those regions that are not open, but are in a transitional state.
The name of the znode is the hash of the region.
Replication HBase replication is a way to copy data between HBase deployments.
It can serve as a disaster recovery solution and can contribute to provide higher availability at the HBase layer.
It can also serve a more practical purpose; for example, as a way to easily copy edits from a web-facing cluster to a MapReduce cluster that will process old and new data and ship back the results automatically.
The basic architecture pattern used for HBase replication is “(HBase cluster) masterpush”; this pattern makes it much easier to keep track of what is currently being replicated since each region server has its own write-ahead log (WAL or HLog), just like other well-known solutions, such as MySQL master/slave replication, where there is only one binary log to keep track of.
One master cluster can replicate to any number of slave clusters, and each region server will participate to replicate its own stream of edits.
The replication is done asynchronously, meaning that the clusters can be geographically distant, the links between them can be offline for some time, and rows inserted on the master cluster will not be available at the same time on the slave clusters (eventual consistency)
The replication format used in this design is conceptually the same as MySQL’s statement-based replication.† Instead of SQL statements, whole WALEdits (consisting of multiple cell inserts coming from the clients’ Put and Delete) are replicated in order to maintain atomicity.
The HLogs from each region server are the basis of HBase replication, and must be kept in HDFS as long as they are needed to replicate data to any slave cluster.
Each region server reads from the oldest log it needs to replicate and keeps the current position inside ZooKeeper to simplify failure recovery.
That position can be different for every slave cluster, as can the queue of HLogs to process.
The clusters participating in replication can be of asymmetric sizes and the master cluster will do its best effort to balance the stream of replication on the slave clusters by relying on randomization.
Life of a Log Edit The following sections describe the life of a single edit going from a client that communicates with a master cluster all the way to a single slave cluster.
The client uses an HBase API that sends a Put, Delete, or Increment to a region server.
The key/values are transformed into a WALEdit by the region server and the WALEdit is inspected by the replication code that, for each family that is scoped for replication, adds the scope to the edit.
The edit is appended to the current WAL and is then applied to its MemStore.
In a separate thread, the edit is read from the log (as part of a batch) and only the KeyValues that are replicable are kept (i.e., they are part of a family that is scoped as GLOBAL in the family’s schema and are noncatalog so it is not .META.
When the buffer is filled, or the reader hits the end of the file, the buffer is sent to a random region server on the slave cluster.
Synchronously, the region server that receives the edits reads them sequentially and separates each of them into buffers, one per table.
Once all edits are read, each buffer is flushed using the normal HBase client (HTables managed by an HTablePool)
This is done in order to leverage parallel insertion (MultiPut)
Back in the master cluster’s region server, the offset for the current WAL that is being replicated is registered in ZooKeeper.
In a separate thread, the region server reads, filters, and buffers the log edits the same way as is done during normal processing.
The slave region server that is contacted does not answer to the RPC, so the master region server will sleep and retry up to a configured number of times.
If the slave region server still is not available, the master cluster region server will select a new subset of the region server to replicate to and will try to send the buffer of edits again.
In the meantime, the WALs will be rolled and stored in a queue in ZooKeeper.
Logs that are archived by their region server (archiving is basically moving a log from the region server’s logs directory to a central logs archive directory) will update their paths in the in-memory queue of the replicating thread.
When the slave cluster is finally available, the buffer will be applied the same way as during normal processing.
The master cluster region server will then replicate the backlog of logs.
Internals This section describes in depth how each of the replication’s internal features operates.
When a master cluster region server initiates a replication source to a slave cluster, it first connects to the slave’s ZooKeeper ensemble using the provided cluster key (that key is composed of the value of hbase.zookeeper.quorum, zookeeper.znode.parent, and hbase.zookeeper.property.clientPort)
It then scans the /hbase/rs directory to discover all the available sinks (region servers that are accepting incoming streams of edits to replicate) and will randomly choose a subset of them using a configured ratio (which has a default value of 10%)
Since this is done by all master cluster region servers, the probability that all slave region servers are used is very high, and this method works for clusters of any size.
Every master cluster region server has its own znode in the replication znodes hierarchy.
The parent znode contains one znode per peer cluster (if there are five slave clusters, five znodes are created), and each of these contains a queue of HLogs to process.
Each of these queues will track the HLogs created by that region server, but they can differ in size.
For example, if one slave cluster becomes unavailable for some time, the HLogs.
When a source is instantiated, it contains the current HLog that the region server is writing to.
During log rolling, the new file is added to the queue of each slave cluster’s znode just before it is made available.
This ensures that all the sources are aware that a new log exists before HLog is able to append edits into it, but this operation is now more expensive.
The queue items are discarded when the replication thread cannot read more entries from a file (because it reached the end of the last block) and that there are other files in the queue.
This means that if a source is up-to-date and replicates from the log that the region server writes to, reading up to the “end” of the current file will not delete the item in the queue.
When a log is archived (because it is not used anymore or because there are too many of them per hbase.regionserver.maxlogs, typically because the insertion rate is faster than the region flushing rate), it will notify the source threads that the path for that log changed.
If a particular source was already done with it, it will just ignore the message.
If it is in the queue, the path will be updated in memory.
If the log is currently being replicated, the change will be done atomically so that the reader does not try to open the file when it is already moved.
Also, moving a file is a NameNode operation; so, if the reader is currently reading the log, it will not generate any exceptions.
By default, a source will try to read from a logfile and ship log entries as quickly as possible to a sink.
This is first limited by the filtering of log entries; only KeyValues that are scoped GLOBAL and that do not belong to catalog tables will be retained.
A second limit is imposed on the total size of the list of edits to replicate per slave, which by default is 64 MB.
This means that a master cluster region server with three slaves will use, at most, 192 MB to store data to replicate.
This does not take into account the data that was filtered but was not garbage-collected.
Once the maximum number of edits has been buffered or the reader has hit the end of the logfile, the source thread will stop reading and will randomly choose a sink to replicate to (from the list that was generated by keeping only a subset of slave region servers)
It will directly issue an RPC to the chosen machine and will wait for the method to return.
If it is successful, the source will determine if the current file is emptied or if it should continue to read from it.
If the former, it will delete the znode in the queue.
If the latter, it will register the new offset in the log’s znode.
If the RPC threw an exception, the source will retry 10 times until trying to find a different sink.
If replication is not enabled, the master’s log cleaning thread will delete old logs using a configured TTL.
This does not work well with replication since archived logs that are past their TTL may still be in a queue.
If it is not found, the log will be deleted.
The next time it has to look for a log, it will first use its cache.
As long as region servers do not fail, keeping track of the logs in ZooKeeper does not add any value.
Unfortunately, they do fail, so since ZooKeeper is highly available, we can count on it and its semantics to help us manage the transfer of the queues.
All the master cluster region servers keep a watcher on one another to be notified when one dies (just like the master does)
When this happens, they all race to create a znode called lock inside the dead region server’s znode that contains its queues.
The one that creates it successfully will proceed by transferring all the queues to its own znode (one by one, since ZooKeeper does not support the rename operation) and will delete all the old ones when it is done.
The recovered queues’ znodes will be named with the ID of the slave cluster appended with the name of the dead server.
Once that is done, the master cluster region server will create one new source thread per copied queue, and each of them will follow the read/filter/ship pattern.
The main difference is that those queues will never have new data since they do not belong to their new region server, which means that when the reader hits the end of the last log, the queue’s znode will be deleted and the master cluster region server will close that replication source.
For example, consider a master cluster with three region servers that is replicating to a single slave with an ID of 2
The following hierarchy represents what the znodes’ layout could be at some point in time.
We can see that the region servers’ znodes all contain a peers znode that contains a single queue.
The znode names in the queues represent the actual filenames on HDFS in the form address,port.timestamp.
The survivors will race to create a lock, and for some reason 1.1.1.3 wins.
Right before 1.1.1.3 is able to clean up the old znodes, the layout will look like the following:
The last region server will then try to lock 1.1.1.3’s znode and will begin transferring all the queues.
Carefully evaluate whether it works for your use case before you consider using it.
This chapter goes deeper into the various design implications imposed by HBase’s storage architecture.
It is important to have a good understanding of how to design tables, row keys, column names, and so on, to take full advantage of the architecture.
Key Design HBase has two fundamental key structures: the row key and the column key.
Both can be used to convey meaning, by either the data they store, or by exploiting their sorting order.
In the following sections, we will use these keys to solve commonly found problems when designing storage solutions.
Concepts The first concept to explain in more detail is the logical layout of a table, compared to on-disk storage.
HBase’s main unit of separation within a table is the column family— not the actual columns as expected from a column-oriented database in their traditional sense.
Figure 9-1 shows the fact that, although you store cells in a table format logically, in reality these rows are stored as linear sets of the actual cells, which in turn contain all the vital information inside them.
The top-left part of the figure shows the logical layout of your data—you have rows and columns.
The columns are the typical HBase combination of a column family name and a column qualifier, forming the column key.
The rows also have a row key so that you can address all columns in one logical row.
The top-right hand side shows how the logical layout is folded into the actual physical storage layout.
The cells of each row are stored one after the other, in a separate storage file per column family.
In other words, on disk you will have all cells of one family in a StoreFile, and all cells of another in a different file.
Since HBase is not storing any unset cells (also referred to as NULL values by RDBMSes) from the table, the on-disk file only contains the data that has been explicitly set.
In addition, multiple versions of the same cell are stored as separate, consecutive cells, adding the required timestamp of when the cell was stored.
The cells are sorted in descending order by that timestamp so that a reader of the data will see the newest value first—which is the canonical access pattern for the data.
The entire cell, with the added structural information, is called KeyValue in HBase terms.
It has not just the column and actual value, but also the row key and timestamp, stored for every cell for which you have set a value.
The KeyValues are sorted by row key first, and then by column key in case you have more than one cell per row in one column family.
The lower-right part of the figure shows the resultant layout of the logical table inside the physical storage files.
The HBase API has various means of querying the stored data, with decreasing granularity from left to right: you can select rows by row keys and effectively reduce the amount of data that needs to be scanned when looking for a specific row, or a range of rows.
Specifying the column family as part of the query can eliminate the need to search the separate storage files.
If you only need the data of one family, it is highly recommended that you specify the family for your read operation.
The store files retain the timestamp range for all stored cells, so if you are asking for a cell that was changed in the past two hours, but a particular store file only has data that is four or more hours old it can be skipped completely.
Rows stored as linear sets of actual cells, which contain all the vital information.
The next level of query granularity is the column qualifier.
You can employ exact column lookups when reading data, or define filters that can include or exclude the columns you need to access.
But as you will have to look at each KeyValue to check if it should be included, there is only a minor performance gain.
The value remains the last, and broadest, selection criterion, equaling the column qualifier’s effectiveness: you need to look at each cell to determine if it matches the read parameters.
You can only use a filter to specify a matching rule, making it the least efficient query option.
Figure 9-2 summarizes the effects of using the KeyValue fields.
The crucial part of Figure 9-1 shows is the shift in the lower-lefthand side.
Since the effectiveness of selection criteria greatly diminishes from left to right for a KeyValue, you can move all, or partial, details of the value into a more significant place—without changing how much data is stored.
Tall-Narrow Versus Flat-Wide Tables At this time, you may be asking yourself where and how you should store your data.
The former is a table with few columns but many rows, while the latter has fewer rows but many columns.
In addition, HBase can only split at row boundaries, which also enforces the recommendation to go with tall-narrow tables.
Imagine you have all emails of a user in a single row.
This will work for the majority of users, but there will be outliers that will have magnitudes of emails more in their inbox—so many, in fact, that a single row could outgrow the maximum file/region size and work against the region split facility.
The better approach would be to store each email of a user in a separate row, where the row key is a combination of the user ID and the message ID.
Looking at Figure 9-1 you can see that, on disk, this makes no difference: if the message ID is in the column qualifier, or in the row key, each cell still contains a single email message.
Here is the flat-wide layout on disk, including some examples:
The same information stored as a tall-narrow table has virtually the same footprint when stored on disk:
The message ID is simply moved to the left, making it more significant when querying the data, but also transforming each email into a separate logical row.
This results in a table that is easily splittable, with the additional benefit of having a more fine-grained query granularity.
Partial Key Scans The scan functionality of HBase, and the HTable-based client API, offers the second crucial part for transforming a table into a tall-narrow one, without losing query granularity: partial key scans.
In the preceding example, you have a separate row for each message, across all users.
Before you had one row per user, so a particular inbox was a single row and could be accessed as a whole.
Each column was an email message of the users’ inbox.
The exact row key would be used to match the user ID when loading the data.
With the tall-narrow layout an arbitrary message ID is now postfixed to the user ID in each row key.
If you do not have an exact combination of these two IDs you cannot retrieve a particular message.
The start key of a scan is inclusive, while the stop key is exclusive.
Setting the start key to the user ID triggers the internal lexicographic comparison mechanism of the scan to find the exact row key, or the one sorting just after it.
Since the table does not have an exact match for the user ID, it positions the scan at the next row, which is:
In other words, it is the row key with the lowest (in terms of sorting) user ID and message ID combination.
The scan will then iterate over all the messages of a user and you can parse the row key to extract the message ID.
The partial key scan mechanism is quite powerful, as you can use it as a lefthand index, with each added field adding to its cardinality.
Make sure that you pad the value of each field in the composite row key so that the lexicographical (binary, and ascending) sorting works as expected.
You will need a fixed-length field structure to guarantee that the rows are sorted by each field, going from left to right.*
You can, with increasing precision, construct a start and stop key for the scan that selects the required rows.
Usually you only create the start key and set the stop key to the same value as the start key, while increasing the least significant byte of its first field by one.
Table 9-1 shows the possible start keys and what they translate into.
You could, for example, use Orderly to generate the composite row keys.
These composite row keys are similar to what RDBMSes offer, yet you can control the sort order for each field separately.
For example, you could do a bitwise inversion of the date expressed as a long value (the Linux epoch)
This would then sort the rows in descending order by date.
This will reverse the dates and guarantee that the sorting order of the date field is descending.
In the preceding example, you have the date as the second field in the composite index for the row key.
This is only one way to express such a combination.
If you were to never query by date, you would want to drop the date from the key—and/or possibly use another, more suitable, dimension instead.
While it seems like a good idea to always implement a composite row key as discussed in the preceding text, there is one major drawback to doing so: atomicity.
Since the data is now spanning many rows for a single inbox, it is not possible to modify it in one operation.
If you are not concerned with updating the entire inbox with all the user messages in an atomic fashion, the aforementioned design is appropriate.
But if you need to have such guarantees, you may have to go back to flat-wide table design.
Pagination Using the partial key scan approach, it is possible to iterate over subsets of rows.
The principle is the same: you have to specify an appropriate start and stop key to limit the overall number of rows scanned.
Then you take an offset and limit parameter, applying them to the rows on the client side.
The approach shown here is mainly to explain the concept of what a dedicated row key design can achieve.
For pure pagination, the ColumnPaginationFilter is also the recommended approach, as it avoids sending unnecessary data over the network to the client.
Read the next limit rows and return to the caller.
Applying this to the inbox example, it is possible to paginate through all of the emails of a user.
Assuming an average user has a few hundred emails in his inbox, it is quite common for a web-based email client to show only the first, for example, 50 emails.
The remainder of the emails are then accessed by clicking the Next button to load the next page.
The remainder of the process would follow the approach we just discussed, so for the first page, where the offset is zero, you can read the next 50 emails.
This approach works well for a low number of pages.
If you were to page through thousands of pages, a different approach would be required.
You could add a sequential ID into the row key to directly position the start key at the right offset.
If you were using epochs, you could compute the value for midnight of the last seen date.
That way you can rescan that entire day and make a more knowledgeable decision regarding what to return.
There are many ways to design the row key to allow for efficient selection of subranges and enable pagination through records, such as the emails in the user inbox example.
Using the composite row key with the user ID and date gives you a natural order, displaying the newest messages first, sorting them in descending order by date.
Time Series Data When dealing with stream processing of events, the most common use case is time series data.
Such data could be coming from a sensor in a power grid, a stock exchange, or a monitoring system for computer systems.
Its salient feature is that its row key represents the event time.
This imposes a problem with the way HBase is arranging its rows: they are all stored sorted in a distinct range, namely regions with specific start and stop keys.
The sequential, monotonously increasing nature of time series data causes all incoming data to be written to the same region.
And since this region is hosted by a single server, all the updates will only tax this one machine.
This can cause regions to really run hot with the number of accesses, and in the process slow down the perceived overall performance of the cluster, because inserting data is now bound to the performance of a single machine.
It is easy to overcome this problem by ensuring that data is spread over all region servers instead.
This can be done, for example, by prefixing the row key with a nonsequential prefix.
You can use a salting prefix to the key that guarantees a spread of all rows across all region servers.
This formula will generate enough prefix numbers to ensure that rows are sent to all region servers.
Of course, the formula assumes a specific number of servers, and if you are planning to grow your cluster you should set this number to a multiple instead.
On the upside, you could use multiple threads to read this data from distinct servers, therefore parallelizing read access.
This is akin to a small map-only MapReduce job, and should result in increased I/O performance.
These reports are subsequently read and analyzed by the Mozilla development team to make their software more reliable on the vast number of machines and configurations on which it is used.
The code is open source, available online, and contains the Python-based client code that communicates with the HBase cluster using Thrift.
The implementation opens up 16 scanners (one for each leading hex character of the salt) simultaneously and then yields the next row in order from the pool on each iteration.
The Python code opens the required number of scanners, adding the salt prefix, which here is composed of a fixed set of single-letter prefixes—16 different ones all together.
Note that an additional heapq object is used that manages the actual merging of the scanner results against the global sorting order.
This approach uses the composite row key concept to move the sequential, monotonously increasing timestamp to a secondary position in the row key.
If you already have a row key with more than one field, you can swap them.
If you have only the timestamp as the current row key, you need to promote another field from the column keys, or even the value, into the row key.
There is also a drawback to moving the time to the righthand side in the composite key: you can only access data, especially time ranges, for a given swapped or promoted field.
Use Case: OpenTSDB The OpenTSDB‡ project provides a time series database used to store metrics about servers and services, gathered by external collection agents.
All of the data is stored in HBase, and using the supplied user interface (UI) enables users to query various metrics, combining and/or downsampling them—all in real time.
The schema promotes the metric ID into the row key, forming the following structure:
Since a production system will have a considerable number of metrics, but their IDs will be spread across a range and all updates occurring across them, you end up with an access pattern akin to the salted prefix: the reads and writes are spread across the metric IDs.
This approach is ideal for a system that queries primarily by the leading field of the composite key.
In the case of OpenTSDB this makes sense, since the UI asks the users to select from one or more metrics, and then displays the data points of those metrics ordered by time.
Randomization A totally different approach is to randomize the row key using, for example:
Using a hash function like MD5 will give you a random distribution of the key across all available region servers.
For time series data, this approach is obviously less than ideal, since there is no way to scan entire ranges of consecutive timestamps.
On the other hand, since you can re-create the row key by hashing the timestamp requested, it still is very suitable for random lookups of single rows.
When your data is not scanned in ranges but accessed randomly, you can use this strategy.
Summarizing the various approaches, you can see that it is not trivial to find the right balance between optimizing for read and write performance.
It depends on your access pattern, which ultimately drives the decision on how to structure your row keys.
Figure 9-3 shows the various solutions and how they affect sequential read and write performance.
In particular, the page that discusses the project’s schema is a recommended read, as it adds advanced key design concepts for an efficient storage format that also allows for high-performance querying of the stored data.
Using the salted or promoted field keys can strike a good balance of distribution for write performance, and sequential subsets of keys for read performance.
If you are only doing random reads, it makes most sense to use random keys: this will avoid creating region hot-spots.
Time-Ordered Relations In our preceding discussion, the time series data dealt with inserting new events as separate rows.
However, you can also store related, time-ordered data: using the columns of a table.
Since all of the columns are sorted per column family, you can treat this sorting as a replacement for a secondary index, as available in RDBMSes.
Multiple secondary indexes can be emulated by using multiple column families—although that is not the recommended way of designing a schema.
But for a small number of indexes, this might be what you need.
Consider the earlier example of the user inbox, which stores all of the emails of a user in a single row.
Since you want to display the emails in the order they were received, but, for example, also sorted by subject, you can make use of column-based sorting to achieve the different views of the user inbox.
The drawback is that you cannot make use of the provided per-table row-level atomicity.
The first decision to make concerns what the primary sorting order is, in other words, how the majority of users have set the view of their inbox.
Finding the right balance between sequential read and write performance.
The email itself is stored in the main column family, while the sort indexes are in separate column families.
You can extract the subject from the email address and add it to the column key to build the secondary sorting order.
If you need descending sorting as well, you would need another family.
To circumvent the proliferation of column families, you can alternatively store all secondary indexes in a single column family that is separate from the main column family.
Once again, you would make use of implicit sorting by prefixing the values with an index ID—for example, idx-subject-desc, idx-to-asc, and so on.
Next, you would have to attach the actual sort value.
The actual value of the cell is the key of the main index, which also stores the message.
This also implies that you need to either load the message details from the main table, display only the information stored in the secondary index, or store the display details redundantly in the index, avoiding the random lookup on the main information source.
Recall that denormalization is quite common in HBase to reduce the required read operations in favor of vastly improved user-facing responsiveness.
Putting the aforementioned schema into action might result in something like this:
In the preceding code, one index (idx-from-asc) is sorting the emails in ascending order by from address, and another (idx-subject-desc) in descending order by subject.
The subject itself is not readable anymore as it was bit-inversed to achieve the descending sorting order.
All of the index values are stored in the column family index, using the prefixes mentioned earlier.
A client application can read the entire column family and cache the content to let the user quickly switch the sorting order.
Another option is the ColumnPaginationFilter, combined with the ColumnPrefixFilter to iterate over an index page by page.
Advanced Schemas So far we have discussed how to use the provided table schemas to map data into the column-oriented layout HBase supports.
You will have to decide how to structure your row and column keys to access data in a way that is optimized for your application.
Each column value is then an actual data point, stored as an arbitrary array of bytes.
While this type of schema, combined with the ability to create columns with arbitrary keys when needed, enables you to evolve with new client application releases, there are use cases that require more formal support of a more feature-rich, evolveable serialization API, where each value is a compact representation of a more complex, nestable record structure.
An exemplary project using Avro to store complex records in each column is HAvroBase.§ This project facilitates Avro’s interface definition language (IDL) to define the actual schema, which is then used to store records in their serialized form within arbitrary table columns.
Protocol Buffers Similar to Avro, you can use the Protocol Buffer’s IDL to define an external schema, which is then used to serialize complex data structures into HBase columns.
The idea behind this approach is that you get a definition language that allows you to define an initial schema, which you can then update by adding or removing fields.
The serialization API takes care of reading older schemas with newer ones.
Secondary Indexes Although HBase has no native support for secondary indexes, there are use cases that need them.
In addition, you can scan a range of rows from the main table, but ordered by the secondary index.
Similar to an index in RDBMSes, secondary indexes store a mapping between the new coordinates and the existing ones.
Moving the responsibility completely into the application layer, this approach typically combines a data table and one (or more) lookup/mapping tables.
Whenever the code writes into the data table it also updates the lookup tables.
Reading data requires either a direct lookup in the main table, or, if the key is from a secondary index, a lookup of the main row key, and then retrieval of the data in a second operation.
First, since the entire logic is handled in the client code, you have all the freedom to map the keys exactly the way they are needed.
The list of shortcomings is longer, though: since you have no cross-row atomicity, for example, in the form of transactions, you cannot guarantee consistency of the main and dependent tables.
The missing transactional support could result in data being stored in the data table, but with no mapping in the secondary index tables, because the operation failed after the main table was updated, but before the index tables were written.
This can be alleviated by writing to the secondary index tables first, and to the data table at the end of the operation.
Should anything fail in the process, you are left with orphaned mappings, but those are subsequently removed by the asynchronous, regular pruning jobs.
Having all the freedom to design the mapping between the primary and secondary indexes comes with the drawback of having to implement all the necessary wiring to store and look up the data.
External keys need to be identified to access the correct table, for example:
The first key denotes a direct data table lookup, while the second, using the prefix, is a mapping that has to be performed through a secondary index table.
The name of the table could be also encoded as a number and added to the prefix.
The flip side is this is hardcoded in your application and needs to evolve with overall schema changes, and new requirements.
Indexed-Transactional HBase A different solution is offered by the open source Indexed-Transactional HBase (ITHBase) project.‖ This solution extends HBase by adding special implementations of the client and server-side classes.
The core extension is the addition of transactions, which are used to guarantee that all secondary index updates are consistent.
On top of this it adds index support, by providing a client-side IndexedTableDescriptor, defining how a data table is backed by a secondary index table.
Most client and server classes are replaced by ones that handle indexing support.
For example, HTable is replaced with IndexedTable on the client side.
It has a new data table using the ordering of a secondary index.
Just as with the client-managed index described earlier, this index stores the mappings between the primary and secondary keys in separate tables.
In contrast, though, these are automatically created, and maintained, based on the descriptor.
Combined with the transactional updates of these indexes, this solution provides a complete implementation of secondary indexes for HBase.
The drawback is that it may not support the latest version of HBase available, as it is not tied to its release cycle.
It also adds a considerable amount of synchronization overhead that results in decreased performance, so you need to benchmark carefully.
Indexed HBase Another solution that allows you to add secondary indexes to HBase is Indexed HBase (IHBase).# This solution forfeits the use of separate tables for each index but maintains them purely in memory.
The indexes are generated when a region is opened for the first time, or when a memstore is flushed to disk—involving an entire region’s scan to build the index.
Depending on your configured region size, this can take a considerable amount of time and I/O resources.
Only the on-disk information is indexed; the in-memory data is searched as-is: it uses the memstore data directly to search for index-related details.
The advantage of this solution is that the index is never out of sync, and no explicit transactional control is necessary.
In comparison to table-based indexing, using this approach is very fast, as it has all the required details in memory and can perform a fast binary search to find matching rows.
However, it requires a lot of extra heap to maintain the index.
The ITHBase project started as a contrib module for HBase.
It was subsequently moved to an external repository allowing it to address different versions of HBase, and to develop at its own pace.
Similar to ITHBase, IHBase started as a contrib project within HBase.
It was moved to an external repository for the same reasons.
The original documentation of the JIRA issue is online at HBASE-2037
Depending on your requirements and the amount of data you want to index, you might run into a situation where IHBase cannot keep all the indexes you need.
The in-memory indexes are typed and allow for more fine-grained sorting, as well as more memory-efficient storage.
There is no explicit control over the sorting order; thus data is always stored in ascending order.
You will need to do the bitwise inversion of the value described earlier to sort in descending order.
The definition of an index revolves around the IdxIndexDescriptor class that defines the specific column of the data table that holds the index, and the type of the values it contains, taken from the list in the preceding paragraph.
Accessing an index is handled by the client-side IdxScan class, which extends the normal Scan class by adding support to define Expressions.
A scan without an explicit expression defaults to normal scan behavior.
Expressions provide basic boolean logic with an And and Or construct.
The preceding example uses builder-style helper methods to generate a complex expression that combines three separate indexes.
The lowest level of an expression is the Comparison, which allows you to specify the actual index, and a filter-like syntax to select values that match a comparison value and operator.
You have to specify a columnFamily, and a qualifier of an existing index, or else an IllegalStateException will be thrown.
You can use it to fine-tune what is included in the scan depending on how the expression is evaluated.
The sorting order is defined by the first evaluated index in the expression, while the other indexes are used to intersect (for the and) or unite (for the or) the possible keys with the first index.
In other words, using complex expressions is predictable only when using the same index, but with various comparisons.
It shares the same drawbacks, for the following reasons: • It is quite intrusive, as its installation requires additional JAR files plus a configuration that replaces vital client- and server-side classes.
It needs extra resources, although it trades memory for extra I/O requirements.
It does random lookups on the data table, based on the sorting order defined.
It may not be available for the latest version of HBase.*
Coprocessor There is work being done to implement an indexing solution based on coprocessors.† Using the server-side hooks provided by the coprocessor framework, it is possible to implement indexing similar to ITHBase, as well as IHBase while not having to replace any client- and server-side classes.
The coprocessor would load the indexing layer for every region, which would subsequently handle the maintenance of the indexes.
The code can make use of the scanner hooks to transparently iterate over a normal data table, or an index-backed view on the same.
The definition of the index would need to go into an external schema that is read by the coprocessor-based classes, or it could make use of the generic attributes a column family can store.
Since this is in its early stages, there is not much that can be documented at this time.
Watch the online issue tracking system for updates on the work if you are interested.
Search Integration Using indexes gives you the ability to iterate over a data table in more than the implicit row key order.
You are still confined to the available keys and need to use either filters or straight iterations to find the values you are looking for.
As of this writing, IHBase only supports HBase version 0.20.5
See HBASE-2038 in the JIRA issue tracking system for details.
Common choices are the Apache Lucene-based solutions, such as Lucene itself, or Solr, a high-performance enterprise search server.‡ Similar to the indexing solutions, there are a few possible approaches:
Client-managed These range from implementations using HBase as the data store, and using MapReduce jobs to build the search index, to those that use HBase as the backing store for Lucene.
Another approach is to route every update of the data table to the adjacent search index.
Implementing support for search indexes in combination with HBase is primarily driven by how the data is accessed, and if HBase is used as the data store, or as the index store.
A prominent implementation of a client-managed solution is the Facebook inbox search.
The schema is built roughly like this: • Every row is a single inbox, that is, every user has a single row in the search table.
The values contain additional information, such as the position of the term in.
With this schema it is easy to search a user’s inbox for messages containing specific words.
Boolean operators, such as and or or, can be implemented in the client code, merging the lists of documents found.
You can also efficiently implement typeahead queries: the user can start typing a word and the search finds all messages that contain words that match the user’s input as a prefix.
An externally hosted project§ provides the BuildTableIndex class, which was formerly part of the contrib modules shipping with HBase.
This class scans an entire table and builds the Lucene indexes, which ultimately end up as directories on HDFS—their count depends on the number of reducers used.
These indexes can be downloaded to a Lucene-based server, and accessed locally using, for example, a MultiSearcher class, provided by Lucene.
Another approach is to merge the index parts by either running the MapReduce job with a single reducer, or using the index merge tool that comes with Lucene.
A merged index usually provides better performance, but the time required to build, merge, and eventually serve the index is longer.
Solr is based on Lucene, but extends it to provide a fully featured search server.
See the GitHub project page for details and to access the code.
In general, this approach uses HBase only to store the data.
If a search is performed through Lucene, usually only the matching row keys are returned.
A random lookup into the data table is required to display the document.
Depending on the number of lookups, this can take a considerable amount of time.
A better solution would be something that combines the search directly with the stored data, thus avoiding the additional random lookup.
HBasene The approach chosen by HBasene‖ is to build an entire search index directly inside HBase, while supporting the well-established Lucene API.
The schema used stores each document field, or term, in a separate row, with the documents containing the term stored as columns inside that row.
The schema also reuses the same table to store various other details required to implement full Lucene support.
It implements an IndexWriter that stores the documents directly into the HBase table, as they are inserted using the normal Lucene API.
Here is an example taken from the test class that comes with HBasene:
The example creates a small test index and subsequently searches it.
You may note that there is a lot of Lucene API usage, with small amendments to support the HBase-backed index writer.
Coprocessors Yet another approach to complement a data table with Lucene-based search functionality, and currently in development,# is based on coprocessors.
It uses the provided hooks to maintain the index, which is stored directly on HDFS.
Every region has its own index and search is distributed across them to gather the full result.
This is only one example of what is possible with coprocessors.
Similar to the use of coprocessors to build secondary indexes, you have the choice of where to store the actual index: either in another table, or externally.
The framework offers the enabling technology; the implementing code has the choice of how to use it.
Transactions It seems somewhat counterintuitive to talk about transactions in regard to HBase.
However, the secondary index example showed that for some use cases it is beneficial to abandon the simplified data model HBase offers, and in fact introduce concepts that are usually seen in traditional database systems.
One of those concepts is transactions, offering ACID compliance across more than one row, and more than one table.
This is necessary in lieu of a matching schema pattern in HBase.
For example, updating the main data table and the secondary index table requires transactions to be reliably consistent.
Often, transactions are not needed, as normalized data schemas can be folded into a single table and row design that does not need the overhead of distributed transaction support.
If you cannot do without this extra control, here are a few possible solutions: Transactional HBase.
The Indexed Transactional HBase project comes with a set of extended classes that replace the default client- and server-side classes, while adding support for transactions across row and table boundaries.
The region servers, and more precisely, each region, keeps a list of transactions, which are initiated with a beginTransac operation then takes a transaction ID to guard the call against other transactions.
ZooKeeper HBase requires a ZooKeeper ensemble to be present, acting as the seed, or bootstrap mechanism, for cluster setup.
There are templates, or recipes, available that show how ZooKeeper can also be used as a transaction control backend.
For example, the Cages project offers an abstraction to implement locks across multiple resources, and is scheduled to add a specialized transactions class—using ZooKeeper as the distributed coordination system.
ZooKeeper also comes with a lock recipe that can be used to implement a twophase commit protocol.
It uses a specific znode representing the transaction, and a child znode for every participating client.
The clients can use their znodes to flag whether their part of the transaction was successful or failed.
The other clients can monitor the peer znodes and take the appropriate action.*
The reason to use Bloom filters at all is that the default mechanisms to decide if a store file contains a specific row key are limited to the available block index, which is, in turn, fairly coarse-grained: the index stores the start row key of each contained block only.
If we further assume your cell size is an average of 200 bytes, you will have more than.
More details can be found on the ZooKeeper project page.
The only way for HBase to figure out if the key actually exists is by loading the block and scanning it to find the key.
This problem is compounded by the fact that, for a typical application, you will expect a certain update rate, which results in flushing in-memory data to disk, and subsequent compactions aggregating them into larger store files.
Since minor compactions only combine the last few store files, and only up to a configured maximum size, you will end up with a number of store files, all acting as possible candidates to have some cells of the requested row key.
Using Bloom filters to help reduce the number of I/O operations.
The files are all from one column family and have a similar spread in row keys, although only a few really hold an update to a specific row.
The block index has a spread across the entire row key range, and therefore always reports positive to contain a random row.
The region server would need to load every block to check if the block actually contains a cell of the row or not.
On the other hand, enabling the Bloom filter does give you the immediate advantage of knowing if a file contains a particular row key or not.
The nature of the filter is that it can give you a definitive answer if the file does not contain the row—but might report a false positive, claiming the file contains the data, where in reality it does not.
This does not translate into an immediate performance gain on individual get operations, since HBase does the reads in parallel, and is ultimately bound by disk read latency.
Reducing the number of unnecessary block loads improves the overall throughput of the cluster.
You can see from the example, however, that the number of block loads is greatly reduced, which can make a big difference in a heavily loaded system.
For this to be efficient, you must also match a specific update pattern: if you modify all of the rows on a regular basis, the majority of the store files will have a piece of the row you are looking for, and therefore would not be a good use case for Bloom filters.
But if you update data in batches so that each row is written into only a few store files at a time, the filter is a great feature to reduce the overall number of I/O operations.
Another place where you will find this to be advantageous is in the block cache.
The hit rate of the cache should improve as loading fewer blocks results in less churn.
Since the server is now loading blocks that contain the requested data most of the time, related data has a greater chance to remain in the block cache and subsequent read operations can make use of it.
Besides the update pattern, another driving factor to decide if a Bloom filter makes sense for your use case is the overhead it adds.
Every entry in the filter requires about one byte of storage.
Now assume your cells are, on average, 1 KB in size; in this case, the filter needs only Bloom filter of a few hundred kilobytes for a store file of one or more gigabyte.
In that case, it seems that it would always be to enable the filter.
The final question is whether to use a row or a row+column Bloom filter.
The row+column Bloom filter is useful when you cannot batch updates for a specific row, and end up with store files which all contain parts of the row.
The more specific row+column filter can then identify which of the files contain the data you are requesting.
Obviously, if you always load the entire row, this filter is once again hardly useful, as the region server will need to load the matching block out of each file anyway.
Since the row+column filter will require more storage, you need to do the math to determine whether it is worth the extra resources.
It is also interesting to know that there is a maximum number of elements a Bloom filter can hold.
If you have too many cells in your store file, you might exceed that number and would need to fall back to the row-level filter.
Figure 9-5 summarizes the selection criteria for the different Bloom filter levels.
Depending on your use case, it may be useful to enable Bloom filters, to increase the overall performance of your system.
If possible, you should try to use the row-level Bloom filter, as it strikes a good balance between the additional space requirements and the gain in performance coming from its store file selection filtering.
Only resort to the more costly row+column Bloom filter when you would otherwise gain no advantage from using the row-level one.
Versioning Now that we have seen how data is stored and retrieved in HBase, it is time to revisit the subject of versioning.
They also expose a few intricacies you should be aware of.
Implicit Versioning I pointed out before that you should ensure that the clock on your servers is synchronized.
Otherwise, when you store data in multiple rows across different servers, using the implicit timestamps, you may end up with completely different time settings.
For example, say you use the HBase URL Shortener and store three new shortened URLs for an existing user.
All of the keys are considered fully distributed, so all three of the new rows end up on a different region server.
Further, assuming that these servers are all one hour apart, if you were to scan from the client side to get the list of new shortened URLs within the past hour, you would miss a few, as they have been saved with a timestamp that is more than an hour different from what the client considers current.
This can be avoided by setting an agreed, or shared, timestamp when storing these values.
The put operation allows you to set a client-side timestamp that is used instead, therefore overriding the server time.
Obviously, the better approach is to rely on the servers doing this work for you, but you might be required to use this approach in some circumstances.† Another issue with servers not being aligned by time is exposed by region splits.
Assume you have saved a value on a server that is one hour ahead all other servers in the cluster, using the implicit timestamp of the server.
Ten minutes later the region is split and the half with your update is moved to another server.
Five minutes later you are inserting a new value for the same column, again using the automatic server time.
The new value is now considered older than the initial one, because the first version has a timestamp one hour ahead of the current server’s time.
If you do a standard get call to retrieve the newest version of the value, you will get the one that was stored first.
Once you have all the servers synchronized, there are a few more interesting side effects you should know about.
This happens when you store more versions than are configured at the column family level.
The default is to keep the last three versions of a cell, or value.
If you insert a new value 10 times into the same column, and request a complete list of.
One example, although very uncommon, is based on virtualized servers.
See http://support.ntp.org/bin/view/ Support/KnownOsIssues#Section_9.2.2, which lists an issue with NTP, the commonly used Network Time Protocol, on virtual machines.
The version is set to a specific value, using the loop variable.
When you run the example, you should see the following output: After put calls...
The older versions of the column are still kept, so deleting newer versions makes the older versions come back.
This is only possible until a major compaction has been performed, after which the older versions are removed forever, using the predicate delete based on the configured maximum versions to retain.
The example code has some commented-out code you can enable to enforce a flush and major compaction.
If you rerun the example, you will see this result instead:
Since the older versions have been removed, they do not reappear anymore.
Finally, when dealing with timestamps, there is another issue to watch out for: delete markers.
This refers to the fact that, in HBase, a delete is actually adding a tombstone marker into the store that has a specific timestamp.
Based on that, it masks out versions that are either a direct match, or, in the case of a column delete marker, anything that is older than the given timestamp.
Store a value into the column of the newly created table, and run a scan to verify.
This sets the delete marker with a timestamp of now.
Store the value again into the column, but use a time in the past.
Flush and conduct a major compaction of the table to remove the delete marker.
Store the value with the time in the past again.
The example shows that there are sometimes situations where you might see something you do not expect to see.
But this behavior is explained by the architecture of HBase, and is deterministic.
Every time you insert a new value you retrieve a new number and use that when calling the put function.
You must do this for every put operation, or the server will insert an epoch-based timestamp instead.
There is a flag in the table or column descriptors that indicates your use of custom timestamp values; in other words, your own versioning.
If you fail to set the value, it is silently replaced with the server timestamp.
When using your own timestamp values, you need to test your solution thoroughly, as this approach has not been used widely in production.
Be aware that negative timestamp values are untested and, while they have been discussed a few times in HBase developer circles, they have never been confirmed to work properly.
Make sure to avoid collisions by using the same value for two separate updates to the same cell.
With these warnings out of the way, here are a few use cases that show how a custom versioning scheme can be beneficial in the overall concept of table schema design:
As an example for a number generator based on ZooKeeper, see the zk_idgen project.
It uses the timestamp value to hold the message ID.
Number generator This follows on with the initially given example, making use of a distributed number generator.
It may seem that a number generator would do the same thing as epoch-based timestamps do: sort all values ascending by a monotonously increasing value.
The difference is subtler, because the resolution of the Java timer used is down to the millisecond, which means it is quite unlikely to store two values at the exact same time—but that can happen.
If you were to require a solution in which you need an absolutely unique versioning scheme, using the number generator can solve this issue.
Using the time component of HBase is an interesting way to exploit this extra dimension offered by the architecture.
You have less freedom, as it only accepts long values, as opposed to arbitrary binary keys supported by row and column keys.
Once you have your HBase cluster up and running, it is essential to continuously ensure that it is operating as expected.
This chapter explains how to monitor the status of the cluster with a variety of tools.
Introduction Just as it is vital to monitor production systems, which typically expose a large number of metrics that provide details regarding their current status, it is vital that you monitor HBase.
But while Hadoop is a batchoriented system, and therefore often is not immediately user-facing, HBase is userfacing, as it serves random access requests to, for example, drive a website.
The response times of these requests should stay within specific limits to guarantee a positive user experience—also commonly referred to as a service-level agreement (SLA)
With distributed systems the administrator is facing the difficult task of making sense of the overall status of the system, while looking at each server separately.
And even with a single server system it is difficult to know what is going on when all you have to go by is a handful of raw logfiles.
But digging through mega-, giga-, or even terabytes of textbased files to find the needle in the haystack, so to speak, is something only a few people have mastered.
And even if you have mad log-reading skills, it will take time to draw and test hypotheses to eventually arrive at the cause of the disruption.
This is obviously not something new, and viable solutions have been around for years.
These solutions fall into the groups of graphing and monitoring—with some tools covering only one of these groups, while others cover both.
Graphing captures the exposed metrics of a system and displays them in visual charts, typically with a range of time filters—for example, daily, monthly, and yearly time frames.
This is good, as it can quickly show you what your system has been doing lately—like they say, a picture speaks a thousand words.
The graphs are good for historical, quantitative data, but with a rather large time granularity it is also difficult to see what a system is doing right now.
This is where qualitative data is needed, which is handled by the monitoring kind of support systems.
They keep an ear out on your behalf to verify that each data point, or metric, exposed is within a specified range.
Often, the support tools already supply a significant set of checks, so you only have to tweak them for your own purposes.
Checks that are missing can be added in the form of plug-ins, or simple script-based extensions.
You can also fine-tune how often the checks are run, which can range from seconds to days.
Whenever a check indicates a problem, or outright failure, evasive actions could be taken automatically: servers could be decommissioned, restarted, or otherwise repaired.
When a problem persists there are rules to escalate the issue to, for example, the administrators to handle it manually.
This could be done by sending out emails to various recipients, or SMS messages to telephones.
While there are many possible support systems you can choose from, the Java-based nature of HBase, and its affinity to Hadoop, narrow down your choices to a more limited set of systems, which also have been proven to work reliably in combination.
For graphing, the system supported natively by HBase is Ganglia.
For monitoring, you need a system that can handle the JMX*-based metrics API as exposed by the HBase processes.
You should set up the complete support system framework that you want to use in production, even when prototyping a solution, or working on a proof-of-concept study based on HBase.
That way you have a head start in making sense of the numbers and configuring the system checks accordingly.
Using a cluster without monitoring and metrics is the same as driving a car while blindfolded.
It is great to run load tests against your HBase cluster, but you need to correlate the cluster’s performance with what the system is doing under the hood.
Graphing the performance lets you line up events across machines and subsystems, which is an invaluable when it comes to understanding test results.
The Metrics Framework Every HBase process, including the master and region servers, exposes a specific set of metrics.
These are subsequently made available to the various monitoring APIs and tools, including JMX and Ganglia.
For each kind of server there are multiple groups of metrics, usually pertaining to a subsystem within each server.
For example, one group of metrics is provided by the Java Virtual Machine (JVM) itself, giving insight into.
Contexts, Records, and Metrics HBase employs the Hadoop metrics framework, inheriting all of its classes and features.
This framework is based on the MetricsContext interface to handle the generation of data points for monitoring and graphing.
Also writes the metrics to a file on disk, but adds a timestamp prefix to each metric emitted.
This results in a more log-like formatting inside the file.
CompositeContext Allows you to emit metrics to more than one context.
You can specify, for example, a Ganglia and file context at the same time.
When using this context, nothing is emitted, nor aggregated, at all.
NullContextWithUpdateThread Does not emit any metrics, but starts the aggregation thread.
Another artifact of HBase inheriting the metrics framework from Hadoop is that it uses the supplied ContextFactory, which loads the various context classes.
The configuration filename is hardcoded in this class to hadoop-metrics.properties—which is the reason HBase uses the exact same filename as Hadoop, as opposed to the more intuitive hbasemetrics.properties you might have expected.
Multiple metrics are grouped into a MetricsRecord, which describes, for example, one specific subsystem.
HBase uses these groups to keep the statistics for the master, region server, and so on.
Each group also has a unique name, which is combined with the context and the actual metric name to form the fully qualified metric:
The contexts have a built-in timer that triggers the push of the metrics on regular intervals to whatever the target is—which can be a file, Ganglia, or your own custom solution if you choose to build one.
The configuration file enabling the context has a period property per context that is used to specify the interval period in seconds for the context to push its updates.
Specific context implementations might have additional properties that control their behavior.
Figure 10-1 shows a sequence diagram with all the involved classes.
The metrics are internally tracked by container classes, based on MetricsBase, which have various update and/or increment methods that are called when an event occurs.
The framework, in turn, tracks the number of events for every known metric and correlates it to the time elapsed since it was last polled.
The following list summarizes the available metric types in the Hadoop and HBase metrics framework, associating abbreviations with each.
A float value representing a rate, that is, the number of operations/events per second.
It provides an increment method that is called to track the number of operations.
It also has a last polled timestamp that is used to track the elapsed time.
The rate is calculated as number of operations / elapsed time in seconds.
The last polled timestamp is set to the current time.
It is used to report the HBase version number, build date, and so on.
It is never reset nor changed—once set, it remains the same while the process is running.
Time varying integer (TVI) A metric type in which the context keeps aggregating the value, making it a monotonously increasing counter.
The metric has a simple increment method that is used by the framework to count various kinds of events.
When the value is polled it returns the accrued integer value, and resets to zero, until it is polled again.
Time varying long (TVL) Same as TVI, but operates on a long value for faster incrementing counters, that could otherwise exceed the maximum integer value.
Sequence diagram of the classes involved in preparing the metrics.
Time varying rate (TVR) Tracks the number of operations or events and the time they required to complete.
This is used to compute the average time for an operation to finish.
The metric also tracks the minimum and maximum time per operation observed.
Table 10-1 shows how the values are exported under the same name, but with different postfixes.
The values in the Short column are postfixes that are attached to the actual metric Time, incrementMaxTime, and incrementAvgTime.
For example, the context-based metrics only expose the AvgTime and NumOps values, while JMX gives access to all four.
Note that the values for operation count and time accrued are reset once the metric is polled.
The number of operations is aggregated by the polling context, though, making it a monotonously increasing counter.
In contrast, the average time is set as an absolute value.
It is computed when the metric is retrieved at the end of a polling interval.
Persistent time varying rate (PTVR) An extension to the TVR.
This metric adds the necessary support for the extended period metrics: since these long-running metrics are not reset for every poll they need to be reported differently.
Values exposed by metrics based on time varying rate Value name Short Description.
Number Operations NumOps The actual number of events since the last poll.
Mininum Time MinTime The shortest time reported for an event to complete.
Maximum Time MaxTime The longest time reported for an event to complete.
Average Time AvgTime The average time for completing events; this is computed as the sum of the reported times per event, divided by the number of events.
When we subsequently discuss the different metrics provided by HBase you will find the type abbreviation next to it for reference, in case you are writing your own support tool.
Keep in mind that these metrics behave differently when they are retrieved through a metrics context, or via JMX.
Accessing the same values through JMX will reveal their reset behavior, since JMX accesses the values directly, not through a metric context.
A prominent example is the NumOps component of a TVR metric.
Reading it through a metric context gives you an ever increasing value, while JMX would only give you the absolute number of the last poll period.
Other metrics are only emitting data when the value has changed since the last update.
Again, this is evident when using the contexts, but not when using JMX.
The latter will simply retrieve the values from the last poll.
If you do not set a poll period, the JMX values will never change.
Figure 10-2 shows how, over each metric period, the different metric types are updated and emitted.
HBase also has some exceptional rate metrics that span across specific time frames, overriding the usual update intervals.
There are a few long-running processes in HBase that require some metrics to be kept until the process has completed.
This is controlled by the hbase.extendedperiod property, specified in seconds.
The default is no expiration, but the supplied configuration sets it to a moderate 3600 seconds, or one hour.
Currently, this extended period is applied to the time and size rate metrics for compactions, flushes, and splits for the region servers and master, respectively.
On the region server it also triggers a reset of all other-rate based metrics, including the read, write, and sync latencies.
Master Metrics The master process exposes all metrics relating to its role in a cluster.
Since the master is relatively lightweight and only involved in a few cluster-wide operations, it does expose only a limited set of information (in comparison to the region server, for example)
Cluster requests (R) The total number of requests to the cluster, aggregated across all region servers.
Split time (PTVR) The time it took to split the write-ahead log files after a restart.
Split size (PTVR) The total size of the write-ahead log files that were split.
Region Server Metrics The region servers are part of the actual data read and write path, and therefore collect a substantial number of metrics.
These include details about different parts of the overall architecture inside the server—for example, the block cache and in-memory store.
Instead of listing all possible metrics, we will discuss them in groups, since it is more each group the meaning is quite obvious and needs only a few more notes, if at all.
The block cache holds the loaded storage blocks from the low-level HFiles, read from HDFS.
Given that you have allowed for a block to be cached, it is kept in memory until there is no more room, at which point it is evicted.
The count (LV) metric reflects the number of blocks currently in the cache, while the size (LV) is the occupied Java heap space.
The free (LV) metric is the remaining heap for the cache, and evicted (LV) counts the number of blocks that had to be removed because of heap size constraints.
The block cache keeps track of the cache hit (LV) and miss (LV) counts, as well as the hit ratio (IV), which is the number of cache hits in relation to the total number of requests to the cache.
Finally, the more ominous hit caching count is similar to the hit ratio, but only takes into account requests and hits of operations that had requested for the block cache.
All read operations will try to use the cache, regardless of whether retaining the block in the cache has been requested.
Compaction metrics When the region server has to perform the asynchronous (or manually invoked) housekeeping task of compacting the storage files, it reports its status in a different metric.
The compaction size (PTVR) and compaction time (PTVR) give details regarding the total size (in bytes) of the storage files that have been compacted, and how long that operation took, respectively.
Note that this is reported after a completed compaction run, because only then are both values known.
The compaction queue size (IV) can be used to check how many files a region server has queued up for compaction currently.
The compaction queue size is another recommended early indicator of trouble that should be closely monitored.
Usually the number is quite low, and varies between zero and somewhere in the low tens.
When you have I/O issues, you usually see this number rise sharply.
Keep in mind that major compactions will also cause a sharp rise as they queue up all storage files.
You need to account for this when looking at the graphs.
Memstore metrics Mutations are kept in the memstore on the region server, and will subsequently be written to disk via a flush.
The memstore metrics expose the memstore size MB metric (IV), which is the total heap space occupied by all memstores for the server in megabytes.
It is the sum of all memstores across all online regions.
The flush queue size (IV) is the number of enqueued regions that are being flushed next.
The flush size (PTVR) and flush time (PTVR) give details regarding the total size (in bytes) of the memstore that has been flushed, and the time it took to do so, respectively.
Just as with the compaction metrics, these last two metrics are updated after the flush has completed.
So the reported values slightly trail the actual value, as it is missing what is currently in progress.
Similar to the compaction queue you will see a sharp rise in count for the flush queue when, for example, your servers are under I/O duress.
The stores (IV) metric gives you the total number of stores for the server, across all regions it currently serves.
The store file index size MB metric (IV) is the sum of the block index, and optional meta index, for all store files in megabytes.
I/O metrics The region server keeps track of I/O performance with three latency metrics, all of them keeping their numbers in milliseconds.
The fs read latency (TVR) reports the filesystem read latency—for example, the time it takes to load a block from the storage files.
The fs write latency (TVR) is the same for write operations, but combined for all writers, including the storage files and write-ahead log.
Finally, the fs sync latency (TVR) measures the latency to sync the write-ahead log records to the filesystem.
The latency metrics provide information about the lowlevel I/O performance and should be closely monitored.
Miscellaneous metrics In addition to the preceding metrics, the region servers also provide global counters, exposed as metrics.
The read request count (LV) and write request count (LV) respectively, summed up for all online regions this server hosts.
The requests (R) metric is the actual request rate per second encountered since it was last polled.
Finally, the regions (IV) metric gives the number of regions that are currently online and hosted by this region server.
The subsystem automatically tracks every operation possible between the different servers and clients.
This includes the master RPCs, as well as those exposed by region servers.
The RPC metrics for the master and region servers are shared—in other words, you will see the same metrics exposed on either server type.
The difference is that the servers update the metrics for the operations to the region server.
On the other hand, you do see all the metrics for all of the administrative calls, like enableTable or compactRegion.
Since the metrics relate directly to the client and administrative APIs, you can infer their meaning from the corresponding API calls.
The naming is not completely consistent, though, to remove arbitration.
A notable pattern is the addition of the Region postfix maps to the splitRegion metric.
Only a handful of metrics have no API counterpart, and these are listed in Table 10-3
As this spans all possible RPC calls, it averages across them.
Monitoring the queue time is a good idea, as it indicates the load on the server.
You could use thresholds to trigger warnings if this number goes over a certain limit.
The remaining metrics are from the RPC API between the master and the region servers, region server initially reports for duty at its assigned master node, and for regular status reports, respectively.
This section discusses what you can retrieve from each server process using the metrics framework.
Every HBase process collects and exposes JVM-related details that are helpful to correlate, for example, server performance with underlying JVM internals.
This information, in turn, is used when tuning your HBase cluster setup.
The provided metrics can be grouped into related categories: Memory usage metrics.
You can retrieve the used memory and the committed memory† in megabytes for both heap and nonheap usage.
The former is the space that is maintained by the JVM on your behalf and garbage-collected at regular intervals.
Garbage collection metrics The JVM is maintaining the heap on your behalf by running garbage collections.
The gc count metric is the number of garbage collections, and the gc time millis is the accumulated time spent in garbage collection since the last poll.
Certain steps in the garbage collection process cause so-called stop-the-world pauses, which are inherently difficult to handle when a system is bound by tight SLAs.
Usually these pauses are only a few milliseconds in length, but sometimes they can increase to multiple seconds.
As soon as you see a sharp increase, be prepared to investigate.
Any pause that is greater than the zookeeper.session.timeout configuration value should be considered a fault.
Thread metrics This group of metrics reports a variety of numbers related to Java threads.
You can see the count for each possible thread state, including new, runnable, blocked, and so on.
System event metrics Finally, the events group contains metrics that are collected from the logging subsystem, but are subsumed under the JVM metrics category (for lack of a better place)
For example, the log error metric provides the number of log events that occured on the.
See the official documentation on MemoryUsage for details on what used versus committed memory means.
The HBase development team has affectionately dubbed this scenario a Juliet Pause—the.
When the server wakes up, it sees that a great mistake has been made and takes its own life.
In fact, all log event counters show you the counts accumulated during the last poll period.
Using these metrics, you are able to feed support systems that either graph the values over time, or trigger warnings based on definable thresholds.
It is really important to understand the values and their usual ranges so that you can make use of them in production.
Info Metrics The HBase processes also expose a group of metrics called info metrics.
They contain rather fixed information about the processes, and are provided so that you can check these values in an automated fashion.
Table 10-4 lists these metrics and provides a description of each.
This usually is the supplied JAR file, but it could be a custom file, depending on your installation.
The values are obviously not useful for graphing, but they can be used by an administrator to verify the running configuration.
Ganglia HBase inherits its native support for Ganglia§ directly from Hadoop, providing a context that can push the metrics directly to it.
As of this writing, HBase only supports the 3.0.x line of Ganglia versions.
This is due to the changes in the network protocol used by the newer 3.1.x releases.
The GangliaContext class is therefore not compatible with the 3.1.x Ganglia releases.
In other words, future versions of HBase will support the newly introduced GangliaCon text31 and work with the newer Ganglia releases.
Advanced users also have the option to apply the patch themselves and replace the stock Hadoop JAR with their own.
The monitoring daemon needs to run on every machine that is monitored.
It collects the local data and prepares the statistics to be polled by other systems.
It actively monitors the host for changes, which it will announce using uni- or multicast network messages.
Ganglia meta daemon (gmetad) The meta daemon is installed on a central node and acts as the federation node to the entire cluster.
Ganglia also supports a hierarchy of reporting daemons, where at each node of the hierarchy tree a meta daemon is aggregating the results of its assigned monitoring daemons.
The meta daemons on a higher level then aggregate the statistics for multiple clusters polling the status from their assigned, lower-level meta daemons.
Ganglia is a distributed, scalable monitoring system suitable for large cluster systems.
See its project website for more details on its history and goals.
Ganglia PHP web frontend The web frontend, supplied by Ganglia, retrieves the combined statistics from the meta daemon and presents it as HTML.
It uses RRDtool to render the stored timeseries data in graphs.
Installation Ganglia setup requires two steps: first you need to set up and configure Ganglia itself, and then have HBase send the metrics to it.
You should try to install prebuilt binary packages for the operating system distribution of your choice.
If this is not possible, you can download the source from the project website and build it locally.
For example, on a Debian-based system you could perform the following steps.
Perform the following on all nodes you want to monitor.
Multicast Versus Unicast While the default communication method between monitoring daemons (gmond) is UDP multicast messages, you may encounter environments where multicast is either not possible or a limiting factor.
The former is true, for example, when using Amazon’s cloud-based server offerings, called EC2
Another known issue is that multicast only works reliably in clusters of up to ~120 nodes.
If either is true for you, you can switch from multicast to unicast messages instead.
This example assumes you dedicate the gmond on the master node to receive the updates from all other gmond processes running on the rest of the machines.
The host0.foo.com would need to be replaced by the hostname or IP address of the master node.
In larger clusters, you can have multiple dedicated gmond processes on separate physical machines.
That way you can avoid having only a single gmond handling the updates.
You also need to adjust the /etc/gmetad.conf file to point to the dedicated node.
See the note in this chapter that discusses the use of unicast mode for details.
This should print out the raw XML based cluster status.
Stopping the daemon is accomplished by using the kill command.
Perform the following on all nodes you want to use as meta daemon servers, aggregating the downstream monitoring statistics.
Usually this is only one machine for clusters less than 100 nodes.
Note that the server has to create the graphs, and therefore needs some decent processing capabilities.
Note the extra --with-gmetad, which is required to build the binary we will need.
The next step is to set up the configuration, copying the supplied default gmetad.conf file like so:
The data_source line must contain the hostname or IP address of one or more gmonds.
When you are using unicast mode you need to point your data_source to the server that acts as the dedicated gmond server.
If you have more than one, you can list them all, which adds failover safety.
These are used to store the collected data in roundrobin databases.
Stopping the daemon requires the use of the kill command.
The last part of the setup concerns the web-based frontend.
A common scenario is to install it on the same machine that runs the gmetad process.
At a minimum, it needs to have access to the round-robin, time-series database created by gmetad.
Ganglia comes fully equipped with all the required PHP files.
You should now be able to browse the web frontend using http://ganglia.foo.com/ ganglia—assuming you have pointed the ganglia subdomain name to the host running gmetad first.
You will only see the basic graph of the servers, since you still need to set up HBase to push its metrics to Ganglia, which is discussed next.
The central part of HBase and Ganglia integration is provided by the GangliaContext class, which sends the metrics collected in each server process to the Ganglia monitoring daemons.
In addition, there is the hadoop-metrics.properties configuration file, located in the conf/ directory, which needs to be amended to enable the context.
If this variable is left out, then the default # is no expiration.
Use this context only if you are certain that your version of HBase supports it (CDH3 does, for example)
When you are using Unicast messages, the 239.2.11.71 default multicast address needs to be changed to the dedicated gmond hostname or IP address.
Once you have edited the configuration file you need to restart the HBase cluster processes.
Usage Once you refresh the web-based UI frontend you should see the Ganglia home page, shown in Figure 10-3
You can change the metric, time span, and sorting on that page; it will reload automatically.
On an underpowered machine, you might have to wait a little bit for all the graphs to be rendered.
Figure 10-4 shows the drop-down selection for the available metrics.
Finally, Figure 10-5 shows an example of how the metrics can be correlated to find root causes of problems.
The graphs show how, at around midnight, the garbage collection time sharply rose for a heavily loaded server.
This caused the compaction queue to increase significantly as well.
It seems obvious that write-heavy loads cause a lot of I/O churn, but keep in mind that you can see the same behavior (though not as often) for more read-heavy access patterns.
For example, major compactions that run in the background could have accrued many storage files that all have to be rewritten.
This can have an adverse effect on read latencies without an explicit write load from the clients.
Ganglia and its graphs are a great tool to go back in time and find what caused a problem.
However, they are only helpful when dealing with quantitative data—for example, for performing postmortem analysis of a cluster problem.
In the next section, you will see how to complement the graphing with a qualitative support system.
The Ganglia web-based frontend that gives access to all graphs.
The drop-down box that provides access to the list of metrics.
In addition to what we have discussed so far regarding Ganglia and the metrics context, JMX also has the ability to provide operations.
These allow you to remotely trigger functionality on any JMX-enabled Java process.
Before you can access HBase processes using JMX, you need to enable it.
This enables JMX with remote access support, but with no security credentials.
It is assumed that, in most cases, the HBase cluster servers are not accessible outside a firewall anyway, and therefore no authentication is needed.
You can enable authentication if you want to, which makes the setup only slightly more complex.# You also need to restart HBase for these changes to become active.
When a server starts, it not only registers its metrics with the appropriate context, it also exports them as so-called JMX attributes.
I mentioned already that when you want to use JMX to access the metrics, you need to at least enable the NullContext WithUpdateThread with an appropriate value for period—for example, a minimal hadoop-metrics.properties file could contain:
This would ensure that all metrics are updated every 10 seconds, and therefore would be retrievable as JMX attributes.
Failing to do so would yield all JMX attributes useless.
There is a loose overlap between the metric context, as provided by the metrics framework, and the MBeans exposed over JMX.
The following MBeans are provided by the various HBase processes:
The HBase page metrics has information on how to add the password and access credentials files.
Note that the port in the name is dynamic and may change when you reconfigure where the master, or region server, binds to.
Use this operation to reset the minimal and maximal observed completion times to time varying rate (TVR) metrics.
You have a few options to access the JMX attributes and operations, two of which are described next.
JConsole Java ships with a helper application called JConsole, which can be used to connect to local and remote Java processes.
Given that you have the $JAVA_HOME directory in your search path, you can start it like so:
Once the application opens, it shows you a dialog that lets you choose whether to connect to a local or a remote process.
Since you have configured all HBase processes to listen to specific ports, it is advisable to use those and treat them as remote processes—one advantage is that you can reconnect to a server, even when the process ID has changed.
With the local connection method this is not possible, as it is ultimately bound to said ID.
Connecting to a remote HBase process is accomplished by using JMX Service URLs, which follow this format:
This uses the Java Naming and Directory Interface (JNDI) registry to look up the required details.
Adjust the <port> to the process you want to connect to.
See the hbase-env.sh file contents shown earlier, which sets a port for every process.
Once you connect to the process, you will see a tabbed window with various details in it.
Figure 10-7 shows the initial screen after you have connected to a process.
The constantly updated graphs are especially useful for seeing what a server is currently up to.
The JConsole application, which provides insight into a running Java process.
Figure 10-8 is a screenshot of the MBeans tab that allows you to access the attributes and operations exposed by the registered managed beans.
See the official documentation for all the possible options, and an explanation of each tab with its content.
The MBeans tab, from which you can access any HBase process metric.
Even the Hadoop project is working on adding some basic support for it.† As an example, we are going to use the JMXToolkit, also available in source code online (https://github.com/larsgeorge/jmxtoolkit)
You will need the git command-line tools, and Apache Ant.
After the building process is complete (and successful), you can see the provided options by invoking the -h switch like so:
You can use the JMXToolkit to walk, or print, the entire collection of available attributes and operations.
You do have to know the exact names of the MBean and the attribute or operation you want to get.
Since this is not an easy task, because you do not have this list yet, it makes sense to set up a basic configuration file that will help in subsequently retrieving the full list.
This configuration can be fed into the tool to retrieve all the attributes and operations of the listed MBeans.
These commands assume you are running them against a pseuodistributed, local HBase instance.
When you need to run them against a remote set of servers, simply set the variables included in the template properties file.
For example, adding the following lines to the earlier command will specify the hostnames (or IP addresses) for the master and a slave node:
When you look into the newly created myjmx.properties file you will see all the metrics you have seen already.
The operations are prefixed with a * (i.e., the star charater)
You can now start requesting metric values on the command line using the toolkit and the populated properties file.
The first query is for an attribute value, while the second is triggering an operation (which in this case does not return a value):
Once you have created the properties files, you can retrieve a single value, all values of an entire MBean, trigger operations, and so on.
The toolkit is great for quickly scanning a managed process and documenting all the available information, thereby taking the guesswork out of querying JMX MBeans.
JMXToolkit and Cacti Once the JMXToolkit JAR is built, it can be used on a Cacti server.
The first step is to copy the JAR into the Cacti scripts directory (which can vary between installs, so make sure you know what you are doing)
The JAR also includes a set of Cacti templates‡ that you can import into it, and use as a starting point to graph various values exposed by Hadoop’s and HBase’s JMX MBeans.
Note that these templates use the preceding script to get the metrics via JMX.
Setting up the graphs in Cacti is much more involved compared to Ganglia, which dynamically adds the pushed metrics from the monitoring daemons.
Cacti comes with a set of PHP scripts that can be used to script the addition (and updates) of cluster servers as a bulk operation.
As of this writing, the templates are slightly outdated, but should work for newer versions of HBase.
Nagios Nagios is a very commonly used support tool for gaining qualitative data regarding cluster status.
It polls current metrics on a regular basis and compares them with given thresholds.
Once the thresholds are exceededing it will start evasive actions, ranging from sending out emails, or SMS messages to telephones, all the way to triggering scripts, or even physically rebooting the server when necessary.
Typical checks in Nagios are either the supplied ones, those added as plug-ins, or custom scripts that have to return a specific exit code and print the outcome to the standard output.
There are many choices for doing so, including the already discussed JMXToolkit.
The advantage of JMXToolkit is that once you have built your properties file with all the attributes and operations in it, you can add Nagios thresholds to it.
You can also use a different monitoring tool if you’d like, so long as it uses the same exit code and/or standard output message approach as Nagios.) These are subsequently executed, and changing the check to, for example, different values is just a matter of editing the properties file.
You can follow the same steps described earlier in the Cacti install.
You can then wire the Nagios checks to the supplied JMXToolkit script.
If you have checks defined in the properties file, you only specify the object and attribute or operation to query.
If not, you can specify the check within Nagios like so:
Note that JMXToolkit also comes with an action to encode text into the appropriate format.
The crucial point, though, is that monitoring and graphing are essential to not only maintain a cluster, but also be able to track down issues much more easily.
It is highly recommended that you implement both monitoring and graphing early in your project.
It is also vital that you test your system with a load that reflects your real workload, because then you can become familiar with the graphs, and how to read them.
Set thresholds and find sensible upper and lower limits—it may save you a lot of grief when going into production later on.
Thus far, you have seen how to set up a cluster and make use of it.
Using HBase in production often requires that you turn many knobs to make it hum as expected.
This chapter covers various advanced techniques for tuning a cluster and testing it repeatedly to verify its performance.
Garbage Collection Tuning One of the lower-level settings you need to adjust is the garbage collection parameters for the region server processes.
Note that the master is not a problem here as it does not handle any heavy loads, and data does not pass through it.
These parameters only need to be added to the region servers.
You might wonder why you have to tune the garbage collection parameters to run HBase efficiently.
The problem is that the Java Runtime Environment comes with basic assumptions regarding what your programs are doing, how they create objects, how they allocate the heap to handle data, and so on.
In addition, the JRE has heuristic algorithms that adjust these assumptions as your process is running.
Even with those in place, the JRE is limited to the implementation of such heuristics and can handle some use cases better than others.
The bottom line is that the JRE does not handle region servers very well.
This is caused by certain workloads, especially write-heavy ones, stressing the memory allocation mechanisms to a degree that it cannot safely rely on the JRE assumptions alone: you need to use the provided JRE options to tweak the garbage collection strategies to suit the workload.
For write-heavy use cases, the memstores are creating and discarding objects at various times, and in varying sizes.
As the data is collected in the in-memory buffers, it needs to remain there until it has outgrown the configured minimum flush size, set with hbase.hregion.memstore.flush.size or at the table level.
Once the data is greater than that number, it is flushed to disk, creating a new store file.
Depending on how long the data was in memory, it resided in different locations in the generational architecture of the Java heap: data that was inserted rapidly and is flushed equally fast is often still in the so-called young generation (also called new generation) of the heap.
The space can be reclaimed quickly and no harm is done.
The latter only affects the region server process (as opposed to the master, for example), and is the recommended way to set these options.
Or you can use the newer and shorter specification which combines the preceding code into one convenient option:
Using 128 MB is a good starting point, and further observation of the JVM metrics should be conducted to confirm satisfactory use of the new generation of the heap.
Note that the default value is too low for any serious region server load and must be increased.
If you do not do this, you might notice a steep increase in CPU load on your servers, as they spend most of their time collecting objects from the new generation space.
Both generations need to be maintained by the JRE, to reuse the holes created by data that has been written to disk (and obviously any other object that was created and discarded subsequently)
If the application ever requests a size of heap that does not fit into one of those holes, the JRE needs to compact the fragmented heap.
This includes implicit requests, such as the promotion of longer-living objects from the young to the old generation.
If this fails, you will see a promotion failure in your garbage collection logs.
It is highly recommended that you enable the JRE’s log output for garbage collection details.
Once the log is enabled, you can monitor it for occurrences of "concur rent mode failure" or "promotion failed" messages, which oftentimes precede long pauses.
Note that the logfile is not rolled like the other files are; you need to take care of this manually (e.g., by using a cron-based daily log roll task)
The process to rewrite the heap generation in question is called a garbage collection, and there are parameters for the JRE that you can use to specify different garbage collection implementations.
The first option is setting the garbage collection strategy for the young generation to use the Parallel New Collector: it stops the entire Java process to clean up the young generation heap.
Since its size is small in comparison, this process does not take a long time, usually less than a few hundred milliseconds.
This is acceptable for the smaller young generation, but not for the old generation: in a worst-case scenario this can result in processes being stopped for seconds, if not minutes.
Once you reach the configured ZooKeeper session timeout, this server is considered lost by the master and it is abandoned.
Once it comes back from the garbage collection-induced stop, it is notified that it is abandoned and shuts itself down.
This is mitigated by using the Concurrent Mark-Sweep Collector (CMS), enabled with the latter option shown earlier.
It works differently in that it tries to do as much work concurrently as possible, without stopping the Java process.
This takes extra effort and an increased CPU load, but avoids the required stops to rewrite a fragmented old generation heap—until you hit the promotion error, which forces the garbage collector to stop everything and clean up the mess.
The CMS has an additional switch, which controls when it starts doing its concurrent mark and sweep check.
The value is a percentage that specifies when the background process starts, and it needs to be set to a level that avoids another issue: the concurrent mode failure.
This occurs when the background process to mark and sweep the heap for collection is still running when the heap runs out of usable space (recall the holes analogy)
In this case, the JRE must stop the Java process and free the space by forcefully removing discarded objects, or tenuring those that are old enough.
It will start the concurrent collection process early enough before the heap runs out of space, but also not too early for it to run too often.
Putting the preceding settings together, you can use the following as a starting point for your configuration:
Note that -XX:+CMSIncrementalMode is not recommended on actual server hardware.
These settings combine the current best practices at the time of this writing.
If you use a newer version than Java 6, make sure you carefully evaluate the new garbage collection implementations and choose one that fits your use case.
It is important to size the young generation space so that the tenuring of longer-living objects is not causing the older generation heap to fragment too quickly.
On the other hand, it should not be too large either, as this might cause too many short pauses.
Although this will not cause your region servers to be abandoned, it does affect the latency of your servers, as they frequently stop for a few hundred milliseconds.
Also, when tuning the block cache and memstore size, make sure you set the initiating occupancy fraction value to something slightly larger.
In addition, you must not specify these two values to go over a reasonable value, but definitely make sure they are less than 100%
You need to account for general Java class management overhead, so the default total of 60% is reasonable.
Memstore-Local Allocation Buffer Version 0.90 of HBase introduced an advanced mechanism to mitigate the issue of heap fragmentation due to too much churn on the memstore instances of a region server: the memstore-local allocation buffers, or MSLAB for short.
The preceding section explained how tenured KeyValue instances, once they are flushed to disk, cause holes in the old generation heap.
Once there is no longer enough space for a new allocation caused by the fragmentation, the JRE falls back to the stop-theworld garbage collector, which rewrites the entire heap space and compacts it to the remaining active objects.
The key to reducing these compacting collections is to reduce fragmentation, and the MSLABs were built to help with that.
The idea behind them is that only objects of exactly the same size should be allocated from the heap.
Once these objects tenure and eventually get collected, they leave holes in the heap of a specific size.
Subsequent allocations of new objects of the exact same size will always reuse these holes: there is no promotion error, and therefore no stop-the-world compacting collection is required.
The MSLABs are buffers of fixed sizes containing KeyValue instances of varying sizes.
Whenever a buffer cannot completely fit a newly added KeyValue, it is considered full and a new buffer is created, once again of the given fixed size.
You can use the hbase.hregion.memstore.mslab.enabled configuration property to override it either way.
If you are still experiencing these pauses, you could plan to restart the servers every few days, or weeks, before the pause happens.
As of this writing, this feature is not yet widely tested in long-running production environments.
The size of each allocated, fixed-sized buffer is controlled by the hbase.hregion.mem store.mslab.chunksize property.
The default is 2 MB and is a sensible starting point.
Based on your KeyValue instances, you may have to adjust this value: if you store larger cells, for example, 100 KB in size, you need to increase the MSLAB size to fit more than just a few cells.
There is also an upper boundary of what is stored in the buffers.
It is set by the hbase.hregion.memstore.mslab.max.allocation property and defaults to 256 KB.
Any cell that is larger will be directly allocated in the Java heap.
If you are storing a lot of KeyValue instances that are larger than this upper limit, you will run into fragmentationrelated pauses earlier.
The MSLABs do not come without a cost: they are more wasteful in regard to heap usage, as you will most likely not fill every buffer to the last byte.
Once again, it’s about striking a balance: you need to decide if you should use MSLABs and benefit from better garbage collection but incur the extra space that is required, or not use MSLABs and benefit from better memory efficiency but deal with the problem caused by garbage collection pauses.
Finally, because the buffers require an additional byte array copy operation, they are also slightly slower, compared to directly using the KeyValue instances.
Measure the impact on your workload and see if it has no adverse effect.
Compression HBase comes with support for a number of compression algorithms that can be enabled at the column family level.
It is recommended that you enable compression unless you have a reason not to do so—for example, when using already compressed content, such as JPEG images.
For every other use case, compression usually will yield overall better performance, because the overhead of the CPU performing the compression and decompression is less than what is required to read more data from disk.
Available Codecs You can choose from a fixed list of supported compression algorithms.
They have different qualities when it comes to compression ratio, as well as CPU and installation requirements.
The provided ones either are part of Java itself or are added on the operating-system level.
They require support libraries which are either built or shipped with HBase.
Note that some of the algorithms have a better compression ratio while others are faster during encoding, and a lot faster during decoding.
Depending on your use case, you can choose one that suits you best.
Before Snappy was made available in 2011, the recommended algorithm was LZO, even if it did not have the best compression ratio.
Snappy has similar qualities as LZO, it comes with a compatible license, and first tests have shown that it slightly outperforms LZO when used with Hadoop and HBase.
Thus, as of this writing, you should consider Snappy over LZO.
With Snappy, released by Google under the BSD License, you have access to the same compression used by Bigtable (where it is called Zippy)
It is optimized to provide high speeds and reasonable compression, as opposed to being compatible with other compression libraries.
It requires that you first install the native executable binaries, by either using a packet manager, such as apt, rpm, or yum, or building them from the source code and installing them so that the JNI library can find them.
When setting up support for Snappy, you must install the native binary library on all region servers.
Lempel-Ziv-Oberhumer (LZO) is a lossless data compression algorithm that is focused on decompression speed, and written in ANSI C.
Similar to Snappy, it requires a JNI library for HBase to be able to use it.
This means that the LZO installation needs to be performed separately, after HBase has been installed.‡
The GZIP compression algorithm will generally compress better than Snappy or LZO, but is slower in comparison.
While this seems like a disadvantage, it comes with an additional savings in storage space.
The performance issue can be mitigated to some degree by using the native GZIP libraries that are available on your operating system.
Java uses the Java Native Interface (JNI) to integrate native libraries and applications.
If not, you will see this message in your logfiles: "Got brand-new compressor"
This indicates a failure to load the native version while falling back to the Java code implementation instead.
An additional disadvantage is that GZIP needs a considerable amount of CPU resources.
This can put unwanted load on your servers and needs to be carefully monitored.
Verifying Installation Once you have installed a supported compression algorithm, it is highly recommended that you check if the installation was successful.
There are a few mechanisms in HBase to do that.
HBase includes a tool to test if compression is set up properly.
This will return information on how to run the tool:
You need to specify a file that the tool will create and test in combination with the selected compression algorithm.
For example, using a test file in HDFS and checking if GZIP is installed, you can run:
The tool reports SUCCESS, and therefore confirms that you can use this compression type for a column family definition.
Note how it also prints the "Got brand-new com pressor" message explained earlier: the server did not find the native GZIP libraries, but it can fall back to the Java code-based library.
Trying the same tool with a compression type that is not properly installed will raise an exception:
The Hadoop project has a page describing the required steps to build and/or install the native libraries, which includes the GZIP support.
If this happens, you need to go back and check the installation again.
You also may have to restart the servers after you installed the JNI and/or native compression libraries.
Even if the compression test tool reports success and confirms the proper installation of a compression library, you can still run into problems later on: since JNI requires that you first install the native libraries, it can happen that while you provision a new machine you miss this step.
This can be mitigated by specifying the (by default unset) hbase.regionserver.codecs property to list all of the required JNI libraries.
Should one of them fail to find its native counterpart, it will prevent the entire region server from starting up.
This way you get a fast failing setup where you notice the missing libraries, instead of running into issues later.
For example, this will check that the Snappy and LZO compression libraries are properly installed when the region server starts:
If, for any reason, the JNI libraries fail to load the matching native ones, the server will abort at startup with an IOException stating "Compression codec <codec-name> not sup ported, aborting RS construction"
Repair the setup and try to start the region server daemon again.
You can conduct this test for every compression algorithm supported by HBase.
Do not forget to copy the changed configuration file to all region servers and to restart them afterward.
Enabling Compression Enabling compression requires installation of the JNI and native compression libraries (unless you only want to use the Java code-based GZIP compression), as described earlier, and specifying the chosen algorithm in the column family schema.
The describe shell command is used to read back the schema of the newly created table.
You can see the compression is set to GZIP (using the shorter GZ value as required)
This is necessary to perform the alteration of the column family definition.
Changing the compression format to NONE will disable the compression for the given column family.
Delayed Action Note that although you enable, disable, or change the compression algorithm, nothing happens right away.
All the store files are still compressed with the previously used algorithm—or not compressed at all.
All newly flushed store files after the change will use the new compression format.
It will rewrite all files, and therefore use the new settings.
Keep in mind that this might be very resource-intensive, and therefore should only be forcefully done when you are sure that you have the required resources available.
Also note that the major compaction will run for a while, depending on the number and size of the store files.
Optimizing Splits and Compactions The built-in mechanisms of HBase to handle splits and compactions have sensible defaults and perform their duty as expected.
Sometimes, though, it is useful to change their behavior to gain additional performance.
Managed Splitting Usually HBase handles the splitting of regions automatically: once the regions reach the configured maximum size, they are split into two halves, which then can start taking on more data and grow from there.
This is the default behavior and is sufficient for the majority of use cases.
There is one known problematic scenario, though, that can cause what is called split/ compaction storms: when you grow your regions roughly at the same rate, eventually they all need to be split at about the same time, causing a large spike in disk I/O because of the required compactions to rewrite the split regions.
Rather than relying on HBase to handle the splitting, you can turn it off and manually invoke the split and major_compact commands.
This is accomplished by setting the hbase.hregion.max.filesize for the entire cluster, or when defining your table schema at the column family level, to a very high number.
Setting it to Long.MAX_VALUE is not recommended in case the manual splits fail to run.
It is better to set this value to a reasonable upper boundary, such as 100 GB (which would result in a one-hour major compaction if triggered)
The advantage of running the commands to split and compact your regions manually is that you can time-control them.
Running them staggered across all regions spreads the I/O load as much as possible, avoiding any split/compaction storm.
You will need in version 0.90.2), discussed shortly, for another way to split existing regions: it has a rolling split feature you can use to carefully split the existing regions while waiting long enough for the involved compactions to complete (see the -r and -o command-line options)
An additional advantage to managing the splits manually is that you have better control over which regions are available at any time.
This is good in the rare case that you have to do very low-level debugging, to, for example, see why a certain region had problems.
With automated splits it might happen that by the time you want to check into a specific region, it has already been replaced with two daughter regions.
These regions have new names and tracing the evolution of the original region over longer periods of time makes it much more difficult to find the information you require.
The only way to alleviate the situation is to manually split a hot region into one or more new regions, at exact boundaries.
This will divide the region’s load over multiple region servers.
As you split a region you can specify a split key, that is, the row key where you can split the given region into two.
You can specify any row key within that region so that you are also able to generate halves that are completely different in size.
This might help only when you are not dealing with completely sequential key ranges, because those are always going to hit one region for a considerable amount of time.
Table Hotspotting Sometimes an existing table with many regions is not distributed well—in other words, most of its regions are located on the same region server.# This means that, although you insert data with random keys, you still load one region server much more often method or shell command to simply remove a region of the affected table from the current server.
The master will immediately deploy it on another available server.
Presplitting Regions Managing the splits is useful to tightly control when load is going to increase on your cluster.
You still face the problem that when initially loading a table, you need to split the regions rather often, since you usually start out with a single region per table.
Growing this single region to a very large size is not recommended; therefore, it is better to start with a larger number of regions right from the start.
This is done by presplitting the regions of an existing table, or by creating a table with the required number of regions.
Work has been done to improve this situation in HBase 0.92.0
HBase also ships with a utility called RegionSplitter, which you can use to create a presplit table.
Required with -c -h                             Print this usage help -o <count>               Max outstanding splits that have unfinished major compactions -r                             Perform a rolling split of an existing region --risky                     Skip verification steps to complete quickly.STRONGLY DISCOURAGED for production systems.
By default, it used the MD5StringSplit class to partition the row keys into ranges.
You can define your own algorithm by implementing the SplitAlgorithm interface provided, parameter.
An example of using the supplied split algorithm class and creating a presplit table is:
In the web UI of the master, you can click on the link with the newly created table name to see the generated regions:
As for the number of presplit regions to use, you can start low with 10 presplit regions per server and watch as data grows over time.
It is better to err on the side of too few regions and using a rolling split later, as having too many regions is usually not ideal in regard to overall cluster performance.
Alternatively, you can determine how many presplit regions to use based on the largest store file in your region: with a growing data size, this will get larger over time, and you want the largest region to be just big enough so that is not selected for major compaction—or you might face the mentioned compaction storms.
If you presplit your regions too thin, you can increase the major compaction interval by increasing the value for the hbase.hregion.majorcompaction configuration property.
If your data size grows too large, use the RegionSplitter utility to perform a network I/O safe rolling split of all regions.
Use of manual splits and presplit regions is an advanced concept that requires a lot of planning and careful monitoring.
On the other hand, it can help you to avoid the compaction storms that can happen for uniform data growth, or to shed load of hot regions by splitting them manually.
Load Balancing The master has a built-in feature, called the balancer.
By default, the balancer runs every five minutes, and it is configured by the hbase.balancer.period property.
Once the balancer is started, it will attempt to equal out the number of assigned regions per region server so that they are within one region of the average number per server.
The call first determines a new assignment plan, which describes which regions should be moved of the administrative API iteratively.
The balancer has an upper limit on how long it is allowed to run, which is configured using the hbase.balancer.max.balancing property and defaults to half of the balancer period value, or two and a half minutes.
You can control the balancer by means of the balancer switch: either use the shell’s balance_switch command to toggle the balancer status between enabled and disabled, it no longer runs as expected.
The balancer can be explicitly started using the shell’s balancer command, or using the this method implicitly.
It will determine if there is any work to be done and return true if that is the case.
The return value of false means that it was not able to run the balancer, because either it was switched off, there was no work to be done (all is balanced), or something else was prohibiting the process.
Instead of relying on the balancer to do its work properly, you can use the move command and API method to assign regions to other servers.
This is useful when you want to control where the regions of a particular table are assigned.
Merging Regions While it is much more common for regions to split automatically over time as you are adding data to the corresponding table, sometimes you may need to merge regions— for example, after you have removed a large amount of data and you want to reduce the number of regions hosted by each server.
HBase ships with a tool that allows you to merge two adjacent regions as long as the cluster is not online.
You can use the command-line tool to get the usage details:
Here is an example of a table that has more than one region, all of which are subsequently merged:
The example creates a table with five split points, resulting in six regions.
It then inserts some rows and flushes the data to ensure that there are store files for the subsequent merge.
The scan is used to get the names of the regions, but you can also use the web UI of the master: click on the table name in the User Tables section to get the same list of regions.
Note how the shell wraps the values in each column.
The region name is split over two lines, which you need to copy and paste separately.
The web UI is easier to use in that respect, as it has the names in one column and in a single line.
The content of the column values is abbreviated to the start and end keys.
You can see how the create command using the split keys has created the regions.
The example goes on to exit the shell, and stop the HBase cluster.
Note that HDFS still needs to run for the merge to work, as it needs to read the store files of each region and merge them into a new, combined one.
Client API: Best Practices When reading or writing data from a client using the API, there are a handful of optimizations you should consider to gain the best performance.
Here is a list of the best practice options: Disable auto-flush.
When performing a lot of put operations, make sure the auto-flush feature of HTable is set to false, using the setAutoFlush(false) method.
Otherwise, the Put instances will be sent one at a time to the region server.
Puts added via HTable.add(Put) and HTable.add( <List> Put) wind up in the same write buffer.
If auto-flushing is disabled, these operations are not sent until the write buffer is.
Use scanner-caching If HBase is used as an input source for a MapReduce job, for example, make sure greater than the default of 1
Using the default value means that the map task will make callbacks to the region server for every record processed.
There is a cost to having the cache value be large because it costs more in memory for both the client and region servers, so bigger is not always better.
Limit scan scope Whenever a Scan is used to process large numbers of rows (and especially when used as a MapReduce source), be aware of which attributes are selected.
If only a small number of the available columns are to be processed, only those should be specified in the input scan because column overselection incurs a nontrivial performance penalty over large data sets.
Close ResultScanners This isn’t so much about improving performance, but rather avoiding performance problems.
If you forget to close ResultScanner instances, as returned by Always have ResultScanner processing enclosed in try/catch blocks, for example:
Block cache usage Scan instances can be set to use the block cache in the region server via the false.
For frequently accessed rows, it is advisable to use the block cache.
Using this filter combination will cause the region server to only load the row key of the first KeyValue (i.e., from the first column) found and return it to the client, resulting in minimized network traffic.
Turn off WAL on Puts A frequently discussed option for increasing throughput on Puts is to call write ToWAL(false)
Turning this off means that the region server will not write the Put to the write-ahead log, but rather only into the memstore.
However, the consequence is that if there is a region server failure there will be data loss.
You may find that it actually makes little difference if your load is well distributed across the cluster.
Configuration Many configuration properties are available for you to use to fine-tune your cluster setup.
There are advanced options you can consider adjusting based on your use case.
Here is a list of the more commonly changed ones, and how to adjust them.
The majority of the settings are properties in the hbase-site.xml configuration file.
Edit the file, copy it to all servers in the cluster, and restart the servers to effect the changes.
Decrease ZooKeeper timeout The default timeout between a region server and the ZooKeeper quorum is three minutes (specified in milliseconds), and is configured with the zookeeper.session.timeout property.
This means that if a server crashes, it will be three minutes before the master notices this fact and starts recovery.
You can tune the timeout down to a minute, or even less, so the master notices failures sooner.
Before changing this value, be sure you have your JVM garbage collection configuration under control, because otherwise, a long garbage collection that lasts beyond the ZooKeeper session timeout will take out your region server.
You might be fine with this: you probably want recovery to start if a region server has been in a garbage collection-induced pause for a long period of time.
The reason for the default value being rather high is that it avoids problems during very large imports: such imports put a lot of stress on the servers, thereby increasing the likelihood that they will run into the garbage collection pause problem.
Increase handlers The hbase.regionserver.handler.count configuration property defines the number of threads that are kept open to answer incoming requests to user tables.
The default of 10 is rather low in order to prevent users from overloading their region servers when using large write buffers with a high number of concurrent clients.
The rule of thumb is to keep this number low when the payload per request approaches megabytes (e.g., big puts, scans using a large cache) and high when the payload is small (e.g., gets, small puts, increments, deletes)
It is safe to set that number to the maximum number of incoming clients if their payloads are small, the typical example being a cluster that serves a website, since puts are typically not buffered, and most of the operations are gets.
The reason why it is dangerous to keep this setting high is that the aggregate size of all the puts that are currently happening in a region server may impose too much pressure on the server’s memory, or even trigger an OutOfMemoryError exception.
A region server running on low memory will trigger its JVM’s garbage collector to run more frequently up to a point where pauses become noticeable (the reason being that all the memory used to keep all the requests’ payloads cannot be collected, no matter how hard the garbage collector tries)
After some time, the overall cluster throughput is affected since every request that hits that region server will take longer, which exacerbates the problem.
Increase heap settings HBase ships with a reasonable, conservative configuration that will work on nearly all machine types that people might want to test with.
This option is set in hbase-env.sh, as opposed to the hbase-site.xml file used for most of the other options.
Enable data compression You should enable compression for the storage files—in particular, Snappy or LZO.
Increase region size Consider going to larger regions to cut down on the total number of regions on your cluster.
Generally, fewer regions to manage makes for a smoother-running cluster.
You can always manually split the big regions later should one prove hot and you want to spread the request load over the cluster.
You could run with 1 GB, or even larger regions.
Keep in mind that this needs to be carefully assessed, since a large region also can mean longer pauses under high pressure, due to compactions.
In this case, you could increase the cache to fit more blocks.
Another reason to increase the block cache size is if you have mainly reading workloads.
Then the block cache is what is needed most, and increasing it will help to cache more data.
The total value of the block cache percentage and the upper limit of the memstore should not be 100%
You need to leave room for other purposes, or you will cause the server to run out of memory.
The default total percentage is 60%, which is a reasonable value.
Only go above that percentage when you are absolutely sure it will help you—and that it will have no adverse effect later on.
Keep the upper and lower limits close to each other to avoid excessive flushing.
When you are dealing with mainly read-oriented workloads, you can consider reducing both limits to make more room for the block cache.
Increase blocking store files This value, set with the hbase.hstore.blockingStoreFiles property, defines when the region servers block further updates from clients to give compactions time to reduce the number of files.
Use monitoring to graph the number of store files maintained by the region servers.
If this number is consistently high, you might not want to increase this value, as you are only delaying the inevitable problems of overloading your servers.
When you have enough memory at your disposal, you can increase this value to handle spikes more gracefully: instead of blocking updates to wait for the flush to complete, you can temporarily accept more data.
Decrease maximum logfiles Setting the hbase.regionserver.maxlogs property allows you to control how often flushes occur based on the number of WAL files on disk.
The default is 32, which can be high in a write-heavy use case.
Lower it to force the servers to flush data more often to disk so that these logs can be subsequently discarded.
Load Tests After installing your cluster, it is advisable to run performance tests to verify its functionality.
These tests give you a baseline which you can refer to after making changes to the configuration of the cluster, or the schemas of your tables.
Doing a burn-in of your cluster will show you how much you can gain from it, but this does not replace a test with the load as expected from your use case.
Performance Evaluation HBase ships with its own tool to execute a performance evaluation.
It is aptly named Performance Evaluation (PE) and its usage details can be gained from using it with no command-line parameters:
Options: miniCluster     Run the test on an HBaseMiniCluster nomapred        Run multiple clients using threads (rather than use mapreduce) rows            Rows each client runs.
Default: One million flushCommits    Used to determine if the test should flush the table.
You can see the default values from the usage information in the preceding code sample, which are reasonable starting points, and the command to run a test is given as well:
The command starts a single client and performs a sequential write test.
The output of the command shows the progress, until the final results are printed.
You need to increase the number of clients (i.e., threads or MapReduce tasks) to a reasonable number, while making sure you are not overloading the client machine.
There is no need to specify a table name, nor a column family, as the PE code is generating its own schema: a table named TestTable with a family called info.
The read tests require that you have previously executed the write tests.
This will generate the table and insert the data to read subsequently.
Using the random or sequential read and write tests allows you to emulate these specific workloads.
You cannot mix them, though, which means you must execute each test separately.
While primarily built to compare various systems, it is also a reasonable tool for performing an HBase cluster burn-in—or performance test.
Installation YCSB is available in an online repository only, and you need to compile a binary version yourself.
The first thing to do is to clone the repository:
This will create a local YCSB directory in your current path.
The next step is to change into the newly created directory, copy the required libraries for HBase, and compile the executable code:
This process only takes seconds and leaves you with an executable JAR file in the build directory.
Before you can use YCSB you need to create the required test table, named usertable.
While the name of the table is hardcoded, you are free to create a column family with a name of your choice.
Multiple files can be specified, and will be processed in the order specified multiple properties can be specified, and override any values in the propertyfile -s:  show status during run (default: no status) -l label:  use label for status (e.g.
Required properties: workload: the name of the workload class to use (e.g.
To run the transaction phase from multiple servers, start a separate client on each.
To run the load phase from multiple servers, start a separate client on each; additionally, use the "insertcount" and "insertstart" properties to divide up the records to be inserted.
The first step to test a running HBase cluster is to load it with a number of rows, which are subsequently used to modify the same rows, or to add new rows to the existing table:
This will run for a while and create the rows.
The layout of the row is controlled by the given workload file, here workloada, containing these settings:
Refer to the online documentation of the YCSB project for details on how to modify, or set up your own, workloads.
The description specifies the data size and number of columns that are created during the load phase.
The output of the tool is redirected into a logfile, which will contain lines like these:
This is useful to keep, as it states the observed write performance for the initial set of rows.
The default record count of 1000 was increased to reflect a more real-world number.
You can override any of the workload configuration options on the command line.
If you are running the same workloads more often, create your own and refer to it on the command line using the -P parameter.
The second step for a YCSB performance test is to execute the workload on the prepared table.
As with the loading step shown earlier, you need to override a few values to make this test useful: increase (or use your own modified workload file) the number of operations to test, and set the number of concurrent threads that should perform them to something reasonable.
If you use too many threads you may overload the test machine (the one you run YCSB on)
In this case, it is more useful to run the same test at the same time from different physical machines.
The output is also redirected into a logfile so that you can evaluate the test run afterward.
Note that YCSB can hardly emulate the workload you will see in your use case, but it can still be useful to test a varying set of loads on your cluster.
Use the supplied workloads, or create your own, to emulate cases that are bound to read, write, or both kinds of operations.
Also consider running YCSB while you are running batch jobs, such as a MapReduce process that scans subsets, or entire tables.
This will allow you to measure the impact of either on the other.
As of this writing, using YCSB is preferred over the HBase-supplied Performance Evaluation.
It offers more options, and can combine read and write workloads.
Once a cluster is in operation, it may become necessary to change its size or add extra measures for failover scenarios, all while the cluster is in use.
Data should be backed up and/or moved between distinct clusters.
In this chapter, we will look how this can be done with minimal to no interruption.
Operational Tasks This section introduces the various tasks necessary while operating a cluster, including adding and removing nodes.
Node Decommissioning You can stop an individual region server by running the following script in the HBase directory on the particular server:
The region server will first close all regions and then shut itself down.
The master will notice that the region server is gone and will treat it as a crashed server: it will reassign the regions the server was carrying.
Disabling the Load Balancer Before Decommissioning a Node If the load balancer runs while a node is shutting down, there could be contention between the load balancer and the master’s recovery of the just-decommissioned region server.
Avoid any problems by disabling the balancer first: use the shell to disable the balancer like so:
A downside to this method of stopping a region server is that regions could be offline for a good period of time—up to the configured ZooKeeper timeout period.
Regions are closed in order: if there are many regions on the server, the first region to close may not be back online until all regions close and after the master notices the region server’s ZooKeeper znode being removed.
HBase 0.90.2 introduced the ability for a node to gradually shed its load and then shut itself down.
When you invoke this script without any parameters, you are presented with an explanation of its usage:
The HOSTNAME passed to graceful_stop.sh must match the hostname that HBase is using to identify region servers.
Check the list of region servers in the master UI for how HBase is referring to each server.
It is usually hostname, but it can also be an FQDN, such as hostname.foobar.com.
Whatever HBase is using, this is what you should pass the graceful_stop.sh decommission script.
If you pass IP addresses, the script is not (yet) smart enough to make a hostname (or FQDN) out of it and will fail when it checks if the server is currently running: the graceful unloading of regions will not run.
The graceful_stop.sh script will move the regions off the decommissioned region server one at a time to minimize region churn.
It will verify the region deployed in the new location before it moves the next region, and so on, until the decommissioned server is carrying no more regions.
At this point, the graceful_stop.sh script tells the region server to stop.
The master will notice the region server gone but all regions will have already been redeployed, and because the region server went down cleanly, there will be no WALs to split.
Rolling Restarts You can also use the graceful_stop.sh script to restart a region server after the shutdown and move its old regions back into place.
You might do the latter to retain data locality.) A primitive rolling restart might be effected by running something like the following:
Tail the output of /tmp/log.txt to follow the script’s progress.
Be sure to disable the load balancer before using this code.
You will need to perform the master update separately, and it is recommended that you do the rolling restart of the region servers.
Here are some steps you can follow to accomplish a rolling restart:
Unpack your release, make sure of its configuration, and then rsync it across the cluster.
Run hbck to ensure the cluster is consistent: $ ./bin/hbase hbck.
If you are running Thrift or REST servers on the region server, pass the --thrift or --rest option, as per the script’s usage instructions, shown earlier (i.e., run it without any commandline options to get the instructions)
This will clear out the dead servers list and reenable the balancer.
Adding Servers One of the major features HBase offers is built-in scalability.
As the load on your cluster increases, you need to be able to add new servers to compensate for the new requirements.
It seems paradoxical to scale an HBase cluster in an all-local mode, even when all daemons are run in separate processes.
However, pseudodistributed mode is the closest you can get to a real cluster setup, and during development or prototyping it is advantageous to be able to replicate a fully distributed setup on a single machine.
Since the processes have to share all the local resources, adding more processes obviously will not make your test cluster perform any better.
In fact, pseudodistributed mode is really suitable only for a very small amount of data.
However, it allows you to test most of the architectural features HBase has to offer.
For example, you can experiment with master failover scenarios, or regions being moved from one server to another.
Obviously, this does not replace testing at scale on the real cluster hardware, with the load expected during production.
However, it does help you to come to terms with the administrative functionality offered by the HBase Shell, for example.
Or you can use the administrative API as discussed in Chapter 5
Use it to develop tools that maintain schemas, or to handle shifting server loads.
There are many applications for this in a production environment, and being able to develop and test a tool locally first is tremendously helpful.
You need to have set up a pseudodistributed installation before you can add any servers in psuedodistributed mode, and it must be running to use the following commands.
They add to the existing processes, but do not take care of spinning up the local cluster itself.
Starting a local backup master process is accomplished by using the local-master-backup.sh script in the bin directory, like so:
In other words, the parameter is required and does not represent a number of servers to start, but where their ports are bound to.
Make sure you do not specify an offset that could collide with a port that is already in use by another process.
For example, it is a bad idea to use 30 for the offset, since this would result in a master RPC port on UI port.
The start script also adds the offset to the name of the logfile the process is using, thus differentiating it from the logfiles used by the other local processes.
For an offset of 1, it would set the logfile name to be:
Using an offset of, for instance, 10 would add that number into the logfile name.
Stopping the backup master(s) involves the same command, but replacing the start command with the aptly named stop, like so:
You need to specify the offsets of those backup masters you want to stop, and you have the option to stop only one, or any other number, up to all of the ones you started: whatever offset you specify is used to stop the master matching that number.
In a similar vein, you are allowed to start additional local region servers.
The script provided is called local-regionservers.sh, and it takes the same parameters as the related local-master-backup.sh script: you specify the command, that is, if you want to start or stop the server, and a list of offsets.
The logfile name has the offset added to it, and would result in:
The same concerns apply: you need to ensure that you are specifying an offset that results in a port that is not already in use by another process, or you will receive a java.net.BindException: Address already in use exception—as expected.
Starting more than one region server is accomplished by adding more offsets:
You do not have to start with an offset of 1
Since these are added to the base port numbers, you are free to specify any offset you prefer.
Stopping any additional region server involves replacing the start command with the stop command:
If you specify the offsets of all previously started region servers, they will all be stopped.
Operating an HBase cluster typically involves adding new servers over time.
This is more common for the region servers, as they are doing all the heavy lifting.
For the master, you have the option to start backup instances.
To prevent an HBase cluster master server from being the single point of failure, you can add backup masters.
These are typically located on separate physical machines so that in a worst-case scenario, where the machine currently hosting the active master is failing, the system can fall back to a backup master.
The master process uses ZooKeeper to negotiate which is the currently active master: there is a dedicated ZooKeeper znode that all master processes race to create, and the first one to create it wins.
This happens at startup and the winning process moves on to become the current master.
All other machines simply loop around the znode check and wait for it to disappear—triggering the race again.
The /hbase/master znode is ephemeral, and is the same kind the region servers use to report their presence.
When the master process that created the znode fails, ZooKeeper will notice the end of the session with that server and remove the znode accordingly, triggering the election process.
The master servers usually share the same configuration with the other servers in the cluster.
Once you have confirmed that this is set up appropriately, you can run the following command on a server that is supposed to host the backup master:
Assuming you already had a master running, this command will bring up the new master to the point where it waits for the znode to be removed.* If you want to start many masters in an automated fashion and dedicate a specific server to host the current one, while all the others are considered backup masters, you can add the --backup switch like so:
As of this writing, the newly started master also has no web-based UI available.
In other words, accessing the master info port on that server will not yield any results.
Once this has happened, they move on to the master election loop.
Since now there is already a master present, they go into idle mode as explained.
If you started more than one master, and you experienced failovers, there is no easy way to tell which master is currently active.
This causes a slight problem in that there is no way for you to know where the master’s web-based UI is located.
Since HBase 0.90.x, there is also the option of creating a backup-masters file in the conf directory.
This is akin to the regionservers file, listing one hostname per line that is supposed to start a backup master.
Adding these processes to the ZooKeeper machines is useful in a small cluster, as the master is more a coordinator in the overall design, and therefore does not need a lot of resources.
You should start as many backup masters as you feel satisfies your requirements to handle machine failures.
There is no harm in starting too many, but having too few might leave you with a weak spot in the setup.
This is mitigated by the use of monitoring solutions that report the first master to fail.
You can take action by repairing the server and adding it back to the cluster.
Overall, having two or three backup masters seems a reasonable number.
Note that the servers listed in backup-masters are what the backup master processes are started on, while using the --backup switch.
This happens as the start-hbase.sh script starts the primary master, the region servers, and eventually the backup masters.
Alternatively, you can invoke the hbase-backup.sh script to initiate the start of the backup masters.
There is an entry in the issue tracking system to rectify this inconvenience, which means it will improve over time.
For now, you could use a script that reads the current master’s hostname from ZooKeeper and updates a DNS entry pointing a generic hostname to it.
Adding a new region server is one of the more common procedures you will perform on a cluster.
The first thing you should do is to edit the regionservers file in the conf directory, to enable the launcher scripts to automat the server start and stop procedure.‡ Simply add a new line to the file specifying the hostname to add.
Once you have updated the file, you need to copy it across all machines in the cluster.
You also need to ensure that the newly added machine has HBase installed, and that the configuration is current.
Then you have a few choices to start the new region server process.
One option is to run the start-hbase.sh script on the master machine.
It will skip all machines that have a process already running.
Since the new machine fails this check, it will appropriately start the region server daemon.
Another option is to use the launcher script directly on the new server.
This must be run on the server on which you want to start the new region server process.
The region server process will start and register itself by creating a znode with its hostname in ZooKeeper.
Data Tasks When dealing with an HBase cluster, you also will deal with a lot of data, spread over one or more tables.
The following describes the possible ways in which you can accomplish this task.
Import and Export Tools HBase ships with a handful of useful tools, two of which are the Import and Export MapReduce jobs.
They can be used to write subsets, or an entire table, to files in HDFS, and subsequently load them again.
They are contained in the HBase JAR file and you need the hadoop jar command to get a list of the tools:
Note that some distributions for HBase do not require this, since they do not make use of the supplied starthbase.sh script.
Valid program names are: CellCounter: Count cells in HBase table completebulkload: Complete a bulk data load.
WARNING: It doesn't work for incrementColumnValues'd cells since the timestamp is changed after being appended to the log.
Note: -D properties will be applied to the conf used.
For example: Additionally, the following SCAN properties can be specified to control/limit what is exported..
The others are optional and can be added as required.
There is an entry open in the issue tracking system to replace the parameter parsing with a more modern command-line parser.
This will change the how the job is parameterized in the future.
You do need to specify the parameters from left to right, and you cannot omit any inbetween.
In other words, if you want to specify a row key filter, you must specify the versions, as well as the start and end times.
This will ensure that the time range is not taken into consideration.
Although you are supplying the HBase JAR file, there are a few extra dependencies that need to be satisfied before you can run this MapReduce job successfully.
MapReduce requires access to the following JAR files: zookeeper-xyz.jar, guava-xyz.jar, and google-collec tions-xyz.jar.
You need to make them available in such a way that the MapReduce task attempt has access to them.
Once the job is complete, you can check the filesystem for the exported data.
Use the hadoop dfs command (the lines have been shortened to fit horizontally):
Each part-m-nnnnn file contains a piece of the exported data, and together they form the full backup of the table.
You can now, for example, use the hadoop distcp command to move the directory from one cluster to another, and perform the import there.
Also, using the optional parameters, you can implement an incremental backup process: set the start time to the value of the last backup.
The job will still scan the entire table, but only export what has been modified since.
It is usually OK to only export the last version of a column value, but if you want a complete table backup, set the number of versions to 2147483647, which means all of them.
First we can get the usage details by invoking the command without any parameters, and then we can start the job with the table name and inputdir (the directory containing the exported files):
You can also use the Import job to store the data in a different table.
As long as it has the same schema, you are free to specify a different table name on the command line.
The data from the exported files was read by the MapReduce job and stored in the specified table.
If you have more than one table, you need to run them separately.
Using DistCp You need to use a tool supplied by HBase to operate on a table.
It seems tempting to use the hadoop distcp command to copy the entire /hbase directory in HDFS.
This is not a recommended procedure—in fact, it copies files without regard for their state: you may copy store files that are halfway through a memstore flush operation, leaving you with a mix of new and old files.
You also ignore the in-memory data that has not been flushed yet.
One way to overcome this is to disallow write operations to a table, flush its memstores explicitly, and then copy the HDFS files.
Even with this approach, you would need to carefully monitor how far the flush operation has proceeded, which is questionable, to say the least.
CopyTable Tool Another supplied tool is CopyTable, which is primarily designed to bootstrap cluster replication.
You can use is it to make a copy of an existing table from the master cluster to the slave cluster.
Options: rs.class     hbase.regionserver.class of the peer cluster specify if different from current cluster rs.impl      hbase.regionserver.impl of the peer cluster starttime    beginning of the time range without endtime means from starttime to forever endtime      end of the time range new.name     new table's name peer.adr     Address of the peer cluster given in the format hbase.zookeeer.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent families     comma-seperated list of families to copy.
CopyTable comes with an example command at the end of the usage output, which you can use to set up your own copy process.
The parameters are all documented in the output too, and you may notice that you also have the start and end time options, which you can use the same way as explained earlier for the Export/Import tool.
In addition, you can use the families parameter to limit the number of column families that are included in the copy.
The copy only considers the latest version of a column value.
Here is an example of copying a table within the same cluster:
The copy process requires for the target table to exist: use the shell to get the definition of the source table, and create the target table using the same.
You can omit the families you do not include in the copy command.
The example also uses the optional new.name parameter, which allows you to specify a table name that is different from the original.
The copy of the table is stored on the same cluster, since the peer.adr parameter was not used.
Note that for both the CopyTable and Export/Import tools you can only rely on row-level atomicity.
In other words, if you export or copy a table while it is being modified by other clients, you may not be able to tell exactly what has been copied to the new location.
Especially when dealing with more than one table, such as the secondary indexes, you need to ensure from the client side that you have copied a consistent view of all tables.
One way to handle this is to use the start and end time parameters.
This will allow you to run a second update job that only addresses the recently updated data.
Bulk Import HBase includes several methods of loading data into tables.
The most straightforward method is to either use the TableOutputFormat class from a MapReduce job (see Chapter 7), or use the normal client APIs; however, these are not always the most efficient methods.
Another way to efficiently load large amounts of data is via a bulkimport.
The bulk load feature uses a MapReduce job to output table data in HBase’s internal data format, and then directly loads the data files into a running cluster.
This feature uses less CPU and network resources than simply using the HBase API.
A problem with loading data into HBase is that often this must be done in short bursts, but with those bursts being potentially very large.
This will put additional stress on your cluster, and might overload it subsequently.
Bulk imports are a way to alleviate this problem by not causing unnecessary churn on region servers.
The HBase bulk load process consists of two main steps: Preparation of data.
The first step of a bulk load is to generate HBase data files from a MapReduce job using HFileOutputFormat.
This output format writes out data in HBase’s internal storage format so that it can be later loaded very efficiently into the cluster.
In order to function efficiently, HFileOutputFormat must be configured such that each output HFile fits within a single region: jobs whose output will be bulk-loaded into HBase use Hadoop’s TotalOrderPartitioner class to partition the map output into disjoint ranges of the key space, corresponding to the key ranges of the regions in the table.
Load data After the data has been prepared using HFileOutputFormat, it is loaded into the cluster using the completebulkload tool.
This tool iterates through the prepared data files, and for each one it determines the region the file belongs to.
It then contacts the appropriate region server which adopts the HFile, moving it into its storage directory and making the data available to clients.
If the region boundaries have changed during the course of bulk load preparation, or between the preparation and completion steps, the completebulkload tool will automatically split the data files into pieces corresponding to the new boundaries.
This process is not optimally efficient, so you should take care to minimize the delay between preparing a bulk load and importing it into the cluster, especially if other clients are simultaneously loading data through other means.
This mechanism makes use of the merge read already in place on the servers to scan memstores and on-disk file stores for KeyValue entries of a row.
Adding the newly generated files from the bulk import adds an additional file to handle—similar to new store files generated by a memstore flush.
In other words, you can bulk-import newer and older versions of a column value, while the region servers sort them appropriately.
The end result is that you immediately have a consistent and coherent view of the stored rows.
HBase ships with a command-line tool called importtsv which, when given files containing data in tab-separated value (TSV) format, can prepare this data for bulk import one row at a time.
Alternatively, you can use the importtsv.bulk.output option so that importtsv will instead generate files using HFileOutputFormat.
Running the tool with no arguments prints brief usage information:
Imports the given input directory of TSV data into the specified table.
The column names of the TSV data must be specified using the -Dimporttsv.columns option.
This option takes the form of comma-separated column names, where each column name is either a simple column family, or a columnfamily:qualifier.
The special column name HBASE_ROW_KEY is used to designate that this column should be used as the row key for each imported record.
You must specify exactly one column to be the row key, and you must specify a column name for every column that exists in the input data.
HFiles of data to prepare for a bulk data load, pass the option: Note: if you do not use this option, then the target table must already exist in HBase.
Other options that may be specified with -D include: of org.apache.hadoop.hbase.mapreduce.TsvImporterMapper.
The usage information is self-explanatory, so you simply need to run the tool, while specifying the option it requires.
It will start a job that reads the files from HDFS and prepare the bulk import store files.
After a data import has been prepared, either by using the importtsv tool with the HFileOutputFormat, the completebulkload tool is used to import the data into the running cluster.
The completebulkload tool simply takes the output path where importtsv or your MapReduce job put its results, and the table name to import into.
The optional -conf config-file parameter can be used to specify a file containing the appropriate HBase parameters, if not supplied already on the CLASSPATH.
In addition, the CLASSPATH must contain the directory that has the ZooKeeper configuration file, if ZooKeeper is not managed by HBase.
If the target table does not already exist in HBase, this tool will create it for you.
The completebulkload tool completes quickly, after which point the new data will be visible in the cluster.
Although the importtsv tool is useful in many cases, advanced users may want to generate data programatically, or import data from other formats.
To get started doing so, peruse the ImportTsv.java class, and check the JavaDoc for HFileOutputFormat.
The import step of the bulk load can also be done from within your code: see the LoadIncrementalHFiles class for more information.
Here we will look at what is required to enable replication of a table between two clusters.
The first step is to edit the hbase-site.xml configuration file in the conf directory to turn the feature on for the entire cluster:
This example adds the new hbase.replication property, where setting it to true enables replication support.
This puts certain low-level features into place that are required.
Otherwise, you will not see any changes to your cluster setup and functionality.
Do not forget to copy the changed configuration file to all machines in your cluster, and to restart the servers.
Setting the scope further prepares the master cluster for its role as the replication source.
The first command adds the ZooKeeper quorum details for the peer cluster so that modifications can be shipped to it subsequently.
The second command starts the actual shipping of modification records to the peer cluster.
For this to work as expected, you need to make sure that you have already created an identical copy of the table on the peer cluster: it can be empty, but it needs to have the same schema definition and table name.
There is one more change you need to apply to the hbase-site.xml file in the conf.2 directory on the secondary cluster:
Adding this flag will allow for it to act as a peer for the master replication cluster.
Since replication is now enabled, you can add data into the master cluster, and within a few moments see the data appear in the peer cluster table with the same name.
No further changes need to be applied to the peer cluster.
The replication feature uses the normal client API on the peer cluster to apply the changes locally.
Removing a peer and stopping the translation is equally done, using the reverse commands:
Note that stopping the replication will still complete the shipping of all queued modifications to the peer, but all further processing is ended.
Finally, verifying the replicated data on two clusters is easy to do in the shell when looking only at a few rows, but doing a systematic comparison requires more computing power.
This is why the Verify Replication tool is provided; it is available as verifyrep using the hadoop jar command once more:
Options: starttime    beginning of the time range without endtime means from starttime to forever stoptime     end of the time range families     comma-separated list of families to copy.
Args: peerid       Id of the peer used for verification, must match the one given for replication tablename    Name of the table to verify.
Other options let you specify a time range and specific families.
Additional Tasks On top of the operational and data tasks, there are additional tasks you may need to perform when setting up or running a test or production HBase cluster.
Coexisting Clusters For testing purposes, it is useful to be able to run HBase in two separate instances, but on the same physical machine.
This can be helpful, for example, when you want to prototype replication on your development machine.
Running multiple instances of HBase, including any of its daemons, on a distributed cluster is not recommended, and is not tested at all.
None of HBase’s processes is designed to share the same server in production, nor is doing so part of its design.
Presuming you have set up a local installation of HBase, as described in Chapter 2, and configured it to run in standalone mode, you can first make a copy of the configuration directory like so:
This is required to have no overlap in local filenames.
You need to assign all ports differently so that you have a clear distinction between the two cluster instances.
Operating the secondary cluster requires specification of the new configuration directory:
The first command starts the secondary local cluster, the middle one starts a shell connecting to it, and the last command stops the cluster.
Required Ports The HBase processes, when started, bind to two separate ports: one for the RPCs, and another for the web-based UI.
This applies to both the master and each region server.
Since you are running each process type on one machine only, you need to consider two ports per server type—unless you run in a nondistributed setup.
Master 60000 The RPC port the master listens on for client requests.
Master 60010 The web-based UI port the master process listens on.
Region server 60020 The RPC port the region server listens on for client requests.
Region server 60030 The web-based UI port the region server listens on.
In addition, if you want to configure a firewall, for example, you also have to ensure that the ports for the Hadoop subsystems, that is, MapReduce and HDFS, are configured so that the HBase daemons have access to them.‖
Changing Logging Levels By default, HBase ships with a configuration which sets the log level of its processes to DEBUG, which is useful if you are in the installation and prototyping phase.
For a production environment, you can switch to a less verbose level, such as INFO, or even WARN.
This is accomplished by editing the log4j.properties file in the conf directory.
Here is an example with the modified level for the HBase classes:
Hadoop uses a similar layout for the port assignments, but since it has more process types it also has additional ports.
Set this class to log INFO only otherwise its OTT ...
This file needs to be copied to all servers, which need to be restarted subsequently for the changes to take effect.
Another option to either temporarily change the level, or when you have made changes to the properties file and want to delay the restart, use the web-based UIs and their loglevel page.
Since the UI log-level change is only affecting the server it is loaded from, you will need to adjust the level separately for every server in your cluster.
Troubleshooting This section deals with the things you can do to heal a cluster that does not work as expected.
HBase Fsck HBase comes with a tool called hbck which is implemented by the HBaseFsck class.
You can get a full list of its usage information by running it with -h:
The details switch prints out the most information when running hbck, while sum mary prints out the least.
No option at all invokes the normal output detail, for example:
The extra parameters, such as timelag and sleepBeforeRerun, are explained in the usage details in the preceding code.
They allow you to check subsets of data, as well as delay the eventual re-check run, to report any remaining issues.
It also scans the HDFS root directory HBase is configured to use.
It then proceeds to compare the collected details to report on inconsistencies and integrity issues.
It is checked whether the region is listed in .META.
It compares the regions with the table details to find missing regions, or those that have holes or overlaps in their row key ranges.
The fix option allows you to repair a list of these issues.
Over time, this feature is going to be enhanced so that more problems can be fixed.
As of this writing, the fix option can handle the following problems:
Assign a user table region to a new server if it is unassigned.
Reassign a user table region to a single new server if it is assigned to multiple servers.
Reassign a user table region to a new server if the current server does not match.
Be aware that sometimes hbck reports inconsistencies which are temporal, or transitional only.
For example, when regions are unavailable for short periods of time during the internal housekeeping process, hbck will report those as inconsistencies too.
Add the details switch to get more information on what is going on and rerun the tool a few times to confirm a permanent problem.
Analyzing the Logs In rare cases it is necessary to directly access the logfiles created by the various HBase processes.
They contain a mix of messages, some of which are printed for informational.
While some of these messages are temporary, and do not mean that there is a permanent issue with the cluster, others state a system failure and are printed just before the process is forcefully ended.
Table 12-3 lists the various default HBase, ZooKeeper, and Hadoop logfiles.
Obviously, this can be modified by editing the configuration files for either of these systems.
When you start analyzing the logfiles, it is useful to begin with the master logfile first, as it acts as the coordinator service of the entire cluster.
It contains informational messages, such as the balancer printing out its background processing:
Many of these messages at the INFO level show you how your cluster evolved over time.
You can use them to go back in time and see what happened earlier on.
Typically the master is simply printing these messages on a regular basis, so when you look at specific time ranges you will see the common patterns.
If something fails, though, these patterns will change: the log messages are interrupted by others at the WARN (short for warning) or even ERROR level.
You should find those patterns and reset just before the common pattern was disturbed.
It gives you a graph showing you where the server(s) started logging an increasing number of error messages in the logfiles.
Find the time before this graph started rising and use it as the entry point into your logs.
Once you have found where the processes began logging ERROR level messages, you should be able to identify the root cause.
A lot of subsequent messages are often collateral damage: they are a side effect of the original problem.
Not all of the logged messages that indicate a pattern change are using an elevated log level.
Here is an example of a region that has been in the transition table for too long:
The message is logged on the info level because the system will eventually recover from it.
But it could indicate the beginning of larger problems—for example, when the servers start to get overloaded.
Make sure you reset your log analysis to where the normal patterns are disrupted.
Once you have investigated the master logs, move on to the region server logs.
Use the monitoring metrics to see if any of them shows an increase in log messages, and scrutinize that server first.
If you find an error message, use the online resources to search#for the message in the that this has been reported or discussed before, especially with recurring issues, such as the mentioned server overload scenarios: even errors follow a pattern at times.
Here is an example error message, caused by session loss between the region server and the ZooKeeper quorum:
You can search in the logfiles for occurrences of "ERROR" and "aborting" to find clues about the reasons the server in question stopped working.
Common Issues The following gives you a list to run through when you encounter problems with your cluster setup.
This section provides a checklist of things you should confirm for your cluster, before going into a deeper analysis in case of problems or performance issues.
The ulimit -n for the DataNode processes and the HBase processes should be set high.
To verify the current ulimit setting you can also run the following:
There’s no particular harm in setting it up to as high as 16,000 or so.
Compression should almost always be on, unless you are storing precompressed data.
Make sure that you have verified the installation so that all region servers can load the required compression libraries.
In the logfiles of the servers, you will see the root cause for this problem (abbreviated and line-wrapped to fit the available width):
The missing compression library triggers an error when the region server tries to open the region with the column family configured to use LZO compression.
The recommended garbage collection settings ought to work for any heap size.
Also, if you are colocating the region server and MapReduce task tracker, be mindful of resource contention on the shared system.
Edit the mapred-site.xml file to reduce the number of slots for nodes running with ZooKeeper, so you can allocate a good share of memory to the region server.
Do the math on memory allocation, accounting for memory allocated to the task tracker and region server, as well as memory allocated for each child task (from mapred-site.xml and hadoop-env.sh) to make sure you are leaving enough memory for the region server but you’re not oversubscribing the system.
You might want to consider separating MapReduce and HBase functionality if you are otherwise strapped for resources.
So even if you have enough memory, check your CPU utilization to determine if slots need to be reduced, using a simple Unix command such as top, or the monitoring described in Chapter 10
In rare cases, a region server may shut itself down, or its process may be terminated unexpectedly.
Double-check that the JVM version is not 1.6.0u18 (which is known to have detrimental effects on running HBase processes)
Check the last lines of the region server logs—they probably have a message containing the word "aborting" (or "abort"), hopefully with a reason.
The latter is often an issue when the server is losing its ZooKeeper session.
If that is the case, you can look into the following:
It is vital to ensure that ZooKeeper can perform its tasks as the coordination service for HBase.
It is also important for the HBase processes to be able to communicate with ZooKeeper on a regular basis.
Here is a checklist you can use to ensure that your do not run into commonly known problems with ZooKeeper: Check that the region server and ZooKeeper machines do not swap.
If machines start swapping, certain resources start to time out and the region servers will lose their ZooKeeper session, causing them to abort themselves.
You can use Ganglia, for example, to graph the machines’ swap usage, or execute.
These columns show the amount of data swapped in or out.
This should help if the total memory allocation adds up to less than the box’s available memory, yet swap is happening anyway.
Check network issues If the network is flaky, region servers will lose their connections to ZooKeeper and abort.
Check ZooKeeper machine deployment ZooKeeper should never be codeployed with task trackers or data nodes.
It is permissible to deploy ZooKeeper with the name node, secondary name node, and job tracker on small clusters (e.g., fewer than 40 nodes)
It is preferable to deploy just one ZooKeeper peer shared with the name node/job tracker than to deploy three that are collocated with other processes: the other processes will stress the machine and ZooKeeper will start timing out.
If you see this, it is probably due to either garbage collection pauses or heavy swapping.
Monitor slow disks HBase does not degrade well when reading or writing a block on a data node with a slow disk.
This problem can affect the entire cluster if the block holds data from the META region, causing compactions to slow and back up.
Again, use monitoring to carefully keep these vital metrics under control.
Also check the data node for log messages containing "exceeds the limit", which would indicate the xceiver issue.
Check both the data node and region server log for "Too many open files" errors.
This appendix lists all configuration properties HBase supports with their default values and a description of how they are used.
Use it to reference what you need to put into the hbase-site.xml file.
The description for each property is taken as-is from the hbasedefault.xml file.
The Type, Default, and Unit fields were added for your convenience.
This is to set an upper boundary for a single entry saved in a storage file.
Since they cannot be split, it helps avoiding that a region cannot be split any further because the data is too large.
It seems wise to set this to a fraction of the maximum region size.
Used mostly as value to wait before running a retry of a failed get, region lookup, etc.
Used as maximum for all retryable operations such as fetching of the root region from root region server, getting a cell’s value, starting a row update, etc.
Higher caching values will enable faster scanners but will eat up more memory and some calls of next may take longer and longer time when the cache is empty.
Do not set this value such that the time between invocations is greater than the scanner timeout; i.e.
For an estimate of server-side memory-used, evaluate hbase.cli ent.write.buffer * hbase.regionserver.handler.count.
Possible values are false for standalone mode and true for distributed mode.
If false, startup will run all HBase and ZooKeeper daemons together in the one JVM.
For any implemented coprocessor methods, the listed classes will be called in order.
For any override coprocessor method, these classes will be called in order.
After implementing your own Coprocessor, just put it in HBase’s classpath and add the fully qualified class name here.
A coprocessor can also be loaded on demand by setting HTableDescriptor.
Setting this to true can be useful in contexts other than the other side of a maven generation; i.e., running in an IDE.
You’ll want to set this boolean to true to avoid seeing the RuntimeException complaint "hbase-default.xml file seems to be for an old ver sion of HBase (@@@VERSION@@@), this version is X.X.X-SNAPSHOT"
Two values are supported now: murmur (MurmurHash) and jenkins (JenkinsHash)
If any one of a column families’ HStoreFiles has grown to exceed this value, the hosting HRegion is split in two.
Useful for preventing runaway memstore during spikes in update traffic.
Without an upper bound, the memstore fills such that when it flushes, the resultant flush files take a long time to compact or split, or worse, we OOME.
Value is checked by a thread that runs every hbase.server.thread.wakefre quency.
This can reduce the frequency of stopthe-world GC pauses on large heaps.
On close, a flush is run under the close flag to empty memory.
During this time the region is offline and we are not taking on any writes.
If the memstore content is large, this flush could take a long time to complete.
The preflush is meant to clean out the bulk of the memstore before putting up the close flag and taking the region offline so the flush that runs under the close flag has little to do.
The time an HRegion will block updates for after hitting the StoreFile limit defined by hbase.hstore.blockingStoreFiles.
After this time has elapsed, the HRegion will stop blocking updates even if a compaction has not been completed.
Larger numbers put off compaction, but when it runs, it takes longer to complete.
Usually in HBase, when writing HFiles, the blocksize is gotten from the table schema (HColumnDescriptor) but in the MapReduce output format context, we don’t have access to the schema, so we get the blocksize from the configuation.
The smaller you make the blocksize, the bigger your index will be and the less you will fetch on a random access.
Set the blocksize down if you have small cells and want faster random access of individual cells.
Set to -1 if you do not want a UI instance run.
The Kerberos principal name that should be used to run the HMaster process.
If “_HOST” is used as the hostname portion, it will be replaced with the actual hostname of the running instance.
These WAL/HLog cleaners are called in order, so put the HLog cleaner that prunes the most HLog files in front.
To implement your own LogCleanerDele gate, just put it in HBase’s classpath and add the fully qualified class name here.
Always add the above default log cleaners in the list.
Used by the client opening proxy to remote region server.
This value equal to hbase.region server.global.memstore.upperLimit causes the minimum possible flushing to occur when updates are blocked due to memstore limiting.
The same property is used by the master for count of master handlers.
Set to -1 if you do not want the RegionServer UI to run.
Enables automatic port search if hbase.regionserver.info.port is already in use.
The Kerberos principal name that should be used to run the HRegionServer process.
If “_HOST” is used as the hostname portion, it will be replaced with the actual hostname of the running instance.
An entry for this principal must exist in the file specified in hbase.regionserver.keytab.file.
Clients must report in within this period else they are considered dead.
This is not a hard limit for the number of regions, but acts as a guideline for the RegionServer to stop splitting after a certain limit.
Default is set to MAX_INT; that is, do not block splitting.
Possible values are false, true, which means only the GET method is permitted.
The URL should be fully qualified to include the filesystem scheme.
Change this configuration else all data will be lost on machine restart.
Implementation of org.apache.hadoop.hbase.ipc.RpcEngine to be used for client/ server RPC call marshaling.
Used as sleep interval by service threads such as log roller.
Change this setting to point to a location more permanent than /tmp (the /tmp directory is often cleared on machine restart)
The number of ticks that the initial synchronization phase can take.
Limit on number of concurrent connections (at the socket level) that a single client, identified by IP address, may make to a single member of the ZooKeeper ensemble.
Set high to avoid ZooKeeper connection issues running standalone and pseudodistributed.
The number of ticks that can pass between sending a request and getting an acknowledgment.
For example, by default, “host1.mydomain.com,host2.mydomain.com,host3.mydomain.com” is set to localhost for local and pseudodistributed modes of operation.
For a fully distributed setup, this should be set to a full list of ZooKeeper quorum servers.
If HBASE_MANAGES_ZK is set in hbase-env.sh, this is the list of servers on which we will start/stop ZooKeeper.
HBase passes this to the ZooKeeper quorum as the suggested maximum time for a session (this setting becomes ZooKeeper’s maxSes sionTimeout)
All of HBase’s ZooKeeper files that are configured with a relative path will go under this node.
By default, all of HBase’s ZooKeeper file paths are configured with a relative path, so they will all go under this directory unless changed.
This is written by the master and read by clients and region servers.
By default, this means the root location is stored at / hbase/root-region-server.
Here is a road map of what is planned in the next releases.
HBase 0.92.0 This upcoming version is being called the Coprocessor Release.
Coprocessors enable users to write code that runs within each region, accessing data directly where it resides.
Distributed log splitting The write-ahead log (WAL) is now split completely distributed on all region servers in parallel.
Running tasks in the UI Previously it was difficult to know what the servers were working on in the background, such as compactions or splits.
This is now visualized in the web-based UIs that the master and region servers provide.
Performance improvements Many miscellaneous performance enhancements were added to this release to make it the best performing HBase ever.
Development for 0.92.0 is still ongoing, even while this book is going into print.
Check with the aforementioned link online to see the complete list of features once this version is released.
This version is scheduled to include the following new features.
This coprocessor-backed extension allows you to create and maintain secondary indexes based on columns of tables.
Search integration This feature lets you create and maintain a search index, for example, based on Apache Lucene, per region, so that you can perform searches on rows and columns.
HFile v2 This introduces a new storage format to overcome current limitations with the existing file format.
Other interesting issues are also being worked on and may find their way into this release.
One of them is the pluggable block cache feature: it allows you to facilitate a memory manager outside the Java JRE heap.
This will reduce the amount of garbage collection churn a large heap causes—which is one of the concerns when running a large-scale HBase cluster with heavy read and write loads.
Upgrading HBase involves careful planning, especially when the cluster is currently in production.
Depending on the version of HBase you are using or upgrading to, you may need to upgrade the underlying Hadoop version first so that it matches the required version for the new version of HBase you are installing.
Upgrading to HBase 0.90.x Depending on the versions you are upgrading from, a different set of steps might be necessary to update your existing cluster to a newer version.
Be sure to remove the hbase-default.xml file from your conf directory when you upgrade.
The hbase-default.xml file is now bundled into the HBase JAR and read from there.
If you would like to review the content of this file, you can find it in the src directory at $HBASE_HOME/src/main/resources/hbase-default.xml or see Appendix A.
If that is the case, you will need to change this.
Failure to run this change will cause your cluster to run more slowly.*
Within 0.90.x You can use a rolling restart during any of the minor upgrades.
Upgrading to HBase 0.92.0 No rolling restart is possible, as the wire protocol has changed between versions.
You need to prepare the installation in parallel, then shut down the cluster and start the new version of HBase.
There are more choices to install HBase than using the Apache releases.
Cloudera makes the distribution available in a number of different formats: source and binary tar files, RPMs, Debian packages, VMware images, and scripts for running CDH in the cloud.
To simplify deployment, Cloudera hosts packages on public yum and apt repositories.
Kickstart users can commission entire Hadoop clusters without manual intervention.
As of CDH3, the following packages are included, many of which are covered elsewhere in this book: HDFS Self-healing distributed filesystem MapReduce Powerful, parallel data processing framework Hadoop Common.
HBase Hadoop database for random read/write access Hive SQL-like queries and tables on large data sets Pig Dataflow language and compiler Oozie Workflow for interdependent Hadoop jobs Sqoop Integrates databases and data warehouses with Hadoop.
Flume Highly reliable, configurable streaming data collection ZooKeeper Coordination service for distributed applications Hue User interface framework and SDK for visual Hadoop applications Whirr Library for running Hadoop, and HBase, in the cloud In regard to HBase, CDH solves the issue of running a truly reliable cluster setup, as it has all the required HDFS patches to enable durability.
The Hadoop project itself has no officially supported release in the 0.20.x family that has the required additions to guarantee that no data is lost in case of a server crash.
Overall, HBase implements close to all of the features described in Chapter 1
Where it differs, it may have to because either the Bigtable paper was not very clear to begin with, or it relies on other open source projects to provide various services and those simply work differently.
HBase stores timestamps in milliseconds—as opposed to Bigtable, which uses microseconds.
This is not much of an issue and can possibly be attributed to C and Java having different preferred timer resolutions.
While we have not yet addressed the specific details, it should be pointed out that both also use different compression algorithms.
HBase uses those supplied in Java, but can also use LZO (with a bit of work; we will look into this later).* Bigtable has a two-phase compression using BMDiff and Zippy.
HBase has coprocessors that are different from what Sawzall, the scripting language used in Bigtable to filter or aggregate data, or the Bigtable Coprocessor framework,† provides.
The details on Google’s coprocessor implementation are rather sketchy, so if there are more differences, they are unknown.
On the other hand, HBase has support for server-side filters that help reduce the amount of data being moved from the server to the client.
But HBase can also work on other filesystems thanks to the pluggable FileSystem class provided by Hadoop.
There are implementations for Amazon S3 (raw or emulated HDFS), as well as EBS.
HBase cannot map storage files into memory, something that is available in Bigtable.
There is ongoing work in HBase to optimize I/O performance, and with the addition.
While writing this book, Google made Zippy available under the Apache license and the name Snappy.
The work to integrate it with HBase is still in progress.
Bigtable has a concept called locality groups, which allow the client to group specific column families together and apply shared features, such as compression.
This is also useful when the contained columns are accessed together, as all the data is stored in the same storage files.
Column families in Bigtable are used for accounting and access control.
In HBase, on the other hand, there is only the concept of column families, combining the features that Bigtable has in two distinct concepts.
Apart from the block cache that both systems have, Bigtable also implements a key/ value cache, probably for cells that are accessed a lot.
The handling and implementation of the commit log also differs slightly.
Bigtable has two commit logs to handle slow writes and is able to switch between them to compensate for that.
This could be implemented in HBase, but it does not seem to be a topic for discussion, and therefore is omitted for the time being.
In contrast, HBase has an option to skip the commit log completely on writes for performance reasons and when the possibility of not being able to replay those logs after a server crash is acceptable.
The METADATA table in Bigtable is also used to store secondary information such as log events related to each tablet.
This historical data can be used to analyze tablet transitions, splits, and/or merges.
HBase had the notion of a historian in earlier versions that implemented the same concept, but its performance was not good enough and it has been removed.
While splitting regions/tablets is the same for both, merging is handled differently.
HBase has a tool that helps you to merge regions manually, while in Bigtable this is handled automatically by the master.
Merging in HBase is a delicate operation and currently is left to the operator to decide what is best.
Another very minor difference is that the master in Bigtable is doing the garbage collection of obsolete storage files.
One reason for this could be the fact that, in Bigtable, the storage files are tracked in the METADATA table.
For HBase, the cleanup is done by the region server that has done the split and no file location is recorded explicitly.
Bigtable can memory-map entire storage files and use them to perform lookups without a single disk seek.
HBase has an in-memory option per column family and uses its LRU cache‡ to retain blocks for subsequent use.
For example, a merging compaction also includes a memtable flush.
Mostly, though, they are the same and simply use different names.
Region names, as stored in the meta table in HBase, are a combination of the table name, the start row key, and an ID.
In Bigtable, the corresponding tablet names consist of the table identifier and the end row.
Finally, it can be noted that HBase has two separate catalog tables, -ROOTand .META., while in Bigtable the root table, since in both systems it only ever consists of one single region/tablet, is stored as part of the meta table.
The first tablet in the METADATA table is the root tablet, and all subsequent ones are the meta tablets.
We’d like to hear your suggestions for improving our indexes.
Cacti server, JMXToolkit on, 416 CAP (consistency, availability, and partition.
Performance Evaluation (PE) tool, 439–440 Persistent time varying rate (PTVR) metric rate,
He has spoken at various Hadoop User Group meetings, as well as large conferences such as FOSDEM in Brussels.
He now works for Cloudera as a Solutions Architect to support Hadoop and HBase in and around Europe through technical support, consulting work, and training.
Named for the district in Scotland where it originates, the breed dates back to the early nineteenth century, when local mares were crossed with imported Flemish stallions.
The horse was bred to fulfill the needs of farmers within the district, as well as to carry coal and other heavy haulage throughout the country.
Due to its reliability as a heavy draft horse, by the early twentieth century, the Clydesdale was exported to many countries, including Australia, New Zealand, Canada, and the United States.
The mechanical age brought a decline in the breed, and although the late twentieth century saw a slight rise in popularity and numbers, the horse is still considered vulnerable to extinction.
However, the appearance of the horse has mostly remained the same throughout its history.
Especially compared to other draft breeds, the Clydesdale has very distinctive characteristics, marked particularly by its feathered legs and high-stepping gait.
It is usually bay, brown, or black in color, and often roan, or white hair scattered throughout the coat, is also seen.
Its darkly colored body stands in contrast to its bright white face and legs, though it is not uncommon for the legs to be black.
The horse is also well known for the size of its feet, which are fitted into horseshoes comparable in size to dinner plates.
Although largely replaced by the tractor, Clydesdales remain an indispensable asset for some agricultural work, and are also ridden and shown, used for carriage services, and kept for pleasure in many places.
In the United States, the best-known ambassadors for the breed are perhaps the horses that make up the team used in marketing campaigns for the Anheuser-Busch Brewing Company.
The text font is Linotype Birka; the heading font is Adobe Myriad Condensed; and the code font is LucasFont’s TheSansMonoCondensed.
Configuration hbase-site.xml and hbase-default.xml hbase-env.sh regionserver log4j.properties Example Configuration hbase-site.xml regionservers hbase-env.sh.
Internals Choosing region servers to replicate to Keeping track of logs Reading, filtering, and sending edits Cleaning logs Region server failover.
