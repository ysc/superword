For online information and ordering of this and other Manning books, please visit www.manning.com.
The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.
Unlike many who made the jump in those early days, I don’t have any EJB horror stories to recount.
My EJB project was too small to have had any serious technology issues—pretty much any technology would have worked.
Although I never fell in love with EJB, my team was able to make it work, so we didn’t have any major complaints.
In 2004 I took a job with a new employer where everybody was using Spring.
It was immediately clear to me that Spring’s POJO- and injection-based approach was simpler to use than EJB and that it resulted in cleaner code.
Moreover, our Spring apps were supporting thousands of concurrent users without issue.
Contrary to the orthodoxy of the day, Spring was certainly ready to take on enterprise demands without EJBs and heavyweight app servers.
My teams and I built a number of Spring apps.
Even as a manager, I did quite a bit of hands-on development, and that’s how I learned the framework.
After a while, though, my management responsibilities made it harder to do as much development as I wanted to do.
I started blogging about Spring (springinpractice.com) to maintain and expand my knowledge of the framework.
Eventually, Manning came across my blog and asked me to write this book.
Nowadays I again do hands-on development, and to this day I use Spring for almost all of my Java development.
It’s a fantastic framework that makes development enjoyable and productive.
PREFACExiv Early in my career, I worked on several large enterprise projects that used EJB 1.0
It quickly became evident to me that enterprise Java development was painful.
The ideas that Rod expressed in his book, and later incorporated into the Spring Framework, struck a chord not only with me but with the Java development community at large.
Because the framework handled the infrastructure code for me, my code was cleaner, simpler, and less error-prone.
It became clear that with Spring, I was more productive and enjoying development again.
I have been an evangelist of the Spring Framework ever since.
As Spring grew, my thirst for knowledge about the framework, its surrounding technologies, and its ecosystem grew as well.
Over the years, I had become an avid technical reader and soon found myself reviewing and providing technical input for other authors’ books.
It wasn’t until Manning provided me with the opportunity to team up with Willie and coauthor this book that I was able to experience being on the other side of the fence.
As a longtime acknowledgments reader, I’m familiar with the typical expressions of gratitude aimed toward one’s significant other, children, and other inconvenienced parties.
But sitting now in the writer’s seat, I more fully appreciate how inadequate even the more vigorous of these expressions are.
I owe my first and largest debt to my wife, Raylene, who supported my efforts far beyond what was fair to ask or expect.
Personal shame prevents me from describing the many sacrifices she made on my behalf, but, suffice it to say that she’s eagerly looking forward to receiving her copy of the book so she can set it aflame.
Thank you, Raylene, for making this book possible—your name belongs on the cover of this book every bit as much as mine does.
And, of course, big thanks to my coauthor, Josh, for helping get the book across.
A special and heartfelt thanks to our development editor, Cynthia Kane.
Besides providing outstanding support on the editorial side, Cynthia was a driving.
ACKNOWLEDGMENTSxvi force in seeing this project through to its successful completion.
Thanks are due both to the technical reviewers and to the MEAP customers for their invaluable questions and feedback during the development process.
And, finally, a special thanks to our technical editor, Doug Warren, whose tireless efforts and attention to detail resulted in many improvements throughout the book.
The core framework is large, and dozens of portfolio projects extend that core, covering things like security, web flow, SOAP web services (REST web services are part of the core), enterprise integration, batch processing, mobile, various flavors of social (Facebook, LinkedIn, Twitter, GitHub, and so on), various flavors of NoSQL (MongoDB, Neo4j, Riak, and so on), BlazeDS/Flex, AMQP/Rabbit, and many more.
If “simple” means something with few parts, then Spring isn’t simple.
As a general rule, it does so by isolating infrastructural concerns (such as persistence management and transaction management) from domain concerns.
The framework handles the former so app developers can focus on the latter.
In this respect, Spring is like JEE and even its earlier J2EE incarxvii.
That Spring simplifies development without itself being simple isn’t paradoxical.
Tools that simplify work don’t themselves have to be simple to learn.
The good news is that Spring keeps the learning curve reasonable in several ways:
The core framework addresses general development needs, such as database development, object/relational mapping, transactions, web development, and so on.
One good way to learn Spring is to learn the basics of the core framework first and then move on to portfolio projects.
Certain approaches and patterns, such as POJOs, dependency injection, templates, AOP-based auto proxying, and so forth, recur throughout the framework.
Learning Spring is a matter of learning a reasonably constrained set of core approaches.
Over time, some developers became grumpy about this: it requires a lot of explicit bean wiring, and XML compares unfavorably to terser formats like JSON, YAML, and perhaps even Java.
Spring addressed this by adding a number of simpler configuration options, including namespace-based, annotation-based, and Java-based.
Spring’s development team pays attention to what’s happening outside the Java world and freely adopts ideas that offer simplifications.
Ruby on Rails has been a particularly rich source of such ideas—Spring Roo and Grails are essentially Rails clones, bringing Rails-like development to Java and Groovy, respectively.
SpringSource leads the development of the Groovy language and the Grails framework.)
Spring has strong IDE support in the form of the Spring Tool Suite (STS), which is a branded and extended version of Eclipse.
This support makes it easier to visualize bean dependencies, to understand where aspect-oriented programming (AOP) advice is being applied, and so on.
Some of the portfolio projects have additional IDE integration, such as Spring Integration with integration visualizations.
This isn’t a complete list, but it gives the basic idea.
Our hope in writing this book is to make the learning curve gentler.
The core framework appears throughout, so you’ll get plenty of practice with that.
But we also pull in several portfolio projects, mostly because they’re appropriate to the problem at hand, but also because seeing them will help you develop a sense for the recurring themes.
By the end of this book, you’ll have a broad understanding of the core framework and many of the portfolio projects.
You’ll also have sufficient practical knowledge to do real work with Spring.
We won’t make you an expert, but you’ll understand enough of the nuts and bolts to know roughly what the answers look like and where to find them.
The first three chapters are background, but are still quite substantial, and we expect most readers will find some new information in them.
Chapter 1 explains the Spring inversion of control lightweight container.
Chapter 2 shows how to work with data access, ORM, and transactions.
Chapter 3 presents an overview of Spring Web MVC, a rich framework for implementing web applications.
Chapter 3 also presents Spring Mobile, which extends Spring Web MVC to provide support for mobile application development.
Although Spring Mobile doesn’t count as background material, it fit fairly naturally with chapter 3, so we went with it.
Chapter 5 uses Spring Web Flow to implement a more sophisticated, multipage registration process.
Here the techniques apply to flow-based interactions generally, such as application processes, checkout processes, and so forth.
Chapter 7 continues with Spring Security by showing how to add authorization.
Chapter 8 covers web-based Contact Us forms, email responses, as well as notifications, mailing lists, and RSS feeds.
Chapter 9 shows how to implement a rich-text comment engine using the PageDown editor, which is the same one that StackOverflow uses.
Chapter 11 presents a configuration management database (CMDB) based on.
Chapter 12 shows how to build an article delivery engine against both Java Content Repository (JCR) and MongoDB.
Chapter 13 covers building a Spring-based help desk system on the inbound.
Chapter 14 demonstrates techniques for building your own Spring-based frameworks, with support for namespace-based configuration, AOP, annotations, and more.
The appendix explains how we’ve organized the book’s source code, as well as how to build, configure, and run it.
Who should read this book? As its title suggests, Spring in Practice aims to help you put the Spring Framework to practical use.
Although we do explain the occasional concept (such as dependency injection) or principle (like preferring whitelisting to blacklisting), there’s comparatively little of that.
Most of the time, we’re showing how to do things.
Accordingly we assume that you come to the book with enough experience to understand what you’re trying to accomplish and why.
We think it makes a nice complement to books that expand more upon the foundations, such as Spring in Action, Third Edition by Craig Walls (Manning, 2011)
Nearly all of the recipes deal with web application development in some way.
This reflects the ongoing importance of web application development, as well as the background of your lead author.
We assume that you know the basics of Java web application development, including HTTP, servlets, JSPs, and tag libraries.
In addition, more recent trends such as mobile, social, and NoSQL are now commonplace in both corporate and noncorporate settings, and some of the recipes in the book treat topics such as GitHub, OAuth, MongoDB, and Neo4j as well.
In general, we assume that you have enough experience to set those up on your own (even if you have to read about them elsewhere), and we focus on the Spring part.
This isn’t a book for absolute beginners; most developers who have been doing Java development for the past few years should find the book useful for expanding their knowledge of Spring.
Code conventions and downloads You can find the source code for all of the examples in the book at www.manning.com/ SpringinPractice or at https://github.com/springinpractice.
The appendix contains more information about how to build, configure, and run the code.
Courier typeface is used to denote code samples, as well as elements and attributes, method names, classes, interfaces, and other identifiers.
Author Online The purchase of Spring in Practice includes free access to a private web forum run by Manning Publications, where you can make comments about the book, ask technical questions, and receive help from the authors and from other users.
To access the forum and subscribe to it, point your web browser to www.manning.com/SpringinPractice.
This page provides information on how to get on the forum once you are registered, what kind of help is available, and the rules of conduct on the forum.
Manning’s commitment to our readers is to provide a venue where a meaningful dialogue between individual readers and between readers and the authors can take place.
It is not a commitment to any specific amount of participation on the part of the authors, whose contribution to the forum remains voluntary (and unpaid)
The Author Online forum and the archives of previous discussions will be accessible from the publisher’s web site as long as the book is in print.
About the authors WILLIE WHEELER currently serves as a principal applications engineer at Expedia, with a focus on continuous delivery and web operations.
If you like this book, you can find more of the same at Willie’s Spring blog, springinpractice.com.
He has worked with and evangelized the use of the Spring Framework since its inception in 2002
The title page is missing from the collection and we have been unable to track it down to date.
The book’s table of contents identifies the figures in both English and French, and each illustration bears the names of two artists who worked on it, both of whom would no doubt be surprised to find their art gracing the front cover of a computer programming book...a bit more than two hundred years.
The seller was an American based in Ankara, Turkey, and the transaction took place just as he was packing up his stand for the day.
The Manning editor did not have on his person the substantial amount of xxiii.
With the seller flying back to Ankara that evening the situation was getting hopeless.
What was the solution? It turned out to be nothing more than an oldfashioned verbal agreement sealed with a handshake.
The seller simply proposed that the money be transferred to him by wire and the editor walked out with the bank information on a piece of paper and the portfolio of images under his arm.
Needless to say, we transferred the funds the next day, and we remain grateful and impressed by this unknown person’s trust in one of us.
It recalls something that might have happened a long time ago.
They recall the sense of isolation and distance of that period—and of every other historic period except our own hyperkinetic present.
Dress codes have changed since then and the diversity by region, so rich at the time, has faded away.
It is now often hard to tell the inhabitant of one continent from another.
Perhaps, trying to view it optimistically, we have traded a cultural and visual diversity for a more varied personal life.
Or a more varied and interesting intellectual and technical life.
We at Manning celebrate the inventiveness, the initiative, and, yes, the fun of the computer business with book covers based on the rich diversity of regional life of two centuries ago‚ brought back to life by the pictures from this collection.
In this chapter, we’ll provide a brief overview of the Spring Framework, beginning with a discussion of what Spring is and giving an overview of its major pieces.
Then we’ll delve into the underlying principles behind the Spring Framework, and talk about inversion of control and how it relates to dependency injection.
Finally, we’ll dive into a small example that shows how to use the Spring Core Container handson.
The framework achieves this goal by providing developers with a component model and a set of simplified and consistent APIs that effectively insulate developers from the complexity and error-prone boilerplate code required to create complex applications.
Over the last nine years, the breadth and depth of the framework has increased significantly, yet it has remained simple to learn and easy to use.
This modularity gives developers the freedom to choose which parts of the framework to use in their applications without the need to include the entire framework.
Let’s begin our tour by looking at each of these functional areas.
In the paragraphs that follow, we’ll give you a brief introduction to each of Spring’s six basic functional areas.
We’ll take a deeper dive into each of these topics as we work through individual recipes later in the book.
We’ll further dissect what dependency injection (DI) is in section 1.2
For now, it’s enough to know that the DI container is at the core of the Spring Framework and provides the fundamental capabilities on which all the other modules are built.
Figure 1.1 A high-level block diagram illustrating Spring’s six basic functional areas.
What is Spring, and why use it? container provides the facility for decoupling the creation, configuration, and management of beans (discussed later) from your application code.
The Spring Framework also supports aspect-oriented programming with both a simpler approach called Spring AOP and the more powerful AspectJ approach.
AOP, which is covered in more detail later, aims to encapsulate cross-cutting concerns (security, logging, transaction management, and so on) into aspects to retain modularity and reusability.
These concerns often can’t be cleanly decomposed from the rest of the system and can result in code duplication, significant dependencies between systems, or both.
Like the DI container, AOP support is both independently useful to developers and used to implement different parts of framework functionality.
For example, Spring implements its support for declarative transaction management through AOP because transactions are a cross-cutting concern.
The JDBC module provides an abstraction layer that relieves developers from having to write tedious and error-prone boilerplate code by automatically managing database connections and connection pools, and by mapping vendor-specific errors into a uniform exception hierarchy.
It also makes it easy to map java.sql.ResultSets to lists of domain objects and execute stored procedures.
If you prefer to use ORM instead of straight JDBC for database access code, you’re in luck.
The OXM module provides an abstraction layer that offers simplified and consistent support for popular Object/XML mapping tools such as Castor, the Java Architecture for XML Binding (JAXB), JiBX, XMLBeans, and XStream.
The JMS module provides simplified APIs for producing and consuming messages.
Finally, the Transaction module provides support for both programmatic and declarative transaction management.
Spring’s Web module provides common web infrastructure code for integrating Spring into web applications, multipart file upload, and web-based remoting capabilities.
A quick note about iBATIS Apache iBATIS was retired in 2010 and has been superseded by MyBatis (mybatis.org)
Last but not least in the framework stack is Spring’s testing support.
This module provides support for using both the JUnit and TestNG frameworks.
Now that we’ve provided a high-level overview of the Spring Framework, let’s discuss the benefits of using the framework.
You may have worked with or even developed other frameworks or APIs that handle one or more of the Spring Framework’s concerns.
Why would you stop to learn something that requires a fairly substantial time investment? In addition to providing you with a component model and a simplified and consistent set of APIs that effectively insulate developers from complexity and error-prone boilerplate code, here are other reasons:
Quality—From the overall design of the modules, packages, class structures, and APIs to the implementation and test coverage of the source code, the Spring Framework is a great example of high-quality open source software.
Promotes best practices—Spring’s plain old Java object (POJO)-based programming model promotes decoupled component models, unit testing, and other best practices.
Modest learning curve—Due to the consistency and simplicity of the APIs, Spring isn’t hard to learn.
As we make our way through the framework, you’ll see that common patterns emerge.
Plus, hundreds of resources online and in print are at your disposal, including message boards where the core developers often participate.
Popularity—As evidenced by myriad publications, websites, and job postings, the Spring Framework is almost ubiquitous.
Spring offers a lot, and it takes time to understand and appreciate the landscape.
But rest assured that the effort is well worth it.
By learning Spring and using it to solve problems, you’ll see how to bring together disparate technologies and incorporate them into cohesive applications.
You’ll keep hardcoded configuration parameters out of your classes and centralized in standard locations.
You’ll design interface-based dependencies between classes and better support changing requirements.
And ultimately, you’ll get more done with less effort and in less time because the Spring Framework handles the plumbing while you focus on writing code to solve business problems.
Now that you have a general idea of what the framework offers, let’s take a deeper dive into the capabilities of the Core Container shown in figure 1.2
Spring’s Core Container provides the inversion of control (IoC) and DI capabilities on which all the other modules are built.
IoC became popular some years back through DI containers like Spring.
Although that might be eons ago in internet time, it’s still a relatively new and unfamiliar concept for many developers.
In this section, we’ll explain what IoC is and examine the forces that produced it.
You’ll even get your hands a little dirty and see how to configure Spring’s container.
Consider the relationship between a data access object (DAO) and the DataSource it relies on in the following code sample.
For the DAO to work with the DataSource, you need to create and initialize the DataSource with various connection parameters within the JdbcAccountDao class:
In this code sample, JdbcAccountDao specifies a dependency on a JDBC DataSource.
As shown in figure 1.3, the code also specifically creates a dependency on a BasicDataSource, a specific DataSource implementation from the Apache Commons Database Connection Pool (DBCP) project.
An obvious problem here is that the JdbcAccountDao class is intimately aware of the DataSource’s implementation, creation, and configuration.
Another potential problem is that it’s likely that many DAOs may need to share this connection information.
As a result of the current design, changing the DataSource’s implementation or configuration may involve multiple code changes, recompilations, and redeployments every time the DataSource implementation or configuration changes.
You could externalize the connection parameters with java.util.Properties, and that would certainly be an improvement.
In the previous code sample, the class is specifying and driving the dependencies.
Let’s look at how you can invert this control by injecting your dependencies instead.
One way to eliminate the concrete dependency on BasicDataSource would be to specify the dependency externally and have that dependency injected into the JdbcAccountDao as a DataSource.
This gives you a lot of flexibility because you can easily change the configuration in one place.
If you want to proxy the DataSource before injecting it, you can do that.
In unit-testing scenarios, if you want to replace the DataSource with a mock, you can do that too.
Again, DI provides a lot of flexibility that you don’t have when the dependency’s construction is hardwired into the components relying on the dependency.
To make DI work, you need to create the DataSource externally and then either construct the DAO with it or set it on the DAO with a setter method, as shown here:
Notice that the DAO no longer has a hardwired dependency on BasicDatasource.
As a result, you’ll notice that the BasicDataSource import has been removed.
Because the dependency is provided via a setter, it’s no longer necessary to provide a constructor to.
Figure 1.3 JdbcAccountDao specifies a dependency on Apache DBCP’s BasicDataSource, which is a concrete DataSource implementation.
Another approach to refactoring this class would have been to provide a DataSource implementation as a constructor argument instead of using a setter.
But you might argue reasonably that you’ve succeeded only in pushing the construction of this dependency elsewhere in the code.
In one respect, you’ve made things worse: you’ve introduced dependencies between AccountService and BasicDataSource—a relationship that is clearly undesirable.
You also have a dependency between AccountService and JdbcAccountDao (a concrete class), so you’re still in the same boat you started in (see figure 1.4)! It’s easy to see how the entire dependency graph for a particular system could become complicated and inflexible with nodes that are hard to swap out.
Figure 1.4 Now JdbcAccountDao has the desired interface dependency on DataSource, but AccountService has dependencies on two concrete classes.
To clean things up, you need to revise what is doing the injecting.
You can move the DI away from client code and over to Spring.
In this scenario, client code doesn’t request or look up an AccountService.
Instead, the AccountService is transparently injected into client code when the client code is initialized.
The following code shows AccountService with a strict interface dependency on AccountDao:
How do you specify the dependency chain? With Spring, one option is to use XML to assemble it declaratively, as in the following listing.
If you’re new to Spring, this configuration might be unfamiliar, but its meaning should be clear enough.
At B you declare the DataSource and set it up with its configuration parameters.
At C you declare the JdbcAccountDao and inject it with the DataSource.
Similarly, you inject the JdbcAccountDao into the AccountService at D.
The end result is that the service now carries the entire dependency chain, and the configuration is entirely transparent to the service.
The cleaned up, new relationship is shown in the class diagram in figure 1.5
As you can see, the class structure has taken on more of a layered approach.
In contrast to figure 1.4, notice that the layers above are dependent only on the layers below, and all the dependencies are expressed as interfaces.
This simplifies the dependency graph and makes it easier to swap out individual nodes.
In the next section, we’ll use an example to illustrate how you can use Spring to manage and inject the concrete implementations of your dependencies.
With the what and the why past you, you’ll get your hands dirty and try Spring Framework DI with a small sample application.
The application won’t be anything serious— just enough to learn the basics of expressing and managing dependencies using Spring.
You’ll build a domain object, a DAO that reads from a CSV file, and a service, and then you’ll wire everything up.
The code will get a list of delinquent accounts for an imaginary utility company.
You’ll define a delinquent account as one with an unpaid balance that hasn’t been credited for 30 days or more.
The service will have the responsibility of finding out which accounts are delinquent, but it will need to delegate to the DAO to get a list of candidate accounts.
Before you create the DAO, you’ll create the domain object it works with.
The toy Account domain object in the following listing has only the fields and methods you need to demonstrate DI through other parts of the sample application.
Figure 1.5 Now the dependencies are interface-based and the concrete classes are configured transparently through Spring.
In a real application, accounts wouldn’t appear out of thin air.
You’d have databases, files, and other systems that you’d store and read them from.
For the example, you’ll create a DAO that parses a comma-separated values (CSV) file with data like this:
In the CSV file, accounts.csv, the first field is the account number, the second is the balance (positive or negative), and the third is the date the account was last credited in MMDDYYYY format.
As mentioned in section 1.2, interface-based dependencies keep things flexible by allowing pluggable, varying implementations.
Before you create the DAO responsible for consuming this file, let’s create an interface for it.
The following interface has only a single read operation to get all accounts from whatever back-end store a particular implementation would read against.
We’re leaving out the rest of the CRUD operations because you don’t need them for this example:
Now you’ll create the concrete AccountDao implementation that reads Accounts from a CSV file.
Assume that the name of the CSV file might change over time; therefore, you won’t hardcode it.
It would be perfectly legitimate to externalize it in a properties file and use java.util.Properties to read it in; but instead of doing that, you’ll configure it with Spring.
The CsvAccountDao declares a csvFile field where Spring populates B.
For Spring to set it for you, you define the required public setter at C.
You have enough code in place to bring Spring into the picture.
There are different ways to configure objects and their dependencies with Spring, and the most popular are XML and annotations.
You’ll use XML in this section; we’ll build on these concepts when we introduce annotation-style configuration in section 1.5
The XML file in the following listing shows how to define and configure a bean using Spring.
By convention, developers usually name the Spring configuration file applicationContext.xml, but it can be named anything.
In real-world applications, it usually makes sense to break out the Spring configuration across multiple files, especially when the applications are large with many bean definitions.
When you do that, the configuration is often broken out by architectural, rather than functional, slice.
You might create configuration files dedicated to DAOs, services, servlets, and security.
The method of configuration used here is called setter injection.
Spring provides other ways to configure beans and wire dependencies, including constructor injection and factory-method injection.
In this book, we mostly use setter injection because it’s the most popular approach.
Omission of error checking and assertions To keep the examples clear, we’ve omitted error checking that you would expect to see in production-ready code.
For example, in the previous sample, production-ready code would assert that the String passed in for the location of the csvFile was not null and that the file existed at the path specified.
A simple bean configuration example you progress through this book, you’ll indeed do this.
Because your needs are minimal to start, a single file will suffice.
Spring ships with different schemas for configuring its different pieces of functionality, such as AOP and transaction management.
For now, you declare the beans schema at B, which is the most fundamental of the schemas in that the functionality that the other schemas provide is generally expressible (albeit in a more verbose fashion) as explicit bean definitions.
The beans schema provides everything you need to define beans of all sorts, configure them, and wire them together.
The class attribute is set to the fully qualified name of the CsvAccountDao.
Spring uses reflection to instantiate the class as you request it through the container (or request other classes that depend on it)
At D, you declare a property element for the csvResource property.
Again, Spring uses reflection to set this to the value in the value attribute.
If you’re following the code examples, you’ll find this file in the src/main/resources directory.
Spring relies on the JavaBeans programming model in order to set the property, so that’s why you declare the setter in the Account domain object.
With the domain object and DAO out of the way, you can build the service that has the responsibility of scanning all accounts and finding delinquent ones.
Listing 1.5 AccountService.java: a service responsible for finding delinquent accounts.
At D, you iterate through all the Accounts that the DAO returns and test whether they’re delinquent.
If they are, you add them to a list and return them.
All you need to do is add a simple bean definition to the applicationContext.xml configuration file as shown next.
Like the CsvAccountDao you defined in listing 1.4, you give the AccountService bean definition id and class attributes.
The subtle but important difference is the way you’re injecting the AccountDao into the service.
Here, you’re using the ref instead of the value attribute.
The ref attribute is used for injecting other beans you’ve defined.
The value attribute injects simple primitive and value object properties.
All you need to do is create a console application to run the code.
Based on the three accounts you’ve defined, the only one that is delinquent is 200
You create an instance of ClassPathXmlApplicationContext at B and pass in the classpath-relative location of the configuration file.
With this class, you can get a reference to any bean you define in the Spring configuration file by the ID you defined in the bean’s definition.
The ApplicationContext interface and its implementations are the gateway into the beans through Spring.
They essentially make up a sophisticated implementation of the factory pattern.
In order for the factory to instantiate beans, the beans must have a no-argument constructor.
The implicit, default no-argument constructor is fine.) Spring provides support for instantiating beans with constructor.
Figure 1.6 AccountService has an interface-based association with AccountDao, and both depend on Account.
At C, you get a reference to the AccountService you’ve defined along with its AccountDao dependency and the AccountDao’s configured csvFile property in one fell swoop.
In this section, you built a simple application with the Spring Framework.
It’s easy to see that a full-blown application with many DAOs, services, and other components configured and wired together through Spring would be more cleanly separated and easier to manage than one that wasn’t.
Now that we’ve piqued your interest with the basics, in the next section we’ll take a more detailed look at the framework’s DI capabilities.
After examining the beans namespace, we’ll look at different ways of injecting dependencies, configuring and externalizing bean properties, bean scopes, and a little syntax sugar to make your configuration more clear and concise.
As you’ve seen, a Spring bean represents a POJO component.
Because the other five functional areas of the Spring Framework (Data Access/Integration, Web, AOP, Instrumentation, and Test) build on the capabilities offered by the Core Container, learning how to wire beans is paramount to understanding and using the Spring Framework.
In this section, we’ll look at using XML to configure Spring.
In the following sections, we’ll discuss two of Spring’s XML namespaces that we’ll be using throughout this book: the core beans namespace and the handy p namespace.
The beans namespace is the most fundamental and deals with DI; it provides a way to define beans and wire dependency relationships between them.
To create a configuration file that uses the beans namespace, you create an XML document and reference the schema:
At this point, all you have is an empty configuration.
You can add object definitions with the inner bean element:
Here you define an AccountService by creating a bean element with id and class attributes.
As described in section 1.3, Spring uses reflection to create a new instance of the class specified as you fetch instances by ID programmatically through the ApplicationContext interface.
You could have constructed the AccountService using the new keyword, but the creation of service layer objects is rarely so straightforward.
They often depend on DAOs, mail senders, SOAP proxies, and whatnot.
You could instantiate each of those dependencies programmatically in the AccountService constructor (or through static initialization), but that leads to hard dependencies and cascading changes as they’re swapped out.
Additionally, you could create dependencies externally and set them on the AccountService via setter methods or constructor arguments.
Doing so would eliminate the hard internal dependencies (as long as they were declared in the AccountService by interface), but you’d have duplicated initialization code everywhere.
Here’s how you create a DAO and wire it up to your AccountService the Spring way:
You’ve injected the AccountService with a JdbcAccountDao by declaring a property element for the dependency.
The property element has a name attribute, which is the name of the property you want to set; and it has a ref attribute that’s set to the id of the bean you want to inject.
Other services and classes can depend on the accountDao bean.
If its implementation changes, say from a JDBC one to a Hibernate one, you just need to update the class attribute in its configuration instead of going to each class with the dependency and swapping it out manually.
Spring supports this type of wiring and can easily work with complex object graphs that are multiple levels deep.
Just as AccountService needs a no-argument constructor so Spring can use reflection.
But, as mentioned previously, Spring lets you instantiate objects with constructor arguments, too.
You can nix the setter method and declare a constructor instead:
The JdbcAccountDao you configured is likely to have additional initialization requirements.
In the example, you’ll register a JDBC driver and set up additional connection information.
In the following code sample, you configure a BasicDataSource with simple properties using the property element instead of the ref attribute:
Of course, in a production system, you’d likely use a javax.sql.DataSource with connection pooling.
We’ll show you how to do that in chapter 2.) Here you’re setting simple String properties on the BasicDataSource.
But what if the properties were numeric or java.util.Date properties? In that case, Spring would attempt to convert the String specified in the value attribute to its appropriate type with a java.beans.PropertyEditor implementation.
Spring ships with quite a few of such implementations and allows you to define your own if you need to.2
In one sense, you could say you’ve done a good thing by configuring the JdbcAccountDao properties externally to the class.
Changing configuration parameters won’t require any recompiling of code; you can just update the XML.
Also, having a centralized configuration makes it easy to change things in one place (or a few logically related places.
You can read more about the built-in PropertyEditors at http://mng.bz/7CO9
But in most environments, you don’t hook straight up to a production database server and start cranking out and running untested code.
Luckily, Spring allows you to handle scenarios like this with an API class: PropertyPlaceholderConfigurer.
In order to use PropertyPlaceholderConfigurer, you first create a properties file (call it springbook.properties):
Then, you define the PropertyPlaceholderConfigurer bean in the Spring configuration.
Finally, as you define beans, you use the placeholder constructs ${} when specifying their property values, so the container can resolve them at runtime.
That’s because the Spring container detects its presence automatically and enables its functionality.
In the location property C of the PropertyPlaceholderConfigurer, the file is located in the root of your classpath.
If you’re following the code examples, the springbook.properties file is located in the src/main/resources directory, which is included in your classpath.
In the previous configuration, you point to a file located in the home directory of whatever computer account is running your application through a Java system environment variable.
Spring will attempt to substitute Java system environment variables in the placeholders if they aren’t found in the  properties files.) This is handy because server admins can store sensitive information in these files, which developers may not need to access.
Also, this makes it easy for multiple developers to work on a project while pointing to their own databases.
At D, you change the hardcoded values over to substitution placeholders.
At this point, you’re beginning to get a solid picture of how beans are configured cleanly in Spring.
Although there’s more than we can possibly hope to cover here, let’s spend a little time going into depth about the important concept of bean scopes.
When defining a bean with Spring, you can specify how you want instances created and managed as they’re retrieved from the container.
There are five such scopes: singleton, prototype, request, session, and global session.
Figure 1.7 Using external properties files to manage configuration for separate environments.
Singleton scope is the default scope for beans declared in Spring.
These singletons are different than Java classes that implement the singleton design pattern.
Declaring a singleton bean in Spring ensures that only one instance exists on a per-container basis, whereas an instance of a class that implements the singleton design pattern will be the only one available on a per-classloader basis.
As you request singleton beans from the container, Spring will create instances and cache them if they haven’t Spring will return already-existing instances from its cache.
Therefore, singleton beans in Spring often don’t maintain state because they’re usually shared among multiple threads (such as in servlet environments)
For example, singleton services often have references to singleton DAOs, and the DAOs might have references to Hibernate SessionFactorys, which are thread-safe.
Note that you generally wouldn’t declare a DAO with prototype scope because you design them for thread-safety.
We’re using a DAO to maintain consistency with previous examples.)
During that time, JdbcAccountDao is created and injected, but never cached.
Figure 1.8 Singleton-scoped beans are shared among dependent class instances.
A singleton referencing a prototype makes the prototype effectively singleton in scope.
But if you were to simultaneously inject JdbcAccountDao into another bean with singleton scope, that bean would maintain a reference to a separate instance.
Spring can manage the complete lifecycle including creation and destruction of singleton-scoped beans, but it can only manage the creation (instantiation, configuration, and decoration through dynamic proxying) of prototype-scoped beans.
It’s up to the client code to clean up, release resources, and otherwise manage the lifecycle of prototype-scoped beans.
In this way, prototype beans are similar to classes created with the new keyword in Java, although it would be irregular to substitute the former for the latter unless there are complex initialization requirements for stateful beans that Spring will make easier to manage.
It doesn’t matter what web framework you’re using; they function identically across all of them.
Request-scoped beans are created each time an HTTP request makes its way into a servlet resource that is injected with one.
Similar to request-scoped variables in servlets, these beans are safe to change and work with because the servlet specification dictates one thread per HTTP request.
Session-scoped beans are confined to the scope of standard session-scoped variables.
These, like request-scoped beans, are safe to modify and work with because although their access isn’t restricted to the same, single thread, it’s restricted to one thread at a time, which is tied to the current session of the client making the requests.
Global session–scoped beans are applicable only in the context of portlet applications.
Like session-scoped beans, they exist throughout an entire session, but they’re shared among all the portlets in a complete portlet web application, whereas sessionscoped beans are created and managed for each individual portlet.
There are a few prerequisites to employing beans with these scopes.
First, if you’re using a framework other than Spring Web MVC, you have to register a RequestContextListener in the servlet deployment descriptor (web.xml):
Figure 1.9 Prototype-scoped beans are instantiated every time one of their dependent classes is retrieved from the container.
But if the dependent class is singleton-scoped, subsequent retrievals of it will return already-cached instances where the prototype-scoped dependency is effectively cached as well (it isn’t reinstantiated)
You can use RequestContextListener in Servlet containers that implement version 2.4 of the Servlet specification or greater.
These listeners and filters enable the required integration between Spring and whatever servlet container you’re using.
Basically, Spring can intercept requests and decorate them with this scoping functionality.
If you’re fetching beans with these scopes from an ApplicationContext, the ApplicationContext implementation must be a web-aware one such as XmlWebApplicationContext.
Otherwise you’ll get an IllegalStateException complaining about unknown bean scopes.
We’ve spent a lot of time with the beans namespace.
As indicated at the outset, this chapter’s treatment isn’t comprehensive, and there’s quite a bit more to learn if you decide to pursue a more in-depth study.
At this point, let’s turn our attention to the p namespace, whose mission is to make your XML configuration a little cleaner than you can make it with the beans namespace alone.
The p namespace extends the beans namespace by providing an alternate propertydeclaration syntax.
Instead of configuring properties as XML elements as you do with JdbcAccountDao, you can declare them as attributes on the bean element.
The p namespace provides you with XML syntax sugar and makes the configuration more concise.
You declare the namespace at B but don’t put a corresponding schema location.
It’s easy to see why, when you look at the namespace usage at C.
The p namespace isn’t defined in an XSD file like the beans namespace.
In addition to specifying simple properties, you can also inject full-blown beans:
The only difference between this attribute declaration and the ones for simple properties is that you append -ref onto the end of this attribute.
In general, we tend to favor the convenience and clarity of using annotations for configuring many cross-cutting concerns (validation, persistence, transactions, security, web services, request mappings, and so on), so we’ll use them quite a bit.
Subsection 1.5.4 discusses some of the debate regarding XML versus annotation-based configuration.) The foregoing techniques are still useful for a couple of reasons: not everything that Spring offers can be configured solely with annotations, and you may prefer to keep your POJOs insulated from configuration concerns, which arguably is the whole point.
Spring 3.1 introduced the c namespace with two goals in mind.
The first was to improve on the existing constructor-injection syntax and clarify which constructor arguments are being set.
The second goal was to provide similar XML syntax sugar for those who prefer constructor-based injection.
It’s no longer necessary to provide a number of constructor-arg elements for constructor injection.
Prior to Spring 3.1, you had to configure this bean as follows:
Similar to the p namespace, the c namespace provides XML syntax sugar and makes the configuration clearer and more concise.
You declare the required namespace at B in the example configuration.
Just like the p namespace, the c namespace isn’t defined in an XSD file.
If you’re using a third-party library where this may not be in your control, you specify the index of each constructor argument.
Just as with the p namespace, you can append the same –ref suffix to reference other beans.
In this section, we took a closer look at how to wire beans using XML and covered a significant amount of ground.
Then we looked at constructor and setter injection as well as configuring and externalizing bean properties.
Finally, we introduced bean scopes and demonstrated how you can use the p and c namespaces.
In the next section, you’ll see how you can use Java annotations to define Spring components and their dependencies without using XML.
Now that we’ve shown you the basics of wiring beans together, we’ll introduce the annotation-based configuration that was introduced in Spring 2.0 and further enhanced in.
The @Autowired annotation can be applied to constructors or fields and allows you to wire relationships by type, without having to explicitly set them up in the XML configuration.
We use autowiring and component scanning often in this book because they’re much more compact (although less explicit).3
In the previous section, you injected a DAO into a service using regular bean declarations along with the p namespace:
With the @Autowired annotation, you can eliminate the p:accountDao-ref attribute.
First let’s look at the annotation, and then you’ll modify the XML configuration to support it:
The @Autowired annotation won’t do you any good until you modify the XML configuration to use it.
You have to reference the context schema in your configuration and declare the annotation-config element as shown in the next listing.
You’ve also removed the setters from the accountDao and accountService beans.
There is a tradeoff between using autowiring and not using it.
One of those is whether the container should enable annotation-based configuration.
After that, turning on annotations is a simple matter of declaring the annotation-config element C.
You can see at D that the p:accountDao-ref attribute is no longer necessary.
Notice that you also split out the DataSource configuration E into its own bean.
As we’ll discuss in the next chapter, notice that the simple String-based configuration parameters are still used to configure the properties of this bean.
If you were to run an application that used these beans and this configuration, you’d see that Spring would inject the AccountService with the JdbcAccountDao.
When a field is @Autowired, Spring looks through the container for beans with a matching type to inject.
Because JdbcAccountDao implements the AccountDao interface and AccountService declares an AccountDao field, Spring automatically wires the dependency.
What happens when there’s more than one bean with a matching type?
As an aside, if you declared the field as an AccountDao array or an AccountDao-typed collection, Spring would populate the array or collection with both DAOs:
Obviously, it wouldn’t make much sense to have an array of DAOs in most cases, but it’s fairly common to have multiple implementations of an interface and a need to specify one at a time.
Fortunately, @Autowired permits you to specify the bean you want:
You qualify by the id attribute of the bean declaration.
This isn’t the only way to do it; the context namespace supplies a qualifier element you could nest in the AccountDao declarations.
The qualifier element would come in handy if you needed to keep the id attributes as they are (maybe because you had a convention in place) and also needed different qualification rules.
For example, you could distinguish a regular Hibernate-based DAO from a high-octane straight-JDBC DAO with hand-crafted SQL that a DBA spent hours tuning for performance.
So far, we’ve shown how to wire beans using @Autowired.
You still have to define the beans themselves so the container is aware of them and can inject them for you.
But with Spring’s stereotype annotations, you can annotate classes and enable component scanning (which we’ll talk about shortly), and Spring will automatically import the beans into the container so you.
Autowiring and component scanning using annotations don’t have to define them explicitly with XML.
The @Component annotation flags a bean so the component-scanning mechanism can pick it up and pull it into the application context.
If you wanted to get rid of the JdbcAccountDao in the XML configuration, all you’d have to do is place @Component above the class declaration:
Now you could completely wipe out the XML bean declaration, and the @Autowired accountDao field in AccountService would be automatically populated with a JdbcAccountDao instance.
Although this is all well and good, there is another, more suitable annotation that provides additional benefits specifically for DAOs.
It not only imports the DAOs into the DI container, but it also makes the unchecked exceptions that they throw eligible for translation into Spring DataAccessExceptions (also unchecked)
The @Service annotation is also a specialization of the component annotation.
Additionally, tool support and additional behavior might rely on it in the future.
Finally, the @Controller annotation marks a class as a Spring Web MVC controller.
It too is a @Component specialization, so beans marked with it are automatically imported into the DI container.
We haven’t talked much about Spring Web MVC yet, but we’ll be using it heavily throughout this book.
That is, you can tell Spring that you want a certain method invoked when a user agent requests one of your application’s URLs.
Just as you need to declare context:annotation-config to turn on autowiring, you need to declare context:component-scan to enable the importing of classes that are annotated with the stereotypes.
The context:component-scan element requires a base-package attribute, which, as its name suggests, specifies a starting point for a recursive component search.
Also, the component-scan element can be declared multiple times and pointed to multiple packages.
It’s common to lay out that case, you’d declare three component-scan elements, each with a base-package attribute pointing to a different package.
Also, when component-scan is declared, you no longer need to declare context:annotation-config, because autowiring is implicitly enabled when component scanning is enabled.
Finally, you no longer need to declare the flagged beans in your configuration.
When component scanning kicks in at application startup, Spring will recurse over all the packages you’ve specified, look for stereotype-annotated classes, and import them into the container.
Then, when declaring the componentscan element, tack on the name-generator attribute and point it to the name of your custom BeanNameGenerator.
Because you’re using a compiled class from a third-party library (BasicDataSource), you’re unable to decorate its code with annotations.
As a result, you must still specify its configuration here.
Many of the recipes use Hibernate, JPA, and Hibernate Validator annotations as well.
This in part reflects the general direction in which the Spring team is steering, and in part reflects the fact that annotations are more convenient and usable in a wide variety of situations.
The choice of whether to use XML or annotations for configuration is the subject of much community debate.
With XML the configuration is centralized into a set of files, and each file is typically dedicated to a particular architectural concern or slice.
For example, if you had a Spring configuration file for DAOs and another for services and you wanted to change something, you’d pull up the appropriate file, scan through it, and make the change.
It’s easy to locate the file, but depending how large it is, it could be difficult to scan through.
Additionally, keeping configuration in the XML keeps POJOs clean, which is a big part of the argument for using Spring in the first place.
Distributing configuration makes it more challenging to replace one piece of infrastructure technology with another.
On the other hand, if you use annotations, your configuration is consolidated according to application verticals.
We prefer this because feature changes are likely to span architectural slices, and it’s nice to be able to change things in one place.
For instance, if you needed to add a field to a domain class, you’d open the class in your IDE, add the field, and add JPA and Bean Validation Framework annotations to it at the same time.
This is easier than opening each respective XML configuration file, scanning through it, and making the necessary changes.
You need to enable component scanning in XML, and Spring ships with several classes that can only be configured with XML.
This chapter has provided a brief overview of the Spring Framework, its major pieces, and its underlying principles, including inversion of control and how it relates to dependency injection.
Through several examples, we’ve demonstrated the benefits of using Spring to manage an application’s dependencies and described how to define these dependencies using both XML and Java annotations.
In the next chapter, we’ll move away from the container and explore some of the (very) useful components that Spring provides around persistence, object-relational mapping, and transaction management.
In many cases, you can incorporate these components into your application in a transparent fashion.
This chapter assembles the data persistence, ORM, DAO, and transactionmanagement infrastructure you’ll be using throughout the rest of the book.
Although there are cases where it’s useful to work directly with JDBC, the ORM approach confers major benefits in terms of simplifying the codebase.
We don’t pretend to offer an exhaustive treatment of Hibernate, but we hope that it’s sufficient to allow you to make sense of the code examples and get started with Hibernate if you aren’t already using it.1 Data persistence, ORM, and transactions.
Here’s an overview of what we’ll be doing in this chapter:
Recipe 2.1 shows how Spring simplifies JDBC-based access to the database using the JdbcTemplate.
In recipe 2.3, you’ll replace your JDBC-based approach with ORM via Hibernate.
Recipe 2.4 shows how to create a data access layer that presents the app with a clean persistence API that hides the mapping details.
Optional) Recipe 2.5 shows how to use the JPA instead of use Hibernate directly.
Although the rest of the book uses Hibernate directly, it’s useful to see how to do things with JPA.
Optional) Recipe 2.6 presents the recent Spring Data JPA project, which simplifies data access even further.
Figure 2.1 presents a visual lay of the land, illustrating the layering in a typical Java-based persistence architecture.
You’ll begin by learning how to use Spring to simplify JDBC-based database access.
In the following recipe and throughout the chapter, you’ll work with a simple contact management application.
The complete code is available from the GitHub master at https://github.com/ springinpractice/sip02/
There is also a GitHub branch (01-06) available that corresponds to each of the recipes in this.
Figure 2.1 The layers involved when implementing a typical Java-based persistence architecture.
Don’t worry if there are parts (even lots of parts) you don’t fully understand, because that’s what the rest of the book is for.
The JDBC API is a useful and well-known approach to accessing data in a relational database.
But it can be somewhat cumbersome to use, because it involves a lot of boilerplate code that acquires connections, creates statements, executes queries, and then closes all those things in the reverse order.
As noted, you’ll use JDBC to talk with the database, but you’ll see how that looks using Spring’s NamedParameterJdbcOperations and RowMapper abstractions.
At B and C you create an SQL update and query with named parameters, respectively.
Then at D you have the JDBC operations object, against which you execute your queries and updates.
You use the row mapper at E to map JDBC result sets to object lists, which is useful for queries that return multiple rows.
You’ll see the row mapper in detail in a minute.
In this case, there is only a single parameter, so you take advantage of the corresponding constructor.
The flanking % characters are SQL wildcards indicating zero or more characters, which is useful for text.
Notice that in making the query, you pass in the contactRowMapper.
This is what lets you map a JDBC result set to a list of contacts.
The next listing shows how to implement the RowMapper interface to achieve this result.
It should be easy to follow what’s happening in listing 2.2 because you’ve no doubt written similar code before.
At B you implement the RowMapper interface so Spring can use it.
At C you perform a standard mapping to extract a Contact from a.
Spring handles the iteration for you, so all you need to do is perform the row-level mapping.
That’s all you need to do as far as actual code goes.
In the next section, we’ll show how to do that.
The NamedParameterJdbcOperations abstraction does a nice job of hiding any configuration messiness.
Once you have the operations object, you’re good to go.
But of course you need to create that object somewhere, and so that’s what you’ll do now.
At B you source externalized configuration properties, as described in chapter 1
This allows you to avoid hard-coding environment-specific information into your app.
As such, DataSources are an important part of most web and enterprise applications.
Finally, you have the standard driverClassName, url, username, and password attributes.
Now that you have a DataSource, you can create the JDBC operations object.
You pass the DataSource into the constructor using Spring’s constructor namespace.
Set up your database using the SQL scripts at /src/main/sql, and create the externalized environment .properties file as described in appendix A.
You should be able to view the contact list and search for contacts by email address.
You should also be able to view, edit, and delete individual contacts.
In this recipe, you configured the DataSource as part of the application configuration.
Without proper configuration externalization (for example, moving URLs and credentials out of the main configuration), you end up tying the app to a particular environment.
If multiple apps in the same container want to use the same DataSource, you have to repeat the configuration for all of them.
This approach doesn’t allow different apps to share a connection pool, which can lead to an inefficient use of system resources.
In the next recipe, we look at a more centralized alternative to managing DataSources.
You’ll learn how to use a container-managed DataSource in recipe 2.2
You can address the issues we raised in the discussion of recipe 2.1 by adopting a centralized approach to configuring the DataSource.
Instead of having each application manage its database configuration, you can configure the DataSource inside the container and then have the applications point to that shared configuration.
In addition to streamlining the configuration, this allows you to share a single connection pool across applications.
Configure the DataSource centrally to simplify configuration management and share a connection pool.
In this configuration, the application container manages the DataSource, and the app makes a JNDI call to get a reference to it.
We assume that you’ve already configured your application server to expose your DataSource through JNDI.
If not, please consult the documentation for your app server for more information.
The sample code for most of the chapters in the book includes a /sample_conf/jetty-env.xml file that shows how to do it for Jetty.
To do a namespace-based JNDI DataSource lookup, you need to declare the jee namespace3 in your app context and include the jee:jndi-lookup element.
In the namespace part, you declare the jee namespace B, which gives you access to some elements related to JNDI and stateless session beans.
Note that the code download includes other namespaces that we’ve suppressed here because you don’t need them for this recipe.)
The id attribute specifies the ID under which the DataSource bean will be exposed; in this case you’re being unimaginative and calling it dataSource.
The jndi-name attribute gives the DataSource’s configured JNDI name that the target object is a resource—an object that the app server makes available as part of the general environment instead of an application-specific object.
Other common resources come from JMS, JavaMail, and JCA.) Because you’ve set this to true, Spring will automatically prepend the JNDI name you specified with the standard java:comp/env/ prefix.
All you’ve really done is replace the app-specific DataSource configuration with a JNDI lookup.
In this recipe and the last, we explored a couple of different ways to set up a DataSource in Spring:
Configure the DataSource directly in your Spring application context using an implementation like the BasicDataSource class from the Apache Commons DBCP library.
Configure your DataSource in your app server and expose it through JNDI, as you just saw.
The former approach is probably more straightforward to set up, because it doesn’t require you to configure anything in the container.
In addition, the DataSource configuration is more portable across containers.
Finally, it’s useful when you want apps to have isolated connection pools so as to ensure that a greedy app doesn’t prevent other apps from getting connections.
The centralized JNDI approach is useful when you want to configure your DataSource one time and share a connection pool across applications.
It also provides a nice way to push sensitive database credentials out of your application sources, although there are other ways to do that as well: for instance, with PropertyPlaceholderConfiguraton, as we showed in recipe 2.1
Next we’ll look at a different model for database interaction: ORM.
In addition to code examples, this recipe offers a high-level overview of ORM with Hibernate.
But this overview should equip newbies to understand what’s happening in the code examples in the rest of the book.
You’ll use the Hibernate ORM framework to establish a simpler, more object-oriented interface for working with the database.
Mapping—You’ll learn how Hibernate makes it easier to work with entities, relationships, and SQL queries by exposing corresponding POJOs and an object-oriented query language.
Querying and updating—You’ll see how Hibernate’s Session API allows you to execute queries and updates against the database.
Transactions—You’ll learn how to use Spring and Hibernate together for transaction management.
The primary function of ORM is to resolve the so-called impedance mismatch between working with Java objects and working with databases.
The idea is that although there are some correspondences (classes versus tables, properties versus columns, instances versus rows, and so on), there are also key differences (inheritance in Java has no natural counterpart in databases; Java has collections but databases don’t)
The first thing ORM helps with is translating Java objects back and forth into database entities, and mapping associations between objects back and forth into database relationships.
As noted, classes correspond to tables, properties to columns, and instances to rows.
With associations, things are quite a bit more involved, because you have to.
Object-relational mapping and transactions via Hibernate account for various concerns (multiplicity, directionality, whether the associated objects have dependent or independent lifecycles, and so on)
We won’t get too deep into the weeds here, but see figure 2.4 for the basic concept.
Once you have the basic framework of mapping objects and relationships, it’s not a huge leap to map queries.
These object-oriented query languages, as you might guess, involve queries on objects and their properties.
In most cases, object-oriented queries (such as JPQL) are more concise than their SQL counterparts.
They also tend to be more natural to developers, who are in many cases more comfortable working with Java objects than with database constructs.
The first order of business is to perform the mapping itself.
The following listing shows how to perform both entity and query mapping.
Figure 2.4 ORM helps you map Java objects and associations to database entities and relationships.
Listing 2.5 Contact.java, illustrating how to map between Java and the database.
There are many such annotations in the javax.persistence package (many more than we’re using here), and you use them to perform the mapping.
The first part of the mapping identifies the Contact class as an entity C and maps it to a table D.
At E you use the @NamedQuery annotation to build a JPQL query based on the entity.
This example is close to the simplest possible query, but it will do for now.
You’ll see more interesting queries over the course of the book.
Then you have an @Column H annotation to map the bean property to a database column.
By default, the column name is the same as the property name, although you can use the name attribute to specify the column name explicitly.
Although you don’t use them here, JPA includes annotations for mapping associations: one-one, one-many, many-one, and many-many.
See Java Persistence with Hibernate for more information on the intricacies of association mapping.
Next you’ll execute queries and updates based on this mapping.
In the foregoing discussion, we’ve focused on what Bauer and King describe as the structural mismatch problem (p.
This is where you map structures in the Java domain to structures in the database domain.
But in addition to structural mappings, you need to deal with the behavioral mismatch.
Lookups and other dynamic behavior based on Java collections and the like are different than database queries and updates, and you need a way to ensure that your database queries end up meeting whatever performance requirements you have.
Hibernate’s Session API is the key piece of the behavioral strategy.
It provides an interface against which to perform persistence operations (basic Create, Read, Update, and Delete [CRUD] queries and updates, transaction control, and context management), and it does so against an internal persistence context, supporting optimizations such as automatic dirty checking (don’t flush to the database unless something changed), transactional write-behind (flush as late as possible to minimize database lock times), and caching (support repeatable reads for free, avoid database hits when nothing has changed, and so on)
More generally, the Session API allows you to transition objects through the persistence lifecycle.
In the next listing you reimplement ContactServiceImpl, this time using Hibernate’s Session API instead of NamedParameterJdbcOperations.
You can also do method-level overrides, although you don’t do that here.
The crucial point is that this is the extent of your explicit treatment of transactions: the against the Hibernate Session, which keeps your code clean.
We’ll look more carefully at transaction management in the next section.
Everything else deals with various persistence operations against the Session.
At with a matching email address by looking up a named JPQL query F, return a single contact G, and delete a contact H.
DAOs provide a way to perform persistence operations on entities, but they aren’t themselves domain logic—they’re pure mechanism.
Instead, domain logic lives in the service layer that sits above the data access layer.
Any given service method might invoke a number of persistence methods, and these might involve multiple operations on multiple entities.
In most cases, you want a service method to operate as a transaction with the standard ACID semantics.
To handle this, your service layer needs a way to embed its database calls within the context of a transaction.4 When a client calls a service method, you want to see something like the following flow:
Create a Hibernate session, and bind it to the transaction.
Notice that step 3 is the only one that’s interesting from an application development standpoint.
Spring’s transaction framework essentially does the boilerplate so developers can focus on the domain logic.
Because the framework handles the actual logic, all you need to do is tell the transaction infrastructure where the transactional boundaries are and which transactional semantics to apply in terms of propagation, isolation, rollback behavior, and so forth.
Basically, you attach the annotation to methods that you want to run within a transaction.
Although the service layer doesn’t typically make direct database calls, the DAO calls that it does make pass through ORM, and these in turn ultimately go to the database.
You usually want these to be part of a single transaction.
Figure 2.6 A single transaction spanning multiple updates to the database.
Object-relational mapping and transactions via Hibernate around the transaction (such as propagation behavior and isolation level) using the same annotation.
This way of specifying transactions is called declarative transaction management, because you attach @Transactional to a method when you want it to be transactional.
Declarative transaction management stands in contrast to programmatic transaction management, where you code up all the logic to create new transactions, bind sessions and connections to them, write if/else logic to decide which exceptions trigger rollbacks, commit the transaction, clean everything up, and so forth.
This is in general not what you want to do as an application developer.
You could have specified additional transactional semantics, such as read-only transactions, but you didn’t do that.
You set Hibernate configuration properties using the util namespace at B.
Then you use them to define an AnnotationSessionFactoryBean at C, which is a factory that produces a SessionFactory.
The SessionFactory knows how to read the Hibernate and JPA mapping annotations that you place on entity classes, as you did with Contact in listing 2.5
You inject the DataSource into the AnnotationSessionFactoryBean because your Sessions ultimately need to read from and write to the DataSource.
You also tell the AnnotationSessionFactoryBean which package(s) to scan for entities.
Here you have only one package, but you can also use multiple comma-separated packages.
This handles the complicated transaction-management logic that you’d rather not have to deal with.
Behind the scenes, the transaction manager invokes transactionmanagement methods on the underlying SessionFactory.
That’s why you used this ID when you defined the transaction manager.) The proxy uses the transaction manager to manage transactions on behalf of the proxy’s target bean.
This is entirely transparent to clients of the target bean, which work with the proxy and are none the wiser.
Declarative, transparent transaction management is arguably one of the most useful features of the entire Spring framework.
With that, you’ve established the data access and transaction background necessary to make sense of what you do in the rest of the book.
Note that Spring’s transaction framework is rich, and there’s much that we couldn’t cover here.
As with recipe 2.2, the changes you’ve made don’t affect the behavior of the app.
Try running the application again to make sure everything is still working.
It’s also possible to weave these with AspectJ, although you aren’t doing that here.
This recipe showed how to clean up your persistence code using Hibernate ORM.
Ideally, your service tier doesn’t have to involve itself in the mechanics of persistence management.
Instead, the service tier should focus on domain logic and delegate persistence concerns to something else.
Persistence is a fairly low-level concern; it’s not part of the domain logic that forms the core concern for your application’s service tier.
Unfortunately, so far your service bean focuses squarely on persistence.
In this recipe, you’ll learn how to isolate domain logic from persistence logic, which creates a cleaner architecture.
The classical solution to separating the domain logic and persistence logic concerns is called the data access object (DAO) pattern.6 The basic idea is to consolidate persistencerelated code in a dedicated architectural tier that sits above the database, and have the service tier (which handles domain logic) defer persistence-related concerns to this persistence-centric tier.
The approach to implementing DAOs is simple: inject the Hibernate SessionFactory into each DAO, and then implement persistence methods backed by the sessions you grab from the factory.
Because a lot of the persistence operations are common, it’s useful to define a general DAO interface and an abstract base DAO, and then derive corresponding entity-specific interfaces and classes as shown in figure 2.8
Figure 2.8 Class diagram illustrating the relationship between generic framework classes and app-specific implementations.
The following listing presents the generic DAO interface that you’ll use throughout the book.
There’s not a single way to implement a generic DAO interface, but this one meets the needs for this book.
First, note that this interface is in the com.springinpractice.dao package B.
We put it there instead of in com.springinpractice.ch02 because you’ll use this interface throughout the book.
To get the sample DAO code, grab the download at https: //github.com/springinpractice/sip-top/
A key feature is the use of a generic type parameter C.
This allows you to adapt derived interfaces to specific domain classes, making them friendlier to use.
You can add entity-specific methods as you please, as shown in the next listing.
You now have a base DAO interface, and you know how to derive entity-specific interfaces from it.
You’re going to do the same thing on the implementation side, defining a generic base DAO class, then subclassing it on a per-entity basis.
Listing 2.10 AbstractHbnDao.java: a generic base class for implementing DAOs.
In line with the Dao interface, the base class uses generics B.
You inject the Hibernate subclasses to perform persistent operations against the Hibernate Session.
Implementing general versions of your CRUD operations and queries requires a reference to the actual domain class.
The following listing shows how to extend AbstractHbnDao to create an entityspecific DAO.
You don’t have to implement the core persistence methods, because they’re available in AbstractHbnDao.
This allows Spring to componentscan the bean, among other things.
The next listing shows the effect of your refactoring on the ContactServiceImpl bean.
As you can see, this “service” is a pass-through layer to the DAO.
In a more realistic application, you’d expect to see service beans include more domain logic.
Sometimes this is domain logic proper, and sometimes it’s other non-domain logic that for whatever reason hasn’t been externalized.
In any event, you’ve successfully moved the persistence concern out of the service bean into a dedicated tier.
This recipe was just a refactoring; rerun the application to ensure that it still functions before continuing on to the next recipe.
You create entity-specific interfaces by extending Dao, and you create entity-specific implementations by extending AbstractHbnDao.
The next two recipes are optional because they present material that you don’t use elsewhere in the book.
But we strongly recommend that you review them, because they present an official framework for building DAOs, similar to the earlier ones, based on JPA and Spring Data JPA.
Spring Data JPA is a fairly recent addition to the Spring portfolio, and we didn’t have time to rework all the examples in the book to use JPA and Spring Data JPA instead of Hibernate and custom DAOs.
Look at the next two recipes, and consider using the approach described in your own projects.
Eventually the idea caught on, and Sun created the Java Persistence API (JPA) standard around ORM, based in large part on Hibernate.
Although you can use Hibernate in a standalone fashion, it’s a compliant.
You can bind an arbitrary persistence provider to the interface (EclipseLink JPA, OpenJPA, Hibernate, and so on)
So you also have the option of using JPA and treating Hibernate as a persistence provider.
This allows you to use JPA-based frameworks like Spring Data JPA, which you’ll pursue in the following recipe.
So far, we’ve described a configuration in which applications (the DAO part of apps, anyway) work directly with the Hibernate API.
Although you do use standardized JPA annotations to declare the mappings on the entities (Hibernate understands them), you’ve been using Hibernate’s SessionFactory, Session, Query, and so forth to implement the DAOs (see figure 2.9)
There’s little danger in doing this because you’ve isolated all such code in the data access layer, and this is the approach used throughout the book.
It’s a more or less straightforward configuration, and it’s nice for developers who are already familiar with the well-known Hibernate API.
But you may prefer to use standardized JPA interfaces such as EntityManagerFactory and EntityManager when you implement your DAOs.
This approach offers additional flexibility with respect to choosing a persistence provider.
You can, of course, continue to use Hibernate, because it’s a mature JPA implementation.
But you have other options, such as EclipseLink and OpenJPA.
The following listing shows how to reimplement ContactDao using JPA.
You begin by using the JPA @PersistenceContext annotation to inject not an EntityManagerFactory (the analog to Hibernate’s SessionFactory) but a shared, thread-safe EntityManager B.
This is analogous to injecting a Hibernate Session, which you can do with JPA.
You also have standard persistence methods, this time implemented against the EntityManager instead of against a Hibernate Session C.
The other piece relevant to Spring/JPA integration is the application context configuration.
As with the DAO, the JPA version of bean-service.xml is similar to its Hibernate counterpart.
You create a LocalContainerEntityManagerFactoryBean (analogous to the AnnotatedSessionFactoryBean) at B.
There are other options, depending in part on what sort of container you want to use, but this is the one that makes sense for web containers such as Tomcat and Jetty.
See the Spring reference documentation for more information.) You don’t have to provide an explicit persistence.xml JPA configuration because you specify the DataSource, provider, and provider-specific properties right here in the Spring configuration.
You create a JpaTransactionManager at C, which takes the place of the HibernateTransactionManager you used formerly.
You can still use Hibernate as a JPA persistence provider, or you can use EclipseLink JPA, OpenJPA, or any other JPA provider.
As with previous recipes, verify that the contact-management app still works before continuing to the final recipe in this chapter.
The next recipe builds on what you’ve done here with JPA.
It presents the Spring Data JPA project, which allows you to simplify your already simple DAO layer even further.
Until recently, it has been up to developers to implement their own DAO framework, as we showed how to do in recipe 2.4
Spring Data JPA provides an official DAO framework as part of Spring itself—one that includes several useful characteristics and features:
Another nice touch is that in addition to the entity class, the ID class is a type parameter.
That way you can use Long, String, or any other Serializable type in a typesafe way.
Spring Data JPA automatically generates concrete DAO classes using dynamic proxies.
It automatically generates query method implementations based on the name of the query method.
Once you’ve updated your app to use JPA instead of using Hibernate directly, you can take advantage of Spring Data JPA.
Replace your custom DAO framework with the simpler and more powerful Spring Data JPA.
Spring Data JPA provides a generic DAO interface called JpaRepository that serves the same function your Dao interface does, although in a more comprehensive way.
Let’s begin by creating a ContactDao interface based on JpaRepository.
To support contacts, you pass the Contact and Long type parameters into JpaRepository B.
Those are the domain class and the ID class, respectively.
You also declare a finder method to support email search at C.
The method name is significant because it’s what allows Spring Data JPA to figure out how to build the corresponding query dynamically.
See the Spring Data JPA reference documentation for more information on how Spring Data JPA maps method names to queries.
We’ve bolded the few parts that have changed from listing 2.14
You add the jpa namespace (and its schema location) to the configuration B.
And you replace the DAO component scan with a <jpa:repositories> definition C, which is almost like a component scan, except that it discovers the DAO interfaces and then automatically provides dynamic proxy implementations for you.
That’s right: you don’t have to write the DAO implementations.
Nor do you need to mark up the interface with @Repository or.
Your data access tier has come a long way, when you think back to recipe 2.1
What started out as a bunch of SQL with named parameter substitutions and row mappings.
Run the app to confirm that it functions as expected with the new Spring Data JPA data access tier in place.
This recipe provides only a flavor of what Spring Data JPA is all about.
But even with this small taste, it’s clear that Spring Data JPA provides a powerful and streamlined approach to building out a data access tier.
For more information on Spring Data JPA, visit the project home page at www.springsource.org/spring-data/jpa.
In this chapter, you learned how to put together a persistence and transaction infrastructure using a DataSource, Hibernate, the data access object design pattern, and declarative transactions.
This popular combination makes for a powerful back end because it cleanly separates the infrastructure from the domain logic.
We also explored building DAOs against JPA rather than coding directly against Hibernate.
Even though this book uses Hibernate directly, it’s useful to see how the JPA-based approach works and how Spring makes it simple to move from one to the other.
Finally, we took a brief glimpse at Spring Data JPA, one of the projects under the Spring Data umbrella.
In chapter 3, we’ll move up the stack to the web tier, where you’ll see how to develop web-based model-view-controller (MVC) apps using Spring Web MVC.
This book is mostly about web application development, partly because that’s where the bulk of your authors’ experience lies, and partly because there’s enough interesting material to support an entire book.
Actually, there’s enough to support a lot of books, as any visit to your local bookstore will reveal.) Therefore most of the recipes involve some amount of web-related code and configuration, and you’ll find that many of the same ideas and techniques recur throughout.
Instead of repeating those over and over, we’ll take a moment to discuss the basics here.
This way, you can easily refer back to this material for review as you work through the recipes ahead.
Despite the large number of Java web frameworks, we’re going to concentrate our efforts around using and understanding Spring Web MVC, which is the web Building web applications with Spring Web MVC.
We’re more concerned with covering a variety of business problems using core Spring technologies and less concerned with covering every available technical option.
No doubt the technical alternatives are interesting in their own right, but given our limited space we’ll leave that treatment for reference manuals.
We’ll begin by laying out background material, including a quick overview of the model-view-controller (MVC) pattern, Spring’s approach to MVC, and its architectural underpinnings.
Next you’ll try your hand at writing some Spring Web MVC code, with your goal being to get an intuitive feel for how things work in Spring Web MVC.
The rest of the chapter offers a more systematic treatment of writing controllers and configuring web applications to use them.
First things first: let’s go over some important background information.
To understand Spring Web MVC, it will be useful to understand web-based MVC frameworks in general, Spring’s version of web MVC, and the highlights of Spring Web MVC architecture, including the key components and control flow through those components.
Model-view-controller (MVC) refers to the architectural pattern in which you separate your business services and domain objects (the model) from the UI (the view) and mediate their interaction through one or more controllers.
You’d like to be able to modify your UI without having to change your business logic and domain objects, and separating the model and view makes it easier to do just that.
Java web applications typically realize MVC in roughly the following way: the model encompasses business-tier code (service beans, POJOs, Enterprise JavaBeans [EJBs], and so forth), the view involves JSPs or similar technologies, and the controller is usually servlet-based.
The handler makes any necessary calls against the service tier and grabs any domain objects1 it needs to populate the view.
Finally, the handler figures out which view to deliver and forwards processing to that view.
As shown in figure in 3.1, an HTTP request comes into the controller B.
The controller accesses the model C, possibly getting data D, possibly updating the model, and possibly both.
The controller then uses the view E to generate a response F, passing any relevant data it pulled out of the model.
The client receives the generated response G, and service is complete.
It’s not always desirable to use domain objects in the presentation layer.
See section 3.3.1 for a more detailed discussion of this topic.
The model is insulated from changes to the view, and the view is insulated from at least certain changes to the model.
Of course, because the view renders model data, the view isn’t completely insulated from model changes.
But in general, separating the model and view into separate components means it’s easier to update one without breaking the other.
We’ll begin by exploring what Spring Web MVC is and how it helps us build web applications.
Its primary job is to support the MVC way of dividing application functionality, so it provides explicit support for organizing the web layer into models, views, and controllers.
Separation between the three concerns is clean; for example, when a controller selects a view, it does so by selecting only a view name (not a View object, not a hardcoded path), and dependency injection makes it possible to treat even view names as injected values.
Besides a clean separation of concerns, another major design goal for Spring Web MVC is flexibility.
There are many ways for you to customize the way it works.
If you want to use POJO controllers, you can do that.
If you prefer defining an interface for controllers, you can do that too.
You can control how requests map to controllers, how view names are generated, and how view names are resolved to views.
You can define interceptor chains and exception handling for your controllers, and you can choose from different strategies for resolving locales, UI themes, multipart resolvers, and more.
Figure 3.1 A conceptual view of control flow in web-based model-view-controller applications.
Speaking of flexibility, one of our favorite things about Spring Web MVC is the tremendous flexibility it provides around handler2 method parameters and return values: if you want to expose an HttpServletRequest to a handler method, just declare the parameter, and it’s automatically provided to the method.
You can do the same thing with a whole host of parameters, as you’ll see.
Don’t worry if that sounds like mumbo-jumbo at this point.
We’ll look at this in detail over the course of the chapter.
Suffice it to say that Spring Web MVC is flexible and capable.
Let’s see some highlights of the Spring Web MVC architecture.
The center of the Spring Web MVC universe is the DispatcherServlet, a front controller3 that dispatches requests to registered request handlers.
The handlers can be UI controllers or endpoints for HTTP-based remote services.
Each handler performs a service and then specifies a view to which the DispatcherServlet passes the request.
In Spring Web MVC, handler is a more general way of referring to a UI controller.
The idea is the same, but handlers include HTTP-based remote service endpoints, which wouldn’t normally be considered UI controllers even though in reality they’re doing roughly the same thing: grabbing data and exporting it in a desired format.
Based on the request path, the DispatcherServlet figures out which of its registered handlers is the right one to service the request C.
It then passes the HTTP request (or, more abstractly, the user command or form data that the request represents) to a handler for processing D.
The handler queries or updates the model (or both) E, and if appropriate the model returns the requested data F.
The handler then returns both the data and a logical view name back to the DispatcherServlet G.
The DispatcherServlet resolves the view name into an actual view H and then passes the model data (if any) along to that view I so it can be used in generating a response J.
Processing is complete, and the DispatcherServlet has serviced the request 1)
We’ve suppressed a lot of details, but now you have a basic understanding of how Spring Web MVC works.
Let’s get right to the good stuff: writing your first Spring Web MVC application (a toy app) and seeing it in action.
Let’s build a simple application (in the broadest sense of the term) to manage a roster of some sort.
We’re going to present this without tons of detailed explanation; our goal is to give you an intuition for how things work in Spring Web MVC, rather than exhaustively cover all the bases.
This leisurely stroll will give you a context against which to understand a more detailed discussion afterward.
The following listing shows a bare-bones web.xml configuration, but it will work just fine.
You’ve defined a minimal DispatcherServlet B, which again is Spring Web MVC’s front controller, and you’ve indicated that you want to send /main/* requests to it C.
You’re not tied to that particular mapping; that’s just what you happen to have chosen.
You didn’t define a ContextLoaderListener in web.xml, so you might be wondering where the app context comes from.
The answer is that each DispatcherServlet instance creates its own local app context using an XML configuration we provide, so here’s that configuration.
DispatcherServlet knows where to find this file by using a convention you can probably guess.4 You can configure the location, but we won’t worry about that right now.
The controller name specifies the requests that the RosterController services.
You also define a ViewResolver C, which allows you to convert logical view names to views.
For now it’s enough to know that a logical view name such as foo is converted to /WEB-INF/jsp/foo.jsp given the definition here.
When dealing with JSP views in particular, it’s a good practice to place them somewhere inside the WEB-INF folder (WEB-INF/jsp is the official recommendation) so clients can’t access them directly.
You’ll create a controller in a few minutes, but in preparation for that, let’s create a domain object to represent a member of your roster.
The following listing shows Member.java, a simple domain object you’ll use in your controller.
You include two constructors because you’ll eventually use both of them.
Now let’s get to the controller, which is more interesting.
To keep things simple, you’ll hard-code some fake roster data directly in the controller.
In a real application, the controller would typically delegate to either a service or a DAO to obtain this data on the controller’s behalf.
There’s a lot packed into this small controller class, and you’re relying heavily on conventions.
Don’t feel bad if you’re not seeing the details of how everything is wired up; you’ll have plenty of time to get to that in the pages ahead.
Because the controller is a POJO, it’s useful for certain purposes (for example, for request mapping purposes) to have an alternative way to flag it as a controller.
You’re not extending any other classes or implementing any interfaces C; you’re effectively defining the contract for this controller.
In a real application, you’d likely grab that from a service bean.
But here you’re trying to see how the MVC part works, so you’re faking the member list.
This is the part we were saying earlier that we like about Spring Web MVC.) You declare a Model parameter E, which means Spring will automatically pass you a Model object (essentially it functions as a Map, even though it doesn’t implement that interface)
Anything you put on the Model will be available to the JSP as a JSP expression language (EL) variable.
Here, you’ve placed the list of members on the Model F.
When you place objects on the model, they’re stored as name/value pairs.
Here, you haven’t explicitly assigned a name to the attribute, so Spring will automatically generate the name by convention.
Here, because the type is List<Member>, the generated name is memberList.
You’ll be able to access it from your JSP using ${memberList}
At G you can see another example of the flexible method signatures in action.
This time you declare that you want to accept an HTTP parameter called id, and you want it to be automatically parsed into an Integer.
We told you it was cool.) And again, with the model you haven’t provided an explicit attribute name, so the name will be autogenerated based on the attribute type.
In this case the name will be member, and it will be available to the JSP using ${member}
You may have noticed that you haven’t explicitly specified any view names.
How does DispatcherServlet know which view gets the request after a given handler method is finished with it.
The answer is that you’re using a convention that automatically translates the request URL to a logical view name.
Because the request URL for when we discuss the DefaultRequestToViewNameTranslator.
Another thing to note is that you define a couple of different actions.
You can define as many as you like, but for now you’re keeping it simple.
Normally you would put closely related functionality together in a single controller as you do here.
The following listing shows list.jsp, which displays the entire roster.
In this listing you reference the memberList attribute you set in the controller through a JSP EL variable B.
You list the members C, with their names being links to expects as you saw in listing 3.4
It’s another example of grabbing data from the Model and displaying it in the JSP B.
Let’s stop at this point to admire the beauty of your work so far.
Now that you’ve seen how to create a simple POJO controller, it’s time to get a little more adventurous and implement form serving and processing functionality.
Let’s pretend you create a form to allow the user to nominate a member for an award.
Yeah, we’re making this example up as we go.) Although it’s entirely possible to mix and match form methods with non-form methods on a single controller, let’s create a new controller because showing rosters and roster members isn’t closely related to nominating members for awards.
Before creating your new controller, you’ll need a form bean.
One of the nice features of Spring Web MVC is that if it makes sense to do so, you can use your domain objects as form beans.
To add a form, the first thing you need to do is create a bean to store the form data.
There are a couple of different approaches to doing this, and they aren’t mutually exclusive.
A best practice for controllers With Spring Web MVC before Spring 2.5, the way you bundled controller methods together was dictated more by the classes in the Controller hierarchy than it was by which methods were closely related.
A best practice is to bundle closely related methods together in a single controller, and to put unrelated methods on other controllers.
One approach is to use your domain objects as your form beans.
This works well when there’s a close match between the fields you require on the form and the properties your domain model has, as is often the case.
The advantage of this approach is that you can avoid creating separate but parallel sets of classes for your domain objects and your form beans, which keeps the clutter down.
With Struts 1, for instance, it was common to have parallel sets of classes like this.)
Another approach is to implement your form bean separately from your domain model.
This can make sense when the domain model has properties for which there aren’t corresponding form fields, when the form has fields for which there aren’t corresponding domain object properties, or both.
A good example (which you’ll see in chapter 4, even though there you use the domain object as a form bean after all) is a user registration form.
Similarly, the domain model often has properties such as confirmed, enabled, and so forth.
When using this second approach, you have to pay attention to how you handle the extra fields on both sides.
If the domain model has extra properties, such as confirmed or enabled, you’ll need to make sure users can’t bind extraneous HTTP parameters to those properties.
Again, the approaches aren’t mutually exclusive, meaning you can use domain objects to back forms in certain cases and dedicated form beans in other cases.
In this case, the Member class is exactly what you’d want from a form bean, so you’ll use it as a form bean.
Is it really OK to use domain objects as form beans? Even in cases where there’s not an exact match between form beans and domain objects, it’s a judgment call and a matter of architectural sensibilities whether you reuse your domain objects for your form.
Some prefer not to do it because they (reasonably) draw a strong architectural distinction between form beans and domain objects.
Struts 1, for instance, works like this: the framework enforces a clean separation between domain objects and form models.
Others, your authors included, don’t mind a little architectural impurity if we can avoid having two parallel sets of strongly similar classes (which carries its own costs)
If our domain objects and form beans mostly overlap, we’re willing to accept (for instance) JPA annotations in our form bean as the cost of having a single class.
In practice we find the option useful and not confusing.
The new form will allow the user to nominate a member for an award.
The controller won’t do anything other than show some log output and forward to a “thanks” page.
You just want to see how to set up a form.
The new controller is dedicated to serving and processing a form.
You define a setter so you can inject a logical view name for a “thanks” page after the user submits a nomination B.
It’s probably obvious why you don’t want to return something like /WEBINF/jsp/nominee/thanks.jsp: that doesn’t give you a good separation between the controller and the view.
But it may be less obvious why you don’t return a logical view name, say thanks.
You’ll see the answer to that when we discuss the redirect-after-post pattern, but for now take it on faith that even with the view name it often makes sense to keep the controller and view separate.
One is marked as handling GET requests and the other as handling POSTs.
In the GET handler C, you return an empty form bean so the HTML form fields have something to bind to.
In this case you aren’t prepopulating the Member form bean, but sometimes it’s useful to do that kind of thing.
Whether or not you prepopulate the form bean with data, by returning it from the method you’re placing it on the Model under the generated attribute name member D.
If you wanted to use another attribute name, such as nominee, you would do something like this:
The result is the same, except that the form bean’s attribute name is nominee instead of member.
At any rate, let’s be happy with the attribute name member and return a Member.
This is yet another example of how flexible Spring Web MVC is.
You include an annotation marking it as such E, and then you have a method signature that takes a Member and returns a String F.
Once again, the signature you use is dependent on your need.
By taking a Member parameter, the submitted form data will be automatically bound to a Member bean, the bean will be placed on the model as an attribute under its generated name (member), and the bean will be passed into the method itself.
Play around with the following, adding and removing the @ModelAttribute annotation, to see how it works:
For the following discussion, assume that you’re using the code from listing 3.8 rather than the modified version.
We wanted to give you a nice way to understand what’s going on with the form bean parameter, as well as exposure to the @ModelAttribute annotation.
Prepopulating form beans The most common case of prepopulating form beans arises in validation scenarios: when the user enters invalid form data, you usually want to re-present that invalid data in the form so the user can correct it, rather than making them reenter the data from scratch.
You might prepopulate a request for information (RFI) form bean with location data based on the user’s IP address.
Any time the return type is a String, DispatcherServlet assumes that the return value represents a logical view name.
You’ll need a couple more JSPs: one to display the form and one to thank the user for submitting the form.
The form is modest and looks a lot like a normal HTML form.
The main difference is that you declare the Spring form tag library B, and you’re using that to represent the form and its inputs.
The primary advantage of using the form tag library is that it provides binding form inputs.
If you were to prepopulate the Member bean with data, that data would automatically be rendered in the corresponding text field.
At C you bind the form to the member attribute that the controller placed on the Model.
This gives you a way to interpret input paths (discussed in a moment) as bean properties.
If you don’t specify an explicit value for modelAttribute, the default form bean attribute name is command, which isn’t descriptive.
At D you have a text field that binds to member.firstName.
The form tag library has tags for all the standard HTML input controls, although you’re using only text fields here.
There’s no tag from the tag library for this because there isn’t anything for the submit button to bind to; that is, the submit button doesn’t have a corresponding bean property.
The only reason we’re looking at this is that it shows you that the Member form bean.
As you can see, adding a form is more involved than actions that grab data and display it.
The framework handles form/bean binding automatically, and you define the controller methods to use just what you need and nothing else.
Let’s finish the form (for the moment) by updating the application context.
You need to make only one change to the main-servlet.xml application context file.
One common pattern to use with web-based forms is called redirect-after-post.
The idea is that when a user submits a form via HTTP POST, it’s nice to force a redirect to minimize the likelihood of a double submit, to avoid browser warnings when the user clicks the back button, to make the resulting page easier to bookmark, and so forth.
To do this, prepend redirect: to the logical view name.
This will cause RedirectView to kick in, and the browser will request whatever page you tell it to request.
This illustrates a good reason for using dependency injection to set logical view names.
Presumably controllers shouldn’t know whether they’re issuing forwards or redirects.
By keeping the view names configurable, you achieve controller/view separation.
Now let’s look at a security issue you need to address when working with forms.
Because Spring automatically binds HTTP parameters to form bean properties, an attacker could conceivably bind to properties that weren’t intended for binding by providing suitably named HTTP parameters.
This might be a real problem in cases where a domain object is serving as the form-backing bean, because domain objects often have fields that are suppressed when used in form-backing scenarios, as we discussed in section 3.3.1
To address this problem, you can define explicit whitelists in your controllers.
If your controller has two forms, it needs two separate @InitBinder methods.)
You have a controller that handles two forms: one to allow users to subscribe to a mailing list using a form called subscriber, and one to allow them to unsubscribe using a form called unsubscriber.
You use these @InitBinder methods to whitelist the form fields:
If there’s only one form, you don’t have to provide an explicit annotation value.
But because you have two forms, you need to specify subscriber or unsubscriber to let Spring know which binder to initialize.
You have to verify that the whitelist has been respected each time the user submits a form.
One simple way to do this is to define a helper class with a static verification method and call that from your form-processing methods.
A more sophisticated way to do this might be to define an aspect that automatically applies the verification to all form-processing methods, although you don’t do that here.
See recipe 4.1 for more information on whitelisting form bindings.
When users submit form data, you typically want to validate it before accepting it for processing.
For example, in the present case you’d want to make sure that the fields aren’t empty, that they aren’t too long, that the e-mail field looks like a real e-mail address, and so forth.
The preferred approach will eventually be to use JSR 303 (Bean Validation) to define annotation-based validation semantics on objects requiring validation.
But JSR 303 isn’t ready at the time of this writing, so the preferred approach until then is to use Hibernate Validator.
Please see recipe 4.2 for a detailed example of how to perform annotation-based form validation in Spring.
Now that you’ve toured some of the capabilities Spring provides for implementing web-based MVC applications, let’s look more carefully at configuration.
We focused on the programming model rather than the configuration model, but now it’s time to address configuration.
Understanding Spring Web MVC configuration amounts to understanding how to configure the DispatcherServlet.
First, because it’s a servlet, you declare one or more DispatcherServlet instances and their corresponding servlet mappings inside web.xml.
Second, each DispatcherServlet instance has its own application context, and by configuring that you configure the DispatcherServlet itself.
In this section we’ll look at web.xml; in the following sections we’ll look at the much more involved matter of configuring the servlet’s application context.
DispatcherServlet is only a servlet, so at a certain level of abstraction there’s no difference between configuring DispatcherServlet and configuring other servlets.
The following listing shows a perfectly simple and valid DispatcherServlet configuration.
You load a root application context from the default location, /WEB-INF/applicationContext.xml B.
Because you haven’t specified a location for the servlet’s dedicated application context configuration, DispatcherServlet assumes that it exists at /WEB-INF/main-servlet.xml; the general pattern for the default location is /WEB-INF/[servlet-name]-servlet.xml.
Finally, you specify the requests that you want the DispatcherServlet to service D.
When DispatcherServlet creates its application context, it uses the root app context as a parent context.
But note that it isn’t necessary to create a root app context.
In that case, DispatcherServlet’s app context will be free-standing (no parent)
The servlet’s application context can “see” beans in the root context, but not the other way around.
Thus a nice way to use the servlet’s app context is to put Spring Web MVC stuff in it rather than in the root context.
We’ll mostly ignore them because they’re fairly esoteric, but one that’s worth knowing is contextConfigLocation.
This works exactly like the same parameter for the root context (see chapter 1), but here it’s a servlet init-param instead of an application context-param.
If you wanted to move the XML file into a WEB-INF/conf directory, you would do something like this:
As with the root application context, you can specify multiple whitespace- or commadelimited locations in the param-value.
The resulting application context will include the beans from all files, with beans defined later in the list taking priority over those defined earlier in the case of naming conflicts.
It turns out that its various JavaBean properties (such as contextConfigLocation, contextAttribute, contextClass, dispatchOptionsRequest, detectAllHandlerMappings, and so on) are all settable via the init-param mechanism.
Pretty cool, right? Servlets don’t usually work that way, but DispatcherServlet descends from HttpServletBean, which provides this special behavior.
That’s about all you need to know about the web.xml configuration.
But the core servlet configuration lives in the servlet’s application context, and we’ll visit that large topic right now.
We’re now going to look at the various DispatcherServlet configuration options at your fingertips.
In calling these DispatcherServlet configuration options, note that we’re talking about Spring Web MVC configuration generally, because DispatcherServlet does play that large a role in Spring Web MVC.
Fundamentally, DispatcherServlet provides a central place for registering controllers you write and an infrastructure for using the controllers to service requests.
The configuration of that infrastructure is strategy-based, meaning Spring Web MVC defines a number of interfaces corresponding to the properties that need to be configured.
Your job as application developers is to select or create appropriate implementations (strategies, in design pattern lingo) for those interfaces.
Spring Web MVC provides several default strategy implementations, and normally you could use them as opposed to being forced to write your own.
But the possibility of writing your own is certainly there, which speaks to the aforementioned flexibility of the design.
Before jumping into the specifics of controllers and DispatcherServlet configuration, we’ll summarize the strategies and their default implementations in table 3.1
Each option requires a given number of implementing beans, as described.
For example, you can have as many HandlerMapping beans as you like, as long as you have at least one.
The annotation-based handler mapping and adapter are created only for Java 5+.)
Some of the beans are discovered by type, and some require the use of well-known names.
In the case of DispatcherServlet, strategy interfaces allowing n implementations do type-based discovery, and interfaces allowing at most one implementation all require the use of well-known names if you want the DispatcherServlet to find your beans.
Now that you have some hint as to how DispatcherServlet configuration works in the application context, let’s consider each strategy interface individually.
Whenever DispatcherServlet receives a new HTTP request, it needs to find a handler (controller) to service that request.
If the process bottoms out without a handler being selected, then DispatcherServlet generates an HTTP 404
There’s more we can say about how HandlerMappings work, but let’s pause to digest what you’ve learned so far.
Let’s see how (most of) the individual HandlerMapping implementations work.
As indicated, BeanNameUrlHandlerMapping is one of the defaults you get if you don’t specify some other mapping.
If you do specify another mapping, then if you want the BeanNameUrlHandlerMapping you have to define it explicitly, because explicit handler mapping definitions displace the defaults.) It’s very simple.
You use the controller bean’s name to specify the handler URLs that map to the controller.
This approach is nice for its simplicity, but it can be verbose if you have a lot of controllers.
ControllerClassNameHandlerMapping allows you to use the name of the controller to implicitly define the URLs that map to the controller.
All you need to do is place the handler-mapping bean on the application context, and it’s activated.
The mapping works for controllers defined using the old Controller hierarchy as well as for those defined using the newer @Controller annotation.
BeanNameUrlHandlerMapping Match the request URL with a handler bean name, which must be URL-like and begin with a slash (/): for example, /contact.do.
This mapping is activated by default if (and only if) you don’t specify mappings explicitly.
ControllerBeanNameHandlerMapping Match the request URL with a plain handler bean name, which is converted into a URL by prepending an optional prefix, appending an optional suffix, and prepending a slash.
ControllerClassNameHandlerMapping Match the request URL with a handler class name, which is converted into a base URL using a certain convention.
DefaultAnnotationHandlerMapping Match based on the presence of the handler-type level and @RequestMapping at the handler-method level.
SimpleUrlHandlerMapping Match according to a map whose keys are URL paths (possibly wildcarded) and whose values are bean IDs or names.
Once you do that, URLs will map to controllers based on controller names.
If you have a controller called ContactController, for example, requests like /contact and /contact/* will map to the ContactController.
The exact mappings depend on the type of controller; the mappings we just described apply to MultiActionControllers and @Controller beans.
This is the second default handler mapping that’s available, although only under Java 5
If you define another handler mapping explicitly, DefaultAnnotationHandlerMapping will be displaced and you’ll need to define it explicitly if you want it.
Under this handler mapping, any annotations discovered on the methods automatically generate mappings to the handler itself.
DefaultAnnotationHandlerMapping typically depends on a type-level @Controller annotation to determine whether a given bean generates mappings, but this isn’t strictly required.
The other alternative is to have a type-level @RequestMapping annotation.
This handler mapping is similar to BeanNameUrlHandlerMapping in the sense that it involves defining explicit URL/handler pairs in the application context file; but SimpleUrlHandlerMapping allows you to define multiple mapping patterns with a single bean; BeanNameUrlHandlerMapping allows only one mapping pattern per bean.
As with the BeanNameUrlHandlerMapping, the URLs are relative to the servlet path.
This handler mapping provides a nice way to combine several mappings in a single bean definition.
You may find yourself needing to use multiple handler mappings in a single DispatcherServlet.
Configuring Spring Web MVC: the application context your main handler mapping strategy, but you want to use SimpleUrlHandlerMapping to cover some cases where your URLs don’t match the controller class name in the way that would be required for the ControllerClassNameHandlerMapping to work.
You can accomplish this by placing any desired handler mappings on the context and defining an order.
DispatcherServlet will find all of your handler mappings, and it will determine handler-mapping priority based on the handler mapping’s order property, which all AbstractHandlerMappings have by virtue of implementing the Order interface.
Here, the lower the number (with Integer.MIN_VALUE being the lowest possibility), the higher the precedence.
When routing a request, DispatcherServlet iterates over its registered handler mappings starting with the highestpriority mapping, trying at each step to generate a handler match.
Once a match is found, request processing continues with the matched handler.
Now let’s look at what happens when the routing actually occurs.
Although in most cases the request goes directly to a controller, that’s not the only way it works.
You can define interceptors around the controller to modify processing both coming in and going out.
One example is WebContentInterceptor, which supports request checks such as checking whether the HTTP method is permissible and whether a session exists (if sessions are required)
You can apply interceptors by injecting them into handler mappings, as shown next.
In listing 3.7, you begin by defining an interceptor B.
Here you’re using one of the ready-made interceptors; you’ll use it to block any HTTP request that isn’t a GET or a POST.
You accomplish this by configuring the supportedMethods property C, which is of course specific to this particular interceptor class.
Although the property type is a String[], Spring knows how to convert your list into an array.
Because this particular handler mapping extends AbstractHandlerMapping, you can define an interceptors property and pass along the list of interceptors E.
As previously mentioned, the interceptors property expects an array, but Spring knows how to convert the list to an array.
The result will be that for any request coming through that handler mapping, if it isn’t either a GET or a POST, it will be blocked.
For more information on interceptors, please see the HandlerInterceptor Javadocs.
Now let’s move on to handler adapters, which allow Spring to be flexible with respect to the type of controllers it permits.
Recall from our earlier discussion that one of the design goals behind Spring Web MVC is to be flexible.
One expression of this flexibility lies in the fact that controllers don’t have to implement any particular interface, at least as far as the app developer is concerned.
Developers can implement interfaces in the Controller hierarchy, or they can add @Controller to their POJOs.
Ultimately, of course, DispatcherServlet needs to have some way to invoke the handlers, and it accomplishes this through the HandlerAdapter interface.
The idea is that as long as there’s a HandlerAdapter implementation that knows how to deal with your specific type of controller, DispatcherServlet is happy and can work with your controller.
HandlerAdapter is therefore more a service provider interface (SPI) that you would implement only if you needed to support a new handler type; you wouldn’t normally make calls against it yourself.
We’ve been discussing handler execution chains, which consist of a handler and its interceptors.
Sometimes you want to define special exception handlers for the handler execution chains, and for that you turn to HandlerExceptionResolvers.
By default DispatcherServlet doesn’t have any, but if you decide you want one or more, it’s easy to do.
You place the desired HandlerExceptionResolver beans in the app context, using the order property to set precedence if you have more than one.
If any given resolver is able to determine an appropriate landing page for the exception, it provides the page; otherwise, it returns null and the next resolver in the chain takes a crack at it.
If all resolvers return null, then normal processing occurs (that is, whatever happens when there’s an exception and no HandlerExceptionResolvers to handle it)
When a controller is done processing a request, it generally returns a logical view name to DispatcherServlet.
The view name serves as a basis for view resolution.
With the adapter design pattern, adapter refers to the implementation code that bridges the two interfaces.
We’ll look at this chaining behavior in a few moments, but first let’s examine the default view resolver, which is InternalResourceViewResolver.
This allows you to map view names to InternalResourceViews, which represent servletbased view technologies such as servlets, JSPs, JSTL-based JSPs, and Tiles pages.
InternalResourceViewResolver converts the logical view name to a physical path by taking the logical view name, prepending a configurable prefix, and appending a configurable suffix.
By default, the prefix and suffix are empty, which means you have to specify full view paths instead of logical view names to use.
If this resolver receives the logical view name contact, for example, it converts that to /WEB-INF/jsp/contact.jsp and then builds a corresponding view.
The view is an InternalResourceView if JavaServer Pages Standard Tag Library (JSTL) isn’t on the classpath or else a JstlView if JSTL is present.
InternalResourceViewResolver automatically detects the presence or absence of JSTL and selects the correct view type accordingly.
Let’s look at a couple of special view names that InternalResourceViewResolver knows how to handle.
InternalResourceViewResolver inherits from its UrlBasedViewResolver superclass an awareness of two special view name prefixes, redirect: and forward:
Best practice: place JSPs in /WEB-INF/jsp or /WEB-INF/views In MVC applications, controllers mediate access to views, and you don’t usually want users hitting JSP pages directly.
A best practice is to place the JSPs under the /WEBINF folder where users can’t get to them.
By configuring InternalResourceViewResolver with the /WEB-INF/jsp/ or /WEB-INF/views/ prefix, you can ensure that logical view names resolve to JSPs inside /WEB-INF/jsp or /WEB-INF/views, respectively.
InternalResourceViewResolver sees a view name that begins with either of these, it short-circuits standard view resolution and instead returns a special view, either a RedirectView or an InternalResourceView according to whether the prefix is the redirect prefix or the forward prefix.
Redirecting is especially helpful after processing form requests, as you saw in section 3.3.5
Let’s look at some other view resolvers you can use.
When it receives a candidate view name, it checks the application context for a bean with a matching name or ID.
If it finds one, it assumes the bean is the desired View and returns it.
Otherwise it returns null, which means the DispatcherServlet will move to the next resolver in the chain.
BeanNameViewResolver is nice for simple applications, but as the number of views grows, XmlViewResolver becomes more helpful.
XmlViewResolver is conceptually similar to BeanNameViewResolver in that it uses named View beans to resolve view names to views.
The difference is that BeanNameViewResolver assumes that the View beans are defined in the application context, whereas XmlViewResolver assumes that they’re defined in a separate file, using the same Spring beans schema.
The difference is that with an unprefixed view name, the returned View is either an InternalResourceView or a JstlView depending on whether JSTL is present.
Because JstlView provides a superset of the functionality that InternalResourceView provides, there isn’t a good reason to use forward:
One possible way of using forward: might be to prevent a view name from being handled by some other view resolver (other than InternalResourceViewResolver) in view resolver chaining scenarios.
We’re a little skeptical of this idea, but it might be useful in some cases.
We’ve alluded several times to the fact that you can chain view resolvers.
The usual scenario is that you want a standard InternalResourceViewResolver to handle most of your view resolution needs, but sometimes you want other view resolvers, such as BeanNameViewResolver or XmlViewResolver, to handle special cases.
Maybe most of your views are JSPs but you also publish an RSS feed.
To handle that, you’d have an InternalResourceViewResolver to handle the JSPs, and you’d probably include a single BeanNameViewResolver for the RSS feed.
To configure multiple view resolvers, you add them to the application context as beans.
The ViewResolver implementations implement the Ordered interface, so you can define a processing order using the resolver’s order property if you like.
The numerically lower the order, the higher the precedence.) If you don’t define an explicit processing order, the InternalResourceViewResolver is automatically assumed to be the last resolver in the chain, because it never returns null (and hence never passes processing along to the next processor in the chain)
We’ve noted that controllers return a logical view name once they’re done processing, and that DispatcherServlet hands that view name over to a chain of view resolvers to generate a corresponding view.
But in many cases it’s easy to automate the generation of logical view names such that controllers don’t have to provide them explicitly.
Its job is to map requests to logical view names.
The controller can always override the generated view name by providing its own explicit view name; otherwise, the controller can let RequestToViewNameTranslator do all the hard work.
It maps request URLs to logical view names in a configurable way: you can configure a prefix and a suffix for the generated view names, and you can configure slash- and extension-stripping behavior.
You can define your own RequestToViewNameTranslator strategy by creating a bean with the well-known name viewNameTranslator.
Effective use of RequestToViewNameTranslator can be a nice way to adopt convention over configuration practices.
In some cases you won’t be able to use it.
This happens, for instance, when your controller method returns a different view name depending on the outcome of processing (for example, returning either a success or a failure page following an attempt to process form data)
In cases where you can use it, it makes your code cleaner.
DispatcherServlet uses a few other resolvers as well: MultipartResolver (for supporting file uploads), LocaleResolver (for supporting internationalization), and ThemeResolver (for supporting skinnable UIs)
We’ll treat each of these in separate recipes later in the book:
MultipartResolver is tackled in chapter 11 when you upload product photos to a product catalog.
LocaleResolver and ThemeResolver appear in chapter 7, where we present general UI recipes.
With that, we’ve completed our examination of the Spring Web MVC configuration.
As you have seen, Spring MVC is a flexible and capable framework.
To show you how easy it is to extend and use this framework, we’ll provide a technology preview of Spring Mobile, a relatively simple but powerful extension of Spring MVC.
Up to this point, we’ve focused on using Spring MVC to create normal web applications.
But what if you were asked to extend the capabilities of an existing Spring MVC application to provide a more customized user experience for mobile users or to create a completely new application that specifically targets mobile devices? In addition to detecting mobile devices, one of the most concerning problems that mobile web applications face is that both the screen size and capabilities of the each web browser vary significantly among today’s smartphone, PDA, tablet, and other mobile devices.
The Spring Mobile project provides extensions to Spring MVC for developing mobile web applications and offers server-side device detection, site preference management, and site-switcher functionality out of the box.
This gives you all the foundational tools necessary to enhance an existing web application or create a web application that provides a more customized user experience that mobile visitors will find more enjoyable and intuitive to use.
The Spring Mobile project provides two approaches for handling mobile devices:
Determine the type of device that initiated a web request.
Provide the information to a web application’s runtime that would provide the opportunity to customize its user experience.
For example, you could customize the layout, Cascading Style Sheets (CSS), and JavaScript based on this information.
Determine the type of device that initiated a web request.
Redirect the user to a separate site that caters specifically to mobile devices.
A common pattern is to redirect users of mysite.com to domains such as m.mysite.com or mysite.mobi where the content is designed specifically for mobile devices.
The ability to detect a mobile device is something that is common to both approaches.
We’ll take a deeper dive into the anatomy of an HTTP request in the next section.
In the pages that follow, you’ll create a trivial Contact List sample application.
At its core, this application is similar to the Spring MVC Roster sample application covered in section 3.2
As such, the process of constructing master/detail views in Spring MVC should already be familiar.
You should focus on how easy it is to use Spring Mobile and a JavaScript library to create an interface that will be recognizable to existing smartphone users.
Instead of building a full-blown application, we’ll focus on building out the pieces that illustrate detecting a mobile device as well as managing site preferences (full versus mobile)
Before we conclude our preview of Spring Mobile, we’ll show you the configuration required to implement the second approach to handling mobile devices.
As of this writing, Spring Mobile version 1.0.0.RC1 has been released.
A word of warning: although the changes to the API have slowed considerably, additional changes may still occur before Spring Mobile finally becomes generally available.
Let’s get started by talking about how mobile devices are detected on the server side using Spring Mobile.
Spring Mobile’s DeviceResolvers use information present in an HTTP request to sniff out the presence of a mobile device.
To give you an idea of how this works and what this information looks like, let’s look at how a typical HTTP request is made.
HTTP, like most network protocols, uses a client-server communication model.
An HTTP client opens a connection and sends a request message to an HTTP server.
The server then returns a response message that normally contains the resource that was originally requested.
After the response is completed, the server closes the connection.
If you were to open your browser and type in http://www.google.com and press Enter, your browser would create a request message that looks something like this:
In the first line of this message, you define that you’re using the GET HTTP method to obtain the resource at / and that you’re using HTTP 1.1
Following this initial request line is a list of header lines.
Each line defines a header in the format Header-Name: value.
Note that in some cases, a header can have multiple values and span multiple lines.
In addition, when using HTTP 1.1, only the Host header is required.
In the example, we used Firefox version 6.0.2 on Windows.
If we were to make the same request using an iPhone, the User-Agent header might look like this:
In the sample User-Agent header, you can now tell that the platform has changed from Windows NT 5.1 to iPhone.
Each device leaves its own request fingerprint that may consist of information from only the User-Agent header or from a combination of information in the HTTP request.
As you’ll see next, you don’t necessarily need to own or have access to these devices to get started developing mobile web applications.
Several browser-based plug-ins/extensions provide the ability to switch the UserAgent header that is supplied by the browser.
For example, User Agent Switcher, a Firefox extension, provides a menu and toolbar button to switch the user agent of a browser to any number of values simulating mobile devices.
If you’ll only be deploying the sample Contact List application to your desktop, you’ll need this or a similar plug-in for testing.
Now that you have an idea of the information available in an HTTP request and a mechanism to manipulate it, let’s see how Spring Mobile detects a mobile device.
Spring Mobile’s server-side device resolution functionality is based primarily on two interfaces, DeviceResolver and Device.
The DeviceResolver interface attempts to determine which device created the current web request.
We have omitted the comments in the code sample for brevity.
The default implementation of the DeviceResolver interface is the LiteDeviceResolver, which attempts to detect the presence of a mobile device based on information in the request headers.
LiteDeviceResolver looks for clues such as the use of the Wireless Access Protocol (WAP) or by comparing the contents of the User-Agent header to a list of 90 or so keywords or prefixes.
For example, LiteDeviceResolver would find the keyword phone in the iPhone User-Agent string (case-insensitive) we discussed earlier.
Just as the name implies, LiteDeviceResolver only aims to determine if the device that created the current request is a mobile device.
LiteDeviceResolver returns an instance of LiteDevice, which implements the Device interface:
This DeviceResolver implementation provides specific device and feature information (screen size and other device specific capabilities)
Now that we’ve talked about Spring Mobile’s server-side device-resolution functionality, let’s get started building an example application.
Because Spring Mobile is an extension of Spring MVC, configuring the Contact List application will be a breeze.
You start by configuring the Spring MVC DispatcherServlet in the web.xml file.
As we talked about earlier, when configuring DispatcherServlet, Spring looks for a file in the WEB-INF directory of the web application by the name of main-servlet.xml B, unless a different name and location are explicitly configured.
A quick note about WurflDeviceResolver WurflDeviceResolver was originally part of the Spring Mobile project.
It was removed after WURFL, which used to be free and open source software (FOSS), was changed to an AGPL license as of version 2.2
The original support for WURFL has been factored out of the Spring Mobile distribution and placed here: https:// github.com/kdonald/wurfl-spring.
The team is currently looking to contribute this integration to the official WURFL project.
The thing added in the Contact List application above and beyond a vanilla Spring MVC configuration is the DeviceResolverHandlerInterceptor at B.
By default, DeviceResolverHandlerInterceptor delegates to a LiteDeviceResolver that resolves the device to a LiteDevice.
If you wanted to use the WurflDeviceResolver we talked about earlier, you would inject this device resolver’s implementation into the HandlerInterceptor via constructor injection.
Based on this configuration alone, you can now detect when a mobile device is requesting a resource from the Contact List application.
In the code, you can obtain a reference to the current device by using the DeviceUtils class:
If you want to have the Device passed in as an argument to one of your @Controller methods, you can configure a WebArgumentResolver.
This is a Spring MVC feature that is new in version 3.1
You can do this by adding the following to the mainservlet.xml file.
The DeviceWebArgumentResolver B allows you to pass in the current device in your @Controller method like this:
We’ll talk more about the SitePreferenceArgumentResolver C in the next section.
Now that you have the ability to detect mobile devices in the Contact List application using Spring Mobile’s server-side device detection, you can control the user experience based on this knowledge.
For example, to optimize a user’s mobile experience, it’s possible to redirect the user to a mobile-specific version of the site.
In most cases, this site may be a thinned-down version of the original site to accommodate a device’s smaller screen size.
But what if the end user wants to visit the normal site?
The Spring Mobile team has provided a facility to handle user site preference management as well.
The code uses a pattern similar to the server-side device-detection.
The SitePreferenceHandlerInterceptor is added to your configuration after the DeviceResolverHandlerInterceptor at B in listing 3.13
The default implementation, StandardSitePreferenceHandler, checks to see if a user specified a SitePreference.
If not, its value defaults to MOBILE if a mobile device has been detected or NORMAL if not.
By default, this value is stored using the CookieSitePreferenceRepository, which is the default implementation of SitePreferenceRepository.
For example, the following code can be used to set a user’s site preference:
Configuring site preference management is similar to configuring Spring Mobile’s server-side device detection.
To configure the site-preference management in the Contact List application, you need to add the SitePreferenceHandlerInterceptor right after the DeviceResolverHandlerInterceptor in your main-servlet.xml file:
Based on this single configuration change, you can now detect an end-user’s explicit or default site preference when a resource is requested from your application.
In your code, you can obtain a reference to the current device by using the SitePreferenceUtils class:
The SitePreferenceWebArgumentResolver allows you to pass in the current device in your @Controller method like this:
You can now detect if a mobile device is accessing your site, and a mechanism lets users manage their own site preference (full versus mobile)
This gives you the information you need to make decisions about how you might want to customize your site to provide a more enjoyable and intuitive user experience for your mobile visitors.
In the next section, we’ll look at how you can use a JavaScript framework to do just this.
These frameworks offer an abstraction layer that simplifies mobile web development by providing a collection of cross-browser UI elements/widgets that often mimic the native device’s look and feel as well as a unified way to access native mobile OS features.
In the Contact List sample application, we chose to use jQuery Mobile (http://jquerymobile.com/)
Measuring in at 12 KB, the framework is relatively lightweight, a feature that is important for devices that may have limited bandwidth.
To experiment with this trivial sample application and see all the items we talk about in action, download and.
Figure 3.4 The Contact List sample application when viewed by a normal browser.
Spring Mobile technology preview run the source code for this chapter and point your browser to http://localhost:8080/sip/main/contact/list.
As shown in figure 3.4, in a normal browser you see a rather vanilla-looking list of contacts.
When you view the same address using a mobile device browser or normal browser with a user-agent switcher to mimic a mobile device, you’ll see the view shown in figure 3.5
Figure 3.5 The Contact List sample application when viewed by a mobile device.
As we mentioned at the beginning of section 3.6, Spring Mobile provides two different approaches to handling mobile devices.
Although trivial, the Contact List sample application you just finished demonstrates how Spring Mobile can provide the information necessary to a web application’s runtime that can allow you to customize the layout, CSS, and JavaScript based on the type of device accessing the site.
Before we conclude our Spring Mobile preview, let’s look at the out-of-the-box site-switching functionality that Spring Mobile provides.
This secondary approach can be useful when you would like to detect mobile users and redirect them to an entire site that might be designed specifically to cater to the needs of mobile users.
As opposed to using SitePreferenceHandlerInterceptor to manage preferences within the same site, you can use SiteSwitcherHandlerInterceptor to redirect mobile users to a separate site.
An example of each is provided in the following sections.
Keep in mind that SiteSwitcherHandlerInterceptor delegates to a SitePreferenceHandler internally so there is no need to configure a SitePreferenceHandlerInterceptor explicitly.
You can use the mDot factory method to create an instance of the SiteSwitcherHandlerInterceptor that redirects users to a domain in the format m.yourdomain.com:
You can use the dotMobi factory method to create an instance of SiteSwitcherHandlerInterceptor that redirects users to a domain in the format yourdomain.mobi:
This concludes our technology preview of the Spring Mobile project.
Over the last several pages, we have discussed how this project provides extensions to Spring MVC for developing mobile web applications and offers server-side device detection, site-preference management, and site-switcher functionality out of the box.
We also talked about how you can use Spring Mobile to detect and customize a single site for both mobile and nonmobile devices or redirect mobile users to a different site.
Spring Mobile provides all the foundational tools necessary to enhance an existing web application or create a web application that provides a more customized user experience that mobile visitors will find more enjoyable and intuitive to use.
Spring Web MVC is closely related to other technologies in the Spring stack.
We’ll mention them briefly here so that if you’re interested, you can do some follow-up study.
The idea is that there are use cases in which it’s necessary to treat a series of user interactions as a single transaction.
Examples include checkout processes (for example, buying a plane ticket, booking a hotel, and buying something from an e-commerce site), multipage user registration and application processes, and product-configuration wizards.
In SWF, you model each process with flows (see chapter 5 for more details)
Flows are essentially state-transition graphs, and they have a hierarchical structure so you can reuse finer-grained flows inside coarser-grained flows.
You might have a user-registration flow and a login flow, and you might incorporate those into a larger checkout flow such that at the end of a checkout process the user is given the option of creating an account or logging in.
Spring JavaScript provides a client-side abstraction over JavaScript toolkits, with an emphasis on progressive enhancement, widgets, and AJAX support.
It began life as part of Spring Web Flow, but eventually it became its own project because it’s not inherently tied to SWF.
Like Spring JavaScript, it originated in the Spring Web Flow project, but became a separate project because it’s not specifically tied to SWF.
Although Spring Security (née Acegi) isn’t inherently tied to Spring Web MVC, it’s worth mentioning here because it includes a great deal of support for securing web applications.
Spring Security isn’t limited to web apps, but it does provide strong support for securing them.
Spring Security primarily addresses two major areas of security: authentication and authorization.11 Concerning web authentication, it provides a built-in username/password login form with optional remember-me functionality, support for CAS-based SSO, OpenID authentication, and others.
Concerning authorization, Spring Security supports both role- and ACL-based authorization at multiple application tiers.
You can selectively display and hide JSP page content using tag libraries.
And you can authorize methods (in any application tier—especially the web and service tiers) using aspects.
Chapters 4–6 present recipes that draw heavily from the Spring Security framework.
Beginning with Spring 3.0, RESTful web services live in the world of Spring Web MVC.
We won’t cover Spring MVC’s REST support in this book.
This has been a whirlwind tour through Spring Web MVC.
We’ve covered a great deal of what the framework provides, so if you understand the material in this chapter, you should be in a good place to understand the recipes in the chapters that follow.
Please refer to them as you work through the recipes any time you need to review the material.
Spring Security addresses other areas too, such as privacy (for example, encryption and SSL, hashing, and salting passwords), but most of the focus is on authentication and authorization.
Web forms provide a means by which we can collect data from end users.
As such, they’re a key aspect of any nontrivial web application.
This chapter shows how to use Spring Web MVC and related technologies to build a simple user registration.
Users establish a relationship with a website or an organization by registering.
The resulting user account allows logins, order placement, community participation, and so on.
The first step in supporting a user registration process is to display a registration form.
In this recipe you’ll use Spring Web MVC to display a user registration form.
You’ll build a user account form bean, a web controller, a registration form, and a confirmation page.
It won’t hurt to have a visual on the UI you’re planning to create in this recipe.
Let’s begin by creating a form bean for your user accounts.
You use a form bean to store form data, as shown in the following listing.
AccountForm is a POJO.1 It has properties for personal B, marketing C, and legal D data.
You also include a observe the form-binding later in the recipe.
By design, you suppress the password here to avoid accidentally revealing it.
You default the marketingOk property to true because you’d like to market to your users unless they explicitly opt out.
On the other hand, you default acceptTerms to false because you want the user’s acceptance of the terms of use to be active rather than passive.
Presumably this gives you a stronger legal leg to stand on in the event of a disagreement with the user.2
You have a form bean, but without a web controller, it’s inert.
Your account controller, which appears in the following listing, handles form delivery and processing.
Disclaimer: We aren’t lawyers! Consult a qualified legal expert if necessary.
Figure 4.1 The simple web-based registration form that you’ll build in this recipe.
At B the @Controller annotation tells Spring that this is a web controller.
You establish a base path for request mapping using the @RequestMapping annotation C.
At D you’re not implementing any special interfaces or extending special classes.
The associated request mapping is /users/ new, which you obtain by combining the class-level /users base path with the methodlevel new mapping.
To override a class-level mapping rather than refine it, place a slash in front of the method-level mapping.) The method itself places a new AccountForm instance on the model under the key account and returns the view name.
You process form submissions at F, specifying the POST request method.
The request mapping is just /users because that’s the result of combining the base path with the empty string.
For now, when users post form data, you log it and redirect them to a view that thanks them for registering G.
We’ll discuss the redirection in more detail later in the recipe.
First you’ll create the view for the registration form, and after that you’ll create the “thanks” page for successful form submissions.
The next listing shows how to implement the registration form from figure 4.1
Note that we’ve suppressed the layout and CSS code; see the code download (src/main/ webapp/WEB-INF/jsp/users/registrationForm.jsp) for the full version.
The registration page uses the form B tag to create an HTML form.
You use form elements are bound to the form bean’s properties in both directions:
Inbound—The form bean is populated with the HTML form element values when the form is submitted and passed to the controller for validation and processing.
Outbound—The form elements are prepopulated with the form bean’s values.
You use this, for example, to set the default value of the marketingOk check box to true and acceptTerms to false.
Form elements are also prepopulated before representing a form to a user for remediating invalid form data; you’ll see this in recipe 4.3
Stars show where form fields and bean properties are bound together.
You use input C, password D, and checkbox E tags from the Spring form tag library to render HTML form elements.
These are essentially form-binding versions of the corresponding HTML elements.
The tag library doesn’t provide anything for submit buttons (there’s nothing to bind to here), so you use standard HTML F.
After the user successfully submits a registration, you need a page to let the user know that the registration succeeded.
In this case, the page doesn’t even need to be a JSP, although you’ll leave it as is because it’s always possible that you’ll want to present dynamic information through the page.
The key part of your web.xml configuration is the following:
This web.xml configuration references a single Spring configuration, called beansweb.xml, associated with the DispatcherServlet.
It goes in src/main/resources/ spring so it will be on the classpath when you package and deploy the app.
You use component scanning to discover the AccountController B based on its @Controller annotation.
You use <mvc:annotation-driven> at C to activate annotation-based configuration inside the DispatcherServlet explicitly.
At D you use <mvc:view-controller> to configure a new controller for the registration success page.
Recall from listing 4.2 that you redirected the request to a success page, but you never specified a controller to display the success page.
That’s what whose job is to accept requests for /users/registration_ok and serve up the logical view name users/registrationOk for view resolution.
You redirect rather than forward to the success page because you want to apply the redirect-after-post pattern to your form submission.
With this pattern, a successful form submission issues an HTTP redirect to avoid resubmissions if the user reloads or bookmarks the page, as illustrated by the sequence diagram in figure 4.3
The figure suppresses the ViewResolver, but the DispatcherServlet uses the instance you created for both view resolutions depicted.
The DispatcherServlet uses the ViewResolver E to convert logical view names into views.
After all, the DispatcherServlet default configuration already has an internal DefaultAnnotationHandlerMapping instance to handle @RequestMapping annotations.
The reason: behind the scenes, <mvc:view-controller> creates a SimpleUrlHandlerMapping to map the ParameterizableViewController to the specified path, and this replaces the DefaultAnnotationHandlerMapping that would otherwise have been created.
You use <mvc:annotation-driven> to indicate that you want the DefaultAnnotationHandlerMapping as well.
You’ll also need a WEB-INF/decorators.xml file for SiteMesh; see the code download for that.
To run the app, run Maven with the jetty:run goal.
On the command line, it looks like this: mvn -e clean jetty:run.
You should see a registration page that looks like the one from figure 4.1
What you’ve done so far isn’t tied to registration forms; this recipe is a blueprint for displaying web forms in general.
As you move forward in the chapter, you’ll continue to target registration forms, but the discussion and techniques are broadly applicable.
In the next recipe, you’ll make your view pages more flexible by externalizing the strings that appear in the JSPs.
It’s often desirable to decouple a view from the specific bits of text rendered in the view.
Externalize the strings that appear in the registration JSPs so they can be managed centrally.
Create a resource bundle that contains the externalized strings, or messages in the Spring vernacular.
First up is the resource bundle, which contains your messages.
The following listing shows how to create a resource bundle for your messages.
This file goes in src/main/resources because you want it to appear at the root of the classpath on deployment.
In this case, you have three sections: one for messages that are common to both pages B, another for registration form messages C, and a third for messages that appear on the success page D.
Next you add a single bean to the beans-web.xml configuration.
This creates a message source, backed by the resource bundle, that you can use to drive dereferencing in the JSP.
The third and final step is to replace the hardcoded strings in the JSP with references.
The next listing shows how to convert hardcoded strings into references using the.
You use the pageTitle variable at D and also inside the following <h1>, and you use the msgAllFieldsRequired variable at E.
At F you use <spring:message> in a slightly different fashion; this time, you dump the message right into the template.
Run the app the same way you ran it in recipe 4.1
Under the hood, you’ve externalized the strings, but you shouldn’t see any behavioral changes.
Besides paving the way for internationalization, it gives you a central place to manage text.
In the following recipe, you’ll fix that with form validation.
No matter how intuitive your registration form, people will accidentally or even intentionally fill it out with invalid information.
You treat such errors as user errors rather than system or application exceptions, meaning you usually want to explain the error to the user in nontechnical language and help them overcome it.
When users submit form data, validate it before performing further processing.
If there are errors, help the user understand what went wrong and how to address the issue.
At the highest level, this recipe addresses two types of validation:
The spring and form tag libraries come from the org.springframework.web.servlet artifact, and the corresponding tag library descriptors are spring.tld and spring-form.tld, respectively.
In general, clients shouldn’t be allowed to submit fields that don’t appear on the form.
Field validation—Ensure that all submitted field values follow validation rules.
When users submit HTML form data, Spring Web MVC uses the form-binding API to bind the HTTP parameters to form bean properties in an automated fashion.
The form-binding API allows you to filter out unwanted HTTP parameters by silently ignoring them during binding.
Spring Web MVC uses JSR 303 to validate form data encapsulated in this fashion, and developers use the Spring validation API (specifically, the BindingResult interface) from within a controller to determine whether the bean is valid.
Sometimes you need to perform a bit of custom validation logic.
Spring’s validation API provides a programmatic interface for implementing such logic.
Recall that Spring Web MVC automatically binds HTML forms to an underlying form bean.
Although this is a major convenience to application developers, it raises a security.
The form-binding API handles field filtering, JSR 303 handles bean validation, and there’s a Spring validation API for custom logic.
Validating form data concern because it allows attackers to inject data into form bean properties that aren’t intended to be accessed via the HTML form.
You’re not in that situation here, but it’s a common state of affairs in cases where a single model object performs double duty as both a form bean and a persistent entity.
In such cases you need a way to guard against data injection.4
The @InitBinder annotation tells Spring Web MVC to call this method when initializing the WebDataBinder responsible for binding HTTP parameters to form beans.
Several steps are involved in adding form validation to your app:
Consider the case where you use a single Account POJO to serve as both an entity and a form bean.
The entity might have an enabled field that indicates whether the account is enabled.
You wouldn’t want clients to be able to manipulate that field by sending a value for the field to the form processor.
The idea is that nothing gets through unless it’s on the whitelist.
With a blacklist, everything gets through unless it’s on the blacklist.
Whitelists are generally more secure, because they start with an assumption of distrust rather than trust.
For example, you might filter out comment spammers using an IP blacklist, because it wouldn’t be practical to use a whitelist for web traffic.
Let’s start at the top of the list and work our way down.
You can therefore move on to the next step, which is marking up AccountForm with validation annotations.
The following listing updates the AccountForm from listing 4.1 by adding validation annotations.
The previous listing uses the Bean Validation (JSR 303) standard and Hibernate Validator to specify validation constraints.
At C you indicate that the username property can’t be null, and its size must be 1–50 characters in length.
At D you use the Hibernate-specific @Email annotation to ensure that the email property represents a valid e-mail address.
At E you require that the acceptTerms property be true for validation to succeed, and you specify a message code to use when the validation fails.
This Hibernate annotation, which was introduced with Hibernate Validator 4.1, allows you to use a script to express validation constraints involving multiple fields.
Here you use JavaScript to assert that the password and confirmation must be equal.
The @ModelAttribute annotation causes the account bean to be placed automatically on the Model object for display by the view, using the key "account"
The @Valid annotation causes the bean to be validated on its way into the method.
Spring exposes the validation result via the BindingResult object C.
This is how the form bean in the method parameter list.
This error code resolves to one of the following message codes, depending on which message codes appear in the resource bundle:
These message codes are listed in priority order: if the resource bundle contains the first message code, then that’s the resolution, and so forth.5 The first message code does in fact appear in messages.properties.
It’s probably worth emphasizing the fact that despite superficial similarities, error codes and message codes aren’t the same thing.
Validation errors have associated codes, and these generally map to a set of resource bundle message codes, which in turn map to error messages.
Finally, once you’ve processed any password errors, you check to see whether there were any validation errors, and route to a success or failure page accordingly E.
Notice that you’re using the view name constants defined at the top of the file.
Let’s take a more detailed look at the error messages here.
First let’s talk about the default JSR 303 and Hibernate Validator messages.
Strictly speaking, you don’t have to override them at all.
But the defaults aren’t particularly user-centric (one of the defaults, for example, references regular expressions), so you’ll change the messages for the constraints you’re using.
You’ll use this resource bundle not only to override the JSR 303 and Hibernate Validator defaults, but also to define an error message specific to the acceptTerms property.
The message for @Size is effectively a template that generates messages with the minimum and maximum sizes substituted in.
And if you do, the default error message is OK because this is a programming error rather than an end user error.) Finally, you define an error message for the acceptTerms property at D.
In addition to the JSR 303 error messages, you need messages for the Spring-managed errors.
You’ll add these to messages.properties because ValidationMessages.properties is for JSR 303 error messages.
Although it can be a little confusing to split the error messages into two resource bundles, it helps to do exactly this.
The reason is that JSR 303 and Spring use different schemes for resolving error codes to message codes, and mixing error messages in a single resource bundle can make it harder to keep message codes straight.
Now you have an error message for the password-mismatch error code you used in the controller.
You’ll use the global error message in the form JSP.
The text fields for properties with errors are visually distinct (they have red borders), although it’s hard to tell if you’re viewing the figure in black and white.
Also, fields are prepopulated with the user’s submitted data so the user can fix mistakes instead of reentering all the data.
The only exceptions are the two password fields, which for security reasons you don’t prepopulate.
To accomplish this design, you’ll need to revise registrationForm.jsp as shown next.
Figure 4.5 The revised registration form, with a global error message and field-level error messages.
By using the <form:input> tag, you get data prepopulation for free.
This time around you include the CSS attributes because there’s something interesting to show off.
This allows you to change the visual appearance of the text field when there’s an error.
In addition to the text field, you want to display the error message, and that’s if desired.
The other fields are essentially the same, so we’ve suppressed them.
Again, please see the code download for the full version of the code.
The last step in the process is to configure the application for validation.
Start up your browser and give the code a spin.
There’s nothing wrong with that, because the constraints you’ve used so far make sense as web tier constraints.
But it’s important to bear in mind that modern validation frameworks like Spring validation and JSR 303 validation abandon the traditional assumption that bean validation occurs exclusively in the web tier.
In the following recipe, you’ll see what validation looks like in the service tier.
So far you’re accepting and validating user registrations, but you aren’t saving the data to a persistent store.
Although you’ll save your form data to a database, you’re not going to save the AccountForm form bean directly.
The main reason is that there’s a mismatch between the form bean and what you’d want out of a domain object:
For security purposes, you don’t want your domain object to have a password property.
You don’t want a bunch of in-memory passwords sitting around.)
Your domain object will have an enabled field that the form bean doesn’t have.
Instead, you’ll create a separate Account domain object and then have the controller translate the AccountForm into an Account before saving the Account.
Why not save the form bean directly? It’s possible to have a single POJO serve as both a form bean and a domain object, but architecturally it’s cleaner to separate the two, especially if there are material differences between them.
Here the security difference seems important enough to warrant two separate classes.
Having said all that, the choice is partly a matter of style.
Especially with traditional designs based on anemic domain objects, it’s common to see a single POJO supporting presentational, domain, and persistence concerns.
This might change, though, if domain-driven design (DDD) catches on in the Spring community.
Spring Roo promotes a DDD approach.) As domain objects get richer, they become less suitable as form beans.
You’ll use a combination of Hibernate, JPA annotations, and JDBC to persist the user registration data.
Hibernate will work nicely for saving the Account domain object, but you need a way to save user passwords as well, and Hibernate won’t help there because the password isn’t part of Account.
The POJO and password data need to be saved as part of the same transaction, and we’ll also show how to do that.
This recipe adds a lot of infrastructure to what you already have.
You’ll start with the database schema, then build out the code and configuration.
The following listing presents the database schema for MySQL, which involves a single table for storing user accounts.
For example, the field-size maximums are generally 50 in both locations.
Speaking of Account, let’s create it, because you’ll need it for what follows.
The next listing presents the Account domain object, with JPA annotations for persistence.
At E you use @Id, that the JPA provider (Hibernate in this case) is responsible for determining the right ID-generation strategy for the underlying database.
IDs might be generated by an autoincrement column, or perhaps by a sequence, and so on.)
For most properties, the column mapping is a matter of attaching an @Column.
In the case of the fullName property, it’s a convenience method rather than a persistent field, so you mark it with @Transient G to prevent Hibernate from trying to persist it.
You can also use JPA to define named queries supporting finder methods.
At B you define a named query to look up accounts by username.
You need both an interface and an implementation for your DAO.
The interface method (recall that the Account doesn’t have a password property) and a finder-byusername:
The DAO implementation in the following listing is more interesting.
You derive it from AbstractHbnDao in chapter 1, but note that it isn’t a pure Hibernate DAO.
You use @Repository B to tag HbnAccountDao as a DAO.
This allows Spring to discover the bean during component scanning.
Hibernate handles everything on the Account POJO, but the password is a standalone field.
You also inject a JdbcTemplate at D to execute the update.
At E you have Hibernate and JDBC working together to save the user account data, including JDBC password update using the JdbcTemplate.
Besides saving account information, you have a finder for looking up an account by username F.
You’ll use this to check for duplicate usernames when the user tries to register an account.
The finder uses the JPA named query you created on the Account domain object in listing 4.12
Now let’s create an account service around the account DAO.
You’ll create a service with a single method for registering valid users.
The idea here is that the if and only if there aren’t any validation errors, either present in the Errors object, or which works fine because BindingResult extends Errors.
You use Errors in the AccountService interface, though, rather than BindingResult, because the service tier doesn’t know anything about web binding.
The reason: when doing form validation, you generally want to know about all validation errors, not just the first one.
So you still check for duplicate usernames even if you already know, for example, that the passwords didn’t match.
A good practice when writing service beans is to associate a read-only transaction definition at the class level B.
This provides a basic layer of safety because individual methods have to override the definition explicitly C in order to write to the persistent store.
Here you have only one method, so it looks a little funny, but this way you won’t forget if you decide to add more methods.
The username validation F uses the finder you created to determine whether the username is a duplicate.
If it is, then you use the errors object to reject the username, specifying the errors.duplicate error code (we’ll define that momentarily) and the username for token substitution.
All you need to do is add a single error message to messages.properties:
The error.duplicate.account.username message code will match the error.duplicate error code as explained in recipe 4.3
There isn’t much you need to do to the controller to make it save accounts, as you’ll see now.
You’ll need to augment the existing configuration to support persistence.
The main part of this effort involves adding a new Spring application context file.
To add persistence to your registration form, you need to add several bits.
You declare a DataSource reference using a JNDI lookup at B.
You’ll need to consult the documentation for your servlet container to see what’s involved with exposing a DataSource with JNDI using that container.
At C you declare the JDBC template you’re using to set the user password.
You’ll need to modify that if you’re using a different RDBMS; see the Javadoc for the org.hibernate.dialect package for more options.
You define a Hibernate SessionFactory at E, using the DataSource and configuration you just created.
As its name suggests, the SessionFactory is a session source.
Sometimes it creates brand-new sessions (for example, when starting a new transaction), and sometimes it returns sessions that have already been created (such as when executing DAO persistence operations)
The transaction manager F provides (you guessed it) transaction management services.
It knows, for example, how to start, suspend, and stop transactions.
The HibernateTransactionManager implementation coordinates transaction management with Hibernate session management.
You use <context:component-scan> to discover DAOs and service beans G.
Just one small tweak to go, and you’ll be ready to run the app.
All you need to do here is add a single configuration element to web.xml.
This tells the Spring Web MVC DispatcherService where to find your beans-service.xml configuration:
In addition to Spring AOP proxies, AspectJ weaving is an option.
In this recipe we’ve shown how to save form data to a persistent store.
That’s of course a common requirement, and now you have a good feel for how to do it.
You even saw how to use Hibernate and JDBC together in cases where the form data doesn’t all fit nicely inside a single domain object.
Because our topic is web forms in general rather than user-registration forms in particular, we’ve neglected some persistence- and security-related topics that a real user form would take seriously.
The good news is that we’ll address them in chapter 6
Spring Security integration—A key reason for user accounts is to support logins.
Hashing and salting passwords—It’s a poor security practice to save passwords as plaintext in the database, because that makes it easier for a malicious person to see those passwords and use them on other websites.
Users often use the same password for multiple websites.) You can use password hashing and salting to mitigate this issue.
We’ll show how to hash and salt passwords in recipe 6.7
In this chapter, you developed a basic registration form with several of the key features you’d expect such a form to have, including string externalization, validation, and persistence.
Although we used user registration as an example, the topics we’ve treated are obviously general concerns when developing web-based forms.
In many cases, registration forms aren’t as simple as the one you developed in this chapter.
Instead they carry the user through a series of steps, implemented as a web flow spanning multiple pages.
In chapter 5 you’ll learn how to implement multistep flows using Spring Web Flow.
Most enterprise Java developers have worked on web applications that have some sort of workflow component to them.
Classic use cases consist of searching for products, booking a flight, and preparing your tax return.
But without the right tools, determining how to manage this workflow can be a challenge.
Model-view-controller (MVC) frameworks work best in situations where the unit of work required to create or update the model can be implemented in a minimal number of views.
By itself, though, the MVC pattern doesn’t provide an efficient mechanism for managing a series of intermediate steps, their rules, and states that span multiple requests.
As a result, page-flow logic typically seeps into both the view Enhancing Spring MVC applications with Web Flow.
An overview of Spring Web Flow and controller tiers whereas the application state required to support the page-flow logic is often spread between a combination of session and request parameters.
As a result, understanding, maintaining, and testing complex page flows in an MVC application can quickly become an arduous task.
In this chapter, we’ll look at Spring Web Flow (SWF) and focus on how you can use this framework to complement your existing Spring MVC application.
Before we dive in, let’s go over some important background information.
In general, workflow engines can route work through any number of sequenced tasks or activities.
Processing in these engines takes place either synchronously or asynchronously using a number of predefined rules and other complex workflow patterns such as forks and joins.
In contrast, SWF provides only a subset of these capabilities.
It’s focused specifically on addressing the problem of navigation (page flows) in the web tier.
Both the relationship and application of these tools should be viewed as complementary rather than competitive.
But this narrow focus makes SWF intuitive to use and easy to learn.
Some examples of where SWF would be beneficial are as follows:
As you’ll see in section 5.6, large flows can be broken down into reusable subflows with their own predefined contracts and lifecycle, making reusability and modularity straightforward.
Over the next several sections, we’ll introduce you to this framework and its core concepts.
You’ll build on this knowledge in section 5.3 by creating a brief demo application.
The remainder of the chapter will focus on making you more productive by examining problems or tasks you’re likely to encounter as you begin developing with SWF.
Although SWF provides JSF and portlet support, we’ll focus on complementing your existing Spring MVC configuration.
Our favorite stories involved going on quests or exploring foreign lands.
The first chapter introduced us as the main character and set up the initial story line.
Subsequent chapters added to the story and presented several options that let us control what we as the main character did next.
We were then routed back and forth through several parts of the book based on the actions we chose.
It was common for these books to have varying endings depending on the choices made.
Flows are similar to these books in that they have a single place to start, followed by several intermediary steps.
Each step can route the user to additional steps or different end points based on information captured in the flow.
In SWF, these flows are potentially reusable components that represent a unit of work and are defined using states, transitions, and flow data.
A flow is defined using an XML-based flow definition language.
Each flow is defined in its own file using the following root element.
In addition to defining a flow, at B you explicitly define the enterSearchCriteria state as being the flow start state.
Each box in figure 5.1 represents an individual step in the flow called a state.
This is where something is displayed to the user, a decision is made, and a flow ends or some other action is taken.
Figure 5.1 In this simplified search flow, users start by entering search criteria and transition to a state where they can view their search results.
The flow transitions to the end state after the user selects an item.
The start state is a marker state that designates another state (view, action, decision, end, subflow) defined in the flow as being the flow’s starting point.
You defined an explicit start-state at B called enterSearchCriteria in listing 5.1
When start-state isn’t explicitly defined, the first state defined in the flow is assumed to be the start state.
Let’s look at how each of these states is defined in SWF’s XML-based flow definition language.
The view state is used to either display or solicit information from the user and is defined using the view-state element in the flow definition XML file.
In the simplified search flow in figure 5.1, you use view states to interact with the end user by presenting a search form or viewing search results.
In the following example, the logical view name is explicitly specified using the optional view attribute.
If the view attribute isn’t specified, the view is given the same name as the state’s required id attribute along with a .jsp suffix.
Although our focus in this chapter is on enhancing your existing Spring MVC application, SWF supports a number of view technologies.
In addition to JSF, SWF can use any of the view technologies that Spring MVC supports out of the box.
When a view is rendered to the user, the flow pauses and waits for another event to occur.
Users can continue flows by generating additional events by either clicking links or submitting forms containing event IDs.
View The view state renders a view to the user and is used to either solicit information from or provide information to the user.
Action An action state is used when you want to perform some type of work and transition to another state based on its outcome.
Decision The decision state is similar to an action state but uses a convenient If…Then…Else statement to determine which state to transition to next.
End The end state represents the end of a flow.
Subflow The subflow state starts another existing flow as a subflow and maps the subflow’s end states to transitions in the current flow.
To specify an event ID in a link, you need to include the event ID using one of the conventions appended to flowExecutionUrl.
The two lines in the following example are equivalent and show how to specify event IDs using links:
When the view is rendered, flowExecutionUrl contains the context-relative URI for the current flow execution and a request variable called execution.
The link specified on the first line in the example may be resolved as.
In the example, the value of the execution request variable is specified in the format eXsY where e indicates the current instance of the flow myFlowId, which is denoted by X.
The letter s indicates the current step that is being executed and is denoted by Y.
Using the example you’re executing the first instance and first step of the flow with flowId myFlow.
We’ll talk more about flow IDs and how they’re specified and resolved later.
When using forms to capture user input, the event ID can be specified either in a hidden field or in the name of a submit button.
The next code snippet provides an example of specifying the event ID using a hidden form field:
Similar to the previous code, the following snippet provides an example of how to specify an event ID using a form submit button:
Notice how this example provides both the event ID and its value together using the underscore character B.
The view state is unique in that it’s the only state where the currently executing flow pauses and waits for a user-generated event to occur.
With the exception of the end state, other states continue the flow by evaluating events generated in the state.
As the name suggests, the action state is where the application does work.
You express the work to be done using an expression in an evaluate statement.
When the expression is evaluated, the value that volunteerAction returns becomes the event ID to transition to.
If you need to execute additional code, you can add evaluate elements.
A decision state provides a subset of the functionality of the action state mentioned earlier.
Instead of checking the return value of one or more expressions to find a match to a local or global transition, the decision state evaluates a boolean expression and transitions to one of two states depending on the result.
The addVolunteer code snippet is rewritten next as a decision state:
Now that we’ve talked about how to start a flow, interact with users, and do work, let’s see how you end the flow.
Once the flow transitions to an end state, it’s terminated:
If a view is specified C, SWF renders the view in addition to ending the flow.
As you’ll see next, if you’re ending a subflow, the end state’s id is used as an event to transition to in the parent flow.
In general, it’s a good practice to break large, complex problems into smaller, more manageable pieces.
As shown in the following snippet, you use the subflow state in the top-level flow to call an existing flow as a subflow:
Here you are calling a subflow with the ID widget/createNew.
As we mentioned earlier, the ID of the subflow’s end state will be used as the event to transition to.
Now that we’ve discussed the five different types of states available in SWF, let’s turn our attention to transitions and see how you can use them to move from state to state.
Transitions can be defined in a given state or globally.
A transition is defined using the transition element and maps an event occurring in the current state to the next state.
At B, you implement a catch-all transition by omitting the event name.
In this example, you transition to newSearchEndState if an explicit match isn’t found for the other transition events.
Here, if a ServiceUnavailableException is thrown, you can transition to a view state to provide the end user with a friendly message.
After writing a flow that contains many different states, you may find that you’re defining certain transitions over and over again.
Perhaps in a given flow, there are many points at which a user can choose to either quit and exit the flow or start over.
Instead of specifying a transition from the quit event to an end state with the ID quitEndState in each location, you could define this transition globally.
Here you use the global-transitions element to define two global transitions that can be used in any view or action state in the flow.
Up to this point, we’ve talked about events, the five types of states, and how to transition between them.
The next section completes our overview by discussing how SWF manages its state.
Then we’ll put it all together in a brief example application in section 5.3
In most MVC applications that contain complex page flows, developers are left to figure out how they should maintain state for each individual step, rule, or entire workflow.
Bugs are often introduced into these systems as a result of having multiple places and approaches to managing this state.
Let’s look at how you define variables in SWF and then at the approaches you can use to store and manage the lifecycle of this state in the flow.
There are several different ways to define variables in SWF.
The <var> element is used at the flow level to define flow-scoped instance variables:
In the example, SWF instantiates an instance of the User class and assigns it to a variable named user.
This variable is accessible from anywhere in the current flow.
When saving the result of an expression into a variable, you must explicitly specify the intended scope for the variable.
We’ll talk more about the scopes used in SWF in the next section.
Because the <evaluate> element determines the event it returns at runtime, it’s ideally suited for driving transitions in an action state:
Like the <evaluate> element, it stores the result of an expression into a named variable using a slightly different syntax:
Here you’re creating a new Widget instance and storing it in the flowScope under the name widget.
Up to this point, all the expressions you’ve seen use the Spring Expression Language (SpEL) which is new to Spring 3.0
Although other expression languages such as Unified EL, OGNL, and JBoss EL exist, SpEL was specifically created to provide developers with a single expression language that can be used throughout the entire Spring stack.
SpEL is a powerful expression language and can be used to express many complex concepts both inside and outside of Spring.
Because of its usefulness and application throughout the Spring stack, we strongly urge you to learn more about programming with SpEL.
For more details about the expression language, its API, or its syntax, consult the documentation for the core Spring project available at www.springsource.org/documentation.
As you may have expected, defining variables in SWF is pretty straightforward.
The data you store as variables in a flow can have different lifecycles based on which of the following five scopes it belongs to:
The data will be available for the duration of the request, which may involve many states and transitions.
They can only be referenced from the view state that created them.
They can only be referenced from the flow in which they were created.
They can be referenced from the flow that defined them as well as all subflows.
With the exception of the <var> element, when assigning variables, you need to be explicit about which scope the variable should be created in.
In this case, SWF uses the scope-searching algorithm illustrated in figure 5.2 to determine which scope the variable is in.
After searching each of the variable scopes, if no variable is found, an EvaluationException is thrown.
This algorithm is similar in concept to how the EL searches through scopes for a named object in a JSP page.
Now that we’ve provided a brief overview of all the major components of SWF, let’s see how you can assemble them into a brief demo application that demonstrates the main features of SWF.
For the remainder of the chapter, you’ll focus on building out a small demo application called Spring Soccer Club.
Every spring and fall, hundreds of kids look forward to the start of a new youth soccer season.
Their parents, on the other hand, dread the existing paper-based sign-up process.
You’re going to help them by defining an online registration process where they can create an account and register their kids for the upcoming season.
Figure 5.3 Before users are allowed to register, you first ask if they have registered in the past.
If they haven’t, users go directly to the registration page.
If they have registered previously, the user is able to search for this registration.
If they still are unable to find their existing registration, users can register again.
Figure 5.2 If you don’t explicitly identify which scope a variable is in, SWF starts looking in the request scope and continues looking in the flash, view, flow, and conversation scopes until the variable is found.
In the diagram, the boxes represent states in SWF and the arrows represent transitions between those states.
Throughout the rest of this chapter, we use the word state and step interchangeably.
In the next several sections, you’ll take an iterative approach to building this demo application, resulting in a working example of how all of SWF’s main features work.
After getting SWF installed and working, your first step will be to define views for each step in the flow.
Then you’ll add dynamic transition logic with action states and decision-state constructs.
Finally, you’ll see how to bind data to forms and perform data validation.
After working through the next several sections, you should have the knowledge necessary to start experimenting with SWF on your own.
To keep the focus on SWF, we’ll minimize extra coding by stubbing out the service code.
In chapter 3, you learned how to set up a basic Spring MVC application.
This recipe builds on this knowledge and shows how to extend and validate this configuration to support SWF.
You would like to install SWF in a new application or extend an existing Spring MVC application.
As of this writing, the latest version of SWF is version 2.3.0
If you’re using Maven, there are two different dependencies you can define in your project object model (POM) depending on whether you’ll be using JSF.
As our focus is on extending an existing Spring MVC application, you’ll use the following for the demo application:
If you were also using JSF, you could use the following Maven dependency instead.
For clarity, the spring-faces artifact includes everything in the Maven dependency plus additional JARs to support JSF development.
For those not using Maven, you can download the SWF JAR files from the project’s site (www.springsource.org/webflow)
Once you’ve unzipped the distribution, you’ll find the following JAR files in the dist directory:
In addition to the Spring 3.x JAR files, you need the web flow and binding JARs for the demo application.
The faces JAR file is only required if you’ll be using JSF.
As of the 2.1 release of SWF, no additional dependencies are required.
In the 2.0 release, the unified EL was the default expression language and users were given the choice to include either a unified EL implementation or Object Graph Navigation Language (OGNL)
As of the 2.1 release, SpEL is the default and preferred expression language going forward.
Although unified EL and OGNL are both supported, it’s strongly recommended that users use or migrate to using SpEL as their expression language of choice.
The next few steps are designed to quickly set up and validate the Spring MVC setup.
Once you validate that Spring MVC is set up, you’ll quickly extend that setup and get SWF working.
Start by adding the following servlet and mapping to your web.xml file:
Starting in Spring 3.04, you can map DispatcherServlet to /
Next, create a file named dispatcherServlet-context.xml in the WEB-INF/spring/web directory with the following contents.
At B, you enabled the @Controller programming model that enables Spring MVC’s annotation-driven features.
Here you are prefixing your logical view names with /WEB-INF/jsp/ and adding a .jsp suffix.
For more information on this, take another look at section 3.5.4
Our annotated controller test class is in the com.springinpractice.ch05 .mvc package.
You’ll test to make sure the Spring MVC is working before moving on.
The controller is straightforward and returns the helloWorld view when called.
Start up your web server, and point your browser at the following address (adjusting for host name and port number): http://localhost:8080/sip/helloWorldControllerTest.
Now that you’ve validated that your Spring MVC configuration is working, let’s extend this configuration to include SWF.
To make things easier to keep track of, you’re going to put the SWF configuration in a new file.
The next listing contains the contents of the webflowContext.xml file.
We’ll discuss the contents of this file in detail in the next several sections.
You use the <flow:flow-builder-services> element at B to customize how the flows are built in the flow registry.
When defining this element, you only need to reference the settings that you want to change from the default.
At C, you configure the flow executor using the <flow:flow-executor> element.
The flow executor, as the name suggests, drives the execution of flow definitions.
Here, you just need to make it aware of the flow registry.
In SWF, each flow is configured in its own XML files.
At D, you make the flow registry aware of your helloWorld-flow.xml.
The Spring Soccer Club demo application file using the <flow:flow-location> element.
Notice that the optional base-path attribute is used to simplify the mapping.
Flow IDs are used to explicitly reference a particular flow.
Without this optional attribute, the flow ID would have been determined to be test.
A more flexible configuration would be to configure the flow registry like this:
Using the mapping, any directory after the base path, represented by the two asterisks, would represent the flow ID.
If you were to use this configuration in the example in listing 5.7, the flow ID would be determined to be test.
All that’s left to do is make Spring MVC aware of SWF.
The FlowHandlerAdapter created at E in listing 5.7 joins the Spring MVC DispatcherServlet and the flow executor.
As you’ll recall from chapter 3, DispatcherServlet determines how best to dispatch a given request by looking at several handler mappings.
The FlowHandlerMapping created at F makes the FlowHandlerAdapter and hence the DispatcherServlet aware of all the flow IDs available in your registry.
To see this in action and to verify your setup, start your web server and point your browser at the following address (adjusting for host name and port number): http://localhost:8080/sip/testWebFlow.
Now that you have SWF up and running, your next step is to create views for each state in the demo application.
Now that you’ve installed SWF and validated that it’s working properly, your next.
You would like to use SWF to manage a complex page flow representing a single unit of work.
Continuing with the Spring Soccer Club theme, your next step is to implement the different flows and states shown in figure 5.3
For a brief discussion of this demo application, refer back to section 5.3
Because searching for an object is normally a reusable flow, you’ll split the overall flow into a registration main flow and a findExistingPlayer subflow.
The following listing contains the contents for the findExistingPlayer-flow.xml file.
We’ll discuss the contents of this file in detail in the next several sections.
Figure 5.6 Being able to search for and select an existing player may be useful in several places in a larger application.
As a result, you’ll separate the findExistingPlayer functionality into its own subflow.
The previous listing contains many of the items we’ve talked about up to this point.
Because this listing is large, let’s start by focusing on the first several lines.
At B, you explicitly set the starting state for the flow.
Recalling our discussion from section 5.2.2, if you don’t explicitly specify a view name, the view name will be the same as the id attribute by convention.
At D you use an Action class to manage your form object.
The action state at E processes the search criteria submitted from the view state at C.
Here you pass the criteria object to your search service.
Because you’re focusing on SWF, this stub implementation always returns a single result.
On the search results view, the end user has an option to perform another search.
Clicking the New Search link fires off a newSearch event that currently transitions to newSearchEndState at H.
Looking at a snippet of code from the parent flow, you can see that newSearchEndState is mapped back to the findExistingPlayer subflow state:
What is happening here is that you’re returning a newSearchEndState event from the subflow and mapping it back to another call to the findExistingPlayer subflow state.
Your original subflow and its state are destroyed, and you repeat the subflow using a new instance.
This approach is useful when you have a complex subflow that has several steps because it ensures that all of the intermediary state gets cleared out.
All you have to do is change G to transition to the newSearch action state.
Because you’re clearing out the state manually, this approach works best when there isn’t a lot of intermediary state being captured.
I demonstrates how a subflow can return information back to the parent flow.
Figure 5.7 shows an unstyled snapshot of what the search results page looks like.
Figure 5.7 Showing the result of the displayFindExistingPlayerResult view state.
If an existing account is found, the user can choose to log in with the account listed.
A link is used at B to fire the existingAccountFound event, which is later transitioned to existingAccountFoundEndState where the user name is returned to the parent flow.
Has anyone in your family registered with us in the past?  If so, enter your information below:
At B, you use the Spring Form tag library to generate your form tags.
At C, you specify the name of the event you want to fire when the form is submitted.
For more information, see the view-state discussion in section 5.2.2
At this point, you should be able to test the flow by pointing your browser at the following address (adjusting for host name and port number): http://localhost:8080/sip/findExistingPlayer.
Keep in mind as you test the flow that because this subflow isn’t getting called from a parent flow yet, when you transition to an end state, the flow ends and all the state for the flow is destroyed.
As a result, you’ll end up on the same URL that started the flow, causing a new instance of the flow to be created.
Watch the value of the execution variable in the browser.
You’ll find that the execution represented by e is incremented every time the flow restarts.
For more information about this variable, flip back to the view-state discussion in section 5.2.2
With the findExistingPlayer subflow working, let’s configure the other states that are left in the example application.
The following listing details the flow definition for the registration flow.
At B, you explicitly define your start state as findExistingPlayer, which calls the findExistingPlayer subflow as its first step.
At C, you see how to retrieve information from a subflow.
The attributes are returned in an AttributeMap, which is essentially an immutable interface that provides the read-only operations you would normally find on a map.
At D, you call a stub that pretends to create an account for the user.
The service returns the username of the newly created account.
We won’t cover the last two views, newAccountForm and confirmNewAccount.
You can test the flow by pointing your browser at the following address (adjusting for host name and port number): http://localhost:8080/sip/registration.
When creating a real system, you would more than likely be coordinating with multiple services and would want more control over error handling and the resulting views that are returned.
To do that, you need to use the <evaluate> element and put your logic into an Action class.
Although it’s certainly easier to call POJOs directly, there are times when you would like to do more.
Perhaps you want to read or set several variables in different flow scopes or invoke complex business logic that might be supported by multiple services.
Those who have experience with Spring MVC may immediately think about controllers.
In Spring MVC, controllers provide a layer of indirection between the service layer and the view.
Controllers typically delegate to application or business services to retrieve necessary data or invoke business operations.
After completing its work, the controller selects a view to be rendered.
In SWF, the flow is considered to be the controller.
Action classes provide a mechanism for you to provide a similar layer of indirection between the service layer and.
Instead of selecting a view to be rendered directly, when an action is executed, it returns an Event providing an outcome that the flow can respond to.
You can create an action class by implementing the org.springframework.webflow.execution.Action interface.
We’ll focus our attention on the three implementations of the Action interface shown in figure 5.8
An action is essentially a command that performs some work and returns an event.
The RequestContext, available as the flowRequestContext EL variable, gives information about the current flow execution and provides access to each of the variable scopes.
Revisiting the Spring Soccer demo, you can create a simple action class that encapsulates the call to the playerService.
The class in the following listing would be configured like any other Spring-managed bean, allowing you to inject any necessary resources.
Figure 5.8 Although several classes implement the Action interface out of the box, we’ll focus on AbstractAction, MultiAction, and FormAction.
With the FindExistingPlayerAction class in hand, the following line in the existing demo application.
When implementing the Action interface, it’s no longer necessary to explicitly pass in the flowRequestContext EL variable:
Although you can implement the Action interface directly, it’s more convenient to take advantage of the convenience methods available in the AbstractAction class.
Specifically, this class provides a default implementation of the InitializingBean interface, which provides a hook for you to do custom initialization or validation after your bean’s properties have been set by the container.
In addition, the AbstractAction class implements the Action interface’s postprocessing.
The class provides several factory methods that create common Events, such as success and error.
If you prefer to group all of your actions together, the MultiAction class builds on the functionality provided by the AbstractAction class and allows you to bundle two or more execution methods in the same class.
As shown next, the signature of each The only difference is that the name can be anything you like:
An example of a MultiAction class can be seen in the following listing.
Here you reimplement the setPlayerService from listing 5.12 in a MultiAction class.
By default, the ID of the wrapping action state is treated as the method to execute.
This code snippet would be functionally equivalent to the more explicit action state identified:
The FormAction class helps simplify working with forms by providing several convenience methods that make it easy to set up, validate, and bind form data.
This method returns a success event unless binding errors occur.
If a validation error occurs, an error event is returned.
There should be no reason to extend the FormAction class directly.
Instead, you can configure the form action as a Spring bean, as show in the following listing.
Three examples of using this bean were provided earlier, in listing 5.8, starting at D.
Look at the comments in the findExistingPlayer-flow.xml file for hints on how to use each.
You can test the flow by pointing your browser at (adjusting for host name and port number) http://localhost:8080/sip/findExistingPlayer.
Up to this point, we’ve talked about view states, but we haven’t had a specific conversation about form data binding.
In this recipe, you’ll use the information you learned in section 4.1 to do data binding in SWF.
When a user submits a form, you would like to control how data is bound to your Java objects.
In a view state, you use the model attribute to specify which object the view should bind to.
The newAccountForm view state is bound to the accountForm object.
Now that this association has been made, SWF will attempt to bind form data to this object by default.
At times, it may be desirable to skip the data-binding process.
By default, SWF will attempt to bind data to every field in the target object.
As we talked about in section 4.1, sometimes that behavior isn’t desirable.
To change the default behavior, you can create a whitelist by identifying each of the attributes you would like to bind using the <binder> element in the view state.
You’re telling SWF that the firstName and lastName attributes of the guardian object are the only fields on the guardian object that should be bound.
In addition, you’re using the optional required attribute, telling SWF that each of these properties is required.
In the next section, we’ll discuss how to take this one step further.
You’ll use the JSR-303 Bean Validation API to declaratively add additional validation constraints to your POJO.
Users interact with websites by clicking links and submitting forms.
All but the simplest systems validate their form data to ensure that information is captured in the format that is expected and that any additional rules, such as required fields, are enforced.
When a user submits a form, you would like to validate the data that was submitted before doing additional processing.
If validation errors occur, you would like to let the user know in a user-friendly way so they may make any necessary corrections before continuing.
JSR-303 defines a standardized metadata model and API for Java bean validation that allows you to define validation constraints without tying you to a specific application tier or programming model.
If you’ve already configured JSR-303 support as part of your Spring MVC setup, you can skip the next section.
As you’ve come to expect, Spring has made bootstrapping a JSR-303 implementation easy.
All that is necessary is to create a single bean definition and have a JSR-303 provider on the classpath.
Validate that you have the following dependency in your POM file:
This Maven dependency adds the hibernate-validator-4.1.0.Beta1.jar file to your classpath.
Once this is done, you can add the following bean definition to your Spring configuration.
Many applications use JSR-303 validation in addition to Spring validation.
The added flexibility allows you to use the same bean definition for both.
Now that your JSR-303 validator is configured, you need to make SWF aware of it.
To do this, add a reference to your new validator to the flow-builder-services element in the webflowContext.xml file:
You’ve established the foundation; let’s look next at adding JSR-303 annotations to a class.
The @Valid annotation at B tells the validator to validate the Guardian child class and aggregate any error messages with those from the AccountForm class as well.
You see similar behavior at C with the Player class.
Instead, it’s a part of the Hibernate Validator framework that is the reference implementation for JSR 303
For more information on the Hibernate Validator, see the documentation at http:// hibernate.org/subprojects/validator.
Now let’s talk about how you can customize any resulting error messages.
When one or more validation rules fail, the validator generates a set of keys that Spring resolves to localized messages.
Looking at example messages from the demo application’s messages.properties file, you’ll find that the naming convention is intuitive:
In this example, the @NotEmpty annotation generates .NotEmpty, which is appended to the end of the full EL-like path of the property.
To create a flow-specific message bundle, add a default message.properties file or a localized version of this file in the same directory as the flow-definition file.
When a model is specified in a view state, validation follows the binding process.
When transitioning to next, information is bound to accountForm and validated.
If an error occurs, the user is returned to the newAccountForm view where the errors are displayed.
Because displaying form errors was covered in section 4.2, we won’t discuss validation process entirely.
You can test your handiwork by pointing your browser at http://localhost:8080/ sip/registration (adjusting for host name and port number) and then navigating to the New Account Creation form.
Earlier in this chapter, we talked about how you can reuse individual flows by calling them as subflows.
We also talked about defining common transitions globally in a flow so they can be reused in a given flow.
You want to reuse common states or transitions in several flows.
A flow definition can contain a lot of configuration information that might be useful for other flows.
Similar to Spring’s bean-definition inheritance, SWF has a built-in mechanism for inheritance at both the flow and state levels.
Flow inheritance is similar to bean-definition inheritance but with a couple of key differences.
Like parent bean definitions, elements defined in a parent flow are exposed and available to a child flow.
As shown at B, a flow definition can be marked as being abstract.
This prevents SWF from trying to use the flow directly.
The next listing defines a simple flow that inherits the elements in your abstract flow definition.
Unlike a bean definition, a flow can inherit from more than one flow.
At B, you use the parent attribute of the flow element to indicate that the flow will inherit from the common flow.
A comma-delimited list can be used to specify multiple flows to inherit from.
When inheriting view states, it’s important to understand how flow inheritance works.
If you’ve ever used static includes in a JSP file, you’ll remember that the source of the included JSP file is copied into the main JSP file at compile time, creating a single composite file.
This makes all relative links to resources (images or other documents) specified in that included JSP file relative to the resulting composite parent file, regardless of where the included JSP file originally was located.
Recall that a view state doesn’t specify a view attribute; by convention the view attribute is assumed to be the value of the id attribute plus the defaultViewSuffix, which is .jsp by default.
Using this convention, the view state at C would have a view state of start.jsp.
The default implementation of the ViewResolver would try to resolve this view in the same directory as the currently executing flow.
Like statically included JSP files, this is always relative to the currently executing flow, not the inherited flow.
For this reason, it’s best to specify absolute paths to views in inherited view states.
The transition at C transitions to a view state that appears in your parent flow (see listing 5.16)
At E, you call an action that does nothing more than throw a DemoRuntimeException.
This action is used to demonstrate how the transition at E is inherited from the common flow.
State inheritance behaves more like bean-definition inheritance, where a given state can only inherit from one parent of the same state type.
In the example, you’re extending the commonView state of the common flow, which is identified as the flow’s parent.
Although it certainly isn’t pretty, you can click through this example by pointing your browser at the following address (adjusting for host name and port number): http://localhost:8080/sip/inheritanceDemo.
Through flow and state inheritance, you can obtain a much higher level of reuse than using subflows and global transitions alone.
For details on the algorithm used to merge each and every element in the SWF definition language, consult the web-flow documentation located at www.springsource.org/spring-web-flow#documentation.
Now that you have a mechanism to authenticate users, you would like to secure specific web flows so that they can only be accessed by authorized users.
Once you’ve configured Spring Security for your application, authorizing individual flows is pretty straightforward.
First we’ll cover the three basic steps required to configure Spring Security.
Then we’ll look at how easy it is to secure individual flows, states, and transitions.
Just as you’ll do in recipe 6.1, you need to configure the DelegatingFilterProxy that will be used to load the Spring Security filter chain.
The following listing focuses on the configuration relevant to Spring Security.
You load a file containing your Spring Security configuration at B.
At C, you configure the DelegatingFilterProxy, which will be used by Spring Security.
At D, you configure the filter to be applied to all URLs.
Starting at B, you enable Spring Security’s auto-config, which sets up Spring Security to use several defaults.
At D, you protect a URL-based resource by creating an intercept-url definition.
Remember that flows, specifically parent flows, are also URL-based resources.
Although you could protect them here, we’ll talk about a better solution to specifically secure a flow in a moment.
Starting at E, you create a simple in-memory UserDetailService with two accounts for testing.
Now that you have Spring Security up and running, let’s start plugging it into SWF.
You do this by updating the webflowContext.xml file we discussed earlier with a new flowexecution-listener.
You add the SecurityFlowExecutionListener defined at C to your collection of listeners defined at B.
To secure an entire web flow or a specific state in a flow, you use the <secured> element; we’ll talk more about this next.
With SWF configured to use Spring Security, you can use the <secured> element to secure a flow, transition, or state.
Here you see that the attributes attribute can take a commaseparated list of roles.
You can click through this example by pointing your browser at the following address (adjusting for host name and port number): http://localhost:8080/sip/ securityDemo.
When you’re asked for an email (username) and password, you can use the values we generated and hardcoded (see www.fakenamegenerator.com) in the applicationContext-security.xml file:
As you’ve seen, with an existing Spring Security configuration in place, adding a SecurityFlowExecutionListener to SWF’s list of flow-execution-listeners is all that is required to make Spring Security aware of SWF.
Securing entire flows, individual states, and transitions using the <secured> element is straightforward.
Don’t worry about the details of listing 5.19 just yet; we’ll take a much deeper dive into Spring Security in the next few chapters.
SWF’s narrow focus on addressing the problem of navigation in the web tier makes SWF intuitive to use and easy to learn.
In this chapter, we introduced you to SWF and familiarized you with its features and functionality.
You started by learning about defining flows, states, how to use transitions, and managing flow data.
Next you learned how to extend your existing Spring MVC application by installing and configuring SWF.
Then we looked at using action classes, how to bind and validate form data, flow inheritance, and, finally, securing web flows.
Our goal was to show how you can use this framework to complement your existing Spring MVC application and simplify the work required to design, maintain, and understand complex page flows in an application.
We touched briefly on using Spring Security to authorize individual states and flows in this chapter.
Chapter 6 takes you much deeper into using Spring Security for authorizing user requests.
In this chapter, you’ll learn how to support this common requirement using Spring Security 3
The first three recipes look at approaches to implementing a login form.
The five remaining recipes look at sourcing user data from a persistent store.
Many applications need a way to allow users to authenticate—that is, to say who they.
Spring Security 3, although a large framework, makes it easy to get started with basic authentication.
This recipe shows what you can do with a fairly minimal configuration.
You’ll use Spring Security 3 to add logins and logouts to a simple web app.
You’ll do this entirely through configuration; that is, you don’t need to write any Java code to make it work.
The app is a simple university portal with nothing more than a home page and a login page (figure 6.1)
To implement it, you’ll need to configure Spring Security, configure web.xml, and add login and logout links to the app.
There’s also a beans-web.xml configuration, but we won’t address that here because it doesn’t contain anything specific to this recipe.
You’ll do the configuration itself first, and after that we’ll dive into some of the behind-the-scenes details.
The following listing shows a simple Spring Security 3 configuration that enables webbased authentication and creates an authentication source.
Figure 6.1 The default login page, complete with Submit Query (or Submit, depending on the browser) button.
The first thing to notice is that beans-security.xml isolates your Spring Security configuration into its own configuration file.
Because you’ve done this, it makes a lot of sense to declare the Spring Security namespace as the default namespace, which you do at B.
That way you don’t have to keep specifying a namespace prefix with each element.
At C you enable web security using the <http> element.
Although this element is responsible for web-based security generally (authentication, authorization, HTTPS, and so on), the focus in this chapter is authentication.
You override the auto-config defaults by placing the desired configuration inside the <http> configuration.
The logout configuration at F is similar to what you did with <form-login>, but this time you’re specifying a target URL for successful logouts.
But you still need an authentication source, and that’s what <authentication-manager> helps you.
When an unauthenticated user tries to access a protected resource, the login.
This element allows you to identify a list of authentication providers that the manager will consult during authentication; authentication succeeds as long as at least one provider successfully authenticates the user.
Note that the specific interfaces and classes are hidden by the namespace configuration, and that’s the point of using namespace configuration in the first place.)
We stated that an authentication manager manages a list of providers.
More exactly, although it’s hard to imagine what else they would reasonably do.
At any rate, from the AuthenticationManager’s point of view, providers are an implementation detail.
You’re telling Spring Security to create a ProviderManager instance when you include the <authentication-manager> element.
ProviderManager maintains a list of AuthenticationProviders corresponding to the authentication sources you want to include.
You happen to be using the DaoAuthenticationProvider, but that’s certainly not the only one there is—not by a long shot.
As figure 6.3 shows, Spring Security provides rich support for authentication.
When you use a DaoAuthenticationProvider, you have to specify a DAO: that is, a UserDetailsService implementation.
DaoAuthenticationProvider is an adapter that allows you to use any UserDetailsService implementation as an AuthenticationProvider.
UserDetailsManager extends UserDetailsService to add write operations as well, although AuthenticationManager doesn’t use that.
Figure 6.3 The AuthenticationProvider hierarchy, which includes tons of provider options.
Figure 6.4 UserDetailsService hierarchy, containing DAOs used by the DaoAuthenticationProvider.
Now you’re in a better position to understand what’s happening with listing 6.1
The net result is an in-memory authentication provider, which is fine for development purposes.
For the sake of completeness, <authentication-manager> also creates a DefaultAuthenticationEventPublisher at E.
The ProviderManager uses this to publish authentication events (successes and failures) so that listeners can respond as needed (for example, redirecting the user to the correct target URL)
Spring Security uses a special servlet filter to secure web resources.
We’ll examine this in more detail momentarily, but first let’s look at the following listing.
At B you reference the beans-security.xml security configuration from listing 6.1
At C you enable Spring Security web security by defining a DelegatingFilterProxy filter, which is part of the core Spring distribution rather than being part of Spring Security itself.3 DelegatingFilterProxy is essentially a trick for injecting servlet filters.
You can point it at any filter on the application context you like by giving the DelegatingFilterProxy a name that matches the target filter’s bean ID.
The target filter, being a bean, is injectable like any other bean.
Although it would certainly be possible to define one DelegatingFilterProxy for each filter you want to use, that would be a hassle.
Instead you define a single DelegatingFilterProxy filter on the web.xml side and a single FilterChainProxy filter on the beans-security.xml side.
Then you create the filters and filter chains you want to use entirely within the Spring Security configuration rather than in web.xml.
See figure 6.6 for a visual overview of what we just described.
Figure 6.7 shows the same thing as a sequence diagram.
Finally, you indicate that you want all requests to pass through the filter D.
Now let’s make sure your JSPs are equipped to display the login and logout links appropriately.
DelegatingFilterProxy was inspired by the FilterToBeanProxy class, which originated with Acegi Security—the precursor to Spring Security.
This is a partial view; we’ve suppressed the full filter chain.
Quick tip: the filter mapping order matters Note that you place the Spring Security filter mapping before the Sitemesh filter mapping for a reason: you want the request to pass through the Spring Security filter first so authentication information will be available on the request when the Sitemesh filter kicks in.
This allows you to display, for example, the user’s name as part of the template.
So far your only special JSP is subhead.jspf.4 The next listing shows a simplified (CSS suppressed) version of subhead.jspf.
You begin by declaring the Spring Security tag library B, because you’re going to use it both to present/suppress links according to role and to display user information.
Next you create a couple of variables to store the default login C and logout D URLs.
The login URL here returns a login form, not the form submission URL.
At F you display a personalized welcome message and the logout link to authenticated users, once again using <security:authorize> and SpEL to perform the test.
The personalized welcome message uses the <security:authentication> tag G, which exposes the user’s authentication information to the JSP.
The property attribute refers to a property on an underlying org.springframework.security .core.Authentication object; see the Javadoc for that class for more information on what’s available.5 By default, the principal is an org.springframework.security .core.userdetails.User, and its properties are available for use as well.
You’ll see how to use a custom principal object in recipe 6.5
The main benefit of the default login form is that it’s easy to set up.
But it’s merely serviceable; it’s not necessarily what you’d want to use for a more polished app:
The login button uses the awkward language Submit Query (or Submit on some browsers, which is somewhat better)
The form includes a reset button, which most users would consider superfluous.
You may want a different page layout, form layout, or styling.
In the following recipe you’ll learn how to customize the login form.
In recipe 6.1, you learned the mechanics of using Spring Security to set up form-based logins, but in most real-life applications you’ll need (or at least want) to modify the form’s appearance.
This recipe shows how to replace the purely functional default login form with one that addresses not only the functional requirements but also those that are more visual or interactive in nature.
As in recipe 6.1, you don’t have to write any Java language code to pull this off.
It isn’t hugely different in appearance than the default Figure 6.8 A custom login page.
Notice that there’s a nav bar at the top of the login form (although all we’ve put there is a Home link)
That nav bar wasn’t part of the default login form from figure 6.1
We’ve also changed the old Submit Query button to a new Log in button, and we’ve gotten rid of the unnecessary Reset button.
The following listing shows how to implement a custom form.
For clarity, we’ve suppressed most of the actual layout and CSS because it’s the form itself that matters.
You use <c:url> to store the form-submission URL at B.
This allows you to avoid hard-coding the context path into the URL, because <c:url> provides it automatically.
The specific URL you’re using is the Spring Security default for login form submissions.
If there’s a login failure, you need a way to say so.
The form at D uses the form-submission URL you created as its action.
You also use specific parameter names for the username, password, and remember-me check box.
But you still need to make it reachable by updating beans-web.xml.
The beans-web.xml configuration (see the code download) already declares the mvc namespace, so all you need to do is add the following view controller:
Listing 6.4 Custom login form, login.jsp, whose appearance you control.
This maps requests for /login.html to the logical view name login, thanks to the DispatcherServlet’s DefaultRequestToViewNameTranslator.
You can also pick a view name explicitly by using the view-name attribute on the InternalResourceViewResolver carries the view name to an actual JSP.
Now that you have a login page, the next step is to tell Spring Security about it.
You need to tell Spring Security where to find your new login page, and you need to ensure that it uses the error-message capability you created in the JSP.
The login-page attribute tells Spring Security where the custom login page is.
It needs this so it can redirect unauthenticated users to the login page when they attempt to access a protected resource.
You don’t currently have any protected pages, so you can’t yet see this in action, but you’ll return to this in chapter 7
The authentication-failure-url attribute, as you might guess, tells Spring Security where to direct the user if the login attempt fails.
You send the user right back to you tell the JSP to display the error message, as you saw in listing 6.4
Now that you’ve handled configuration, let’s update the navigation JSP fragment.
All you have to do in subhead.jspf is replace the /spring_security_login path with /login.html.
This recipe showed how to improve upon the default login page that Spring Security provides by creating a custom login page.
In the next recipe, you’ll consider a third way to handle login forms.
This time, instead of using a login link that points to a separate login page, you’ll build the login form right into the page navigation.
Figure 6.9 From request path to view with a view controller.
Here we’ll consider a third way: the always-resident login form.
It allows the user to log in with one less click.
Display a login form that appears as part of the page template (and thus on every page) until the user logs in.
Normally an unauthenticated user either clicks a login link or attempts to access a protected resource, after which Spring Security redirects them to a login page.
Here you don’t have that; every page has a login form, and there isn’t any login page.
Fortunately, Spring Security is sufficiently flexible that you can pull it off.
The main thing you care about is having somewhere to post the form data, no matter where the form lives.
The next listing updates the subhead.jspf file from listing 6.3
As before, we’ve simplified the layout and CSS for clarity’s sake; see the code download for the full version.
Listing 6.5 subhead.jspf updated to include an always-resident login form.
Figure 6.10 A login form that displays on every page until the user logs in.
You specify the login submission URL at B and use it to create a login form at C.
This is a special case of the custom login form from recipe 6.2
The first is a login-required page that you present to unauthenticated users when they attempt to access protected resources.
The second is a login-failed page to display when (yup) a login attempt failed.
Normally, when an unauthenticated user attempts to access a protected page, you send the user to a login page, perhaps with some verbiage to the effect that they need to log in.
Here you don’t have a dedicated login page, so you need to do something different.
The subhead.jspf include has a built-in login form, so the user can log in from this page.
Once again you’ll create a simple page to do that, this time called loginFailed.jsp:
Implementing an always-resident login form technical support for further assistance.
Let’s add them to beans-web.xml because you’ll need to reference them from beans-security.xml.
Here all you need do is add a couple of view controllers to beans-web.xml, like so:
Notice that this time around you’re using the view-name attribute to specify view names explicitly, because, for example, /login-required.html wouldn’t map to /WEB-INF/jsp/ loginRequired.jsp under the implicit mapping.
You could have named the JSPs loginrequired.jsp and login-failed.jsp, but you happened not to do that.
With the JSPs and MVC configuration complete, let’s update the Spring Security configuration.
All you need are a couple of tweaks to your existing beans-security.xml configuration.
Once again it’s the <form-login> element you need to change:
This time login-page points to the login-required page and authentication-failure points to the login-failed page.
You can give it a test drive using the same URL as before.
Bear in mind that you don’t yet have any way to exercise the login-required page, because the sample app doesn’t include any access controls.
The always-resident login form described in this recipe is an alternative to the more typical login link.
It works well if you have enough screen real estate to present it and if you want to avoid an unnecessary click.
The Spring Community Forums (http:// forum.springsource.org/), for example, use this login style.
Always-resident login forms are good for highlighting the fact that a given website supports user registrations and logins.
They’re often located near links for registration and resetting forgotten passwords.
Once the user logs in, it’s common to replace the login form with account settings information or links as well as a logout link.
This helps to establish the piece of screen real estate as containing account/session information and options.
Specifically you’re ready to connect the login form to a backing database.
In real applications, you want to source authentication data from a persistent store, and a database is a common choice.
In this recipe you’ll see how to replace your inmemory user service with one that’s backed by a database.
We covered both the DaoAuthenticationProvider class and UserDetailsService interfaces in recipe 6.1, so we won’t rehash that here.
Instead we’ll jump right into the changes you need to make.
The most straightforward approach to using a database back end is to replace the InMemoryDaoImpl user service with a JdbcDaoImpl user service.
To do this, you’ll need to perform the following steps:
It’s fine to grant all permissions to that user on the sip06 database.)
Expose the database through JNDI in your servlet container environment.
When all is said and done, you’re targeting the bean graph shown in figure 6.11
You’ll see more details as you work through the recipe.
Although it’s possible to use a custom database schema, in this recipe you’ll use the default Spring Security user schema.
The default user schema has only two tables: users and authorities, for user credentials and roles, respectively.
Figure 6.12 shows the entity-relationship diagram (ERD) for the user schema.
Here’s the MySQL DDL for the schema in figure 6.12:
Figure 6.11 Bean dependency graph for a JDBC-backed authentication manager.
You can find the schema and data SQL scripts in the sample code in the src/main/sql folder.
The next step is to use JNDI to expose the database in the servlet container environment.
The specifics of this step are strongly dependent on the servlet container you’re using.
If you haven’t already done so, consult your container documentation for instructions on how to do this.
For the sake of this exercise, assume that you’ve bound the DataSource to the jdbc/Sip06DS JNDI name.
Next you’ll make the DataSource available to Spring so you can use it as an authentication source.
For this step, you’ll create a new application context configuration file.
Although it would be possible to place the DataSource lookup directly in beans-security.xml, in general you’d expect to use the DataSource for general persistence needs and not merely security needs.
You also need to update web.xml to point to the new Spring configuration.
Next you need to update the Spring Security configuration to use it to source authentication data.
By now you may be noticing the common theme that you can make fairly major changes with minimal effort.
Swapping a JDBC-backed user service for the in-memory user service is another case in point.
Table 6.1 shows the default SQL queries that JdbcDaoImpl uses for looking up users and roles.
Now you have a database back end for your authentication source.
In this recipe, you saw how to source authentication data from a database.
Although there was a little setup to do with respect to the database and DataSource configuration, presumably most of your apps have to do that anyway.
The default user schema that JdbcDaoImpl expects may or may not serve your needs in any given case.
It’s simple and minimalistic, so it won’t be long before you’re looking for ways to customize, expand, or replace it.
Table 6.1 Default SQL queries that JdbcDaoImpl uses to retrieve user data.
In most instances, you’ll want to use something a little more beefy than the default JdbcDaoImpl user schema.
You can handle this by configuring <jdbc-user-service> to use custom queries, as follows:
This configuration uses the same JdbcDaoImpl user service you’ve been using, but you replace the two lookup queries with custom queries that reflect the underlying schema.
Be sure to run the SQL scripts in the src/main/sql directory of the sample code before starting up the app.
Then point the browser at http://localhost:8080/sip/ home.html, and you should be running successfully against the new database schema.
Being able to customize the database schema is certainly a useful thing to do, but sometimes the customizations you desire are more involved.
Recall from the discussion following listing 6.3 that the default principal object for Spring Security user services is an org.springframework.security.core.userdetails.User.
Even when you customize the database schema as just described, you’re still stuck with the default User object, which is once again minimalistic.
Ideally you’d like to be able to query the principal object for the user’s first and last names, email, address, and so forth.
The next recipe shows how to overcome this limitation by sourcing your user data from the account service you implemented in recipe 4.4
In general, user representations include important information beyond credentials and account flags.
This might include demographic data (first name, last name, email address), preferences, and more.
Because Spring Security makes the authenticated user principal available in the security context, it would be nice to use a more full-featured user instead of the default org.springframework.security.core.userdetails.User.
In this recipe, you’ll learn how to do that with a custom user service.
Enhance the user principal with first name, last name, email address, and so forth.
For this recipe, you’ll connect the account service you created in recipe 4.4 with Spring Security.
You did most of the heavy lifting in that recipe, but there’s still a bit more work to do.
Adapt the Account domain object from recipe 4.4 to the Spring Security UserDetails interface.
Create a DAO to get the password for the new UserDetails object, because neither Account nor AccountDao exposes the password.
Update subhead.jspf to take advantage of the new UserDetails object.
You’ll show the user’s full name instead of only their username.
Figure 6.14 is a class diagram that shows several of the key pieces for this recipe.
To perform authentication, Spring Security needs your user principal to implement the UserDetails interface.
Although you could add implements UserDetails to the Account object from recipe 4.4, this would be invasive.
Instead you’ll create an adapter to make the Account object conform to the UserDetails interface, as shown in the following listing.
In this recipe, you’ll implement the adapters that allow you to use the account service from recipe 4.4 as a UserDetailsService.
The listing is an adapter to make Account act like a UserDetails.
Obviously that involves implementing the UserDetails interface B and accepting a backing Account C.
You expose special Account properties like firstName D, lastName, fullName, and email because you want these to be available to the app for rendering or other purposes.
You must also implement the methods that UserDetails expects, such as username, password, various flag properties, and authorities.
The password property is different because Account doesn’t include a password property (recall that you left it out for security reasons), so you implement it here and include a setter F because the database has a password column.
The flag properties other than enabled, on the other hand, have neither corresponding Account properties nor corresponding database columns, so you return true for all of them G.
Finally, you support the authorities property by mapping roles to authorities H.
You now have a custom user principal class, but you still need a way to get the password out of the database.
This DAO exists solely for the purpose of obtaining user passwords from the database.
Although you could go back and modify the account service, that would again be invasive.
We’ll continue to show how to adapt the account service without changing it.
The next listing is a JDBC-based implementation of the UserDetailsDao interface.
This DAO uses JDBC instead of Hibernate because you’re not doing any ORM.
You inject the JdbcTemplate at B and define a simple query at C.
You query for the password at D by specifying the SQL, the username, and the String class.
With that, you have a user principal and a means to populate all of its properties.
But what you don’t have is an implementation of the UserDetailsService to inject into the DaoAuthenticationProvider.
In the same way that you adapted the Account class to the UserDetails interface, you need to adapt the AccountService interface to the UserDetailsService interface.
You give it a name B so you can reference it from the Spring Security <authentication-provider> configuration.
As an adapter, it implements UserDetailsService C and accepts a backing AccountService D.
It also accepts a UserDetailsDao E so you have a way to look up the user’s password.
Then you create the UserDetailsAdapter I, inject the password J, and return the user principal.
The Java part of your effort is now complete, so let’s look at the configuration changes you need to make.
Now that you’ve added the custom user service from recipe 4.4, you need to update the configuration to support it.
There’s also some other configuration to handle, such as the configuration for the JdbcTemplate.
Here’s what you need to do for the data tier.
Listing 6.9 beans-data.xml, updated to support the custom user service.
But now you add a JdbcTemplate C to help with the password lookup.
You also add Hibernate D and scan for DAOs E because your user service will need those.
In addition to configuring your data tier, you must configure your service tier.
The previous listing is of course entirely standard, and there’s not much to say about it.
The third Spring configuration file you need to update is beans-security.xml.
Replace the old <authentication-provider> definition with a new one, as shown:
Figure 6.15 shows the bean dependency graph for the DaoAuthenticationProvider bean and its supporting infrastructure.
This is a visual summary of the work you’ve already done.
The authentication provider at the top is something that Spring Security provides, and the account service at the bottom is what you did in recipe 4.4
The adapter layer, which you just implemented, is the glue that holds these two things together.
But you do need to add the entry for beans-service.xml to web.xml:
Figure 6.15 Bean dependency graph for DaoAuthenticationProvider and its dependencies.
You also need to modify one of the JSPs slightly, so let’s do that next.
In previous recipes, you displayed the user’s username in the navigation area because you didn’t have access to user properties beyond those provided by the default Spring Security UserDetails implementation.
But now you’re using the UserDetailsAdapter class from listing 6.6, which gives you access to nonstandard properties such as the user’s first name, last name, full name, and email address.
You can address the user by their full name by changing subhead.jspf as follows:
This gives you a great deal of flexibility as regards your back-end authentication source, and it allows you to use rich user principals in your code and JSPs.
All the examples so far have involved storing passwords as plain text.
Recipe 6.6 Using a custom user service and user principal (code dependency only)
When storing passwords, you need to take steps to ensure that nobody can see them, yourself included.
This recipe shows how to use password salting and hashing to prevent anybody from viewing user passwords in the database.
Although this recipe doesn’t inherently depend on using a custom user service or user principal, you do build on the sample code from recipe 6.6
Prevent people (including you) from viewing user passwords stored in the database.
You’ll use cryptography to secure user passwords in the database, or at rest.
The specific technique you’ll use is hashing, which performs a one-way encryption of a user password; that is, hashing doesn’t provide a corresponding decrypt operation.
When a user creates an account, you hash the user’s password before.
That way, nobody except the user knows what it is, even if the database is compromised.
When the same user wants to log in, you take the submitted password, hash it, and compare that hash to the one stored in the database.
The authentication succeeds if and only if the hashes are the same.
This works because different messages (here, passwords) are extremely unlikely to hash to the same value.
Figure 6.16 illustrates the process of hashing a plaintext message.
Spring Security makes it easy to add password hashing to your app, but to learn.
The code download for this recipe combines the login work you’ve done in this chapter with the user registration code from recipe 4.4
First you’ll update the registration process to store password hashes instead of plain text.
Figure 6.16 Hashing a plaintext password into a digest for improved security at rest.
The only other thing to do is add a PasswordEncoder in the app context.
You’ll use it both for the initial password creation as well as for logins.
To do this, you need to choose an appropriate hashing algorithm.
MD5 in particular is no longer considered a secure hash algorithm.
You use the ShaPasswordEncoder and its constructor to specify SHA-256
For logins, you use the <password-encoder> configuration to endow the DaoAuthenticationProvider with the ShaPasswordEncoder.
The <password-encoder> element has an optional hash attribute that you can use to specify a hash algorithm (for inject the bean into the HbnAccountDao as noted.
Each as well rebuild the database with the database scripts.
Be sure to run both the schema and the data script.) See the src/main/sql in the code download.
If for whatever reason you don’t want to do that, you can as an alternative use this:
At this point, if you run the app, new registrations will create hashed passwords, and logins will hash tendered passwords before comparing with the database hashes.
You’ll need to create new accounts (with hashed passwords) through the registration process to log in.
The new password-storage scheme offers considerably better security than plain text, which is completely insecure.
The most significant is that despite the fact that hash algorithms don’t support a decrypt operation, you (wearing your black hat) can do something that’s almost as good: precompute hashes for all the words in a dictionary and use the result as a reverse-lookup.
Although this won’t necessarily allow you to recover every password in the database, it will allow you to recover any password that’s a dictionary word, and plenty of users use dictionary words for passwords.
You need something to defend against socalled dictionary attacks, and that’s where salt comes in.
The idea behind salt is to add extra bits to user passwords to ensure that they aren’t dictionary words, thus making dictionary attacks harder to execute.
One best practice is to associate a large, random set of bits with each user and append it to each password before hashing it.
This makes it much more costly to precompute reverse-lookup tables because a table must be created for each possible combination of bits.
Here you’ll do something that’s weaker but still an improvement over simple hashing: you’ll use the user’s ID as a salt.
We can best explain how Spring Security will use this by way of example.
Suppose you have a user with ID 27 and password maxmax.
Spring Security will incorporate the chosen salt (here, the ID) into a nondictionary password as follows: maxmax{27}
This is weaker because you can easily imagine somebody precomputing a catalog of reverse-lookup tables to attack apps that use Spring Security and this particular salt scheme.
But it would require a table for each ID, which means more effort than a single table of dictionary word hashes.
But if your security needs are more stringent, you would be well advised to consider a stronger salt scheme, such as the large random bitset we described.
Figure 6.17 shows how you’ll encode passwords using salting and hashing.
To add salt, once again you need to add it to both the password-creation and authentication processes.
Figure 6.17 Improving security at rest by incorporating a variable salt.
What you’re doing is similar to what you did with the PasswordEncoder.
You inject a SaltSource and then use it to generate a salt from the account in a way you’ll see.
Note though that you need to wrap the account with a UserDetailsAdapter because.
You’ve added a ReflectionSaltSource bean to generate salts from the user’s id property, and as with the PasswordEncoder, Spring automatically injects this into the HbnAccountDao so you can use it during account creation.
You also configure the salt source into the DaoAuthenticationProvider so you can use it during logins.
Once again, this configuration effectively invalidates any existing accounts, because their passwords won’t be recognizable anymore.
Quick tip: preserving legacy passwords If you want to upgrade a plaintext password-storage scheme to a hashed storage scheme, that’s easy enough: you replace the plaintext passwords with hashed versions.
But what if you want to upgrade a legacy hashed password-storage scheme to a salted, hashed scheme? Here you don’t have the original passwords, so you can’t recompute the passwords.
The answer is to use two authentication providers: the one with the salt source and the one without.
Your password-storage scheme is now significantly more secure than the plaintext scheme you’ve been using up to this point.
This recipe illustrated the link between the account-creation process and the authentication process.
In general, the two processes need to be coordinated so that created accounts can serve in authentication contexts.
Something you’ll notice if you run the sample code for this recipe is that you aren’t automatically logged in after you register an account.
That’s an annoyance and another example of where it makes sense to coordinate account creation with authentication.
The final recipe in this chapter shows how to fix that.
Users expect applications to authenticate them automatically after creating an account.
This recipe shows how to do this with Spring Security.
This recipe builds on the code from recipe 6.7, but it doesn’t depend on hashing or salting.
The sample code for recipe 6.7 is convenient in that it includes both the registration component and the login component.
Automatically authenticate the user after a successful new user registration.
This requires a couple of minor modifications to the sample code from recipe 6.7:
Update the AccountController to perform the auto-authentication immediately following a successful user registration.
Update beans-security.xml to support injecting your authentication manager into the controller.
Note that javax.inject also has an @Qualifier annotation, but it has different semantics.
You also have to modify AccountController to use the authentication manager to perform the auto-authentication:
The part you care about starts where you create the authentication request.
To authenticate the token, you need to pass it into the succeed because you’ve just created the associated account.
After you have the authenticated token, you place it on the SecurityContext.
There’s one small detail you need to handle in the beans-security.xml configuration.
Because there are (as we mentioned) two AuthenticationManagers, you can’t rely on type-based injection.
You used @Qualifier to identify the bean you want to.
The problem is that the bean’s actual ID is org.springframework.security.authenticationManager, and that’s an internal ID that Spring Security gives it rather than a published ID.
To reference this bean, you need to give it an explicit alias.
With those changes, you now have a registration process that automatically authenticates the user after a successful registration.
This final authentication recipe showed how to handle an important detail when integrating registration with authentication: auto-authenticating the user on a successful registration.
You were able to accomplish this using the AuthenticationManager in a programmatic fashion.
You should now have a reasonable understanding of how to implement logins and logouts using Spring Security.
We began by looking at different login UI options, then moved on to back-end options as well.
Toward the end, you saw how to integrate different aspects of account creation with authentication, such as hashing, salting, and auto-authentication.
In chapter 7, we’ll consider one of the major use cases for authentication: authorization.
Authorization is the process of determining for a given user whether they’re allowed to access a given resource.
Once again, Spring Security provides a rich set of tools to handle the job.
Authentication establishes the principal’s identity, and authorization decides what the principal is allowed to do.
This chapter continues the treatment of Spring Security we began in chapter 6, this time exploring its authorization features.
On the one hand we have authorization targets, which correspond to what is being protected: methods, views, and web resources.
On the other we have authorization styles, or how we’re protecting the targets: via authentication levels, roles, and access control lists (ACLs)
Conceptually we’ll break authorization into the grid in table 7.1
Each recipe addresses one of the cells in the table.
Authorization is the area of security that deals with protecting resources from users.
Before tackling the recipes, let’s discuss the authorization targets and styles.
Spring Security allows you to authorize the three target types that appear as columns in table 7.1:
Methods—We often need to protect Java methods from unauthorized access.
Views—JSPs frequently contain navigation and content that we want to show or hide according to the user’s permissions.
For example, if the user doesn’t have access to an admin console, it probably doesn’t make sense to show the link to the admin console in the first place.
We also need a way to suppress page content when the user isn’t allowed to see it.
Web resources—We can grant or deny access to different HTTP requests based on the associated URLs and HTTP methods.
The two rows in table 7.1 deal with how access is determined.
Authentication-, role-, and permission-based authorization—Authentication-based authorization uses authentication levels to determine access.
Authentication levels range from anonymous (not authenticated) to remembered (authenticated via remember-me) to fully authenticated.
Role-based authorization involves assigning roles to individual users and then using those roles to determine what users can see and do, ideally through associated permissions.
ACL-based authorization—ACLs control access to an application’s domain objects based on user permissions attaching to those domain objects.
For example, I have access to my inbox but not yours.
Because ACLs attach to domain objects, people often refer to this authorization style as domain object authorization or domain object security.
The following recipes center around a discussion forum sample application.
There are multiple reasons to protect Java methods from unauthorized access:
To consolidate the definition of access rules so they can be managed in one place instead of being distributed and repeated across multiple clients.
To avoid having to rely on potentially untrusted clients to enforce access rules ■ To provide for defense-in-depth, which is a security best practice.
Spring Security allows you to define access rules, using either XML or annotations, that you can apply to Java service methods.
Secure service methods by defining authentication- and role-based access rules.
Approaches to securing Java methods involve defining rules that specify who has access to which methods.
You’ll define access rules using annotations, although XMLbased rules are also an option.
Consult the Spring Security reference documentation for information on using security pointcuts and AOP to secure Java methods.
The first thing to understand is how to define an access rule.
As of Spring 3, you can use the SpEL to define who has access to a target (whether a method or otherwise)
Table 7.2 shows the predicates for defining authentication- and role-based access rules.
With the annotation-based approach, you add security annotations to your service beans, and then you add a single line to your beans-security.xml configuration.
You’ll secure the ForumServiceImpl service bean in the sample app.
This service allows clients to access forum-related functionality, like getting forums, updating messages in a forum, and so on.
For now, your rules will be simple, although later you’ll refine them.
Table 7.2 SpEL predicates for defining authentication- and role-based access controls.
Listing 7.1 shows what ForumServiceImpl.java looks like with the annotations in place.
Despite its simplicity, this code snippet uses two key techniques that we need to discuss in a little detail.
Let’s cover the raw mechanics, then we’ll look at the bigger picture.
The class-level @PreAuthorize annotation defines a default denyAll rule for the methods in the class B.
That may seem like a strange thing to do, but it’s one of the key techniques that we’ll discuss momentarily.
The default applies unless individual methods override it with method-level annotations.
Of course, denying access to everybody doesn’t make for a useful method.
Broadly speaking, there are two approaches to authorization: whitelisting and blacklisting.
Whitelisting denies requests unless they’re explicitly granted (on the whitelist), whereas blacklisting grants requests unless they’re explicitly forbidden (on the blacklist)
Whitelisting, as the more paranoid, generally makes for better security, and security professionals consider it a best practice.
If somebody adds a new method to ForumServiceImpl and forgets to attach an access rule, the default rule prevents anybody from using the method.
The goal behind this technique is to avoid embedding security policy decisions in the code.
Such decisions should be set at runtime because they vary across customers, they vary over time, and sometimes they need to be changed immediately (for example, in response to a security breach)
The first rule breaks when somebody decides that teaching assistants, parents, faculty trainers, accreditors, or any number of other roles should gain access, or that one of the roles should lose access (for instance, faculty-only forums)
The roles may be different for different customers using the software, and many of the roles may not make any sense for some customers.
The second rule is more resilient in the face of such changes, because in essence it says that a user can access a given forum if they have read access to forums generally.
As a general rule, prefer permission-based rules to role-based rules.
There are exceptions (you’ll see an example in recipe 7.2), but it holds in general.
Spring Security 3 appears schizophrenic on the issue of separating roles and permissions.
But the examples in the Spring Security reference documentation tend to steer you toward using roles directly, which is at best a questionable practice for the reasons already given.2
Apparent schizophrenia aside, Spring Security makes it easy to do the right thing.
The sample code, for instance, uses a custom UserDetailsService backed by the user/role/permission schema shown in figure 7.2.3 The src/main/sql/schema.sql script contains this schema, but it’s just an example.
Even if you’re using JdbcDaoImpl instead of a custom UserDetailsService, you can take advantage of the Spring Security group schema to separate roles and permissions.4
The sample code uses BIGINTs but in real life you’ll almost certainly want to use a smaller type, like INTs or even smaller.
Of course, if you do need room for 18 quintillion accounts, then BIGINT is the data type for you.
The basic idea is that groups are roles and granted authorities are permissions.
In addition to the schema, you also need sample data so you can test the security configuration.
Figure 7.3 shows the roles and permissions that each sample user has, as contained in the src/main/sql/data.sql script.
To do that, you add a single line to the beans-security.xml configuration from recipe 6.1:
You’ve enabled Spring Security’s pre- and post-annotations, disabled by default, because they allow you to use SpEL to define access rules in an elegant fashion.
There are a couple of other options, which we’ll list for completeness:
Although these are standard, they support only simple role-based rules and aren’t nearly as powerful as Spring Security’s pre/post annotations.
Figure 7.3 Users, roles, and permissions for the sample application.
Go into one of the forums, select a message, and try to block it (the link is available at the bottom of the message)
You should get an error message in a dialog the PERM_ADMIN_MESSAGES permission, which the student role doesn’t have.
You’ll be able to access the edit page and the delete confirm box.
You should be able to execute all of them, because the admin role has the required permissions.
The first authorization recipe showed how to create authorization rules and apply them to Java methods.
You’ve focused on applying them to Java service beans, although it’s also possible to use Spring Security with AspectJ to attach authorization rules to domain objects when implementing a domain-driven design.
The next recipe shows how to secure views by applying authorization rules to code fragments inside a JSP.
In many cases it’s necessary to suppress JSP fragments.5 One case would be where the fragment contains content the user isn’t authorized to view.
We’ll use a basic forum application to illustrate the techniques involved.
Show or hide JSP page fragments based on the user’s authentication level and role.
Here we mean subsets of the content inside a JSP, rather than .jspf files per se.
One such example would involve trying to entice the user to purchase premium features or content.
Recipe 7.1 showed how to accomplish authorization based on authentication levels.
We’ll show how to add authorization based on roles and permissions.
The main tool here is the Spring Security tag library.
The Forums link is present if the user has the read forums permission.
The My Account and Logout (not displayed) links appear only if the user is authenticated; otherwise a Login link appears.
Finally, the Admin link appears only if the user has the admin role.
The tag displays its body if and only if the predicate evaluates true against the current principal.
Listing 7.2 shows how you use the predicates to implement the desired display controls.
Figure 7.4 Navigation display based on user authentication levels, roles, and permissions.
You use the same predicates to control the visibility of the My Account, Login, and Logout links.
Recall that recipe 7.1 mentioned that sometimes it does make sense to build access rules based on roles rather than permissions.
Here, it makes sense to tie the Admin page directly to the admin role, because that rule is unlikely to require revision: the page specifically targets the role.
There is a second way to specify access conditions for <security:authorize> tags.
Instead of defining an access attribute, you can define a url attribute, along with an optional method attribute for HTTP methods, to establish an access condition.
The idea is that the tag displays its body if and only if the user has access to the URL via the method in question.
You’ll see how to define access controls on a URL and method in the next recipe.
This approach is useful when rendering navigation, because it allows you to bind the visibility of a given navigation item to its access definition instead of duplicating what’s essentially a single rule.
This recipe showed how to hide links and content based on user authentication levels and roles.
In the case of links, developers sometimes use the technique of hiding links as a substitute for proper access controls.
These are deprecated; use either access or url and method instead.
To defend the link targets against unauthorized access, it will help to have a way to selectively permit or deny HTTP requests, and that’s what the next recipe shows how to do.
In recipe 7.2, you saw how to display web-page content based on user authentication levels, roles, and permissions.
This is often used as a way to keep the UI streamlined; in most cases it doesn’t make sense to show users links that they aren’t allowed to use.
Just because the link is hidden doesn’t mean users can’t get to it.
To have real security, you must protect your web resources with access rules.
Control users’ access to web resources according to their authentication level, roles, and permissions.
You’ll continue using Spring Security 3 to implement authentication-, role-, and permission-based authentication for the web URLs in your sample forum application.
You use the intercept-url element to define access rules for web URLs as shown in the following listing.
Try it using http://localhost:8080/sip/main/admin.html and some account other than juan.
You’ll find that you can still get to the admin page.
To define access rules, you once again take advantage of SpEL.
That approach is now legacy, so we won’t cover it here.
Before looking at the specific rules, let’s discuss the high-level approach and some details about the mechanics of defining a rule.
As with recipe 7.1, you adopt the best practice of defining access on a whitelist model.
To implement a whitelist, bear in mind that rules are evaluated on a firstmatch basis; that is, the first rule in the list with a pattern that matches the request URL is the rule that applies.
Therefore you need to place more specific patterns before more general patterns.
You make your list a whitelist by making the most general rule one that denies access to all resources using denyAll.
To specify rules, you use a URL pattern, an optional HTTP method (GET, POST, PUT, indicates that the pattern and method don’t require authentication, authorization, or any other security services.
The second option allows you to use SpEL to define the conditions under which the user can use the URL/method pair in question.
The patterns have two possible syntaxes, which you choose by setting the path-type attribute on the containing <http> element.
Next you have rules for specific URL and method combinations.
Although the methods are optional, it’s a good practice to specify them explicitly to avoid opening up access unnecessarily.
This is very much in line with the whitelist approach.) At D you use permitAll to specify that GET requests for the home page are always to be granted.
At E you use hasRole('ROLE_ADMIN') to ensure that only administrators have access to the admin page.
You define additional access rules for forums and messages at F and G.
Although it would have been possible to use Ant’s ** wildcard to shorten the list of rules, doing so would effectively create a blacklist inside of the whitelist, and from a security perspective it’s safer to require grants to be explicit.
At H you create the denyAll rule that makes your rule list a whitelist.
Try to access http://localhost:8080/sip/admin.html anonymously by manually entering it in the browser’s address bar.
If you log in as juan, the request should succeed; otherwise, you should get the access-denied page.
Log into the app as elvira using the normal login process.
Once you’ve done so, manually enter the admin URL from the previous bullet.
Unlike in recipe 7.2, you should now get the access-denied page.
There’s one loose end to tie up before we move on to the next recipe.
Recall that the idea is to bind the display of a navigation item to the user’s access to that item such that you can define the access rule in a single location (such as beans-security.xml) instead of defining it in two.
Indeed, let’s update subhead.jspf to take advantage of this feature (see listing 7.2 for the original listing):
Now, when you change the rules in beans-security.xml, the <security:authorize> tag’s behavior automatically reflects the change.
You can define rules authoritatively in the Spring Security configuration file instead of duplicating rule definitions in the JSPs.
In the preceding three recipes, you’ve assumed fairly simple requirements around access, mostly around application-wide roles and permissions.
For example, you’ve assumed that users either do or don’t have permission to edit messages.
You might want each forum to have a moderator with administrative privileges over that forum, rather than having just a single site administrator.
For any given message, you might want the site admin, the forum moderator, and the original author to have edit permissions, but no one else.
You might want the site admin and forum moderator to be able to block, unblock, and delete messages, but nobody else.
The requirements just described require machinery more powerful than that offered so far, because they’re based not on simple application-wide permissions but on relationships between a principal and a domain object being accessed.
A given user either is or isn’t the moderator for a given forum, and the answer makes a difference in terms of what the user is allowed to do with messages in that forum.
This is the first of two recipes that deal with authorizing access to and displaying specific domain objects that exist in your system.
Keeping with the discussion forum example, imagine that you want to allow the author, the forum moderator, and the site admin to edit a message after it’s been posted, and nobody else.
Additionally, moderators must be able to block, unblock, and delete messages in forums they moderate.8
The idea is that each domain object has an associated list of access rules specifying who is allowed to do what with the object.
Each rule, more formally known as an access control entry (ACE), specifies an actor, an action, and either grant or deny, indicating whether the actor may perform the action on the.
You’ll handle part of this requirement around blocking, unblocking, and deleting messages in this recipe.
See figure 7.5 for a visual explanation of access control lists.
Actors (like Daniel) want to perform actions (like editing) against domain object targets (like message 106)
You need to decide in any given instance whether the desired action is permissible.
With role-based authorization, you manage predicates on principals without worrying about specific targets.
Can Daniel edit messages generally?” ACL-based authorization is finer-grained and involves managing relationships between actors and targets.
Figure 7.6 shows the difference between role-based and ACL-based authorization.
With that background, you’re ready to tackle the rest of the recipe.
Authorize service method invocations involving domain objects based on the relationship between the principal and the domain object(s) in question.
Specifically, the author, the forum moderator, and the site admin must be able to edit existing posts.
You also need to lay the groundwork allowing moderators to block, unblock, and delete posts, although that will require additional work in recipe 7.5
The author has one set of permissions with respect to the message, and other users have other permissions.
You’ll use the Spring Security ACL module to implement domain object authorization.
Defining domain object ACLs—First we’ll cover the basics of granting (or, less commonly, denying) user permissions on domain objects using the ACL database schema to drive the discussion.
Defining ACL-based access rules for Java methods—You’ll see how to use annotations to define ACL-based access rules, specified in terms of user permissions, on Java methods.
Configuring Spring Security for ACLs—You need configuration to activate domain object security.
Optimizing ACL definition—We’ll look at ways to simplify and streamline ACL definitions.
Manipulating ACLs programmatically—We’ll look at programmatic manipulation of ACL data, which is often necessary in cases where the app creates new domain objects.
For example, give the author of a new message permission to edit the message even after it has already been posted.)
Figure 7.6 With rolebased authorization, a user might have permission to (say) edit messages in general.
With ACL-based authorization, you can set user edit permissions on a messageby-message basis.
Because the ACL module is database-driven, you’ll start there both to get a good grasp of the key concepts and details on the database schema.
Note that the schema we’re about to cover replaces the one from the earlier recipes, so you should rebuild the database using the scripts at src/main/sql at this time.
To begin, you need to understand some key ACL concepts and how to define user permissions on domain objects so you can define access rules in terms of said permissions.
The ACL database schema is the logical place to start both because it highlights the concepts and because it’s where you define the user permissions.
The ACL module has four tables to store actors, actions, and targets (domain objects)
Together they provide a framework for defining access rules for specific domain objects.
Spring Security uses the access rules to make access decisions.
Figure 7.7 is the E/R diagram for the ACL module.
It’s unlikely in the extreme that you’ll require BIGINTs for your dataset, and using them consumes space unnecessarily.
It pays to think about your expected data volumes and choose data widths accordingly.
Let’s go over the tables in detail because it will help you better understand how ACLbased security works.
As an example, you’ll define a rule that grants user daniel permission to edit message 106
A security identity (SID) can be a principal such as an end user or a system or a granted authority such as the admin role or even a system-wide permission as noted in recipe 7.1
The principal column is a flag indicating whether the SID is a principal or a granted authority, and the sid column contains a username or granted authority name accordingly.
Together the principal and sid columns allow Spring Security to associate SIDs in the ACL schema with app users and granted authorities, as figure 7.8 shows.
To be concrete, consider user daniel with a role called ROLE_USER.
In the acl_class table, there are only two columns: id and class.
Figure 7.8 How Spring Security links its ACL schema to its user schema.
This allows Spring Security to support custom app objects because the Spring Security user schema uses Java interfaces.
There’s a lot going on in acl_object_identity, whose rows are referred to as OIDs.
Each OID is Spring Security’s representation of an application domain object.
The object_id_identity column is for the domain object’s native ID, which Spring Security assumes to be numeric and exposed by an id property on the domain object.
Continuing with the example, suppose you want to represent message ID 106
These are more advanced in nature, so we’ll ignore them in this first pass and return to them shortly.
Actions are what SIDs want to do with OIDs, such as creating or editing them.
Each row in the acl_entry table, or ACE, is essentially an assertion to the effect that such-andsuch SID either does or doesn’t have permission to perform such-and-such action on such-and-such OID.
Each ACE involves a SID, an OID, a permission, and either grant or deny.
The simplest way to think about permissions is that the BasePermission class defines five default permissions with associated codes (masks), as shown in table 7.6
The precise interpretation of the specific permissions depends on the application, but they correspond to the standard CRUD (create, read, update and delete) operations plus an administrative operation.
Principals with the administrative permission are able to manage the corresponding object’s ACL (add or remove ACEs, change parent, change ownership, change audit information, and so on)
There are other ways to be able to manage object ACLs, and you’ll see those later in the recipe.
If you create additional permissions, their masks must be powers of 2.10
Our current project, for instance, is a software deployment automation system.
Permission to deploy to environments like dev, test, and prod varies with role; for example, developers can deploy to the development environment but not to production.
We distinguish the ability to update the environment domain object from the ability to update the real-world environment.
We use the standard WRITE permission for the former and a custom DEPLOY permission for the latter, because the ability to update an environment’s description (say) is distinct from the ability to deploy a change to it.
Table 7.7 shows a sample ACE, one that grants the edit (a.k.a.
The org.springframework.security.acls.model.Acl interface doesn’t specify how the order matters for the default AclImpl implementation.13
You’re now ready to learn how to define access rules on Java methods in terms of user permissions like the one you’ve just defined.
It’s fairly complicated, but the basic idea is that the first matching ACE is the one that wins.
But that’s a simplification; again, consult the Javadoc for details.
Bitmasks are a well-established and economical way to do that.
Unfortunately, despite the use of the mask terminology in the database table and in the API, the default ACL implementation, AclImpl, doesn’t support arbitrary bitmasks.
You can’t grant READ and WRITE using a single ACE with the mask set to 3
Instead you need to create an ACE for the READ permission and a second ACE for the WRITE permission.
The reason is somewhat difficult,11 but suffice to say that many Spring Security beginners understandably but incorrectly assume that they can place bitmasks in the mask column.
In recipe 7.1 you defined rules in terms of authentication status, roles, and system-wide permissions.
But now you’re going to define rules in terms of permissions on domain objects.
As it happens, Spring Security supports several access-control annotations, as shown in table 7.8
As we just mentioned, the expressions can reference domain objects.
Let’s look at a few examples from the class ForumServiceImpl in the sample code.
In the snippet, the rule is to allow entry into the method if and only if the current principal has the delete permission on the message being passed in.
Spring Security checks this by matching on the Message class and domain object ID as.
Filters a collection of domain objects before passing it into the annotated method.
Predicate indicating whether the current principal has a given permission on a given domain object.
Legal values for permission are read, write, create, delete, and admin (no quotes)
Sometimes you want to drive access decisions based on something other than object IDs.
For instance, the sample app allows forum moderators and site admins to block messages.
Only users with the admin permission on a blocked message should be able to see it.
The following code snippet shows how to use the @PostAuthorize annotation to accomplish this:
In this example, you have no way of knowing whether the message is visible based on the message ID alone.
You have to get the message (using the special returnObject term), check for the read permission (using the hasPermission predicate), and then check its visibility.
You have to override that annotation so calls can enter the method.
The sample app doesn’t use the @PreFilter annotation, and it’s not commonly used, but for completeness we’ll say something about it here.
Prefiltering can be useful where bulk operations on domain objects are concerned.
It allows you to remove a domain object from the bulk operation when the current principal doesn’t have permission to perform the operation on that domain object.
The domain objects need to be part of a java.util.Collection (an array won’t work), and the Collection needs.
Suppose, hypothetically, that you wanted to support a bulk delete on messages.
But after that, you’d use the @PreFilter annotation with the special filterObject (representing an arbitrary collection element) and hasPermission expressions to exclude messages for which the current principal lacks the delete permission.
In this case you have only one collection parameter, but if you had more than one, you’d use the filterTarget element to choose one.
If you’re wondering how Spring Security knows the parameter name, the answer is that it uses compiler debug information.
You must compile the code with the debug local variable information turned on in order for this feature to work.
When a method returns a collection of domain objects, sometimes you want to filter out individual elements before handing them over to the caller.
That’s what and only if—the principal has read permission on the forum:
As with @PreFilter, you use filterObject and hasPermission to perform the desired filtering.
The annotations define the access rules, but you need to activate them to make them do anything.
There are a couple of different pieces to the configuration.
First, you make a minor tweak to the <global-method-security> definition in beans-security.xml:
You address that with a new beans-security-acl.xml configuration as shown next.
Figure 7.10 is the same expression handler configuration as a bean dependency graph.
You use the beans namespace in listing 7.4 rather than the security namespace.
The security namespace doesn’t (at the time of this writing, anyway) directly support ACL configuration, so you need to do everything explicitly.
It’s still the default expression handler, but you’re giving it a nondefault configuration by injecting a permission evaluator the beans-security-acl.xml configuration.
The specific permission evaluator you need is the AclPermissionEvaluator, so you create one at C.
The permission evaluator relies on an ACL service for ACL CRUD operations.
You use a JdbcMutableAclService D because your ACLs are in a database (that’s the JDBC part) and you want to be able to create, update, and delete ACLs as you create, update, and delete the corresponding domain objects (that’s the mutable part)
The service also uses caching for performance, so you inject an Ehcache-backed cache too G.15
For simplicity you use a BasicLookupStrategy E, which is based on ANSI SQL.
It doesn’t have DBMS-specific optimizations, but it attempts to be performant within the confines of ANSI SQL.
The BasicLookupStrategy uses the same data source and cache that the JdbcMutableAclService uses.
You also inject a console logger F for logging purposes.
The BasicLookupStrategy also has an AclAuthorizationStrategyImpl H, which it injects into the AclImpls that it returns from lookups.
The AclAuthorizationStrategyImpl supports ACL administration by authorizing ACL modification attempts.
Its constructor takes an array of three granted authorities as described in table 7.10
You’ll learn a little more about how these constructor arguments work when we discuss programmatic ACL management, but the idea is that not just anybody can.
Don’t forget to configure your Ehcache, at least for production.
See http://ehcache.org/documentation/ configuration.html for information on how to do that.
We already mentioned in connection with table 7.6 that one way to manage an object’s ACL is to have the administrative permission on that object.
Another way is to have the authority or authorities injected into the AclAuthorizationStrategyImpl constructor, because each entails a special ACL management permission.
These aren’t to be confused with the normal permissions that appear inside the ACLs.
You can think of the special permissions as metapermissions that sit outside the ACLs, determining who can change the ACLs.) In practice, the authority you pass into all three constructor slots is a system-wide administrative authority, and indeed that’s what you’ve configured here.
Now let’s look at some optimizations you can make to your ACL definitions.
These columns allow you to simplify the management of your ACLs and also to save what could potentially be a lot of storage space by creating OID hierarchies and then allowing OIDs to inherit ACEs from parent OIDs.
For example, in the sample app, you want the administrator to have the write permission (among others) for all messages in all forums, and you want forum moderators to have the write permission for all messages in forums they moderate.
There are lots of users and lots of messages, and if you have to create a new set of ACEs for every new user or new message, things will get out of control in a hurry.
Establish a parent-child relationship between forums and messages using the parent_object column on message OIDs.
Set entries_inheriting true on the message OIDs so that messages automatically inherit ACEs from their parent forums.
Give the site admin the write permission on the individual forums, and give forum moderators the write permission on the forums they moderate.
The messages will inherit ACEs from the forum ACLs, allowing you to avoid creating lots of message ACEs.
That is, the specified authority has the special AclAuthorizationStrategy.CHANGE_OWNERSHIP permission.
That is, the specified authority has the special AclAuthorizationStrategy.CHANGE_AUDITING permission.
Authority able to change other ACL and ACE details (for example, inserting an ACE into an ACL or changing an ACL’s parent)
That is, the specified authority has the special AclAuthorizationStrategy.CHANGE_GENERAL permission.
In figure 7.11, your user has the write (or edit) permission on each message, and.
Using this approach, if you want to give the user read permission on the messages, you need to create another set of ACEs granting the read permission.
If you decide to revoke the write permission, you’ll need to delete all the ACEs.
In figure 7.12, you establish parent/child relationships between the forum and its.
Then you give the user the write permission on the forum.
Finally you use inheritance to grant the user the write permission on the messages by setting entries_inheriting to true.
Notice that this solves the issues we mentioned: you can add and revoke permissions on the messages by adding and revoking a single permission on the forum.
We need to cover one additional topic: manipulating ACLs programmatically.
Because applications create, update, and delete domain objects, you need to ensure that you keep the various ACL schema objects in sync.
If you create a message, you need to create a corresponding OID and ACL.
If you delete a message, you need to delete the corresponding OID and ACL as well.
Figure 7.12 Best practice: establish parent/child relationships, then inherit permissions.
You included a JdbcMutableAclService in the configuration in listing 7.4
The following listing shows how you can use a MutableAclService to manage the creation, updating, and deletion of messages.
Listing 7.5 is ForumServiceImpl, which we referenced earlier when going over the security annotations.
Like the other service beans, you declare it to be @Transactional B, which matters here in part because you want your domain modifications to execute in the same transactional context as your ACL modifications.
You have different message modification methods that create, update, and delete sync with the domain model.
OID, then use it to issue a forum ACL lookup against the ACL service H.
Then you create a new message ACL and corresponding OID I.
By default, the JdbcMutableAclService assigns OID ownership to the current principal, which may or may not be the author, as you’ll see.
Spring Security doesn’t specify what it means to own a domain object, but by default the owner is allowed to perform.
Now that you’ve created the message ACL, you make the forum ACL its parent.
The current principal can do this because it owns the current OID, as noted.
At J you give the author write permission on the message if the message is visible.
The idea is that the author can edit their own messages as long as an administrator hasn’t put an administrative block on the message (that is, an administrator hasn’t made it invisible)
You change the message ACL owner to the author at 1)
This requires the AclAuthorizationStrategy.CHANGE_OWNERSHIP metapermission, which the current principal has because it’s the current owner.
In most cases, the author will already be the posting a message.
But you’ll see momentarily that in some cases an administrator is creating the ACL, and in such cases you need the administrator to relinquish ownership to the author.
You update the ACL in the persistent store at 1!
You now have a new message OID and ACL with the right parent, author write permission, and owner.
In addition to creating ACLs, you have to be able to delete them.
You also have to be able to update ACLs 1#
This can happen if the original author updates the message (not shown in listing 7.5, but it’s in the code download), or it can happen if an administrator blocks the message.
With that, you’re able to manage ACLs alongside your domain objects.
In this recipe, you learned how to add fine-grained authorization to your application.
With the earlier recipes you had to be content with fairly crude rules, such as the rule that normal users can read and create messages in general and that the site admin can also edit, block, unblock, and delete messages in general.
But now you can create more specific rules, such as the rule that relevant authors and forum moderators should also be allowed to edit specific messages.
You’d like forum moderators to be able to block, unblock, and delete messages in their forums, but the app doesn’t yet support this.
Under the hood you have the ACLs in place for it, but on the display side, the JSP doesn’t yet know how to take the ACLs into account.
Displaying web navigation and content based on ACLs hinges on coarse-grained permissions rather than on fine-grained ACLs.
Also, the app always shows the Edit Message option, even if the user doesn’t have that permission for that message.
The final recipe in this chapter, recipe 7.5, addresses these issues.
In addition to protecting URLs from being accessed and methods from being called, it’s often useful to show or hide page navigation and content according to a user’s authorization to view them.
Spring Security’s tag library allows you to display page navigation and content based on ACL data.
Show or hide web navigation and content based on domain object ACLs.
Specifically, you need to show or hide the edit, block, unblock, and delete message options depending on whether the user has permission to perform those operations on the messages in question.
In previous recipes, the sample app always displays the Edit Message link, even if the user doesn’t have permission to perform that operation.
And it uses crude, general permissions to decide whether to display the block/unblock/delete options:
Instead, you want to show or hide those options based on the ACLs you created in recipe 7.4
The basic approach is to use the <security:accesscontrollist> JSP tag to decide whether to hide or show the edit link.
The tag references the ACLs you created in the previous recipe.
Now that we’ve discussed the structure of the Spring Security ACL module, you’re in a good place to understand the JSP that contains the ACL-based JSP tags.
Let’s look at that so you can keep the goal in mind.
The following listing shows the relevant part of the JSP that displays individual forum posts.
The part shown is the list of options at the bottom of the message.
Under normal circumstances, the only thing a user would see is the Reply link.
First you declare the Spring Security tag library B because you’re going to use the attributes: domainObject and hasPermission.
The domainObject attribute points to a domain object (no surprises there), and hasPermission describes individually sufficient permissions that the current user must have with respect to the domain object.
In other words, you enter the body tag if and only if the current user has at least one of the listed permissions.
Here you’re basically displaying the <ul> only if there’s at least one option to show.
The user role doesn’t directly come into the picture here.
Note that in E, even though the JSP generates HTML for both the block and unblock options, JavaScript ensures that exactly one is visible.
Listing 7.6 message.jsp, showing how you show or suppress privileged features.
User roles are indirectly involved, though, because they factor into which general permissions are in place, and those in turn factor into specific permissions.
In this specific case, the site admin role has all ACL permissions on the site object, and those propagate down to forums and messages through the ACL inheritance mechanism.
Log in to the application as user julia, and go to the Algebra I forum, which she moderates.
Julia should be able to perform all operations on the messages in that forum.
Then go into the Calculus II forum, which she doesn’t moderate.
She can edit messages that she wrote, but she can’t do anything other than read messages that somebody else wrote.
Log out, and then log back in as user juan, the site admin.
You should be able to perform all operations on all messages across all forums.
As with role-based authorization, ACL-based authorization involves collaboration between an underlying authorization model and view-related controls based on that model.
In this recipe, you learned how to use the <security:accesscontrollist> JSP tag to make display decisions based on an underlying ACL apparatus.
Once that’s in place, showing or hiding page content based on ACLs is a piece of cake.
With this chapter, we’ve completed our two-chapter tour of security-related problems.
The issues we’ve examined are of a general nature and reappear in application after application.
You’ve also seen that Spring Security provides a nice framework for solving such problems, providing services around authentication and authorization, including a rich ACL infrastructure.
In the next chapter, we switch gears and discuss how to build features for communicating with customers and end users.
How it works: AccessControlListTag The <security:accesscontrollist> tag is backed by the AccessControlListTag class.
AccessControlListTag looks for an AclService on the application context, which it uses to find the ACL attaching to the domain object in question.
Then AccessControlListTag delegates the access decision to Acl.isGranted(...) in much the same way that AbstractSecurityInterceptor delegates to an AccessDecisionManager.
This is different than how the AccessDecisionManager makes its decisions.
We noted that AccessDecisionManagers (at least, the ones that ship with Spring Security) use a voting mechanism (in the form of AccessDecisionVoter) combined with a conflict-resolution mechanism (in the form of AccessDecisionManager) to yield access decisions.
If the user has at least one of the permissions, access is granted.
The success of a website is often closely tied to your communication with users and customers.
You need to understand their needs, desires, and concerns to provide the level of service required to acquire and retain them.
By the same token, you need a way to communicate news and announcements, new products and services, marketing and PR information, operational issues, and so forth to people who need the information.
It’s clear that good communication is crucial to the success of many websites and businesses.
The ability to communicate depends of course on the existence of an appropriate technical infrastructure.
Usually you want to have multiple channels available because users vary widely in how they prefer to contact you and to be contacted.
This chapter covers ■ Creating customer feedback forms with email.
In this chapter, we’ll look at how to use Spring to support some common communications requirements, including Contact Us forms, mailing lists, and news feeds.
Most websites let users contact the organization behind the website.
Users may need to ask questions, report issues, or leave general feedback.
Although it’s easy to put an email link on a web page, often this isn’t the ideal way to collect user communications.
Sometimes you need extra structure around the communication, such as asking whether it’s a tech-support issue or a suggestion.
This can be useful for routing the request and for analytic purposes.
Sometimes users don’t want to use their work email account to correspond with your business.
Finally, if you post an email address, you have to worry about spammers having it.
In this recipe, we’ll look at a common alternative to the mailto link: the web-based form.
You can collect whatever information you like, users can provide whatever return email address they like, and you don’t have to post an email address that spammers will spam.1
You’ll need to update the Jetty configuration in this recipe for Maven, including specifying the username and password for the MySQL database (sip/sip).2
Create a web-based form so users can send you comments, questions, issues, and so forth.
You’ll implement the form in figure 8.1 using Spring Web MVC for the controller, a POJO service bean, and a POJO data access object backed by Hibernate.
For now, you’ll save the user’s message in the database.
In the next recipe, you’ll see how to automatically generate emails for the user and the site administrator.
Note that spammers will probably still spam your contact form—it’s just that they won’t have an actual email address to spam.
You can reduce the amount of email spam by using a CAPTCHA.
The bean-dependency graph in figure 8.2 shows the main part of what you’re doing in this recipe.
It’s only a controller, a service bean, and a DAO.
Let’s begin with the web tier, where you’ll build a controller and a JSP.
The following listing shows ContactController, which is responsible for serving the form and processing the user’s submission.
Figure 8.1 This is the simple contact form that you’re creating.
Figure 8.2 Key contact service dependencies: a standard controller/service/DAO stack.
At B you define the form whitelist, which you definitely want in this case because the UserMessage bean includes properties that aren’t part of the form.
In the form-serving method C you grab the referer (yes, that’s the correct spelling— it’s an HTTP header that gives the referring page, if any) of the request and insert it into the form bean.
In the JSP (see listing 8.2), you’ll expose the referer as a hidden field.
The reason for storing the referer is that it’s often useful to know which page the user was on just prior to asking a question or reporting a problem.
You only get one chance to capture the referer, and this is it, so you grab it and store it.
Because you didn’t explicitly specify an attribute name, Spring applies a convention that stores the UserMessage bean under the name userMessage.
But there is also an addAttribute(String name, Object value) method if you want to specify a different name.
In the form-processing method D you attach the @Valid annotation to the UserMessage E to tell Spring to validate the form data on the way in.
Otherwise, you set a bunch of other properties on the form bean F.
Like the referer, these provide useful context when answering questions and troubleshooting.
You handle these in the form-processing method instead of in the form-serving method because you can—they don’t change from request to request (or if they do, it doesn’t matter), so you can avoid having to create hidden form fields by placing them in the form-processing method.
This redirectafter-post pattern allows you to minimize duplicate form submissions.
We’ve suppressed most of the CSS, but it’s in the code download if you’re interested.
Listing 8.2 contactForm.jsp, a JSP to present the contact form.
Here you’re using the Spring form tag library to generate the HTML form and to bind it to the userMessage form bean.
First you declare a form with the model attribute set to userMessage B.
This specifies the form bean, and the name is the one that Spring assigned by convention as discussed.
The next bit of code displays your global error, if any C.
You use nested form:errors tags as described in chapter 3
If you do a View Source in the browser, you should be able to see the field set to whatever referer brought you to the form page (it will be empty if you accessed the form directly)
At E you bind a text field to the form bean’s name property, and at F you use nested form:errors tags to display any property-specific error messages.
We’ve suppressed the interface, but it’s exactly what you’d expect.)
In recipe 8.2, the service bean will become more interesting, but for now it’s just a pass-through to the DAO.
Let’s look at the UserMessage domain object and its persistence mapping.
Like the service bean, the combined form bean/domain object is nondescript.
UserMessage is a domain object, but you’re using it as a form bean as well to avoid multiplying classes beyond necessity.
The UserMessageDao implementation, HbnUserMessageDao, extends AbstractHibernateDao<UserMessage> without adding anything extra.
Therefore we won’t show it here, but you can get it, as well as the user_message DDL, in the code download at the book’s website.
Here’s the single MySQL table you need to support user messages:
Because all you’re doing is connecting a plain controller to a plain service bean and connecting that to a plain DAO, there’s nothing to discuss.
See the code download for the web.xml, beansweb.xml and beans-service.xml configurations.
You should be able to fill out the contact form, and the results should end up in the table you created.
Creating a simple web-based Contact Us form is straightforward and isn’t unlike other forms you’ve already seen.
This recipe does show some useful techniques in action, such as using a domain object as a form bean (using @InitBinder to keep the domain data secure), prepopulating a form bean with metadata (in this case, referer data), and postpopulating the form bean with metadata before storing it in the database.
As mentioned previously, contact forms are useful for supporting multiple communications processes, including support and collecting suggestions and feedback.
Although we didn’t show it here, if users are contacting you in sufficient volume, you may want to add a type property to UserMessage and use it to route messages accordingly.
In the following recipe, you’ll augment the contact form by creating emails from email templates and autogenerating email responses and notifications.
In recipe 8.1, you saw how to create a web-based contact form, which serves as an alternative to placing mailto links in a website.
Despite the fact that sometimes email isn’t the best communications medium, it’s obvious that other times email is an entirely appropriate way to communicate with your users and customers.
In this recipe, you’ll build on recipe 8.1 by adding autogenerated email messages, both to the user who completed the contact form and to an administrative mailbox.
You’ll generate the emails from Velocity templates, then you’ll send them using JavaMail.
It’s important to update the Maven configuration (sip08) for this second recipe because it contains additional Jetty configuration for JavaMail using a Gmail mail server, which wasn’t included in the first recipe.
Also, a username and password will need to be specified for the Gmail account to be used.3
When a user completes a web-based contact form, send two template-based emails: a confirmation to the user who completed the form and a notification to an administrative mailbox indicating that a user has submitted a contact form.
You’ll use Velocity to define a couple of email templates and stamp out individual email messages.
The next listing shows how to create a confirmation message template using the Velocity Template Language (VTL).4
This is an auto-generated response to let you know that we received your message.
As you would expect, most of this is static content, with some placeholders for dynamic content B.
We won’t get deep into VTL syntax here, but you can see that you can access bean properties using dot notation, just as with JSP EL.
You have your template (or two templates if you’ve downloaded the code), so let’s update the ContactServiceImpl bean to use it to create emails.
In recipe 8.1, ContactServiceImpl was boring, but we indicated that you’d be building it out.
The following listing is an implementation of the design in figure 8.3
There’s a lot happening here, so let’s take a look.
At C you declare a dependency on VelocityEngine, which you’ll use to stamp out emails from a template.
Two of them are flags that indicate who (if anybody) should get an email when a user submits a contact form.
You can choose whether or not to send the user a confirmation, and you can choose whether or not to send the site administrator an email notification.
If you send these emails, you’ll need to have a couple of email addresses handy: a noreply address and the administrator’s address.
The process is similar for the notifyAdmin flag and notification emails to the admin.
You’ll use Velocity to generate the email text, so you start by creating a simple model to resolve references in the email templates I.
That takes care of the text, but you have to set the other fields on the email as well.
Most of this is straightforward; the only exception is setting the sender’s name, which can possibly generate an UnsupportedEncodingException.
If the name is provided, you try to set it, but if you.
That’s the guts of what you’re doing in this recipe, but let’s see what beansservice.xml looks like.
You need to add definitions for the JavaMailSender and VelocityEngine dependencies to beans-service.xml.
The next listing shows what you need to add to beans-service.xml to achieve this.
For the JNDI lookup B, be sure to add the jee namespace.
You’ll also need to configure a JavaMail Session in your server.
The code download includes a sample Jetty configuration at sample_conf/jetty-env.xml.5 The mailSender bean C references that session.
If you’re paying attention, you’ll notice that the velocityEngine dependency in ContactServiceImpl is a VelocityEngine, but a quick look at the Javadocs for VelocityEngineFactoryBean reveal that it isn’t a VelocityEngine, either directly or indirectly.
Because of this, it isn’t itself injected into other beans (that’s why it (which is declared by the FactoryBean interface) is injected into other beans.
Also, because you want to set a bunch of properties on the ContactServiceImpl bean, you add the util:properties definition E and the corresponding util namespace.
If you don’t want to use JNDI to look up a mail session, you can configure the JavaMailSenderImpl directly.
Configure the application context so the two ContactService flags are true, and set the admin email address to one of your own.
The service should generate two emails: one sent to whatever email address you provide in the form (again, use one of your own) and one to the admin email address.
In this recipe you learned how to use JavaMail to send email and also how to use the Velocity template engine to create them.
In the next recipe, we’ll look at a simple but interesting enhancement to the email functionality you’ve just added.
The reason is that it takes a while to send the confirmation and notification emails, which you’re sending in a synchronous fashion on the calling thread.
But there’s no good reason to make the user wait, especially because email is an asynchronous form of communication.
In this recipe, you’ll see how to use the Spring Task Execution API to fork the calling thread when sending the emails.
Minimize the noticeable delay that the user experiences after submitting the contact form.
Keep in mind that when the user submits the form, you do two things.
First, you save the contact form data in the database.
You want to leave the database call on the calling thread because you need to be able to alert the user when problems occur.
For now a stack trace will suffice, but obviously you’d have a proper error page in a production application.) You want to move the emails to a separate thread.
The easiest approach would be would cause the database save to happen on the forked thread, which would mean the user would have no way to know if the save failed.
So a little refactoring is in order: the ContactServiceImpl will call a dedicated ContactMailSender component that does the mail sending, and you’ll wrap the ContactMailSender with the proxy.
That way, your service bean can do the database call on the calling thread and then fork the email send.
You’ll refactor ContactServiceImpl by splitting it into two pieces: a thin piece that saves to the database (by calling HbnUserDao) and a slightly larger piece that sends the email.
At B you inject the ContactService, which for you is a proxy.
At C you call the DAO on the calling thread, which means that if the call fails, the user will find out job asynchronously as you’ll see momentarily.
The next listing shows the ContactMailSender, which is the bean you want to proxy with forking behavior.
An alternative approach that turns out not to work would be to annotate the former directly to the bean rather than jumping outside the proxy and coming back in.
As you can see, you haven’t changed much as compared to when this code was in.
Fortunately the new configuration is easy, as we show next.
You declare the namespace B and its schema location C.
The definition at E creates a Spring ThreadPoolTaskExecutor, which is a convenient front end for a Java ThreadPoolExecutor that provides the threads.
Table 8.1 presents the configuration options; see the Javadoc for ThreadPoolExecutor for details.
Now you should be able to see the effect of the work you’ve done.
Start the server, submit the contact form, and pay attention to how quickly you receive the response.
Even after the response is complete, you should see in the server console output that the server is sending mail on a separate thread.
The original version of this recipe showed how to create an aspect to handle thread forking in a generic fashion.
Note that you may need to adjust your server’s security configuration in order to create threads for the thread pool.
This is because thread creation is an expensive operation, and some server configurations disallow it.
In the next recipe, we’ll look at mailing lists and how to implement the subscription process, which turns out to be interesting due to some of the security-related elements we need to consider.
Either a single integer or a range such as “5-10.” The lower end of the range is the core pool size: the size the pool tends toward as its steady state.
For marketing purposes, it’s often beneficial to provide a way for users to sign up for an email mailing list.
Because mailing lists are an opt-in communication, and because users generally understand that such lists serve marketing purposes, this can be an easy way to identify high-value customers and keep them up-to-date on company news, your products and services, helpful articles, and so forth.
Although many users prefer to subscribe to a news feed (we’ll discuss that in recipe 8.5), others are happy to provide their email address to a trusted site and receive email updates.
Another benefit of providing a mailing list is that it gives you a nice goal to track if you’re using analytics tools such as Google Analytics.
You can treat a confirmed subscription as a goal conversion, and that’s a way to gauge interest over time in your organization, site, products, articles and blog posts, and so forth.
This is similar to tracking feed subscriptions through a service like FeedBurner.
In this recipe, you’ll create a mailing list using Spring.
This recipe will handle only the subscription piece, but the sample code treats unsubscriptions as well.
Also, we’re not treating the topic of sending marketing emails to mailing list members.
We do cover confirmation emails, though, so you should have the tools you need.)
Users also need a way to subscribe to and unsubscribe from the list.
Subscriptions and unsubscriptions must require email confirmation to prevent attackers from falsely subscribing or unsubscribing people.
At first it might seem that mailing lists aren’t very interesting from a technical perspective.
But they’re reasonably interesting after all, owing to the requirement that you avoid false subscriptions (signing up victims to a bunch of mailing lists to spam them) and unsubscriptions (unsubscribing victims from mailing lists they care about)
You’ll create a MailingListService to handle subscriptions and unsubscriptions, although due to space limitations you’ll focus on the subscription case.
The code download includes code for unsubscription as well.) It’s a controller/service/DAO stack like the ContactService, but the implementation details are a little different.
Figure 8.6 User flow when subscribing to the mailing list.
In the happy path flow, the user first clicks the site’s Mailing List link, which presents a subscription form.
The user completes the form, supplying their name and email address, and submits it.
The response is a page that tells the user to go to their inbox and click the link in a confirmation email the site just sent.
There are some sad path flows as well, such as when the user takes too long to confirm the subscription.
You’ll begin at the top with the controller, which appears in the following listing.
Allowing users to subscribe to a mailing list HttpServletRequest request,
The controller handles three different requests: the initial subscription form, the form submission, and the confirmation.
Initial subscription form—For the subscription form, you have a method to serve up the initial form D.
There’s nothing to do other than put an empty Subscriber form bean on the model and return the logical view name.
You specify an actual value because the code download supports an unsubscriber bean as well, and specifying a value.
Inside the method is a standard form field whitelist C.
After that, Spring validates the request and calls the request handler method E.
You’re including the IP address mostly for your own information, but you’ll use the date to assign an expiration date to your confirmation emails, so you need to set it before saving it to the database.
After accepting the submission, you redirect to a preconfirmation page G that offers the user instructions on how to confirm the subscription.
If the call fails because the subscription request has expired or for some other reason, you deliver an appropriate failure page.
In the case of a failure, you set an appropriate model attribute to tell the JSP (and ultimately the user) what the problem was.
Now you’ll see the subscription form, which serves two separate roles in your mailing-list module.
Your subscription form appears, of course, at the beginning of the subscription process.
You also use it during the confirmation process when confirmations fail; for example, due to expiry.
The next listing shows the form and how it supports both roles.
To subscribe you will need to complete a new subscription request using the form.
If you copied the URL from your confirmation e-mail into the browser, please make sure you copied the entire URL.
Otherwise, you can complete a new subscription request using the form.
This form is similar to forms you’ve seen in other recipes, so we’ll go over only the two tests; if you want to see more, you can download the code from the book’s website.
The first test checks for the existence of an attribute called expired B.
You saw in listing 8.11 that you set the corresponding model attribute.
The second test checks for an attribute called failed C.
We’ll talk about the conditions under which a confirmation might expire or fail in the next subsection, which covers the service bean.
Before we examine the service bean code, let’s talk about a couple of security requirements:
Authentication—You need to prevent people from maliciously subscribing other people to your mailing list, generating what the victim might consider to be spam.
Essentially this is an authentication requirement: the subscriber must prove that he’s the one being subscribed.
Privacy—In cases where privacy is a concern, whether for regulatory or other reasons, it must be impossible for attackers to use the subscription process as a way to query the system for who’s on the mailing list.
The UI must respond to all subscription requests in exactly the same way (by telling the user to look for a confirmation email)
We’ll start with the authentication requirement; we’ll worry about privacy later.
Your tack is the standard one: you capture the subscriber’s email address and send them a confirmation email containing a link that, when clicked, activates the subscription.
The challenge is that you need to know that the person clicking the link is the person who subscribed to the mailing list.
A naïve approach might be to include in the link the subscriber ID as an HTTP parameter.
Clearly it’s too susceptible to guessing, especially if the subscriber IDs are drawn from a sequence.
A much better way to do this would be to generate a large random string, save it with the rest of the subscriber information, and include both the ID and the random string as HTTP parameters in the email link.
This approach makes it much more difficult for an attacker to guess the confirmation link.
But it turns out you can avoid the extra database column, which of course is better still.
What you do is inject a secret key into the service bean (the key should be random-looking and not prone to dictionary attacks), then concatenate that with the subscriber ID before hashing it.
The resulting string is called a digest, and you include the ID and digest in the email link.
For any given ID, attackers can’t compute the digest because they don’t know the secret key.
And from any given digest, attackers can’t uncover the secret key because hashes are one-way and the key is sufficiently obscure that it wouldn’t be found in a dictionary attack.
Yet by passing the ID and digest to the server, the server can determine whether the digest is correct because it knows the secret key that was used in generating the digest.
This is easy: all you have to do is timestamp the subscription request and check the timestamp when processing confirmation requests.
We hope that explanation makes some sense; but if not, that’s OK, because you have the code.
The next listing shows MailingListServiceImpl.java, which contains the core mailing-list service logic, including the authentication feature described.
Figure 8.7 Using a digest to authenticate subscribers without having to store subscriber-specific secrets.
The service bean so far includes two high-level functions: the ability to accept a subscription request and the ability to confirm a subscriber.
Here you take the unconfirmed Subscriber and save it in the database, and then.
Allowing users to subscribe to a mailing list send a confirmation email.
Note that you’re using the @Async annotation you saw in method, the idea being that this helps you meet the privacy requirement described earlier in the recipe.
If the subscription already exists, a constraint violation occurs; but the user never sees it because it’s on another thread.) It would be somewhat cleaner to emulate the approach from recipe 8.3 where the calling thread does the database access; in this case, you’d report any exception other than a duplicate subscription exception, which you’d suppress.
We covered the mechanics of creating an email in recipe 8.2, so we won’t rehash that here.
But as indicated, you need to create a secure link that the user can click to confirm their subscription.
To do this, you create a digest D and include that as part of a URL you create E.
In addition to sending confirmation emails, you process the result when a user the subscriber ID concatenated with a secret key (the key is arbitrary, and you inject it into the service bean) I, and the expiration period is set to one day J.
To save space, we won’t look at code listings for Subscriber.java and mailingListSubscribe.vm.
Neither file is particularly interesting for the purposes of this recipe.
If you want to see these, download the code from the book’s website.
Similarly, the database table mirrors the structure of the Subscriber class, so see the code download for that.
The Spring configuration extends the beans-service.xml configuration used in recipe 8.3
There’s little to add because you’re using annotations for component scanning and dependency injection.
The corresponding mailingListService.properties file looks like this (but feel free to use your own property values):
At this point you should be able to run the app.
Sign up for the mailing list, and work through the flow we described earlier.
The end result (once you confirm) should be that you have a record in the database with the confirmed flag set to true.
Mailing lists are a little more involved than they might initially seem, owing to the need to prevent malicious subscriptions and unsubscriptions.
The sample code shows how to do unsubscriptions as well.
They may not be able to click the confirmation link.
It helps to provide a non-hyperlinked version of the confirmation URL that the user can cut/paste into a browser.
Note that because the URL is long, it’s possible that it may line wrap, and the user may not copy/paste it correctly.
You can handle this by including explicit instructions in the email and by advising the user about potential causes in the event of a failed confirmation.
In recipe 8.4, you saw how to create a mailing list for your website.
Mailing lists are nice from the website operator’s perspective, because they involve collecting certain useful bits of information from your customers, such as their name and email address.
If you have actual user account information, you can even target the user based on demographics, behavior, stated preferences, and so forth.
But many users want to be able to keep up to date on your site without having to give you any personal information.
Users can subscribe to news feeds (for example, RSS feeds and Atom feeds) without revealing personal information other than, say, their IP address, user agent information, and whatever else the browser sends in the request headers.
This is yet another example of a growing trend in web-based business (and indeed business everywhere): the customer is king.
Publish a news feed from your website or web application.
Spring 3.0 introduces a set of View classes for publishing feeds.
These have their origin in the AbstractRssView class from the defunct Spring Modules project (http:// java.net/projects/springmodules/), but now the View classes have made their way into the framework proper.
Like the Spring Modules class, the new View classes are based on Sun’s open source ROME API (http://rometools.org/)
Here you’ll publish an RSS feed, which means you’ll be working with AbstractRssFeedView.
But note that there’s also an AbstractAtomFeedView for (yep) Atom feeds, and they share a common subclass in AbstractFeedView.
This allows you to keep the controller, service bean, and news items ignorant of the fact that you’re going to render the news items as an RSS feed.
We may as well start with the AbstractRssFeedView implementation, because that’s the core piece.
You implement the view, sans an @Component annotation (you want to configure this bean explicitly in beans-web.xml), by extending the AbstractRssFeedView class B.
You include a few metadata properties C so you can configure those externally when you build your feed D.
Besides building the metadata, you need to build the feed items themselves E.
You first grab the domain objects (here, a List of NewsItem objects, which is your own class) off the model F.
Then you iterate over the NewsItems, converting them to FeedItems that you add to a list G.
Note that you do an explicit null check to handle the case where the list of items is empty (which, as it turns out, means Spring can’t automatically discover the model key, which in turn means the corresponding attribute is null)
AbstractRssFeedView takes care of building the RSS Channel and serializing it to the servlet output stream.
You grab the List of NewsItems from the service bean and add it as an attribute to the Model B.
Publishing a news feed didn’t specify an explicit attribute name, Model applies a naming convention that generates the name newsItemList for this particular attribute.
As we hinted, if the list of news items is empty, Spring can’t autodiscover the model key, and so the news items won’t be stored under the desired key.
You handled this in listing 8.15 by doing an explicit null check.
As you’ll see in a moment, you’ll map that view name to an actual bean using the BeanNameViewResolver.
There isn’t any need to review the NewsItem, NewsServiceImpl, and other related classes because this shows how the RSS piece works.
If you want to see the other classes, consult the sample code.
We do need to look at the application context, though.
Then map the “news” logical view name to this view.
To do this, you’ll use the BeanNameViewResolver, which maps the logical view name to the bean having the same name (or ID), which is why you called your view “news” in the snippet.
Once you put the news back end in place, direct your browser to.
In this recipe, we’ve explored Spring’s support for feed publication.
We hope you’ll agree that Spring makes publishing feeds both easy and clean.
There are some useful websites you should know about if you’re going to publish an RSS feed.
One is www.feedvalidator.org, which will tell you whether your feed is valid.
Another is www.feedicons.com, which has numerous icons you can use to make your feed look great.
Once your feed is in place, consider fronting it with FeedBurner or a similar service.
FeedBurner acts as a proxy to your feed, serving at least two useful purposes.
You don’t have to worry about load issues, because subscribers hit FeedBurner, not you, and FeedBurner updates its cache of your feed or feeds only on a periodic basis (roughly every half hour, but we’ve observed quite a bit of variability in our server logs)
It’s useful to have visibility into that kind of data without having to write it yourself, which is more involved than you might guess because you have to account for multiple subscribers coming out of the same aggregator (Netvibes, Google Reader, and so forth)
We’ve come to the end of what we hope has been an interesting chapter on ways to keep in touch with your users and customers.
It’s a tried and true way of being successful on the web and in business, so it helps to know some of the standard options available.
In chapter 9, we’ll extend the present discussion to cover additional ways in which your users and customers can communicate not only with you, but also with one another.
Many websites and organizations are finding that building and taking advantage of community is an important way to understand user characteristics, likes, and dislikes; to generate content and traffic; and in general to generate value.
Examples include sites like Facebook (social networking), LinkedIn (career networking), Wikipedia (online encyclopedia), Digg (social bookmarking), DZone (tech-specific social bookmarking), Amazon (retail), Pinterest (photo sharing), and YouTube (video sharing)
One important way to create a sense of community is to allow users to submit comments.
The comments might target articles, blog posts, user submissions, products, or anything else where it’s useful to respond to items in a public fashion.
It will be generic in that it won’t care what kind of item serves as a target.
A striking characteristic of several successful modern websites and web applications.
Finally you’ll write tests that deal with the security issues that rich-text editing raises.
Allowing users to comment on articles, blog posts, products, other users, and so forth is useful in many ways.
If you’re trying to build a sense of community, there’s nothing like seeing users out and about, speaking their minds.
That can be helpful if you’re trying to choose the topic of your next article, or if your users are trying to decide which laptop to buy.
And giving your community a voice will keep your users coming back.
Comments can also help generate traffic by improving search-engine rankings, tying your site into a larger community of related sites, and making your pages more attractive link targets by providing a well-balanced view of the subject under discussion.
You’ll build it in a generic way that allows you to use it for arbitrary targets, whether articles, products, or something else.
Allow users to read and submit comments for articles, blog posts, products, and so on.
For now the comment engine should support only plain text comments, not rich text (but see recipe 9.3)
Once a user posts a comment, the engine must notify the site admin by email.
Figure 9.2 shows the beans you’ll use and how you’ll wire them all up.
Again, in this recipe you’re building the comment engine, not integrating it with.
One key goal for the comment engine is that it be agnostic with respect to the sorts of targets (articles, products, and so on) that comments can attach to.
You’re building a generic comment engine, not one for any particular target type.
Figure 9.1 The comment engine, with a comment list and a place to enter comments.
This shows what the comment service needs to filter comments and send notification emails.
Figure 9.3 shows a two-table E/R diagram for the database schema.
You need a way to attach those to an arbitrary target, but you don’t want to use a foreign key to something specific like an article; instead the generic comment_target table represents anything you might want to comment on.
See schema.sql in the code download for the DDL for this pair of tables.
Now let’s see the Comment and CommentTarget domain objects that correspond to the two tables.
You’re using it primarily as a domain object representing an individual comment, although you’ll also use it as a form-backing bean when you get to the Spring Web MVC part in recipe 9.2
This simple schema allows you to attach comments to arbitrary targets.
As noted, Comment performs double-duty as a domain object and a form-backing bean.
The various annotations reflect the dual roles: you have JPA annotations to store Comments in the database, and you have JSR 303 Bean Validation annotations to validate user-submitted form data.
At B you define Comment as implementing the Comparable interface.
This is because you’ll want to sort comments by date for display purposes.
The next listing contains CommentTarget.java, which represents an arbitrary target against which users can create comments.
Listing 9.2 CommentTarget.java, representing anything that can be commented on.
CommentTarget provides a way to group comments associated with a single target.
Although in theory you could link comments to their articles, products, and other targets directly, the problem is that you want to keep the comment engine generic.
You have a list of comments B with cascading persistence operations, meaning that if you save, update, or delete (there are some other operations as well) a CommentTarget, Hibernate to mark individual Comments for deletion when they’re removed from their respective CommentTargets.
You do this because individual Comments don’t have their own lifecycle and don’t have shared references.
Now it’s time to check out the service bean for the comment engine.
The simple comment service is responsible primarily for saving user comments.
Finally, the method sends an email using the CommentMailSender E.
Those details are of course available in the code download.
You’re assuming the client will already have a transactional context in place if one is necessary.
In this because you wouldn’t want it unnecessarily extending the transaction’s duration.
You’ll see more about PostCommentCallback in recipe 9.2, because that’s where you’ll link the comment engine to a simple article-delivery system.
For now you’ll use a simple text-filtering strategy, treating user input as plain text using the filter in the following listing.
This isn’t the most time-efficient implementation of plain-text filtering, but it’s simple and easy to speed up should you need to do that.
You need to provide a couple of JSP fragments as well, to make it easier for client services to display comment lists and the form for posting new comments.
The next listing presents a JSP fragment for displaying a comment list.
We’ve simplified the HTML to streamline the code; please see the code download for the full version.
You loop over each of the comments, using <c:out> at B and C instead of JSP EL to ensure that you escape user-provided data appropriately for HTML.
This helps prevent search engines not to follow the link.
In addition to viewing a comment list, users need a way to post comments.
The next listing contains the form for posting a new comment.
We’ve suppressed parts of it here; see the code download for the full version.
We covered what you’re doing here throughout chapter 4, so refer to that if anything application to supply this URL so it can process posts as necessary.
You need to do some bean configuration for the comment engine, as shown next.
This configuration should be easy to follow given what we’ve already discussed.
First, you externalize your environment-specific properties into an environment.properties file B, and you use them to define a comment-notification email message C.
You use the task namespace D to activate the @Async annotation and define a thread pool to send email asynchronously.
At E you scan for the comment service and the mail sender, and at F you create a plain-text comment filter for the comment service to use.
Let’s discuss the comment engine briefly and then see how to use it.
The design is such that you can use it for multiple purposes in a single application; it’s not tied to articles, product reviews, or anything else.
Having said that, the comment engine doesn’t provide any value unless you integrate it with some other service.
By using the PlainTextFilter strategy for text filtering, you’ve taken the draconian step of preventing cross-site scripting and user-generated nuisance issues by disallowing HTML markup.
But obviously there are plenty of situations in which you want to be more flexible.
Integrating the comment engine with an article-delivery service then you probably want a nice way for users to submit formatted code, and that probably involves <pre> tags.
In short, although it’s true that you want to minimize your exposure to security issues, you don’t want to throw the baby out with the bath water.
In many cases, you’d like to offer the end user the ability to enter rich text while simultaneously limiting security issues.
The comment engine you created in recipe 9.1 is generic, meaning you can use it to add comments to targets such as articles, blog posts, and products.
But to use it, you have to integrate it with a relevant service.
In this recipe, you’ll create a basic article-delivery engine and hook the comment engine up to it.
We won’t get into the details of the article-delivery engine because the point is to show how to use the comment engine, and the articles are incidental to that goal.
If you’re curious about those details, please look at the code download for the chapter.
Also, chapter 12 revisits article delivery with a focus on different back-end strategies.
You’ll need to update the Jetty configuration in this recipe for Maven, including specifying the username and password for the MySQL database.3
You have an article-delivery engine (or some other such service) that doesn’t support user comments.
You’d like to integrate it with the comment engine from recipe 9.1
Because the purpose of this recipe is to show how to integrate the comment engine with a service you already have, you’ll be minimalistic with respect to the service.
That way we can keep the discussion focused on the integration, and we can minimize the number of assumptions we have to make about how the service works.
For the sake of discussion, we’ll assume you have an article-delivery service; but we intend this recipe to be more generally useful than that.
Let’s begin by considering the bean-dependency graph in figure 9.4
This graph builds on figure 9.2 by adding article-specific context.
Now you have a DAO, a service bean, and a controller to support article delivery.
Specifically, you inject the comment service into the article service.
Next let’s look at the slightly revised E/R diagram in figure 9.5
It’s the same as figure 9.3, but it adds a couple of tables to support articles.
The schema is designed to support a 1-1 relationship between articles and comment targets in the domain model.
If you look closely at the diagram, you’ll see that you support this relationship by defining a foreign key from the article table to the comment_target table.
Although you certainly could do that the other way around, that would make the comment engine aware of articles (the comment_target table would know about the article table), which would in turn undermine the goal of keeping the comment engine generic.
So, the article table references the comment_target table instead of the reverse.
You’ll need to create an Article class and integrate it appropriately with CommentTarget.
Figure 9.5 E/R diagram for the comment engine, with an article table added.
You connect the tables using a foreign key in the article table that points at comment_target.
We’ll suppress the details (including the related ArticlePage class), focusing instead on the integration with CommentTarget, which is the real goal.
The Article class has a getter B and setter for the commentTarget property, but notice that you make them private.
Instead, you want them to be able to method C.
This listing contains the DAO you use to save both articles and their comments.
Listing 9.8 Article.java: an example of something that can be commented on.
The only operations relevant to the current purpose are the ones defined in AbstractHibernateDao.
The finders mentioned support article delivery and aren’t related to comments.) Also, because you’ve specified cascading persistence operations need separate DAOs for CommentTarget and Comment.
Instead you add the comments to the article and use the ArticleDao to save them.
See the code download for the interface.) It isn’t part of the comment engine; it merely shows how to use the comment engine.
This is entirely legitimate; CommentService is a lower-level service, and it often makes sense for higher-level services to call lower-level services.
With respect to the comment engine, the expected use is for target-specific services (ArticleService, BlogPostService, ProductService, and so on) to delegate most of the work of posting comments to CommentService.
As you probably know, by default Hibernate loads collections lazily.
Although it’s possible to configure Hibernate to load any given collection eagerly, you mostly want the collection to be loaded lazily except in certain use cases.
For example, when you load the master list of articles, you don’t want to load all the comments, because you aren’t displaying those with the article list.
But when you load a single article, you do want to load the comments.
There are different techniques to accomplish this with Hibernate, but one of the nicer ones.
With Hibernate, there are various ways to specify whether a given association should be lazily or eagerly loaded.
One way is to specify in the configuration that such-andsuch association should always be lazily loaded, or else always eagerly loaded.
That takes care of the article service bean and the bulk of the integration.
The following listing shows the single controller, which handles article-related requests.
We’ve suppressed code that isn’t directly related to the comment engine, so if you want to see the whole thing you can check out the code download.
When loading a master list of articles, you don’t want to load the comments at all because you don’t display them in that context.
On the other hand, when you load a single article, you do want to load the comments.
This is a servlet filter that binds a Hibernate Session to the request-processing thread for the duration of the request.
If the JSP needs to display something (say, a comment) that hasn’t already been loaded, the Session is available and the data is loaded just-in-time.
We don’t care for this approach, because it often triggers lots of database queries if you’re not careful.
Imagine, for instance, that you were to display comment counts with the master list of articles.
The OpenSessionInViewFilter approach would generate a database query for each article.
Our preference is to define the association-loading behavior on a per-service-method don’t necessarily load the comments, whereas a call to getArticle(Long id) definitely does load the associated comments.
This approach, which involves using some argue that it couples the service too tightly to the UI in that changes to the UI might necessitate changes to the service.
This doesn’t bother us because we don’t find it surprising that adding new data elements to a UI might require support on the back end.
You require controllers to set the IP address before posting the comment C.
Let’s pull together the comment list and comment form in a JSP.
The next listing shows a JSP that presents an individual article page.
We’ve suppressed most of it, showing only the part that’s relevant for displaying comments.
At B you show the list of comments, and immediately after that you show the form for posting a new comment C.
To make it all work, you’ll need a few Spring configuration files: beansresources.xml, beans-service.xml, and beans-web.xml.
Because none of those illustrates anything you haven’t already seen, we won’t cover them here; see the code download.
You learned here how to integrate the generic comment engine from recipe 9.1 with an arbitrary service, such as an article service.
You did that using the template method pattern coupled with Spring’s support for programmatic transaction management.
In recipe 9.3, you’ll add pizzazz by introducing support for rich-text editing, courtesy of the RequireJS and PageDown JavaScript libraries.
You’ll see other goodies as well, including server-side JavaScript via Rhino and Spring’s support for incorporating external resources (such as JavaScript files) into your apps.
Recipe 9.2 Integrating the comment engine with an article-delivery service.
In recipe 9.1, we looked at creating a simple comment engine.
One basic requirement with comment engines is to prevent XSS attacks.
The simplest way to do this is to prevent users from entering any markup, but in many cases that isn’t an acceptable approach.
A lot of times users need to be able to post links, images, code snippets, and so forth, and yet the requirement to prevent XSS doesn’t go away.
In this recipe we’ll show how to support rich text in the comment engine while simultaneously guarding against XSS.
Support XSS-free rich text and content in the comment engine.
You’ll do a number of fun and interesting things in this recipe.
Principally you’ll use the PageDown4 rich-text editor from the folks at Attacklab and Stack Overflow to.
What is cross-site scripting (XSS)? XSS is a technique for attacking vulnerable web-based applications.
Typically an attacker enters an HTML <script> tag into a web form, and the app redisplays it without filtering it.
Most of us are potential targets because we allow JavaScript to run in the browser.
Despite the name, XSS also refers to attacks that inject malicious nonscript HTML.
This could be anything from adding unmatched HTML tags (thus potentially messing page) to creating malicious iFrames.
Although web forms are the typical attack vector, they’re by no means the only one.
Any userprovided input you display onscreen is a potential candidate for an XSS-based attack.
We won’t go into the syntax of Markdown here because that’s available on the web and because you can easily discover it by using the button bar interface.
Instead let’s preview the overall technical approach for this recipe:
In the web browser, you’re running a few PageDown scripts: Markdown.Editor.js (presents a rich-text editor), Markdown.Converter.js (converts Markdown to HTML), and Markdown.Sanitizer.js (cleans up the HTML and guards against XSS attacks)
Users can create Markdown with the UI controls or enter it directly.
When the user submits the form, you send Markdown to the server, not HTML.
First, if there’s any kind of validation error, you need to be able to prepopulate the <textarea> with the user’s original Markdown, not with HTML.
Second, if you wanted to support user edits after the comment had already been saved, you’d want the user to be able to edit the Markdown rather than the HTML.
Once a valid form reaches the server, you use server-side JavaScript (via Rhino) to convert the Markdown to HTML.
Specifically you use RequireJS (http:// requirejs.org/) to set up a CommonJS (www.commonjs.org/) environment for PageDown, and then use PageDown (Markdown.Converter.js and Markdown.Sanitizer.js) to convert it to sanitized HTML.
You save both the Markdown and the HTML to the database.
Again, saving the Markdown makes it possible to support user edits, and saving the HTML allows you to avoid retranslating Markdown to HTML every time you display a comment.
Figure 9.7 Bean-dependency graph for the version of the comment engine.
Here the text filter supports rich text—specifically, Markdown (which allows embedded HTML)
We’ll discuss that shortly, but first let’s see the updated application context configuration.
In the listing you grab a couple of JavaScript resources off the classpath and inject them into the RichTextFilter B.
The rich-text filter, RichTextFilter, converts user-entered Markdown to appropriately filtered HTML.
You do this using Rhino, a helper object called JsRuntimeSupport (you’ll see it in a bit) and a couple of scripts.
JavaScript resources, and r.js adapts it to the Rhino JavaScript engine that ships with Java 6
You use RequireJS here because PageDown needs a CommonJS environment to run on the server side.
The sample code contains the original and modified PageDown scripts at src/main/resources/pagedown.
The modified scripts wrap the originals in a function to allow them to run correctly under RequireJS.
Thanks to user sperumal on Stack Overflow for assistance in getting RequireJS to run on Rhino.
At E you do the same thing with a custom convert.js script, which loads the Markdown.Sanitizer.Modified module, converts the Markdown into sanitized HTML, and places the result in the global html variable so you can return it at F.
You don’t have to load the Markdown.Converter.Modified module explicitly, because Markdown.Sanitizer.Modified loads it for you.
The next listing shows the JsRuntimeSupport class that we mentioned.
Start the app, and point the browser at http://localhost:8080/sip to try the new editor.
Otherwise, use the button bar on the editor to create Markdown.
Rich text offers important features, such as the ability to style text, add images and links, and so forth.
But such features carry their own risks, because they allow ill-intentioned parties to create XSS if you’re not careful.
To that point, it’s important from a security perspective that you test the filter.
The next recipe shows how you can use JUnit and Spring’s TestContext framework to get a handle on this task.
Whether or not you’re a proponent of test-driven development (TDD) or even unit and/or integration testing in general, there’s no doubt that in some cases it makes a lot of sense to write such tests.
One case is when you’re writing code that’s addressing security needs, such as the HTML filter.
Write tests to ensure that the comment engine filters out potential XSS attacks.
You’ll get a slight jump on the content from chapter 10 and implement the first integration test here using JUnit and the Spring TestContext framework.
The rich-text comment engine is exactly the sort of place where testing is critical, so we’ll show how to do that here.
It should be easy to follow, but if you have questions, continue on to chapter 10
The next listing presents an example of what an integration test for RichTextFilter might look like.
This isn’t a full-blown test, but it gives you an idea of what would be involved in developing a more comprehensive test.
In the listing we’ve suppressed most of the test cases (even in the code download, the cover is thin), but we’re focusing on how to conduct the test.
The main issue for integration-testing beans is that you need access to the Spring application context so you can inject any necessary dependencies.
Recall from recipe 9.3 that convert.js loads Markdown.Sanitizer.Modified, which in turn loads Markdown.Converter.Modified.
Ultimately these latter two give you the conversion and sanitation that you want to see.
At B you specify the locations for your Spring app context configuration files.
You’re using src/main/resources/spring/beans-service-richtext.xml, which is a simplified version of the beans-service.xml configuration from recipe 9.3:
Note that this involves moving the filter definition out of the existing beansservice.xml file into beans-service-richtext.xml and updating web.xml to reflect the new beans-service-richtext.xml file.
At C you extend AbstractJUnit4SpringContextTests, a support class for the ApplicationContextAware interface.
At D you get the filter bean under test from the application context.
Finally, at E you verify that the filter converts and sanitizes Markdown input the way you expect.
The test in this recipe is an integration test because you’re not testing the RichTextFilter: you’re testing how it works in conjunction with several key external scripts.
Usually you want to run unit tests and integration tests separately from one another.
In a continuous integration context, you generally want to run unit tests (perhaps with a handful of strategically chosen integration tests) to get fast feedback about whether you broke the build.
Integration tests, because they access external resources, are much slower than unit tests, and you should typically separate them.
Here you’ve added the test to the src/test folder, which is where you also place your unit tests.
Also, you haven’t used many of the features of the Spring TestContext framework, such as the ability to run any given test in a transaction that you roll back at the test’s conclusion to maintain a known test data baseline.
The following chapter explains how you can segregate integration tests from unit tests, and how you can take advantage of some of the more advanced features of the Spring TestContext framework.
In this chapter we’ve taken a detailed look at implementing a rich-text comment engine.
In the course of doing that, we explored RequireJS, PageDown, Rhino, and Spring resources.
You also learned how to use Spring’s TestContext framework to remove XSS vulnerabilities from your rich-text filter.
Dependency injection is particularly useful in unit testing, where you desire to test small units of code in isolation from other code.
With dependency injection you can inject mock dependencies with prescribed behavior into code units, which allows you to isolate faults to the code under test.
The idea here is to test collaborations between units of code.
An integration test can be fairly narrow, perhaps involving a single collaboration between a web controller and a service bean (with DAO dependencies mocked out)
At the other extreme it can involve deploying the code under test to a servlet container (or other container), sending HTTP requests and parsing HTTP responses for the expected output.
This chapter presents Spring’s TestContext framework, which offers features useful for both unit and integration testing.
Here we’ll focus on integration testing because the features in this area are especially powerful.
Ideally, testing is a core activity of the practicing software developer, and it so happens that the dependency-injection approach to software design is especially useful.
You typically want to be able to run the tests during project builds, so you’ll see how to do that with Maven.
Concerning the test framework itself, you have two options: JUnit and TestNG.
Although Spring provides support for TestNG tests, you use JUnit here because the key Spring TestContext features don’t depend on the underlying test framework, and JUnit is probably more familiar to most readers.
You’ll build integration tests for the simple contact-management application that’s available as part of the code download for this book; see figure 10.1
We won’t present any of the code for the sample app because it’s the test code we’re interested in, but again you may download the code if you wish.
Your first task is Maven setup, mostly around declaring plug-ins and dependencies.
Maven is one of the more popular build-automation tools in the Java world, and as you might expect, it supports integration testing.
Unfortunately, its out-of-the-box configuration leaves a couple of things to be desired where integration testing is concerned:
Although Maven’s default lifecycle includes an integration-test phase, by default no goals bind to it.
You could place integration tests and their associated resources in the src/test/ java and src/test/resources folders, respectively, but it would be nice to separate integration tests from unit tests more cleanly.
Figure 10.1 A simple contact-management application for which you’ll write integration tests.
This is a Maven recipe rather than a Spring recipe, but you need it to set the stage for the recipes that follow.
Configure Maven to run integration tests, keeping integration tests separate from unit tests.
Before diving into the specifics of the Maven plug-in configuration, it will be helpful to review the phases of the default Maven lifecycle, which represents something like a standard build.
We’ve taken the liberty of collapsing closely related phases into a single box on the diagram to improve clarity.
We don’t need to peer too closely at the details (this is after all a Spring book), but suffice it to say that the phases provide hooks onto which you can hang additional goals related to integration testing.
Let’s start by creating new source and resource folders for your integration tests.
Your first task is to create dedicated source and resource directories for your integration tests to keep yourself from getting them mixed up with unit tests.
To accomplish this, you’ll use the Build Helper Maven plug-in as illustrated in the following listing.
You’re placing this in the top-level project object model (POM) because you want to enable integration testing across all projects.
Normally there is a single test source directory (src/test/java) and a single test resource directory (src/test/resources)
In effect what you’re doing in the previous listing is adding a second test source directory and a second test resource directory.
At B you add the src/it/java (“it” stands for integration test) test source directory to the build.
This is where you’ll place the Java code for the integration tests.
You accomplish this small feat using the plug-in’s add-test-source goal.
By default the plug-in binds this goal to the generate-test-sources lifecycle phase, and you don’t have any reason to change that.
This time you’re adding the src/it/ resources test resource directory to the build.
Listing 10.1 Adding new source and resource directories to Maven.
In this case you rely on the plug-in’s add-test-resource goal, which this time is bound to the generate-test-resources phase.
Build Helper has indeed helped your build by allowing you to keep your project structure nice and tidy.
The next concern is how to go about running your integration tests.
Rather than using the standard Surefire Maven plug-in (which is designed for unit tests), we’ll turn to the Failsafe Maven plug-in (designed specifically for integration tests)
Before going over the Failsafe configuration, it will help to understand the difference between Surefire and Failsafe.
You might reasonably ask why you can’t run your integration tests using Surefire.
The major difference between the two is that Surefire fails the build immediately when a unit test fails; Failsafe allows the build to proceed to a cleanup phase.
With unit tests, there’s no general environmental setup (there are only test fixtures specific to individual test cases, and these don’t reference external resources), so there’s nothing to tear down if a test fails.
But with integration tests, there may very well be a general environmental setup, such as building a clean test database or perhaps deploying a package to a remote server.
If an integration test were to fail, you’d want the build to perform the cleanup before ending the build.
That background should help you understand better how Failsafe works, so let’s look more closely.
In figure 10.2 we offered a high-level overview of the default Maven lifecycle, but we’ll zero in on the specific phases we care about for integration testing.
Maven doesn’t distinguish between unit-test, integration-test directories The new test source and resource directories are nothing more than an attempt to make the project structure more developer-friendly.
We happen to prefer keeping unit tests and integration tests separate, and that’s what the Build Helper Maven plug-in is helping you do.
Note that from Maven’s point of view, you’ve added new test source and resource directories.
Maven has no idea that you intend to store unit-test goodies in one pair of directories and integration-test goodies in the other.
First, the test-compile phase compiles all the test code at once, whether it’s unit-test code or integration-test code.
Second, to run unit tests and integration tests separately (and that is a big deal, because you typically want to run unit tests with every build but not integration tests, because they’re slower), you need some way to distinguish unit-test cases from integration-test cases.
An integration-test goal, binding by default to the integration-test phase, that runs the integration tests.
A verify goal, binding by default to the verify phase, that verifies that the integration tests passed.
The Failsafe plug-in doesn’t bind anything to either the pre-integration-test or the post-integration-test phase.
You won’t need to do anything during those phases for the purposes of this chapter because you’ll handle environment setup and (as needed) teardown as part of the integration tests.
The following listing shows how to deploy the Failsafe plug-in.
There’s not a lot to say here, but one thing to point out is that because you’re using SpringSource’s Enterprise Bundle Repository (EBR), you need to help the Failsafe plug-in deal with the fact that the EBR renames the JUnit artifact (or TestNG if you’re using that)
We mentioned in the earlier sidebar that Maven doesn’t distinguish unit-test directories from integration-test directories.
Listing 10.2 Binding integration test goals to the default lifecycle.
Figure 10.3 The key Maven lifecycle phases for integration testing.
Here you rely on filename conventions that the Surefire and Failsafe plug-ins use.
Basically, if a test case’s filename matches a Surefire pattern, then Surefire will run it; and if it matches a Failsafe pattern, then Failsafe will run it.
Table 10.1 lists the default patterns for Surefire and Failsafe.
You can change these patterns in your plug-in configuration, but here you’ll go with these.
For integration tests, you’ll use the **/*IT.java pattern for no special reason.
You need to do only one more piece of setup: declare test dependencies.
The plug-ins you just installed take care of the Maven integration-testing infrastructure, but you need to include integration testing dependencies as well.
You’ll learn more about these dependencies in the upcoming recipes, but we can make a few high-level remarks here.
JUnit C is a unit-testing framework, but it’s also useful for writing integration tests.
Mock objects are especially useful for unit testing, but sometimes you need mocks in integration tests too.
In this recipe, you’ve prepared your Maven configuration for the integration-test recipes that follow.
Although it’s possible to run integration tests outside of a build context, in practice they’re almost always included as part of a build, and that’s what we’ve shown how to do.
Recipe 10.2 builds on this base to create simple but nontrivial happy-path integration tests involving transactional access to a live test database.
Among the simplest sort of integration tests are those that test for routine, nonexceptional behavior, often called happy-path behavior in testing circles.
This might involve, for instance, requesting an object from a web-based interface, having all the back-end transactional magic happen (for example, hitting a database), and then verifying that the returned result is what’s expected.
Because this type of test forms the basis for more sophisticated tests, it makes a good starting point, so we’ll explore happy-path integration testing in this recipe.
Write happy-path integration tests to verify correct integration between web, service, DAO, and database components.
As a general statement, integration testing involves selecting large vertical slices of an application’s architecture and testing such slices as collaborating integrated wholes.
Ideally you’re able to reuse as much of the app’s native configuration as possible, partly to minimize code duplication, but more fundamentally to put the configuration to test.
It is, after all, part of what makes the app work.
In the normal situation, you fall short of that ideal because you don’t want to run integration tests against live production databases.
If you can identify the relevant slices and make it easy to choose between the test and production databases, you have a winner.
We’ll begin with details of how to implement the strategy just outlined using Spring.
Spring’s approach to configuration makes it easy and elegant to do this.
In figure 10.4 we highlight the part of the stack we’ll address with our approach to integration testing.
Although you don’t quite hit 100% of the stack (you’re excluding the DispatcherServlet and JSPs from the scope), what you get represents a reasonable.
As illustrated, the stack starts from the controller and pushes all the way back to the database.
It bears repeating that you can write integration tests that are more aggressive about coverage—tests that include the DispatcherServlet and JSPs, for example.
And in the case of the DispatcherServlet, there are strong reasons for doing so, among them the desire to verify that controller annotations (@InitBinder, do.
But you’d take a hit for expanding that coverage, either by making the testing more complicated (you’d have to provide the DispatcherServlet configuration, which is more involved than controller configuration) or by slowing down the execution (for instance, by running the tests in-container)
This chapter opts for the stack shown in figure 10.4, although it’s useful to know that other options are available.
Now that you’ve identified the stack, you need to figure out how to implement the wiring only one time.
You’ll reuse that wiring across both normal app use and integration test contexts.
First, figure 10.5 shows the application’s bean-dependency diagram, using the normal data source.
In essence, two things differ between the app’s normal configuration (the one shown in figure 10.5) and its integration-testing configuration:
You want to carve off the DataSource definition from the rest of the configuration and select a definition based on the context.
When all is said and done, you want the integration-test configuration to look like figure 10.6, and you want to factor out as much of what’s common to the main and integration-test configurations as you can.
The case for testing JSPs in this fashion is, in our opinion, somewhat weaker because view components change more often and are thus more prone to cause spurious test failures.
In our opinion the approach we present here is more straightforward.
Figure 10.4 You’ll write integration tests for the stack that start from the controller and go all the way back to the database.
This is the normal app configuration, not the integration-test configuration.
The DataSourceInitializer sets up the test database by running DDL and test data DML.
The controller, service, and DAO are suppressed because they’re exactly the same as in figure 10.5
For normal app execution, the app uses the bean configuration shown in the next listing.
That’s just the DataSource lookup; you configure the DataSource via whatever container-specific means the container makes available.
The sample code uses /WEB-INF/ jetty-env.xml to configure the DataSource.
The application pulls this configuration into the fold using the normal contextConfigLocation means available through web.xml.
You’ve seen that configuration so many times by now that we won’t repeat it here, but look at web.xml in the sample code if you’d like to see it again.
You don’t have a JNDI environment available, so you need to both build and configure a DataSource, as shown in listing 10.5
At B you observe the standard practice of externalizing volatile configuration such as username/password information.
Next you build a BasicDataSource C using that configuration, instead of performing a JNDI lookup, because you aren’t in a Java EE container environment.
Next is something new for which there’s no counterpart in listing 10.4
At a high level, this part of the configuration D resets the test database to a known state, which you obviously desire in order to have predictable and repeatable testing.
The Spring 3) causes Spring to run the referenced SQL scripts in the given order against the referenced DataSource whenever you load this application context, typically just attribute says that if a script attempts to drop a table and fails (perhaps because the table doesn’t yet exist), continue running the script.
You haven’t yet seen the integration-testing counterpart to web.xml for specifying the configuration files you want; that comes when you write the integration test case (listing 10.8)
But before you do that, let’s look quickly at the SQL scripts you’re using to reset the test database prior to running the integration tests.
The next listing contains simple test data that you’ll use to populate the contact table.
The DDL script drops the table and recreates it, which should provide a solid reset.
The DML script feeds the table with test data that you can use for your integration testing.
In the following subsections, you’ll write three separate happy-path integration tests, demonstrating different key capabilities of the Spring TestContext framework.
The first is the simplest test, which involves asking for a specific contact and verifying that you got the information you expected to get.
The bare-bones sample contact-management app is a master list of contacts with editable details pages corresponding to individual contacts.
All you have is a name and an email address.
Your first integration test tests (the next listing) this contact details page feature.
Figure 10.7 A details page prepopulated with subterranean, homesick contact data.
The first thing to notice in the listing is that the test case class is an annotated POJO E.
The test execution environment (Failsafe, for instance) uses this class to.
We’ve chosen SpringJUnit4ClassRunner, which is effectively how you activate the Spring TestContext framework.
Figure 10.8 shows that Failsafe runs tests using the JUnit API and the Spring TestContext framework implementation of the JUnit Runner.
Loads the test context (that is, bean definitions) from locations you’ll specify, reusing most of the application context configuration files.
Injects test fixture instances, and resolves any Spring EL expressions.
Caches the test context between test executions unless otherwise directed.
Wraps tests in transactions as directed, typically rolling back the transaction.
ReflectionTestUtils allows test cases to inject dependencies into private fields, much as autowiring would do.
At C you use the @ContextConfiguration annotation to tell the test-execution environment where to find the bean configuration files.
Notice that you’re getting the DataSource from beans-datasource-it.xml instead of beans-datasource.xml, thanks to the work you did earlier in separating the DataSource configuration.
This is the test case counterpart to the contextConfigLocation parameter from web.xml.
A path starting with a slash is treated as a fully qualified class path location, e.g.: "/org/springframework/ whatever/foo.xml"
A path which references a URL (e.g., a path prefixed with classpath:, file:, http:, etc.) will be added to the results unchanged.
At D, the @Transaction annotation indicates that you want to wrap each test with a transaction.
That way you can roll back any database mischief you create at the end of each test.
You’ll reuse the transactional machinery from the application’s bean-configuration files.
You might be seeing at this point why the TestContext framework is nice to have around.) If you wanted to specify a custom PlatformTransactionManager bean name (the default is transactionManager) or turn off the default rollback-ontest-completion behavior, you could add a @TransactionConfiguration annotation here, but you don’t want to do either.
We covered E, so let’s start looking in the class.
It’s not just the controller under test, but rather the entire stack under the controller.
The controller gives you something to poke and prod, and you can watch it to see what happens.
The framework resolves the EL at G to give you an expected view name.
The test setup method creates a request and a model.
With all that test-case setup (whew!), you’re finally ready to write your first happypath test.
You exercise the code by asking the controller to get the contact with ID 1, put it on the model, and return a view name J.
At 1) you verify that the expected view name matches the actual view name.
Before you can run the tests, you need a test database.
After that you can run the tests from the command line as follows:
You launch the verify phase rather than the integration-test phase because you want to ensure that you run any cleanup goals bound to the post-integration-test phase.
In this case nothing is bound to post-integration-test, but it’s a good habit to launch integration tests using verify anyway just in case that phase happens to be performing cleanup.
This last subsection was a lot to digest, so please be sure to review as necessary before moving on.
If you’re ready, you’ll do a slightly more interesting happy-path test, this time updating a contact.
Updating a contact is what you do when you click the Save button on the form in figure 10.7, so it’s the logical next test.
The following listing shows an updated version of ContactControllerIT.java, suppressing code you’ve already seen.
The test for updating a contact looks a lot different than the test for getting a contact, and it is.
But that’s not the part that looks different, because you can’t see that at all.
The framework handles that transparently, which is one of its major selling points.
The part that looks different is that you have to deal with some complexity that Hibernate adds.
The basic idea is that when you update the controller, this updates a service bean, which updates a DAO, which then updates a Hibernate session.
For optimization purposes, Hibernate doesn’t flush every change it receives immediately to the database.
It collects changes and flushes them either automatically at appropriate points or manually on demand.
The integration test deals with all of this, as you’ll see.
Figure 10.9 The TestContext framework automatically rolls back transactions after each test completes, ensuring that the test database is in a clean state for the next test.
In the setup, you create a new SimpleJdbcTemplate using an injected DataSource B.
You’ll use this to verify database changes directly instead of relying on Hibernate’s report, because Hibernate session state is somewhat decoupled from database state.
You must simulate an update request from a web client.
To do this, you create a Contact C with the submitted update (here, the original first name Robert is being updated to Bob), create a dummy BindingResult to keep the under test E.
Once you’ve exercised the method, it’s time to verify the result.
You verify the view name F, and then you want to determine whether you were successful in updating the first name.
Checking with the controller G, it will appear (on running the test) that the answer is yes.
But you can use SimpleJdbcTemplate to determine whether the change made its way all the way back to the database, and the answer will be no H.
You’re still in the middle of a transaction, and Hibernate hasn’t flushed the change to the database yet.
You can flush the change manually I and check the database again J.
Normally you wouldn’t write the first database check (the check to verify that the change didn’t make it), although it doesn’t hurt anything.
But we wanted to show how Hibernate works and how you’ll need to flush the session and use the SimpleJdbcTemplate if you want to ensure that your code did what it was supposed to in the database.
This time, let’s delete the contact you’ve been working with.
In the following listing, you test the deletion of the good Mr.
By now you’ve seen this a couple of times, so we’ll blast through it.
You start with a quick check to make sure the contact you’re about to delete (it’s Bob Dylan again) exists B.
Then you exercise the code C and verify the view name D.
Next you try to get the contact from the controller E.
You expect a ResourceNotFoundException; you fail the test if you don’t get one.
Finally, you run through the same JDBC routine where you flush the session, then verify that the contact is removed from the database F.
Once again, you didn’t have to check the database twice; you did that in this case to demonstrate that the flush is required.
Even though we focused on the basic happy-path integration test, you learned how the Spring TestContext framework supports test database resets, configuration reuse, dependency injection of test fixtures, transactions and rollbacks, mocks, assertions against database state, and more.
Fortunately, this basic training was the most grueling you’ll see.
In the following recipes, you’ll be able to build in a more leisurely fashion on the knowledge you’ve just gained.
Testing the happy path is only one part of what integration testing is all about.
Happy-path integrations are exercised repeatedly in the normal course of development, which in many cases reduces the likelihood that a defect will make it out of dev.
You still want the testing because defects can easily go unnoticed.)
This can happen when different people with different assumptions write the code on either side of the integration point.
It can also happen when the integration point involves communicating with a resource (like a database) that may be unavailable.
When exceptions occur, you want to ensure that you control the way the system responds, rather than letting the exception determine what happens.
Write an integration test that verifies the proper handling of an exceptional condition.
The following listing shows how to write a test to verify that an expected exception is thrown.
In the code, you want to verify that HibernateExceptions arising out of the SessionFactory pass through the DAO, service bean, and controller.
First, you need a way to induce a failure condition so you can test the system response.
To do this you’ll mock out a broken Hibernate SessionFactory B to simulate a Hibernate failure when trying to get a session.
At C you create the mock object using the Mockito framework.
You specify at D that the mock is using Mockito’s intuitive API.
With the setup complete, you can now write the test.
You use Spring’s @ExpectedException annotation E to declare the expectation that the test method will throw a HibernateException.
For instance, if the service bean were to catch the HibernateException and rethrow it as something else, the test would fail.
Normally the TestContext framework loads the application context one time at the beginning and caches it so it can be reused across all test methods in the test case.
The @DirtiesContext annotation tells the framework to mark the app context as being dirty so it will be automatically reloaded before running the next test.
You use this annotation because you want to modify the contact DAO to use the mock SessionFactory instead of the real one, but you don’t want that change to survive outside of this test.
You mark the test method as one that dirties the app context, and the reload will follow when it’s needed.
In the sample code, there is no setter for the SessionFactory; it’s a private field.
Again, the test passes if and only if this call to the controller throws a HibernateException.
At this point we’ve covered most of the basics: you’ve set up your integration-testing infrastructure, implemented happy-path tests, and implemented a test that demonstrates a desired response to an exceptional condition.
In recipe 10.4, we’ll examine a more advanced technique that allows you to create tests that run only in specific environments.
You often want to be able to verify that code runs within specified time bounds.
This can be challenging, because a given piece of code has different performance characteristics on different machines and in different environments, and so it may be unclear what response time to specify as the expected response time.
You probably shouldn’t expect production-level performance in a development environment, for example.
Although that won’t generally provide a good sense for performance in production, it can at least catch certain cases of performance drift and provide advance warning of issues.
Therefore you’d like to have performance tests that run during continuous-integration builds but not during private builds (that is, builds on an individual developer’s machine)
This isn’t a perfect technique, and it’s certainly no substitute for proper performance testing in a system test environment.
But it can flag issues and set a minimum bar.
Create time-bounded integration tests that run during continuous-integration builds but not during private builds.
The first is to write the tests, as in the following listing.
This repetition provides an extra level of assurance when tests pass because execution times can vary somewhat from run to run.
But note that it’s more likely that at least one test will fail, so your time bounds must be fairly conservative to avoid excessive failure rates.
The first test uses the JUnit @Test annotation to set a timeout D.
This approach applies timeouts to individual test iterations: each iteration must complete within 200 ms.
If the test fails on your machine under this setting, adjust the timeout upward so your test will pass.
It works on our laptop, and our laptop isn’t particularly fast.)
The second test uses the TestContext framework’s @Timed annotation E, which sets a timeout that applies to the overall set of iterations, including their respective setups and teardowns.
In addition to writing the new tests, you need to pass system properties from the environment to the test runner.
To do this, add the <systemPropertyVariables> section to the Failsafe configuration.
Ignoring a test then the test runner will skip the two tests you just added, because you.
Configure your CI server (Hudson or Bamboo, for example) to run the and you’re ready to go.
Keep in mind that code runs faster or slower depending on the environment, so the techniques described here are inappropriate for production performance testing.
Performance testing should generally be conducted under production-like loads, with production-like data and data volumes (so the DBMS generates production-like execution plans) and so forth.
What these techniques do allow you to do is see that a test that once ran without issue now exceeds the timeout period.
The next recipe presents a cleaner alternative to commenting out broken tests.
Ideally you would never have occasion to ignore a test.
You might adopt test-driven development (TDD) practices,6 creating or changing tests before changing the corresponding code to make the lights turn from red to green.
Code would never break a test, and there wouldn’t be cause to ignore tests.
Somewhat short of this ideal would be the situation in which you change some code, you break a few tests, you fix the tests, and then you continue coding.
But in the real world (or at least in our real world), sometimes you change code, it breaks stuff, and you aren’t ready to go back and fix everything you just broke.
In the extreme case, this lack of discipline leads to way too many tests being commented out, but it’s fair to say that sometimes you want to make a code change without fixing all the resulting broken tests at that moment.
You’d prefer to do that in a clean way, rather than commenting everything out all over the place.
This one is easy, and it uses a standard JUnit annotation.
Apply the @Ignore annotation to the test method, as illustrated in the next listing.
Even though applying the @Ignore annotation is cleaner than commenting out test code, it’s still subject to abuse.
Ignoring too many tests might be a sign that you weren’t disciplined in preserving your tests.
At some point you may pay the price through increased regression.
Setting up a code-coverage tool like Clover, EMMA, or Cobertura can be a big help with managing this issue.
By ignoring a test, you prevent it from contributing to the coverage percentages that coverage tools typically provide as part of their reporting functionality.
Having a code-coverage tool in place can help alert you to excessive ignoring, among many other things.
The final recipe shows how to run integration tests against an embedded database.
There you used it to initialize the test database to a known state because integration tests depend on that.
This recipe shows how to take advantage of the <jdbc:embedded-database> tag.
Simplify the infrastructure needed to support integration testing by replacing real test databases with an embedded test database.
It’s also possible to implement support for other DBMSs, although here we’ll stick with HSQL.
Basically you use the jdbc namespace to create an embedded DataSource that the Hibernate SessionFactory can use.
First add the following dependency to the Maven project object model (POM):
Next you need to add a new DDL script, because the existing MySQL script won’t work with HSQL.
The following listing shows the roughly equivalent DDL script for HSQL.
You’ll also need a copy of the DML script in the hsql directory.
Copy the MySQL version (the same DML works here) to sip10-test-data-hsql.sql.
Finally, you need to modify and even refactor your Spring configuration a bit.
Until now, both the app and integration-test configurations have been using MySQL, so it hasn’t been an issue that you’re specifying the SQL dialect in beans-service.xml (which is shared across both configurations)
But now the two configurations use distinct dialects, so you want to push the dialect selection out of beans-service.xml and into the two data-source configuration files.
All you’ve done here is replace the previous hibernateProperties configuration with an external reference, which the individual data-source configurations will provide.
At B you replace the previous BasicDataSource definition with a new HSQL definition based on <jdbc:embedded-database>
The tag accepts a type attribute with valid values HSQL, H2, and DERBY.
The default is HSQL if the type isn’t explicitly specified.)
You reference the DDL and DML scripts at C and D respectively to initialize the embedded database with the desired schema and test data.
You’re using the util namespace for this, so don’t forget to add the corresponding namespace and schema location declarations to the top of the file.
You’re almost done, but the change to beans-datasource-it.xml broke the app because there’s no app-side target for the hibernateProperties reference yet.
You’ve added the necessary hibernateProperties configuration, along with the supporting util namespace declarations.
At this point, you should be able to run both the app and the integration tests.
If everything is good, the app runs against the MySQL database, and the integration tests run against the embedded HSQL database.
Using an embedded database is a useful technique for reducing the amount of setup necessary to get going with integration tests.
The integration tests are more self-contained, and you don’t have to worry about starting up a database, having a network connection to the database (if it’s remote), and so forth.
Also, in many cases working with an embedded database offers performance benefits over working with nonembedded databases.
Keep the following points in mind before using an embedded database:
By using an embedded database, you’re no longer including the database in the scope of your integration test.
Yes, you’re still using a database, but it’s functioning essentially as a test double for the real database.
You must keep the SQL for the real and embedded databases in sync.
This can be painful if there are lots of database changes.
One approach to dealing with this is to start off using only an embedded database (not only for the integration tests but also for the app) while the schema is still under heavy development.
As schema changes slow down, introduce a real database on the app side.
Embedded databases may not support all the features you’d expect to see from a non-embedded database.
Our preference is to run integration tests against a local, nonembedded test database, mostly because we prefer to include the database in the scope of our integration testing, but you may find the embedded-database option useful.
It’s well-known that Spring’s dependency injection lends to designs that are easy to unit-test.
In this chapter, you’ve seen that Spring additionally provides several features to make integration testing simple and effective.
If you work with a technical infrastructure of any size, you may have run into a variety of common issues:
Poor visibility into system configuration—It may be difficult to know exactly what’s deployed, which apps live on which servers, who to call when apps fail, and so on.
When multiple teams are involved (for example, multiple development and operations teams), they may not have the same way of referring to applications, making communication, planning, and incident response difficult.
This chapter covers ■ Creating a configuration database using Spring Data.
Configuration drift—Things aren’t configured the way you expect them to be.
This could include servers that are supposed to match but don’t, firewall ports being closed when they should be open, and so forth.
Manual change processes—Deploying software, for example, involves a large team performing manual releases late at night.
Because they’re manual, they take a long time, it’s easy to miss steps (such as forgetting to update a configuration file on one of the servers), some team members may be absent, and so on.
Manual change processes are a major contributor to configuration drift.
Rogue changes—Another major cause of configuration drift, rogue changes are changes that somebody makes outside the official change process.
Perhaps someone needs a firewall port opened and emails his networking buddy, who opens the port without logging the change, without making the corresponding change in other environments, without a proper security review, and so on.
Inability to reset to a known state—It’s important to be able to reset your environment, or elements of your environment, to a known good state.
Examples include testing (tests should run against a known state), adding capacity to a farm (new servers should have the same configuration as existing servers), disaster recovery (DR environment requires standing up servers according to a known configuration), and more.
These issues are a small sample of those that arise without proper configuration management in place.
Configuration management is a set of practices around ensuring that you have good visibility into and control over the technology assets (infrastructure, middleware, apps, and so on) underlying your services.
When the technology footprint is smaller, it’s certainly possible (although arguably still not best practice) to use manual deployment and release processes, to make changes in the environment as they’re needed, to document configuration information on a wiki, to build new servers from wiki documents, and so forth.
But once there are hundreds or even thousands of servers in place, the approaches that work in smaller environments don’t scale.
In sum, configuration management is useful for any organization wanting visibility and control over its technology, and it becomes more and more essential as your technology footprint scales up.
This chapter shows how Willie is using Spring to implement several features in an open source configuration management database (CMDB) called Zkybase1 (http:// zkybase.org/)
A CMDB serves as the foundation for a more general configuration management capability by supporting data, tool, and process integration.
In particular, it supports various sorts of automation, including build, test, deployment, and operations automation.
The app is currently called Zkybase, but its former name was Skybase.
It appears as “Skybase” in the various figures in this chapter.
The architecture and use case are just one possibility among many; the point is to highlight the foundational role that a CMDB plays in such a system.
The Heliopolis team at the Apollo Group, of which Willie was delighted to be a member, established a configuration management architecture along these lines.
Figure 11.1 A hypothetical configuration management architecture built around a CMDB, supporting auto-deployment to dev servers on commit.
The developer makes any necessary updates to the CMDB and to the app configuration.
For the CMDB, this might include specifying instance IP addresses (in cases where instances already exist) or the desired number of instances (in cases where instances will be launched as part of the deployment)
For app configuration, this could be anything you typically see in an app configuration, such as setting passwords, setting log levels, adding security rules, and so on.
Usually the developer doesn’t need to do anything here because the configuration is already in place from previous deployments.
The developer commits the code to a source control, such as Subversion or Git.
It also updates the CMDB with information about the new package.
The deployment engine pushes the assets across the WAN to a staging repository sitting at the target site.
As the instances come up, they grab the required assets from the staging repository and then perform the required installation and configuration.3
The deployment engine updates the CMDB with information about the deployment (for example, the IP addresses of any instances that were newly provisioned)
There are lots of others, such as self-service deployments to development and test environments, continuous delivery, and runbooks that source data from the CMDB.
Alternatively, the deployment engine can push a bootstrapper onto the instance, which then takes care of getting the assets from the staging repo.
Spring 3.1 was released as we were drawing the book to a close, and it includes new features that we wanted to present.
This chapter presents several of those, including new MVC namespace configuration elements, producible and consumable media types, constructor injection, and profiles.
We’ll call out the new features as we use them.
There’s a ton to cover, so let’s dive right into establishing the foundation for the entire system: the configuration management database.
A CMDB centralizes system configuration, broadly conceived, in a way that enhances visibility, security, and management.
Although there isn’t a single, universally agreed-on definition, the general concept is that it manages the so-called configuration items (CIs) that allow you to keep your environments in an operational steady state, and also to manage changes to those environments.
CIs can be anything you want to manage in this context: an entire data center can be a CI, as can an individual NIC on a physical server.
A CMDB isn’t necessarily a single physical database, and there’s nothing saying that it has to be a relational database or any other type of technology.
It’s more conceptual: it brings together the configuration necessary to stand up and maintain your technical environment.
Strictly speaking, the app configuration repository in figure 11.1 is part of this logical CMDB concept because it contains configuration of the sort under discussion.
But for present purposes we’ll separate the environmental configuration from the app configuration, and we’ll generally use the term CMDB to refer to the former.
Because there’s no gold copy of the configuration for a given set of server instances, the deployer has to go onto each server and update the configuration manually.
In addition to being laborious (especially if you have farms that are many servers wide), it’s also highly error prone.
It’s easy to make a single-character password mistake when you’re slogging through a late-night release window with 10 more app releases to go.
Instead, we prefer to adopt an approach more like the one in figure 11.3 where we have a different situation.
The deployer updates a single configuration master in the CMDB (or app config repo; in this case we’re using the term CMDB more generically), and then the automation deploys it.
Figure 11.2 The deployer updates configuration in place on all target servers, requiring lots of manual, error-prone work.
Creating a simple configuration item to make a mistake, it’s no longer possible for the configuration copies to be out of sync.
If there’s a mistake, all instances will have it, and a single fix will fix all instances.
Centralization has therefore eliminated an important source of configuration drift.
Firewall rules (at least in environments where you need the control) prevent anybody other than the deployment automation from getting on the target servers.
You can place appropriate access controls and auditing at the CMDB to ensure that only authorized activity takes place.4
Figure 11.3 The deployer edits the configuration in one place, and then the automation pushes it through to the target servers.
Unlike many of the other NoSQL databases, Neo4j is fully transactional.
Just as with relational databases, you want to represent entities and relationships, but instead of using tables, columns, foreign keys, and such, you use graph nodes and edges.
Because you’ll build several CIs (one in this recipe and more in the next), you’ll build out some framework code in this recipe to make subsequent CIs easier to implement.
Even though we’re only looking at the application CI here, your CMDB will have lots of different CI types, so it’s sensible to define an appropriate abstraction.
You need a base class for implementation as well, as shown next.
You can add nodes and edges to the database without worrying about database schema conflicts.
Alternatively, you have to worry about the schema in the app, because the database won’t help.) This is particularly nice for a CMDB, which often has a rich set of entities and relationships that evolve over time to support maturing development and operational processes.
It’s easier to make the corresponding changes to the underlying data when there isn’t an explicit schema in the way.
Your abstract CI uses annotations from the Spring Data Neo4j project to help perform the object/graph mapping.
Now that you have a simple interface and abstract base class, let’s implement the application CI.6
The next listing shows how to create an application CI using the AbstractCI class you just created.
In other cases, it’s useful to represent an entity with an edge.
An example might be a membership entity in a team roster, where you have a relationship between a person and a team, along with an additional role attribute attaching to the membership.
It offers a data-access API, the ability to generate dynamic DAO implementations, and annotations for mapping Java objects to Neo4j graphs.
You have only a couple of properties and no relationships at all.
The main point is that you extend the AbstractCI base class B.
Next you’ll want a DAO for CRUD operations and queries.
Spring Data Neo4j provides a way to generate DAOs dynamically using the GraphRepository abstraction.
The following listing creates ApplicationRepository, which is your DAO for persistence operations on applications.
This listing shows how simple it is to create a DAO using Spring Data Neo4j.
All you have to do is create an interface extending GraphRepository<Application> B, which provides various CRUD methods and finders.
You can even define a custom finder C using method-naming conventions.
Spring Data Neo4j generates a DAO implementation dynamically for you, including the custom finder.
There’s only one more thing left to do, and that’s the configuration.
At B you tell Spring where to find the Neo4j database.
With that, you’ve created your first Neo4j-backed CI and repository.
In this recipe you got your feet wet with Spring Data Neo4j, creating a simple CI and a simple DAO for performing persistence operations.
Although everything you’ve done has been straightforward, you have the basis for a powerful way of working with CMDB data.
Recipe 11.2 builds on the work you did here by elaborating both the data model and the application stack.
But real CIs aren’t quite so simple: they generally have relationships to other CIs.
It’s also possible to run Neo4j as a standalone server, but we’re not pursuing that deployment approach here.
To do anything useful with your CMDB, you need to know how to create relationships between your CIs.
To illustrate a variety of techniques for creating relationships, you’ll create CIs for modules, packages, and teams.
In this recipe you’ll lay down the code for the expanded domain model.
After that, you’ll create the DAOs as well, because you’ll need them for subsequent recipes.
Finally, with the persistence layer in place, you’ll create a transactional service layer for your CIs.
First, your application CI now has a one-many relationship to a new module CI and a many-many relationship to a new team CI.
The following listing shows how you can establish the relationships in question.
This involves choosing a name for the relationship type; here you choose APPLICATION_MODULE.
Because the edges are directed, one end of the edge is the start node and the other end is the end node.
It’s up to the app either to impose a meaning or to ignore the directionality, although.
At C you have another relationship, this time between applications and teams.
But notice that the definition here looks slightly different than the definition in B.
In this case you’re dealing not with a simple relationship but with a relationship entity.
A relationship entity is a relationship that exists as a first-class entity with an explicit class definition.
In this case, what’s happening is that you’re relating the Application CI with a Team CI through a relationship entity called ApplicationTeam.
The main use case for relationship entities is that you can attach properties to the relationship.
You’ll see in listing 11.7 that you can assign a team type to the relationship: a given app might have a development team, a test team, a release team, and an ops team.
Another example might be a role property on the relationship between a team and a person.
A second use case for relationship entities would be wanting to have a way to connect two entities without either one knowing about the other.
For example, you might want to constrain individual apps to be deployable to specific server farms, but you might not want apps to know about server farms and vice versa.
Relationship entities are useful here even if you don’t have any properties defined on the relationship.
In fact, Neo4j refers to such edges as relationships, as opposed to saying that edges represent relationships.
Naming relationship types Note that relationship types need unique names.
If, for example, you have a relationship called CONTAINS between an application and a module, you can’t have another relationship called CONTAINS between a server farm and a server.
When dealing with relationship entities, you still need a type.
You can specify that or in the relationship entity class itself.
In the present case, we’ve opted to specify the type in the relationship entity class; see listing 11.7
Spring Data Neo4j allows you to use either a Set or an Iterable, depending on whether you want the collection to be read/write or read-only.
Unfortunately, Lists are unsupported.) You use a Set here so you can add modules directly.
You certainly could use a method creates the ApplicationTeam instance and returns it in case the caller wants to do anything with it.
The following CIs are more of the same, but with variants on how you establish relationships.
Modules provide a way to decompose an application into its service, packaged as a JAR), and a domain module (domain classes shared by the web and client modules, once again packaged as a JAR)
As with the application CI, you extend AbstractCI B from listing 11.2
You also annotate the groupId and moduleId fields with @Indexed C, which will allow you to perform lookups against them later.
Notice that even though there’s a relationship between applications and modules (you established it in your application CI), your module doesn’t know anything about applications.
The reason is that you might want to allow CIs beyond applications to have modules: you might define, for example, a WebService CI that has modules.
If you were to do that, you wouldn’t want the module to know directly about applications and web services.
A package is a JAR, a WAR, an EAR, and so forth, and it’s the result of building a module.
As such, it references a module, but it has a specific version too.
It’s not unlike other object persistence frameworks, such as Hibernate, in this respect.
The previous listing demonstrates the use of the @Fetch annotation B, which eagerly loads the field so annotated.
In this case, it makes sense to do so, because there aren’t many (if any) contexts in which a module-free package would be useful.
Once again, you use @Indexed C to make the field available as a key for performing lookups.
Now that your apps have modules and packages, let’s switch gears and create a team CI that you can attach to your apps.
The intent is to establish a many-many relationship between applications and teams.
The next listing is a basic team CI, having nothing more than a name (for example, Zkybase Development Team or Zkybase Operations Team)
A real team CI would have team members, but we’re suppressing those here because you don’t need them for the current purpose.
Even though you want applications and teams to be related, notice that neither the application CI nor the team CI knows anything about the other.
The reason is that whenever you relate an app to a team, you want to attach a team type to qualify the relationship.
This entity, unlike the previous ones, represents a relationship between CIs, so you use the @RelationshipEntity annotation to reflect that B.
In this case you’re assuming that only two types of teams are available, so you create a typesafe enum for the team type C.
It doesn’t matter which CI is the start node and which is the end node.
Here, you choose Application to be the eagerly fetch that as well.
But relationship entities are useful in at least one other situation: when you want to decouple the two node entities in question.
Suppose, for example, that you want a many-many relationship between packages and server farms, where a package and a farm are related if the package is deployed to the farm.
It may be that you don’t want packages to know about farms and vice versa.
Here you could create a PackageFarm relationship entity to establish the relationship, even if PackageFarm doesn’t have any additional attribute.
You’ve seen a variety of relationship scenarios with your small stable of CIs.
You’ll create DAOs for modules and packages, because you’ll need those later in the chapter.
As with the ApplicationRepository, you have a custom finder B.
This time you find a module based on two properties.
As with the DAOs, you have two custom finders: one to find a list of packages for a given module B and one to find a specific package for a given module and version C.
It’s time to create a service layer for your CIs.
This will allow you to add transactions, and it will let you execute certain bits of business logic.
As with your CIs, there are several service beans, so you’ll want to have useful abstractions in place.
The following listing shows the interface for your CI service beans.
Your CI service interface exposes various CRUD operations, although individual services can, of course, add more specific methods.
The GraphRepository interface the two because you can more conveniently target RESTful POSTs and PUTs that way.
We flagging duplicate CIs for creates or nonexistent CIs for updates.
You’ll want a base class for implementing CI service beans, and that’s what the next listing provides.
You annotate the base class with @Transactional B to ensure that you run your service methods in a transactional context.
At C you inject a Neo4jTemplate, which subclasses can use because it has protected visibility.
This allows the AbstractCIService to implement general CRUD operations against the repository, as the listing shows.11
You can see the service interface and base class in action by implementing a package service, which you’ll use in some of the following recipes.
You implement a package service interface in the following listing.
It’s little more than a wrapper interface around the package repository you created earlier.
This gives the PackageService access to the custom finder methods you specified.
You expose it as a GraphRepository<Package> C so the base class can make calls against it.
You implement a custom duplicate-check method D, throwing an exception if a lookup by module and version yields an existing package.
Zkybase has other service beans, but this should be enough to give you a sense for what they do and how to implement them.
Don’t forget to add a <context:componentscan> to beans-service.xml to capture the new service bean.
In recipe 11.1 you created a basic CI and a corresponding repository.
In the current recipe, you elaborated the model to include more CIs, relationships between them, more repositories, and transactional service beans.
Although this treatment merely scratches the surface of what a fully fledged CMDB offers, it’s enough to equip you to build your own domain models and services.
In the next recipe, you expose your package data through a RESTful web service.
One of the major points of a CMDB is to enable the process, tools, and data integration that support a mature operational capability.
This recipe shows how you can expose configuration management data through a RESTful web service.
Expose configuration management data through a RESTful web service, supporting both XML and JSON.
In recipe 11.2 we showed how to create a simple package CI for your CMDB, with a focus on data modeling and persistence.
This is important because you want to make it possible to create automation around packages:
The continuous-integration build automation needs to be able to create package records in the CMDB following successful builds.
The deployment automation needs to be able to see which packages are available for deployment.
You’ll create both XML and JSON representations for your packages because both formats are popular and because it’s easy to support them both.
First, you need to annotate your package and module CIs with JAXB annotations.
Recall that packages reference modules, so you need to take care of the module mapping too.) Second, you want to create a package controller with appropriate endpoints.
Finally, you need to update your Spring configuration to support your web service.
Don’t you need to include Jackson annotations for JSON mappings? Nope.
Although it’s true that Jackson provides JSON-specific annotations for object/ JSON mapping, you can configure Jackson to understand JAXB annotations, and this allows you to avoid parallel sets of JAXB and Jackson annotations.
Among other things, you want a way to view individual packages and lists of packages as XML and as JSON.
For the most part, this is a matter of adding JAXB annotations to the package and module CIs and letting some mapping frameworks (namely, JAXB and Jackson) do their thing.
In the special case of mapping XML lists, JAXB needs a little help.
First you’ll create a ListWrapper interface that you can use for this purpose.
The idea is that if you want a root-level XML list, you have to create a wrapper object and map it.
Unfortunately, with JAXB, unless you want every such list to have the same root element name (it would be something generic, like <list>), you have to create separate wrapper objects with separate root element names.
The point of ListWrapper is to provide structure around these list wrapper objects.
You have a bunch of CIs, and you want to support XML and JSON mappings for all of them.
You add only a couple of things to the original class.
You indicate that the mapper shouldn’t automatically map either fields or JavaBean properties B; you’ll specify all mappings explicitly.
At C you use @XmlAttribute to map the id property to an XML.
Listing 11.18 AbstractCI.java, updated to support XML and JSON mappings.
Adding a RESTful web service attribute, at least where XML is concerned.
The next listing shows how you map the package CI, including how to use the AbstractCI base class and the ListWrapper to map package lists.
For more information about the mapping annotations, please consult the JAXB 2 documentation.
The updated package CI is mostly the same as before, except now it includes annotations and an inner class to support object/XML and object/JSON mapping.
You use @XmlRootElement to mark this class as one that can be so mapped B.
You also use @XmlType C to specify the order of the properties in the mapping output.
As a last step, you create a root-level package list E by implementing the ListWrapper interface.
The next listing shows the same thing for the module CI, which we include for completeness.
Again, this is pretty much the same as what you saw with packages, so there’s no need to study the details.
Let’s turn now to the task of creating web service endpoints for your packages.
You could create a variety of endpoints, but for now you’ll focus on the following readonly endpoints:
In recipe 11.4 we’ll look at an endpoint that creates a CI.) The following listing shows how to implement each of these endpoints using an abstract base controller class.
Again, note that the controller in the listing is an abstract base-controller class.
Each controller instance provides CRUD operations for a given type of CI B.
Here we’re focusing on the web service operations; we’ve suppressed the others.
The code for those is available in the code download.
One of the things complicating the base controller is the fact that you’re trying to handle operations in a general fashion.
By putting the complexity in the base class, you help keep subclasses simple.
To that end, you want to autodiscover the CI class based on the type argument in the subclass, because this turns out to be useful.
You’ll see an example shortly.) At C you have a ciClass field, and the constructor uses reflection to set it at D.
A subclass extending AbstractCrudController<Package>, say, would have the Package class object as its ciClass.
Another thing you need to be able to do is inject the right CI-specific service into your controller.
You’d want to inject a PackageService into your PackageController, for instance.
You handle this by leaving the injections up to the subclasses13 and accessing the service through an abstract method E.
This allows you to implement generic operations against the service in the AbstractCrudController while giving subclasses access to type-specific methods.
With that setup done, you can get to the real meat.
You handle requests to list all CIs of the relevant type as JSON F.
You assume that the subclass will provide a base path through a class-level @RequestMapping, so the method-level value is the empty string (meaning you won’t append anything to the class-level path)
But you need a way to route map paths like /applications.json, /packages.json, and so forth here, but there’s currently no generic way to do that.14 But you can do something that’s almost as good,
It filters out requests whose Accept header is incompatible with the specified media type (here, application/json)
Second, it ensures that the generated output has the correct media type.
In this case, it turns out that the method works without the produces definition, but there’s no harm in being paranoid.15
The @ResponseBody annotation G tells Spring to take the handler method’s return value and directly map it to the response output, without involving either models or views.
Normally, you put your objects on the model and return a logical view.
You can’t inject CIService<T> in the base controller due to type erasure.
Spring gets the mapping right because the MappingJacksonHttpMessageConverter happens to be the first match for the CI list, so the output is JSON.
We’ll discuss the mapping when we look at configuration, because mapping is determined by that configuration.16
The XML list H is similar to the JSON list, but this time you have to deal with the XML list wrapper issue we mentioned.
Once again, paranoia drives you toward an explicit produces definition, and you use @ResponseBody to effect a direct mapping of your list wrapper to XML.
In the case of the JSON details view, here you need the produces definition to avoid having the XML converter preempt the JSON converter.
Because you did most of the hard work in the AbstractCrudController, there’s little to do here.
You also extend the base controller, passing in the Package type argument C.
At D you inject your PackageService so the controller can use it in a typesafe way.
As noted earlier, the produces element appeared in Spring 3.1
If you’re using an earlier version of Spring, then produces isn’t available.
An alternative is to have the JSON details method call the Jackson ObjectMapper directly, writing its output to the response stream.
That way you don’t have to depend on Spring applying the correct HTTP message converter.
The CRUD controllers for other CIs are essentially the same, so we don’t have to review them all here.
The following listing shows how you configure your web application context for your nascent RESTful web service.
The web service needs a couple of mappers: an object/XML mapper and an object/JSON mapper.
Ultimately you use the mappers to convert your CIs into JSON and XML.
You’re using the oxm namespace to define a JAXB2 marshaller, which happens to be an unmarshaller too.
That is, it’s a full object/XML mapper.) You enumerate the CIs that you want to map using <oxm:class-to-be-bound>.18
You also need an object/JSON mapper, but as we noted earlier, you need it to understand JAXB annotations.
That’s not a problem if you use the Jackson mapper.
First you create a JaxbAnnotationIntrospector D, and then you feed it to your Jackson mapper E on both the serializing side and the deserializing side.
It’s not a common technique, but Spring supports it, and in certain situations it’s helpful.
Creating new SerializationConfig and DeserializationConfig objects is nontrivial, and we’d much rather inject the introspector into the existing SerializationConfig and DeserializationConfig objects.
To activate your mappers, you need to define what are known as HTTP message converters.
Each such converter has a mapper that it uses for mapping objects to some format or other.
You can define these implicitly using the <mvc:annotation-driven> annotation F, which places default HTTP message converters on the application context, among them one for XML and another for JSON.
The problem with using the default JSON converter, though, is that it doesn’t know anything about the JAXB-enabled object mapper you just created.
Spring 3.1) to specify any HTTP message converters that you’d like to have override the default.
In your case, you create a MappingJacksonHttpMessageConverter, injecting the JAXB-enabled object mapper H.
Finally, you component-scan your controller I the same way you generally do.
At this point you have a simple but functional web service that returns package list.
Creating RESTful web services in Spring is similar to creating ordinary HTML-based services in the sense that you do it with controllers, @RequestMappings, and so on.
But by and large, if you’re comfortable with Spring Web MVC for HTML pages, it shouldn’t be a stretch to wrap your head around RESTful web services with Spring.
The current recipe intentionally started with the simplest web service endpoints.
In the following recipe, you’ll elaborate your web service to support package creation, and you’ll build a Maven plug-in that uses a Spring REST client to invoke the new endpoint.
In some organizations, when the test team wants a new build for testing, they have to coordinate with developers and, potentially, release engineers to get one.
The tester asks for a build, the developer makes one available a few hours later, and the release.
The converters associated with the conversion service are not the same thing as the HTTP message converters.
By the time the build appears in test, several hours may have passed.
A much better approach is for the tester to be able to deploy good builds on demand.
Such systems eliminate needless delay by allowing workers to pull work instead of waiting for somebody else to push it.
To implement a pull system for deployments, it’s useful to set up a CI server (such as Bamboo, Hudson, or Jenkins) that makes good builds available for deployment.
One critical piece of this is to push good builds to a package repository (for example, Nexus, Artifactory, or even a simple fileshare) where the deployment process can find them.
In CMDB-based systems, another important step is to create a record for the package in the CMDB.
This is useful if you want to join other data to the package.
You might want, for instance, to associate development issues with the package, to track which packages are deployed to which servers in which environments, to schedule future automated deployments, and so forth.
In this recipe, you’ll look at how to create a package record in the CMDB.
Following a successful CI server build, create a package record in the CMDB.
Assume here that the CI process involves running a Maven-based build.
After Maven successfully compiles and tests the code, you want it to create a corresponding package record in the CMDB if one doesn’t already exist.
To do this, you’ll implement a custom Maven plug-in that makes a web service call to the CMDB.
You need to do two things: create an endpoint on the CMDB and create the Maven plug-in.
Your first task is to create an endpoint capable of receiving web service requests to create new packages.
The following listing shows how to add this to your PackageCrudController.
You need the base URL to create a Location header in response to create requests.
The consumes element C tells Spring that you want to match requests having the application/xml content type (Content-Type header)
You pair this with the you can map to a Package instance.
When creating the package, you need to associate the package with a module having an ID.
You use the group ID and module ID to look up the fully hydrated module E, then replace the module data transfer object with the result.
One issue you need to deal with is duplicate packages.
You can either silently ignore attempts to create a duplicate package or throw an exception.
Here we’ve chosen to ignore such attempts because the Maven plug-in won’t know whether the package has been created.
You try to create the package F, and if the attempt succeeds,
Otherwise, you ignore the attempt, responding with the OK status H.
Next is a client that calls the Zkybase web service.
The client is effectively a wrapper around Spring’s RestTemplate B, which applies the template pattern to the task of calling web services.
Having a client allows you to build the Maven plug-in that you want to build.
The next listing is a Maven plug-in package goal that calls the web service endpoint you just created.
You’ll configure it to record the existence of the new package in the CMDB as part of the Maven build lifecycle’s deploy phase.
We won’t go into the details of writing Maven plug-ins here, because this isn’t a book on Maven, but do check the Maven reference if you’re interested in learning more.
To configure the plug-in for use in your project (presumably a project that you run as part of a continuous-integration build), include the following in your project’s pom.xml file:
This causes Maven to run the plug-in during the deploy phase of the Maven build lifecycle.
From a tools perspective, this recipe showed how to build a Maven plug-in that calls the Zkybase web service and creates a package.
The larger process significance, though, is that you now have a way to make package information available to interested processes.
Although Maven can deploy packages to artifact repositories such as Nexus or Artifactory, it’s useful to be able to deploy to a CMDB so you can more readily associate packages with application modules, deployment definitions, server farms, and so on.
In the remaining recipes, you’ll follow the social trend in application development and integrate the Zkybase CMDB with GitHub, a social software development web site.
When designing a CMDB, it’s not necessary that all the data be in a single physical data store.
In fact, in larger organizations, it makes a lot of sense to assume that the data won’t be in a single store.
In a federated design, data lives in multiple stores, and you bring it together in a unified UI.
This recipe considers GitHub integration.19 This is useful in cases where you have applications whose source code repos live at GitHub.
You can present repo data (such as the GitHub URLs, commits, repo collaborator, repo watchers, and so on), user data (such as followers or whom a user is following), and more, alongside other app-related data, such as build, deployment, and operational data.
It’s even possible (although we won’t pursue this here) to create automation on the basis of this data, such as automation that provisions a continuous-integration server based in part on GitHub data.
To be concrete, we’ll show how to display a repository’s watchers in your CMDB interface.
We’ll focus on the data-integration piece; the code for the UI isn’t anything special.
Figure 11.4 shows what it will look like when you’re done.
Getting the watcher data from GitHub requires a web service call to the GitHub API (http://developer.github.com/v3/)
It’s a REST-based API that uses JSON for data transfer.
You’ll use Spring Social to make the call to GitHub.
Spring Social provides a programmatic, Java-based interface for making calls against social websites like Facebook, Twitter, LinkedIn, and GitHub.
SpringSource and other providers create bindings for individual social sites, which makes it easy for Spring-based applications to interact with the site in question.
It will help if you are familiar with Git and GitHub, although that may not be strictly required.
GitHub is a socially oriented Git hosting website at https://github.com.
We’re going to use it here anyway to show how Spring Social works.
The more mature Facebook, LinkedIn, and Twitter bindings work in the same fashion.) We’ll also look at the actual framework implementation, based on Spring’s RestTemplate, just in case you decide you want to do the integration but you’re not comfortable building your app on top of an unreleased piece of software.
You’ll see that it’s easy to use the RestTemplate to accomplish your goals.
By the time you read this, the situation may have changed, but at the moment Spring Social GitHub has only snapshots.
With that repository defined, you can now declare your Spring Social and Spring Social GitHub dependencies:
The following listing presents a controller that loads an app from the app repository, then uses the associated GitHub username and repo name to load the repo watchers from GitHub itself.
Willie contributed the watcher code under discussion to Spring Social GitHub.
You start with a couple of injections: the application repository B from the previous recipe and the GitHub client C from Spring Social GitHub.
At D you get the app from the repository, including the GitHub user and repo as we noted.
Finally, you use the GitHub client to grab the watchers E.
In Spring Social, the general pattern is to have interface names like Facebook, LinkedIn, Twitter, and GitHub, and to have implementation classes named FacebookTemplate, LinkedInTemplate, TwitterTemplate, and GitHubTemplate.
We noted that at the time of this writing, Spring Social GitHub hasn’t yet been released.
If you prefer not to use unreleased software, you can of course still call the GitHub API, but you’ll need to use Spring’s RestTemplate, as you’ll see next.
The Spring Social GitHub implementation, like most other Spring Social implementations, uses Spring’s RestTemplate as a client for making RESTful web service calls.
Here’s the relevant code from Spring Social GitHub, edited to highlight the use of the RestTemplate.
The listing shows how to call the GitHub API using a RestTemplate B.
As with other templates in the Spring Framework, it takes care of the boilerplate (for example, establishing and terminating the connection, performing the object mapping, and so on) so developers can focus on what’s interesting.
You convert the array to a List before returning it.
There are lots of other RestTemplate methods; see the Spring Javadoc for details.
This recipe introduced the GitHub API as well as two different ways to call it: via Spring Social GitHub and using the Spring RestTemplate directly.
We showed the latter approach because the Spring Social GitHub binding is currently (early 2012) in its infancy.
Check on it from time to time, because it may have matured by the time you read this.
You used the GitHubTemplate to invoke public endpoints on the GitHub API.
This is fine if all you need to do is call public endpoints.
But if you want to use private endpoints, you need to introduce authorization.
Keep in mind that even though Spring Social GitHub is in an early stage, other bindings under the Spring Social umbrella have production-ready releases.
Craig Walls, the author of Spring in Action (Manning, 2011), is the Spring Social project lead.
In recipe 11.5, you learned how to call public endpoints on the GitHub API.
This might include sensitive read endpoints or perhaps endpoints that modify data.
To accomplish this, you need to add authorization to the mix.
Allow individual end users to authorize Zkybase to perform sensitive GitHub operations on their behalf.
GitHub has a concept of service hooks, which are integration points between GitHub and other applications.
You might create a hook, for instance, that posts a message to Twitter every time somebody commits code to a repository.
In this recipe we’ll show how Zkybase retrieves hooks from a GitHub repository.
Hooks are private information, so authorization is part of the story.
In the following subsection we’ll take a high-level look at OAuth 2, and then we’ll dive right into the code.
In the case at hand, you want Zkybase to be able to read a GitHub repo’s service hooks, which aren’t normally public.
That way you can display the hooks in the Zkybase UI, and maybe even use them for other purposes, such as provisioning continuous-integration servers with the hooks established.
But there are other sensitive operations as well, such as editing repos, writing gists, and so on.
Zkybase doesn’t have carte blanche to perform sensitive operations for arbitrary GitHub users.
Also, the user can revoke the authorization at any time using the GitHub admin console.
A bit of a protocol is involved in establishing the desired authorization.
The first thing is that you’ll need to go to your GitHub account settings and register the Zkybase application.
That allows Zkybase to get an access token as part of the OAuth process.
It will help to consider the flow in terms of screenshots.
Figure 11.6 shows what the account page looks like at the beginning of the process, before you’ve connected Zkybase to GitHub.
After you click the Connect button, Spring Social redirects you to GitHub for authorization.
If you aren’t already authenticated into GitHub, you’ll obviously need to log in first.
But then you’ll get a GitHub screen that looks like figure 11.7
Here GitHub explains to the user exactly which capabilities it will grant to Zkybase, should the user allow the authorization.
GitHub creates a record of that authorization in its own database.
As noted, the user can revoke this at any time.
GitHub then redirects the user back to the Zkybase app, passing the authorization along.
From here, Zkybase invokes GitHub again, exchanging the authorization for an access token.
Conceptually, the token is the user’s connection, and it’s what allows Zkybase to make authorized calls against the GitHub API.
Once Zkybase has the token, it stores it in its database.
Among other things, this record associates the current Zkybase user with the access token that Zkybase uses to issue requests against the GitHub API.
As long as the token remains valid (not revoked or expired), Zkybase can do what it needs to do for the user.
The Zkybase user account page now has the GitHub information shown in figure 11.8
Zkybase can use the access token to perform sensitive GitHub API operations on.
Figure 11.8 The Zkybase user account page after establishing the current user’s connection to GitHub.
Now that you’ve seen how the flow works, you’re ready to look at some code.
As a preliminary, Zkybase needs a place to persist user connections once the user has established them.
This table is the backing store for your connection repository, which you’ll see presently.
First we’ll show how to use Spring Social GitHub to get the currently authorized user.
Recall that the user account page displays either a Connect or a Disconnect button, depending on whether the user has established the connection between Zkybase and GitHub.
The first thing you need then is a way to tell whether the connection exists.
Figure 11.9 Zkybase displays the repository hooks it read from GitHub on behalf of the current Zkybase user.
Your first authorized service call grabs the current user’s profile from GitHub.
To do this, you need a connection repository B and an authorized API client, which you acquire from the connection D.
With these, getting the profile is a simple method call C against the GitHub client.
The next listing shows how to get hooks from a repository.
As you can see, the process is essentially identical to that of getting the currently authenticated user.
Spring Social includes some web components, including a connection-management controller and a simple JSP tag library, to make it easier to build UIs for social apps.
The Spring Social project provides a ConnectController that manages the flow of requests involved with the OAuth dance via a RESTful API.
At B you have the URL for interacting with the GitHub API service (Spring Social calls it a provider), which you’ll use to connect to and disconnect from the service.
In generating the UI for the user account page, the user is either connected to GitHub or not.
When the connection doesn’t exist C, you present a button that allows the user to establish the connection D.
The controller expects an HTTP POST here, so you use a form.
GitHub also expects you to request a certain scope for authorization.
The GitHub OAuth API describes the scopes in detail; here you want the user to grant Zkybase to perform arbitrary operations on GitHub users, repos, and gists on that user’s behalf.
You specify that scope to the ConnectController E, which in turn passes it along to GitHub.
In the case where the connection exists F, you want to show data from the user profile such as the user’s blog URL G and geographic location.
You also want to display a button that allows the user to disconnect from GitHub.
The controller expects an HTTP tells the service that this is an HTTP DELETE rather than an HTTP POST.
There’s one more piece to the puzzle, and that’s the Spring configuration.
The last mile here is to make certain bits of the Spring Social infrastructure available to the application.
You need a way for the app to acquire connections for any given user request, and this in turn involves having a connection repository (to store the user connections), as well as a way to establish new connections if they don’t exist in the repository.
To begin, you declare the c namespace B, which is new with Spring 3.1
This namespace allows you to perform constructor injection in the same way the p namespace allows you to perform property injection.
At C you define a JDBC data source, which you’ll need for storing GitHub user connections.
You have a connection factory registry D, and you register a lone GitHub connection factory E.
If you needed connections for other Spring Social providers, such as Facebook, Twitter, or LinkedIn, you’d register them here as well.
Notice that you’re using the c namespace to perform constructor injection into the GitHubConnectionFactory F.
The names you use must match the parameter name; in this case they’re clientId and clientSecret.
This grabs user-specific connections from the database, creating them if necessary.
Again you’re using constructor injection, this time with a reference H.
The connection repository I is a little different than the user-connection repository.
The reason it’s request-scoped is that different requests will have different Spring Security user principals, and you want to get Language (SpEL) to perform the injection.
You have a no-op text encryptor 1) to use for encrypting access tokens that Zkybase gets from GitHub.
Spring Social applies this encryptor before storing the access tokens in the database.
A no-op encryptor is fine for development but not for production.
Recapping, you have a request-scoped connection repository that grabs user-specific connections from a user-connection repository, which in turn has references to a connection factory and a persistent connection store.
You also need to add a Spring Social ConnectController to the beans-web.xml configuration file.
This reasonably modest configuration takes care of the OAuth 2 dance.
In this recipe and the previous one, we showed how to use Spring Social to make calls against GitHub’s public and private API.
In the first case, all you need to do is make a GitHubTemplate available to the app, and you can use it without having to configure it.
For making private API calls, the process is considerably more involved because it requires OAuth 2, but Spring Social does a nice job of making this as simple as possible.
Spring Social GitHub is still a work in progress at the time of this writing (no 1.0 release yet), and the number of apps that want to interact with GitHub is probably relatively small compared to those that want to talk with Facebook, Twitter, or LinkedIn.
Fortunately, the Spring Social projects for those providers are much more mature.
You can easily apply what you’ve learned about Spring Social GitHub to the other Spring Socials, because they all use the same general approach.
The next recipe addresses a security issue with your work so far: you aren’t protecting the access tokens in the database.
In recipe 11.6, you used a no-op text encryptor for GitHub access tokens.
That’s fine for development, but it’s no good for production-deployment scenarios where you don’t want prying eyes to see user access tokens.
An attacker with an access token can perform sensitive actions against the victim’s GitHub account.
Encrypt access tokens for production, but leave them unencrypted for development.
You’ll take advantage of the new profile feature in Spring 3.1
The idea behind this feature is to support different bean definitions depending on the environment.
This goes beyond using different property values in different environments; here you want to use different beans depending on the environment.
Recall from recipe 11.6 that you used a no-op text encryptor:
Here you’ll continue using that for development, but you want to use the following for production instead:
In listing 11.33, the text encryptor was defined alongside the other beans.
In listing 11.34 that’s not true: it’s defined instead inside the development profile B, which you indicate in the production profile C.
The values for security.encryptPassword and security.encryptSalt come from your environment.properties file.
Note that the text encryptor expects the salt to be a valid hex-encoded value (for example, CAFED00D3141— but choose your own for actual production use)
You need a way to tell Spring which profile you want to use when you’re running the app.
There are different ways to do this, but in this case you want to pass the profile in as a command-line argument when starting up the server because you obviously don’t want to bake it into the configuration.
You can do that with a JVM system property as follows:
Other alternatives are system environment variables (generally useful because most systems target a specific profile), web.xml servlet context parameters (not as useful because you don’t want to hardcode the profile in the configuration), and JNDI entries.
Note that you can specify multiple profiles with a comma-delimited list.
This recipe showed how to secure access tokens by encrypting them.
Doing so involves using different encryptor beans in the development and production profiles.
There are other examples where this sort of thing happens.
For instance, an app may use database-based authentication on the local machine but CAS-based authentication for dev, test, staging, and production.
Note that in a great many cases, the only things that change from environment to environment are property values, such as passwords or web service URLs.
In such cases, there’s no need to use profiles: just use different properties files for different environments.
This chapter concludes one of our favorite topics: configuration management.
In it you learned how to create a CMDB with CIs backed by Neo4j.
You also saw how to create a web service for your CMDB, as well as how to integrate tools like Maven with your CMDB via the web service.
Finally, you learned how to make outbound calls to social apps like GitHub.
These integrations are key to establishing an effective configuration management infrastructure.
The next chapter turns to the topic of content delivery in the form of an articledelivery engine.
In it, we continue our exploration of NoSQL data stores, this time involving the Java Content Repository (JCR) and MongoDB/Spring Data MongoDB.
Examples include articles (news, reviews, and so on), announcements, press releases, product descriptions, and course materials.
In this chapter, you’ll create an article-delivery engine following three recipes.
Recipe 12.2 builds a web front end to upload and display the articles.
Among the tasks a content-management system (CMS) must support are the.
Although it’s not strictly required, to get the best value from this chapter you’ll find it helpful to have experience with JCR, Jackrabbit, and MongoDB.
If those are new areas for you, be prepared to do some self-study because we won’t be covering them in detail.
But we do provide sample code—be sure to make use of it, especially as regards the nontrivial Maven setup.
For JCR and Jackrabbit, see the Jackrabbit website at http://jackrabbit.apache.org/
Before we continue, let’s pause to discuss Spring Modules JCR.
There is a Spring Data JPA project, though, so you never know…
A word about how we’re using Spring Modules JCR Spring Modules is a defunct project that includes several useful Spring-style libraries for integrating with various noncore APIs and codebases, including Ehcache, OSCache, Lucene, and JCR.
Unfortunately, various promising attempts to revive Spring Modules, either in whole or in part, appear to have stalled.1
It’s unclear whether Spring will ever directly support JCR,2 but there’s a lot of good Spring/JCR code in the Spring Modules project, and we wanted to take advantage of it instead of starting from scratch.
Toward that end, we forked an existing Spring Modules JCR effort on GitHub to serve as a stable-ish basis for the book’s code.3 We’ve made some minor enhancements (mostly around cleaning up the POM and elaborating support for namespace-based configuration) to make Spring/JCR integration easier.
But note that we don’t plan to elaborate this fork beyond merging any useful pull requests that people might want to submit.
You’ll begin your explorations by setting up the article repository.
Your first order of business is to establish a place to store your content, so let’s start with that.
Future plans4 are to support more advanced capabilities such as article authoring, versioning, and workflows involving fine-grained access control.
Although it’s often fine to use files or databases for content storage, sometimes you must support advanced content-related operations such as fine-grained access control, author-based versioning, content observation (for example, watches), advanced querying, and locking.
A content repository builds on a persistent store by adding direct support for such operations.
You’ll use a JSR 283 content repository to store and deliver the articles.
You’ll use the open source Apache Jackrabbit 2.x JCR reference implementation at http://jackrabbit.apache.org/
This isn’t a book on JCR, so we’ll limit our treatment of JCR to an overview.
For more information, please see the Jackrabbit website or check out the JSR 283 home.
The future plans merely motivate the choice of JCR as opposed to a simpler approach.
It’s about Spring/ JCR integration, and we use Spring Modules JCR because it’s helpful and it’s currently a sensible approach to take if you need to perform Spring/JCR integration.
The reality is that integrating Spring and JCR requires extra effort because there isn’t to date an established project for doing that.
The JCR specification aims to provide a standard API for accessing content repositories.
A content repository is a high-level information management system that is a superset of traditional data repositories.
Architecturally, so-called content applications (such as a content authoring system, a CMS, and so on) involve the three layers shown figure 12.2
Do you need JCR just to import and retrieve articles? No.
If all you need is the ability to import and deliver articles, JCR is overkill.
But we’re assuming for the sake of discussion that you’re treating the minimal delivery capability you establish in this chapter as a basis on which to build more advanced features.
Given that assumption, it makes sense to build JCR in from the beginning because it’s not especially difficult to do.
If you know you don’t need anything advanced, you might consider using a traditional relational database back end or even a NoSQL document repository such as CouchDB or MongoDB.
We don’t pursue the relational approach here (the rest of the book equips you to pursue that yourself), but recipe 12.3 shows how to use a MongoDB document repo instead of JCR.
Content apps make calls against the standardized JCR API, and repository vendors provide compliant implementations.
These might be CMS apps that content developers use to create and manage content, or they might be content-delivery apps that content consumers use.
The API specifies capabilities that repository vendors either must or should provide.7
It allows content apps to insulate themselves from implementation specifics by coding against a standard JCR API instead of a proprietary repository-specific API.
Apps can of course take advantage of vendor-specific features, but to the extent that apps limit such excursions, it will be easier to avoid vendor lock-in.
The content repository is organized as a tree of nodes.
You can represent individual articles and pages as nodes, for instance, and article and page metadata as properties.
That’s a quick JCR overview, but it describes the basic idea.
Let’s take a fast look at the article repository, and after that you’ll start on the code.
At the highest level, you can distinguish article development (for example, authoring, version control, editing, and packaging) from article delivery.
Our focus in this recipe is article delivery, specifically the ability to import an article package (assets plus metadata) into a runtime repository and deliver it to readers.
Obviously there has to be a way to do the development too, but here you’ll assume the author uses their favorite text editor, version-control system, and zip tool.
In other words, development is outside the scope of this chapter.
See figure 12.3 for an overview of this simple article-management architecture.
If you’re knowledgeable about Jackrabbit, feel free to configure it as you wish.
Otherwise, the chapter’s code download has a sample repository.xml Jackrabbit configuration file.
It’s in the sample_conf folder.) Create a fresh directory somewhere on your filesystem, and drop the repository.xml configuration file there.
You shouldn’t need to change anything in the configuration if you’re trying to get something quick and dirty to work.
Eventually you’ll point the app at the directory you just created.
The app, on startup, will create an embedded Jackrabbit instance against your directory.
A content repository is a back-end system that provides a content storage abstraction and also various services around that, such as read/write, querying, transactions, and versioning.
A CMS provides a UI for working with a content repository.
The JCR spec defines three levels of compliance, and providers target one of those levels.
To model the articles, you’ll need a couple of domain objects: articles and pages.
The following listing shows an abbreviated version of the basic article domain object covering the key parts; please see the code download for the full class.
Figure 12.3 An article CMS architecture with the bare essentials.
The development environment has authoring, version control, and a packager.
The runtime environment supports importing article packages (for example, article content, assets, and metadata) and delivering it to users.
It would probably be a nice addition to add a title to the page domain object, but this is good enough for your current purpose.
Next we want to look at the data access layer, which provides a domain-friendly API into the repository.
Even though you’re using Jackrabbit instead of using the Hibernate back end from other chapters, you can continue to use the Dao abstraction.
Figure 12.4 is a class diagram for the DAO interfaces and class.
The Hibernate DAOs had an AbstractHbnDao to factor out some of the code common to all Hibernatebacked DAOs.
In the current case you haven’t created the analogous AbstractJcrDao because you have only a single JCR DAO.
But if you had more, it would make sense to do the same thing.
You’ll want a couple of extra operations on the ArticleDao, as the next listing shows.
The other pages have placeholder objects to ensure that the page count is correct.
The JcrArticleDao class illustrates ways in which you can use Spring to augment JCR.
You implement the ArticleDao interface from listing 12.3 and also extend JcrDaoSupport, which is part of Spring Modules JCR.
JcrDaoSupport gives you access to JCR Sessions, a JcrTemplate, and a convertJcrAccessException(RepositoryException) method that converts JCR RepositoryExceptions to exceptions in the Spring DataAccessException hierarchy.
You also declare the @Repository annotation to support component scanning and the.
At C you inject an ArticleMapper, which is a custom class that converts back and forth between articles and JCR nodes.
After all, you usually define transactions on service beans because any given service method might make multiple DAO calls that need to happen in the scope of a single atomic transaction.
But in this chapter, you don’t have service beans—you’ll wire the ArticleDao right into the controller.
The reason is that the service methods would pass through to ArticleDao, and in that sort of situation there’s no benefit to going through the ceremony of defining an explicit service layer.
If you were to extend this simple app to something with real service methods (as opposed to data access methods), you’d build a transactional service layer.
You implement the DAO methods using the template method pattern common throughout Spring (JpaTemplate, HibernateTemplate, JdbcTemplate, RestTemplate, and so on)
In this case, you’re using the Spring Modules JCR JcrTemplate (via This template is helpful because it automatically handles concerns such as opening and closing JCR sessions, managing the relationship between sessions and transactions, and translating RepositoryExceptions and IOExceptions into the Spring DataAccessException hierarchy.
Finally, to maintain consistency with JcrDaoSupport’s exception-translation mechanism, you throw a DataIntegrityViolationException G (part of the aforementioned DataAccessException hierarchy) in the event of a duplicate article.
In our discussion of the JcrArticleDao, we mentioned an ArticleMapper component to convert between articles and JCR nodes.
The listing is more concerned with mapping code than with Spring techniques, but we’re including it to give you a sense for what coding against JCR looks like in case.
The next listing brings everything together with the Spring configuration.
As always, you begin by declaring the relevant namespaces and schema locations.
In this case you need to declare (among others) the Spring Modules jcr B and jackrabbit C namespaces so you can use the custom namespace configuration they provide.
Listing 12.6 beans-jcr.xml: Spring beans configuration for the JCR repository.
You need to pull in a couple of externalized properties so you can configure a Jackrabbit repository without resorting to hardcoding.
Of course, you’ll need to adjust the values according to your own environment.) Note that the property values here are Spring resources, and they don’t necessarily have to be file resources.
You can, for instance, create classpath resources, network resources, and so forth.
You use the <jackrabbit:repository> element to do that E, along with the repository.conf and repository.dir properties you grabbed from environment.properties.
Behind the scenes, this reads the Jackrabbit repository.xml configuration file we mentioned earlier and then builds a repository at whatever home directory you specify.
The DAOs need a way to get JCR sessions, and for that you need a Spring Modules SessionFactory.
For the repository.xml configuration it’s fine to use dummy credentials, so that’s what you do.
If you want to use real credentials, update repository.xml and beans-jcr.xml appropriately.
Pass the repository and credentials into the SessionFactory G, and then component-scan ArticleDao H, which automatically injects the SessionFactory.
For transactions, you define a Jackrabbit LocalTransactionManager (courtesy of Spring Modules) I and use <tx:annotation-driven/> J to activate declarative transaction management.
Figure 12.5 shows the bean-dependency diagram for the configuration we just reviewed.
You now have a JCR-backed DAO for the article-delivery engine.
This recipe showed how to use Spring Modules JCR to integrate Spring and Jackrabbit, the JCR reference implementation.
You followed this book’s usual practice of defining a DAO interface and then implementing the DAO using a specific persistence technology.
The DAO is, of course, only the first piece of what you need to build.
In the next recipe, you’ll create a simple, web-based article-delivery engine that uses the DAO you just created.
In the previous recipe you created a repository for storing articles.
To make the repository useful, however, you need a front end to present the articles to users.
Create an article-delivery engine supporting article importing, a master list view, and a details view.
Of the three required features, the master list and the details views are both straightforward.
They aren’t much more than calls straight to the DAO.
Let’s start by looking at the controller so you can get the master list and details views out of the way.
In the controller, you inject an ArticleConverter B for article importing (we’ll get to that in a moment) and also the ArticleDao C you created in the previous recipe.
Next we’ll look at the JSPs for viewing the article master list and the article details.
The article master list shows the articles in a simple list, with the title, author, publication date, and description.
The titles are links to the first page of the individual articles.
The code for the article master list is in the next listing.
We’ve simplified the HTML (by removing CSS stuff) to keep the listing short, but the full JSP is available in the code download.
The article details page is somewhat more involved but still not too complicated.
You need navigation, and the first page of the article should show the title, author, publication date, and description, as shown in figure 12.7
Again, we’ve taken some liberties with the actual code listing to highlight what’s essential.
Listing 12.9 articlePage.jsp: displays a single article page with page navigation.
Figure 12.7 The article details view must show page navigation, and the first page shows the title, author, publication date, and description.
As you can see, the page navigation appears at the top and bottom of the page.
Next is the JSP include that generates the page navigation.
With that, we’ve covered the article master list and details views.
We still need to examine the topic of importing articles, so we’ll do that next.
Recall from figure 12.3 that the article-import functionality assumes you’re importing an article package.
In this case, an article package is a zip file whose contents are the following:
The sample code includes three sample article packages; unzip them to see their contents.
For your convenience, here’s an example of an article.xml file.
Use Spring Security 2 to store your user passwords securely.
The HTML pages are normal HTML pages, although they use certain CSS classes to achieve various presentational effects.
To understand how article importing works, you need to be familiar with the flow starting from an article package on the user’s desktop to an article ending up in the article repository.
Listing 12.11 Example of an article.xml file for the article package.
Figure 12.8 How an article package on the user’s machine ends up as an article in the article repository.
Let’s look at the article-upload form first and then the ArticleConverter.
From a UI perspective, you’ll put the article-upload form at the top of the article master list.
To implement this, you need to add a form to articleList.jsp.
You’ll add a couple of alert messages as well to indicate success or failure when uploading an article.
Review the controller in listing 12.7 to see how you activate these alerts.
Here’s a slightly simplified version of what you need to add to articleList.jsp; see the code download for the full version:
The important parts are the form’s enctype attribute and the file-input field—these are what make this a file-upload form.
When the user uploads an article package, the browser sends the package to the MultipartFile into an Article.
See figure 12.10 to understand what’s happening behind the scenes.
Listing 12.12 ArticleConverter.java: extracts an Article from the file upload.
Figure 12.10 How ArticleConverter converts an article package to an Article.
File articleDir; tempDir, "article-" + throw new RuntimeException( "Can't create a temporary directory.
The ArticleConverter class contains a fair bit of code, and we’ll go over the key elements here.
The Converter interface is for classes that convert objects from one type to another.
You need ServletContextAware to get a reference to the ServletContext, which gives you a way to get the servlet context’s temporary directory.
At a high level, its logic follows the logic presented in figure 12.10
It creates a temporary directory F, unzips the article package into the temporary directory G, and, finally, builds the Article from the article package contents H.
Note that you use OXM to build the Article I, but you build the pages manually J.
This is because you care about the XML structure in article.xml, but you don’t care about anything other than what’s between the <body> tags in the HTML files.
To make this work, you need to revisit the Article class and add some annotations.
Because the structure of the XML matches the structure of the Article class almost exactly, you don’t have to add many annotations.
In article.xml, the ID is an attribute rather than an element, so you add the corresponding annotation:
Finally, you aren’t OXM mapping the pages, so you add the following:
To make everything work, you need to create a beans-web.xml configuration.
The following listing shows how to put everything together at the web tier.
The previous listing is similar to other beans-web.xml configurations you’ve seen.
You handle OXM by declaring the namespace B and using it to declare a JAXB marshaller D, which happens to double as an unmarshaller.
To enable file uploads, you place a CommonsMultipartResolver C on the application context under the ID multipartResolver.
You use some of the configuration options available; see the Javadoc for CommonsMultipartResolver for the full list of options.
You can find the article packages in the sample_articles directory in the code download.
Over the last two recipes, you’ve put together a basic article-delivery engine based on JCR, a powerful repository technology.
The engine doesn’t currently use any of the advanced features of JCR, but the idea behind using JCR is that it allows you to elaborate the article engine into a more full-blown article-management system, with authoring, versioning, fine-grained access controls, and more.
But it may be that all you want is an article-delivery engine of the sort you just built.
It is, after all, a useful application as it stands, and technical users might prefer to edit content using a text editor, manage it using an existing source-control system, and approach on the back end.
In the next recipe, you’ll see how to replace the JCR repository with MongoDB using Spring Data MongoDB.
The approach was to provide a basis for building a more sophisticated set of management capabilities, such as authoring, versioning, fine-grained access controls, and more.
But if that’s not required, then there are repository options that greatly simplify the dataaccess code.
This recipe shows how to do that using MongoDB and Spring Data MongoDB.
It’s a short recipe, because Spring Data MongoDB does almost all of the work.
You’ll find it helpful to review the Spring Data JPA overview in chapter 2 if you’re unfamiliar with Spring Data.
All you need is article delivery, not advanced content-management functions.
Choose a repository technology that allows you to simplify the data-access code.
You’ll replace the JCR repository with MongoDB, a scalable document repository with much to recommend it.
It has a nice query API and it’s very easy to set up and administer.
MongoDB is a good match for your needs because the articles are documents.
In chapter 2, we presented a brief overview of Spring Data JPA.
You saw how the framework automatically generates DAO implementations for you, simplifying the creation of the data access layer.
The Spring Data portfolio includes several other projects, including Spring Data MongoDB, which assists with the creation of a data access layer based on MongoDB.
As we mentioned in the background for this recipe, there isn’t much to do here.
We’ll break it down into a handful of short, easy steps, a few of which are absolutely trivial.
Go to the MongoDB website (www.mongodb.org/), download the MongoDB package, install it, and start up the instance.
Even if you’ve never heard of MongoDB, you should be able to get the entire thing up and running in under 30 minutes using the MongoDB Quickstart guide.
If you completed recipe 12.1 before coming to this recipe, you’ll want to get rid of the JCR-related code.
You can take out ArticleDao, JcrArticleDao, ArticleMapper, and the beans-jcr.xml configuration.
Between JcrArticleDao and ArticleMapper, that’s a decent chunk of code you’re eliminating.
You can also remove JCR-related dependencies from the Maven POM.
It’s similar to the one it replaces, but this time it uses Spring Data MongoDB.
All you have to do is parameterize the MongoRepository interface with the domain object type (Article) and the ID type (String)
Also, you don’t have to implement the interface yourself because Spring Data MongoDB will generate an implementation for you automatically using Java’s dynamic proxy machinery.
Finally, you don’t need to worry about mapping the Java Article object to MongoDB’s native BSON format,8 because Spring Data MongoDB also handles that for you.
Because you’ve changed the method names on the ArticleDao, you need to make the corresponding changes in ArticleController, which calls the methods.
Only three such changes are required, and they’re straightforward, so we won’t cover them here.
The next listing is effectively the replacement for beans-jcr.xml from recipe 12.1
To use Spring Data MongoDB, it’s useful to employ the namespace configuration, so you declare the namespace and the schema location B.
Various instance configuration options are available; D is a partial list.
Once you have an instance, you need a MongoTemplate to access the instance E.
This template is like other templates in Spring, such as HibernateTemplate and RestTemplate.
You call the template mongoTemplate to support automatic discovery, which you’ll see shortly.
You pass the mongo instance into the constructor, and you choose techsite as the name of the database you want to use in the MongoDB instance.10
At F you tell Spring Data MongoDB to scan com.springinpractice.ch12.dao for DAO interfaces (that is, interfaces that extend the MongoRepository interface) and to generate implementations dynamically.
Finally, G tells Spring to activate exception-translation in the generated DAO implementations.
This will translate MongoExceptions into exceptions in the DataAccessException hierarchy.
Figure 12.12 is the bean-dependency diagram corresponding to the configuration.
An instance hosts a set of databases, and each database has a set of collections.
Don’t forget to update web.xml to reflect the new app context configuration file:
Restart the app, import the article packages, and be amazed.
Other than ArticleConverter (the component that extracts an article from a file upload), the Java code in the app is close to trivial.
This recipe showed how to simplify the data access layer using Spring Data MongoDB.
You saw that the framework handles DAO implementation and persistence mapping for you.
The Spring Data portfolio includes support for many of the new NoSQL offerings that have been appearing over the past few years.
We looked at JPA in chapter 2 and MongoDB (an example of a document repository) here.
Other database types include key/value stores, big data stores, graph databases, and several other general categories.
See the Spring Data section of the SpringSource website for more information (www.springsource.org/spring-data)
At the time of this writing, Spring Data CouchDB is planned, but it doesn’t exist.
Several comparisons between CouchDB and MongoDB are available on the web, so read those and choose the document store that makes the most sense for your project.
In this chapter, you learned how to use Spring to create an article-delivery engine.
The work you did here falls under the more general category of content management, and you can apply the techniques you used here to building other CMS domains and functions.
It’s also useful if you need to offer support for different back-end stores, because JCR is a standard implemented by several vendors in the content-management space.
But be sure to explore the newer NoSQL document stores, including MongoDB and CouchDB.
They’re promising alternatives to some of the more traditional approaches to content management, including relational back ends and JCR.
In addition, because NoSQL stores are especially well-suited to addressing issues of scale, there is a lot of industry excitement around their use.
Thus you can expect over time that NoSQL stores will become even more capable.
In the next chapter, we take a whirlwind tour of enterprise integration, building a Spring-based help desk system.
We’ll use Spring Data REST, Spring HATEOAS, Spring Integration, and Spring AMQP/Rabbit to complete our task of building an external-facing but internally developed customer portal.
In a perfect world, there might be a fixed set of business processes, and the systems chosen to support those processes would play nicely together.
There are many reasons why the systems in an environment might be a jumbled mess.
A single vendor may offer a highly integrated tool suite, but the IT organization.
An enterprise of any size might have hundreds of different software systems, such as monitoring tools, ticketing systems, collaboration platforms, and so forth.
And despite the difficulty, the need for systems integration is very much alive and well.
In this chapter you’ll imagine that you’re building a Spring-based help desk system to support an external-facing, internally developed customer portal.
Customers must be able to create tickets in a self-service fashion.
The help desk system must have an internal-facing UI that lets support reps create tickets on behalf of customers who call the help desk on the phone.
You need to support self-service ticket creation through a legacy email address.
On the outbound side, you’ll suppose that the help desk must send the customer a confirmation email regardless of whether the customer or a support rep creates the ticket.
Figure 13.1 shows a conceptual overview of your eventual goal in this chapter.
A real help desk system has lots of other functionality, such as workflows, reporting, and so on.
But we’re more concerned with showing how to perform integrations than with creating an actual ticketing system, so we’re sticking to a basic structure.
That being the case, let’s pause for a quick tour.
Although it’s outside the scope of this chapter to present SI comprehensively,1 a better picture will emerge as you work through the recipes.
Figure 13.1 Overview of your help desk system and its integrations as they appear at the end of the chapter.
Note that we assume some familiarity with these patterns and the integration domain in general; see www.eaipatterns.com/ for more background if you require it.
Something interesting about SI is the way it uses dependency injection.
The typical Spring application uses DI in what might be called a vertical fashion: the application is organized as a set of layers, and you inject beans at layer n into beans at layer n+1
For example, you inject DAOs into service beans and service beans into web controllers.
The idea is to build an integration layer just above an application’s services and implement the integration layer in the pipes and filters architectural style.2 In SI, the pipes are called channels and the filters are called endpoints, and the overall pipeline is a messaging system supporting the integration of connected services.
Endpoints perform message routing and processing, whereas channels convey messages from endpoint to endpoint.
You can build an entire messaging system out of channels and endpoints by injecting channels into endpoints.
This horizontal use of DI is entirely compatible with the vertical use.
Especially when you’re integrating internally developed apps, it’s sometimes helpful to have service activators in the integration layer invoke your Java service beans directly.
But in general, SI uses dependency injection as a way to build a horizontal layer of integration flows.
Most of the communication with underlying services happens through the application’s coarse-grained external interfaces (for example, web services and messaging endpoints) rather than through the fairly fine-grained mechanism of Java DI.
Note that although SI provides ready-made implementations of the various EIP patterns, there are architecturally different approaches to applying those patterns, and SI isn’t prescriptive about it.
You can use SI to implement a simple, in-process messaging.
Integration and services: an architectural perspective One of the things I (Willie) struggled with when I was first learning about integration was understanding just what it is that’s being integrated.
It can get confusing because there are multiple ways of talking about services, and they often come up in a single conversation.
The channels correspond to pipes in the pipes-and-filters architectural style, and the endpoint corresponds to a filter.
Integrating applications via a shared database infrastructure that a given app uses to communicate with external resources.
Alternatively, you can use it to communicate with an external message broker (ActiveMQ, RabbitMQ, SonicMQ, and so on), which usually provides better decoupling, flexibility, and resiliency.
You’ll see examples of both of those in the recipes ahead.34
The first recipe starts you off by illustrating a common form of application integration: a shared database.
In many cases, multiple apps need to work with the same data.
One app might capture leads, another might qualify the leads, and a third might present the leads to salespeople trying to close.
Especially when the apps in question are developed internally, it’s often easiest and most appropriate to have the apps all work with a shared database.
There are major, revenue-critical systems that work exactly this way.
We won’t explore that here, but by the end of this chapter it should be obvious how to do it.
Services in an integration context refers to the coarse-grained interfaces that different apps/ systems happen to expose to the world and that provide integration hooks.
This could be SOAP/REST web services, messaging endpoints, HTML forms, file-based, emailbased, and so forth.
Obviously this is a permissive conception, and not one you would use to prescribe for a green field, service-oriented architecture.
But for integration it’s appropriate because it’s sometimes the case that the only way to talk to an app is to post data into an HTML form that it provides.3
Services in this sense refers to something larger than the service beans we often find in Spring-based applications, because (for example) RESTful endpoints live above the service beans.
There are certainly cases where the integration layer has direct access to an application’s Java code, and so the integration layer might call that code directly, but this is a special instance.
The more general case is an integration layer interacting with apps/systems through whatever interfaces they expose.
You’ll see in this chapter, for instance, how creating a ticket in the portal app causes the creation of a ticket in the help desk app, which in turn generates a confirmation email.
These apps and systems were independently conceived, but you use integration to make them work together.
Integrate applications by having them work against a shared database.
For your current purposes, assume that your help desk and portal applications need to work with common customer and ticket data.
Customers need to be able to create tickets using the portal, and those tickets need to show up in the help desk app for processing by support representatives.
The approach here, as noted, will be to use a shared database.
This approach generally requires that you control the apps in question because you have to be able to make them agree on the database schema.
But where that’s true, a shared database is an option to consider.
At small scale, the coordination between apps on schemarelated issues is generally manageable, and this allows you to avoid having to build out separate abstraction layers such as web services.
There isn’t anything too special about having two different apps use the same database.
You have to pay attention to transactions, but that’s usually true for single apps.
Point your apps at the same database, and you’re in business.
Your real goal in this recipe is to establish a code baseline for subsequent recipes.
To that end, you use Spring Data JPA as described in recipe 2.6, rather than working directly with the Hibernate API.
You still use Hibernate as a JPA provider.) The reason is that you want to set the stage for using Spring Data REST in recipe 13.2, and this depends on Spring Data JPA.
Feel free to review recipe 2.6 or look at the sample code.
One special case comes up in the context of shared database integrations, and it’s worth investigating.
Even though this is a recipe about having multiple apps work with a shared database, sometimes that plays out via an app with a dedicated database, and the app needs data from another app’s database as well.
Now you have an app that has to work with two databases.
In this case, the portal app owns the customer database, and the help desk app owns the ticket database.
But the help desk app needs customer data so it can resolve customer usernames in the tickets to the customer’s full information (name, contact information,
So the help desk app will use the customer database in addition to its own ticket database.
One possibility for working with multiple databases is to use distributed transactions.
Even though you’re only reading customer data, in theory transactions could be useful for proper isolation, setting lock modes, and setting timeouts.
You might use them, for instance, to handle the case where the portal changes a customer username or deletes a customer entirely, either of which would break the soft association (based on usernames) between tickets and their customers.
But in fact there’s no worry here, because the portal doesn’t allow users to change their usernames, and it also doesn’t delete customers.
With customer data, you’d probably want soft deletes, where you keep the customer record but use a flag to indicate whether the customer has been deleted.) And even if you were for whatever reason to allow these things, they would presumably be insufficiently common to warrant the performance overhead of distributed transactions.
Instead, your help desk will use two different transaction managers: one for working with tickets and the other for working with customers.
You haven’t seen that yet, so let’s look at how the help desk app does it.
First, the following listing shows the help desk’s configuration for the ticket database.
Figure 13.4 The portal and help desk applications share customer data through direct access to a shared customer database.
For smallscale integrations, this can be a clean and simple solution.
You’re using Spring Data JPA, so you set up a factory for JPA entity managers at B.
At C you’re explicit about the entity manager factory (EMF) because you want to avoid ambiguities with the EMF you’re about to create in the next listing.
You’re also explicit at E and F where you define the Spring Data JPA repositories.
Spring Data JPA will apply the specified EMF and transaction manager to the repositories it generates.
Your single transaction manager can handle multiple repositories just fine, because repositories correspond to tables, not to databases.5
The help desk app has a repository configuration file for the customer database as well.
The JtaTransactionManager can handle multiple databases in support of distributed transactions, but you aren’t using that here.
In the previous listing you’re explicit about the EMF and transaction manager, just as you were in listing 13.1
Sometimes there’s only one database, and all the apps use it, but the configurations show how to deal with multi-database scenarios where distributed transactions aren’t a concern.
You’re dealing with two separate applications here, and each has its own configuration location and configuration files.
Because you’re dealing with two applications here, the URLs are different than other URLs in the book:
You should see that they’re both sharing the portal’s customer data.
In this recipe we looked at the common approach of using a shared database to integrate multiple applications.
This didn’t require any special technology beyond what the database natively provides with respect to transaction management.
Despite its simplicity, it’s important to consider reasons why you might choose not to adopt this approach, even initially:
A shared database works well enough at small scale, but as you add applications it becomes increasingly difficult to coordinate changes.
One app may need a schema change that would break other apps, and so the change can’t occur until all apps are ready for it.
Depending on the database technology, it may be difficult to scale out as you add apps.
The shared database quickly becomes a single point of failure.
By the time you decide to pursue a different integration approach, the apps are.
If you don’t control the apps in question, then a shared database probably isn’t.
Even if you do control the apps, there’s a good chance that you have third-party.
So you may need to come up with an integration solution that doesn’t require a shared database anyway.
Web services are a popular and effective solution for some of the challenges described.
They make it possible to decouple the client view of the data from the details of the actual database implementation, which provides needed flexibility.
The next recipe shows how to integrate applications using RESTful web services.
In recipe 13.1 you saw that although shared database integrations may be simple at small scale, the approach can quickly run into both development and operational issues with increasing scale.
Web services are a battle-tested technique for decoupling clients from the data they use.
Originally SOAP-based web services were the “official” approach, but over time the growing consensus was that SOAP was too heavyweight for many purposes, and so the REST-based approach to web services took the lead.
But SOAP or REST, the idea is to create an abstraction layer in front of core capabilities and data so the owners of those capabilities and data can make back-end changes without breaking their clients and without being held hostage by those clients (that is, being prevented from making changes)
With the REST approach in particular, service designers try to minimize the amount of knowledge that clients must have to work with the service.
For example, REST’s Hypermedia as the Engine of Application State (HATEOAS) principle says that clients shouldn’t require anything beyond a general knowledge of working with hypermedia-based systems.6 This constraint, properly observed, further decouples clients from the services they use.
Integrate applications without the tight coupling entailed by shared database integrations.
This recipe takes a first step in the direction of decoupling your apps by eliminating their common dependency on the details of the shared customer database schema.
Use Spring Data REST to implement (as part of the portal app) a RESTful web service API in front of your customer data.
Use Spring HATEOAS to implement data transfer objects ■ Use RestTemplate to implement (as part of the help desk app) a client for your.
In the sample code we introduce a dependency in the other direction as well: the portal needs to get data (ticket statuses, ticket categories) from the help desk’s ticket database to allow customers to create self-service tickets through the portal UI.
Because both directions are entirely symmetrical, we’ll cover only one direction here: the one where the help desk calls the portal web service API to get customer data.
Refer to the sample code if you want to see the other direction as well.
Spring Data REST (SDR) is a relatively recent addition to the Spring family.7 It builds on Spring Data JPA (SDJ) to expose SDJ repositories through a RESTful, JSON-based web service API.
Let’s look first at the portal app’s single repository interface, UserRepository.
At B you use the SDR @RestResource annotation to specify a path to the resource, relative to the servlet path.
You’re specifying "users" because otherwise SDR defaults to "user" (based on UserRepository), which you don’t want.
At C you attach an SDR path to your custom SDJ finder query.
You do the same thing at E and F for a collection-driven custom finder query.
At the time of this writing, it’s still a release candidate, so it may change a bit by the time you read this.
Listing 13.3 UserRepository: SDJ repository annotated for exporting by SDR.
The apps hide their databases from one another but expose RESTful web service APIs for data access.
All that remains is to define the exporter servlet in web.xml.
True to HATEOAS form, the resources specify links that you can follow to get further results.
Let’s see what happens when you hit the search endpoint:
To take advantage of the new web service, the help desk app will find it useful to have data transfer objects (DTOs) for data binding.
Besides the actual payload, Spring HATEOAS supports links, which you would expect because that’s a big part of the HATEOAS idea.
On the client side, you want those links because SDR uses URIs for resource identification (again, no surprise) rather than the database IDs.
You can use Spring HATEOAS to implement the desired DTOs on the client side.
Because both individual users and collections of users have associated link information, you need separate DTOs for each.
The next listing shows the resource for an individual user, which the help desk refers to as a customer rather than a user because the help desk’s users are support representatives.
You extend the Spring HATEOAS ResourceSupport class at B, which does the heavy linking around generating links and such.
The DTO’s fields need to be public in order for data binding to work, so that’s what you do at C.
Here all you do is extend the Resources class, specifying CustomerResource as the type argument.
The last step is to implement a client for the help desk app.
In integration parlance, a gateway provides an application with an interface to the underlying messaging infrastructure without the application realizing it.
Because you want the help desk to be able to get customer data from the portal without realizing that it’s making a web service call, you’ll create a PortalGateway interface for the help desk to use, along with an implementation that makes the web service call using RestTemplate and your resource DTOs.
Listing 13.7 PortalGateway: hides messaging details from the help desk app.
You use Spring’s RestTemplate B to invoke the portal’s web service API, located at the baseUrl C.
The gateway has two methods, corresponding to the two custom queries you implemented for the web service.
The first one returns a single customer, which the help desk app uses to resolve a username to a customer record on a ticket details page.
You use the CustomerResources D you wrote earlier to get the wrapper container, because Spring Data REST doesn’t know that customer usernames are unique.
The second method returns a collection of customers corresponding to a collection of usernames.
You use this one for the ticket summary page, because there are a bunch of tickets, each with its own username.
You build out the URL from scratch at F because there are arbitrarily many usernames (limited generally by paging the ticket summary)
Finally you return the underlying collection of CustomerResource instances I to match the API signature.
Although we didn’t cover it here, the portal app has an analogous TicketGateway and TicketGatewayImpl to handle calls against the ticket service’s REST API.
One final note on gateways before we close the recipe.
We mentioned earlier that gateways hide the details of the messaging approach from the app.
Gateway implementations are part of the integration infrastructure, not part of the app.
They allow you to change the integration approach with minimal disruption to the app.
Note that you have to run both apps at the same time in order for either to work, because they make web service calls against one another.
The RESTful web service approach adopted in this recipe decouples the apps from the shared database from recipe 13.2
It also decouples the apps to some extent from one.
Unfortunately, the apps are still fairly tightly coupled to one another:
From a development perspective, they need to coordinate with each other on changes to the web service API.
From a configuration perspective, both apps have to know each other’s location.
So instead of a single point of failure, you have two.
In addition to the coupling that remains, there’s another problem.
The point-to-point integration approach breaks down because it scales as O(n2) in the number of applications to be integrated.
If you have more than a handful of applications to integrate, you’ll need to manage a lot of development, configuration, and operational linkages, as you can see in figure 13.6
Recipe 13.3 shows how you can use a centralized messaging infrastructure to alleviate much of the pain.
Recipe 13.2 Decoupling applications with RESTful web services Familiarity with the integration domain and Enterprise Integration Patterns in particular; see www.eaipatterns.com/ for background.
Recipe 13.2 explored the use of RESTful web service APIs as a way to decouple applications from one another.
Web service APIs provide a layer of abstraction over underlying capabilities and data, which makes it easier to change the implementations without impacting clients.
In addition, the RESTful approach supports decoupling by reducing the knowledge that clients must have of the services they consume.
We noted in the discussion that improvements are possible in two major areas.
First, the apps still have to know quite a bit about each other from development, configuration, and operational perspectives.
Implementing a message bus using RabbitMQ and Spring Integration scales poorly as you incorporate different apps.
This recipe presents a broker-based approach that addresses both of these issues.
Further decouple your apps, and address scalability issues associated with the point-topoint integration strategy.
The solution is to use a centralized message broker to serve as the basis for your application integrations.
Message brokers are specifically designed to address application integration, and they solve the previous issues as follows:
Centralizing the integration infrastructure allows you to transform the O(n2) integration topology to an O(n) topology.
Each app has a link to the central integration infrastructure.)
The characteristics of message brokers promote decoupling through the use of asynchronous messaging with guaranteed delivery, well-known messaging endpoints, and so forth.
Message brokers are generally reliable, runtime-configurable, and horizontally scalable, which helps with availability and performance.
This largely mitigates the single-point-of-failure issues associated with being a central location in the architecture.
There are lots of options for message brokers, but you’ll use RabbitMQ, which implements the AMQP messaging protocol.
The advantage over the Java Message Service (JMS) API is that using a protocol decouples messaging clients from the broker.
With JMS, the clients are Java clients (although any given broker has APIs for other platforms as well)
With AMQP, any platform with an AMQP client can communicate with the broker, in much the same way that any web browser can communicate with any web server, regardless of the client and server platforms.
Because most platforms have AMQP clients,8 AMQP has outstanding interoperability.
We won’t go into the details of RabbitMQ; fortunately it’s fast and easy to set up a development instance.9 You can also consult RabbitMQ in Action by Alvaro Videla and Jason J.W.
Recall from the previous recipe that you used gateways to hide the messaging system from the help desk and portal apps.
There are fewer moving parts if you remove the web services and connect the apps directly to the broker.
In other situations it might be desirable to keep the web services around.
Let’s get started by looking at message buses and canonical data models.
You’re going to use RabbitMQ to implement the message bus integration pattern.
The idea behind this pattern is to provide a central medium through which applications can communicate with one another.
Conceptually it’s based on the hardware bus concept: plug in, and you’re good to go.
A Message Bus is a combination of a Canonical Data Model, a common command set, and a messaging infrastructure to allow different systems to communicate through a shared set of interfaces.
You’ll use message queues as the shared set of interfaces.
In the previous recipe, the two apps had their own data representations.
Now you’ll standardize those by creating a separate Maven module for the CDM.
In real life there are sometimes significant business, technical, and organizational challenges surrounding the creation of a CDM, but you can ignore those because you’re lucky enough to have a simple data model.
You’ll use XML for your format because it’s widely supported, although JSON would be another plausible option.
Ideally you’d create XML schemas for the model, but you won’t mess around with that here.
Figure 13.7 Integrating applications via a centralized RabbitMQ message broker.
Implementing a message bus using RabbitMQ and Spring Integration around RESTful web services), define XML bindings, and treat the implied schema as constituting your CDM.
You have a handful of message types, but it will suffice to look at one.
It’s a bare-bones DTO with some JAXB annotations to bind it to the CDM’s XML representation.
You have DTOs for other message types as well, such as ticket categories, ticket statuses, customers, and so forth.
Now that you have a CDM in place along with a central set of DTOs, you need to make an interesting design decision.
One possibility is for the existing apps to continue using their existing data models, and perform translations as messages enter and exit the bus.
The other is for apps to adopt the central DTOs as their own data model, at least in cases where you have control over that (for example, internally developed apps)
In this case the choice is fairly clear because the Spring HATEOAS DTOs are more oriented to support RESTful web services.11 The benefit is that you can avoid message translation between apps.
You do need to modify the gateways to use the new DTOs, so let’s do that now.
In recipe 13.3 you worked on the help desk side with the PortalGateway.
For variety, this time you’ll work on the portal side with the TicketGateway.
It happens that the gateway interfaces are slightly leaky: through the DTOs they use, they expose the fact that you’re designing for Spring Data REST-based implementations with URIs instead of database IDs.12
Let’s replace the Spring HATEOAS DTOs with the ones you created for the CDM.
See the sample code for a similar treatment of the PortalGateway.
You can of course do the same thing for the help desk.
But you don’t have any use for links here, so you’ll go with plain-vanilla DTOs.
Spring HATEOAS may be useful for implementing message-bus CDMs in addition to REST APIs.
Implementing a message bus using RabbitMQ and Spring Integration app.
In effect, you can use SI to create app-specific adapters to the RabbitMQ messaging infrastructure.
This section focuses on implementing the portal app’s outbound messages; that is, you’ll implement the TicketGateway interface.
To complete the circuit, you also need to handle inbound messages into the help desk, so you’ll do that as well.
We won’t cover requests originating from the help desk app because the logic involved is more of the same.
Refer to the sample code if you want to see it.
Let’s start by implementing the integration logic for the portal’s self-service ticket creation feature.
TicketGateway has a createTicket(Ticket) method that serves as a nice starting point because it’s fairly straightforward.
The idea is that the customer creates a ticket using the portal’s web interface, and the portal passes it along to TicketGateway.
Behind the scenes, the gateway does an asynchronous fire-and-forget at the messaging infrastructure, meaning that the call returns immediately.
Later we’ll look at the message-handling code on the help desk side, but to keep things simple let’s focus on the portal’s fire-and-forget code.
Figure 13.8 shows what this looks like using the EIP graphical language.
Note that the Spring Tool Suite generates these diagrams automatically from the SI configuration files; click the IntegrationGraph tab in the configuration file editor.
At the front end is a TicketGateway proxy that accepts requests from the application through the TicketGateway interface.
It passes ticket creation requests to the AMQP channel adapter by way of a channel, and the channel adapter in turn pushes the message to a RabbitMQ exchange.13 In the case of ticket creation, all of this is completely asynchronous, so control returns to the portal immediately after invoking the TicketGateway.
The following listing shows how to implement the pipeline using SI, Spring Rabbit, and Spring AMQP.
If the exchange concept is new to you, you might want to take a few minutes to read up on it.
Quite a bit is happening in this listing, but you can break the configuration into three sections: RabbitMQ, Object/XML mapping (OXM), and SI.
First, the RabbitMQ configuration begins with a connection factory B.
Note that the default credentials for a fresh RabbitMQ installation are guest/guest.)
You use <rabbit:admin/> at C to create queues dynamically if they don’t already exist.
At D you declare a single queue for ticket-creation requests.
At E you create a template for sending messages to Rabbit.
This follows Spring’s general practice of template-based communication with external systems and resources.
The template uses a MarshallingMessageConverter (part of Spring AMQP) at F to perform OXM on message payloads.
By default, the Rabbit template uses a SimpleMessageConverter, which handles strings, Serializable instances, and byte arrays.
Because you want an XML-based CDM, you need a converter that performs OXM.
At H you define a dynamic proxy for the TicketGateway interface.
The configuration at I routes tickets coming in through AMQP outbound channel adapter 1) receives it and pushes it to Rabbit’s default exchange, because you haven’t specified an exchange explicitly.
Gateways support bidirectional, request/ reply messaging, but you don’t require that here.) The channel adapter’s routing key is set to createTicketRequest.queue, so the default exchange routes it to that queue.
The message payload is ticket XML in the canonical format because the adapter uses the Rabbit template, which in turn uses the MarshallingMessageConverter.
That takes care of the fire-and-forget implementation of ticket creation on the portal side.
Now there’s a message with an XML ticket payload sitting in a request queue on your bus.
The next step is to implement integration logic on the help desk side to receive and service the request.
See figure 13.9 for a diagram showing how this works.
An inbound channel adapter receives the request from Rabbit, maps the ticket XML to a ticket DTO, and passes it to a processing chain.
The chain’s first endpoint is a transformer (SI’s terminology for EIP’s message translator) that maps the DTO to a returns the saved instance, but the chain discards that message by dropping it onto the global nullChannel, which is essentially a black hole like /dev/null in Unix.
Figure 13.9 A help desk integration pipeline that receives ticket-creation messages from the bus and creates tickets in the help desk database.
As with the portal application, you have an initial RabbitMQ configuration B, although this time you don’t need a template.
This time around you have some transformers (more on that in a minute), so you scan for them at D.
In listing 13.11 you had an AMQP outbound channel adapter to send messages to the bus, so here you have the inbound counterpart E.
The inbound channel adapter receives ticket-creation requests from createTicketRequest.queue and passes them via a channel F to a chain G.
A chain is a linear sequence of endpoints connected by implicit channels.
The first endpoint is a transformer H that transforms the ticket DTO into a ticket entity, as you’ll see.
The second endpoint is a service activator I that saves the ticket entity to the Spring Data JPA ticket repository using a Spring Expression Language (SpEL) expression.
The variables headers and payload are available for use, although you’re the original caller; you send it to the global nullChannel G, which sends the message to a black hole.
Next is the transformer that converts ticket DTOs into ticket entities.
Although it’s possible to use annotations to configure SI components, you’re using XML because I (Willie) find it easier to understand when the SI configuration is in one place.
When there’s a single public method, you don’t have to specify the transformer method explicitly in the XML, but you do it anyway.
Because the ticket DTO has references to category and status DTOs, you delegate the transformation to corresponding transformers C and D.
With that, you have a full asynchronous flow from the portal application through the message bus and ending with the help desk.
To be sure, there are some details we’ve neglected, such as error handling.
The next section looks at a more complex case: implementing synchronous finder methods.
Finder methods involve a request/reply communication style, which takes more effort to implement in a messaging environment than the fire-and-forget style does.
In this case you’ll implement synchronous request/replies, meaning the caller will block until the reply arrives; but note that SI also supports asynchronous request/replies, which are based on a callback mechanism.
Integration and services: an architectural perspective You might fairly ask why you would implement synchronous request/reply on top of a fundamentally asynchronous messaging infrastructure.
Wouldn’t it be simpler to have the caller invoke a web service on the target system?
In many cases it’s indeed simpler to make a web service call.
You can avoid implementing a bunch of integration patterns on the bus, as well as avoid forcing the request and reply messages to pass through the message bus.
Originally you had a single path to an AMQP outbound channel adapter.
This time you add a couple of new paths to an AMQP outbound gateway.
Channel adapters and gateways are alike in that they’re both interfaces to external systems, but not alike in that channel adapters are unidirectional (fire-and-forget) while gateways support request/reply communications.
In this case, the external system is the message bus.
The following listing shows how to implement the pipeline in figure 13.10
In the asynchronous case, it doesn’t matter if a message receiver is offline when the sender sends the message, because the messaging system queues the message until the receiver is available.
With synchronous communications, the receiver must be available when the sender sends it a request.
We won’t settle the issue here, but suffice it to say there’s a design decision to consider.
The rest of the recipe shows how to implement synchronous messaging without necessarily claiming that it’s the right approach for all cases.
Figure 13.10 The portal’s outbound pipeline with support for the TicketGateway’s finder methods.
Implementing a message bus using RabbitMQ and Spring Integration ...
You add three new queues at B to support your new finder methods.
At C you add more classes to be bound to the OXM configuration.
You’ll see why you’re adding the dummy payload and special request objects in a moment.
You specify that by default all requests coming into the gateway will land on the helpDeskRequestChannel.
You also set a default reply timeout, expressed in milliseconds, because now you’re expecting replies.
On replies: unless you specify an explicit default-reply-channel (which you’re not doing here), the gateway creates for any given request a temporary, anonymous reply channel, and adds the channel to the request message as a header called replyChannel.
That way, reply-generating downstream endpoints know where to place the reply.
The reason you create a special request DTO is that you need the request to be XML.
This is because the AMQP gateway expects an XML reply from the bus (recall your CDM), which it maps to an object via the AMQP template, which in turn uses the MarshallingMessageConverter.
The template applies the converter to both the request and the reply, so the request needs to be a mappable DTO as opposed to a simple string.
In addition to the SpEL payload, the finder method definition includes a custom requestType header (custom in the sense that you invented it)
Both SI and RabbitMQ support message headers, but here, the header is an SI header.
You’ll use this header to route finder requests to the right queue, as you’ll see.
This time you get a list containing all ticket categories, which is useful for populating the category drop-down in the new ticket form.
By default, SI treats no-arg gateway methods as connecting to pollable (receive-only) channels, as opposed to no-arg request/reply (sendthen-receive) channels.
To implement a request/reply communication, you need to provide a dummy payload using payload-expression.
Normally you can pass in a dummy string or a Date:
But here that doesn’t work because you’re using MarshallingMessageConverter, which expects payloads to be mappable XML.
That’s why you have the DummyPayload class, and you use payload-expression to create an instance here.
Once again you enrich your message with a requestType header for routing purposes.
The third finder retrieves a specific ticket category by ID G.
Once again you need to represent the payload ID using XML rather than a Long.
You override the gateway’s default request channel with a new channel called findTicketCategoryRequestChannel and then pass the message over that channel H to the transformer at I.
Here you take advantage of the transformer’s expression attribute to wrap the Long ID in a mappable request DTO.
Finally the message goes to the helpDeskRequestChannel J like the other help desk requests.
You use the AMQP template to do the actual request and reply.
First you use routing-key-expression to specify a dynamic, message-driven routing key that allows Rabbit’s default exchange to route messages to queues.
In this case, the expression is a SpEL expression that appends .queue to the value of the requestType SI header you’ve been using.
You use mapped-request-headers to indicate that you want the SI requestType header to appear as an AQMP header as well once the message hits the bus.
This is because you’ll have further use for this header for routing on the help desk side.
As with the initial gateway, the AMQP outbound gateway generates a reply.
For any given request, the outbound gateway creates a temporary reply queue and sets the AMQP message’s reply_to property to the queue’s name.
This tells downstream endpoints where to place the reply when it materializes.
Once the reply appears in that queue, the AMQP outbound gateway grabs it and places it on the request message’s reply channel.
You’ll recall from our discussion that the request message maintains a reference to the reply channel as the value of its replyChannel header.
Now the portal sends finder requests to the bus, so the help desk needs to pick those up and service them.
Figure 13.11 The help desk’s inbound pipeline to support the TicketGateway’s finder methods.
Although it’s not shown here, each chain contains a service activator followed by a transformer.
This help desk pipeline receives finder requests at an AMQP inbound gateway and forwards them to a router, which uses the requestType header to pass the request to one of three chains.
Each chain invokes a finder method on the TicketRepository and uses a transformer to convert the result into a DTO before returning it to the caller.
Here’s the configuration you use to implement the help desk pipeline.
As was true with the portal SI configuration, you declare the three queues for finder requests at B to ensure that they exist.
You also declare the same set of DTOs for OXM at C because you’ll need to convert back and forth between the bus CDM and Java.
You need a few transformers to convert the entities you find into DTOs, so you scan for them at D.
The entry point for synchronous messages into the pipeline is the AMQP inbound gateway at E.
You specify its three feeder queues using the queuenames attribute.
Just as you used mapped-request-headers in listing 13.14 to convert the custom SI requestType header into an AMQP header, you use it here to convert the AMQP header back into a custom SI requestType header.
When the gateway receives a message from a queue, it creates an anonymous reply channel and attaches it to the message using the replyChannel message header.
Eventually some downstream component responsible for producing the reply will place the reply in that channel.
The gateway passes requests to a router that uses header values to drive routing F.
As you’ve guessed, you’re using the requestType header for that.
Once the request leaves the router, it goes to one of three chains you’ve defined, corresponding to the three finder requests.
First is a chain for the ticket status requests G.
The chain has an expression-driven service activator that unpacks the key from the request object (recall that you wrapped the key with a TicketStatusRequest The result is an entity, so you use a transformer to convert the entity back to a DTO for.
See the sample code for the transformers, which are similar to the one from listing 13.13
Because you haven’t specified an explicit output channel for the chain, the chain sends the transformer’s output to the channel you’re storing under the replyChannel header.
The circuit is now complete: the help desk AMQP inbound gateway receives the reply from the channel and sends it to the specified exchange and queue (as specified by the routing key)
The portal AMQP outbound gateway receives the reply from the queue and places it on the replyChannel.
Finally the initial portal gateway receives the reply and returns it to the caller.
The chains at H and I are essentially similar to the one at G.
You now have the plumbing on both the portal and the help desk sides to support both asynchronous and synchronous communications over Rabbit.
Although we didn’t cover it here, note that the help desk also requests customer information from the portal, using largely the same set of patterns, but in the opposite direction.
Over the past three recipes we’ve shown how to integrate applications in a progressively more decoupled way.
Though we’ve considered only two apps here, the architecture’s power becomes more obvious as you place additional apps on the message bus.
The number of potential integrations grows quadratically in the number of apps, but the integration complexity increases only linearly.
In this recipe you used RabbitMQ as the bus-implementation technology and SI as a way to implement app-specific bus adapters.
In the recipes that follow, you’ll reposition the help desk’s SI pipeline as an application bus in its own right and then add both inbound and outbound email by attaching them to the application bus.
Although many sites offer a formbased option to better structure the ticket and to avoid email spam, email can be an attractive option because it’s so easy to implement: all it requires is an inbox.
In this recipe, imagine that you only recently rolled out the form-based approach from recipe 13.1, but you still want to support a legacy support email address that was.
Sourcing tickets from an IMAP store your primary ticket source prior to introducing the form.
We’ll show how to create help desk tickets based on incoming customer email.
One question you might be asking is why you wouldn’t attach inbound email to the RabbitMQ bus instead of attaching it to the help desk’s SI adapter.
After all, the portal is a ticket source, and you’ve attached it to the RabbitMQ bus.
And in the help desk, you’re using SI as an adapter to the RabbitMQ bus, so accepting email from a source other than the bus seems to conflict with this design.
You could certainly do that, but one reason you’re not is that you’d need a separate adapter to connect the inbound email channel to RabbitMQ, and that’s a complexity you don’t currently require.
Only the help desk cares about inbound email, and if the help desk isn’t available to receive email, then messages sit in the mailbox until the help desk is available again.
In effect, the mailbox functions as a persistent message queue.)
As to the design conflict, the conflict is only apparent.
All the help desk sees are the gateway interfaces you happen to have in place; the app doesn’t know anything about SI or RabbitMQ.
Instead of thinking of the SI pipeline strictly as an adapter to the RabbitMQ bus, you can consider it to be an application bus in a federated bus.
Figure 13.12 You’ll add an emailbased ticket channel using SI’s support for inbound email.
Figure 13.13 presents graphically the pipeline you’re going to create.
You’re adding an IMAP inbound channel adapter to receive email messages from an IMAP mailbox.
Then you pass the email messages to a transformer, which converts them into ticket DTOs.
The DTOs are the canonical data model for the help desk’s application bus.) The transformer drops the DTOs onto the existing createTicketRequestChannel, which allows you to take advantage of the downstream chain for saving tickets you created in recipe 13.3, and also to use it as a single location for making changes to the integration logic.
You’ll see this benefit in action when you add confirmation emails in recipe 13.5
Listing 13.16 shows what you need to add to your help desk beans-integration.xml configuration to support the pipeline depicted.
Spring Integration supports multiple integration architectures The preceding discussion highlights the fact that SI is flexible; it doesn’t prescribe a specific integration architecture.
You can have a single central message broker with SI adapters if you like.
Or you can even use SI itself as a central bus.
Figure 13.13 You’ll augment the inbound pipeline to include support for IMAP messages.
Now that you’ve read the warning and created a test account, please see the following listing to add support for inbound email.
This is a special IMAP IDLE adapter, which supports the IMAP IDLE notification mechanism.
If your provider doesn’t support IMAP IDLE, then you can use a standard IMAP inbound channel adapter with a poller:
In any event, you specify the IMAP store (mailbox) URI and also tell the channel adapter to go ahead and delete messages from the mailbox after pulling them down.
Listing 13.16 Help desk’s beans-integration.xml, with support for inbound email.
The IMAP channel adapter treats every email in the mailbox as a message to be processed and deleted.
Please use a test email account, not your personal or work account.
I (Willie) learned this the hard way by stupidly deleting several years of Gmail messages from my personal inbox.
Notice the use of IMAPS, which is IMAP over SSL (standard port is 993).15 Obviously you need to replace username and password with the actual credentials associated with the account.
After the channel adapter receives an email message, it sends it to a transformer C so that it can be converted into a DTO.
You’re using the transformer’s expression attribute to select the transformation.
After that it goes to the createTicketRequestChannel, where the chain from recipe 13.3 receives it and saves it to the TicketRepository.
The transformer code is important, so let’s look at that.
The transformer is an updated version of TicketTransformer from listing 13.13
If you run into PKIX/certificate trust issues, you may need to import the Gmail IMAP certificate into your truststore.
This discussion involves SMTP, but with minor modifications it applies to IMAP as well.
You use the @PostConstruct annotation B to declare a method for Spring to run after creating and injecting the bean.
You use this to preload the General ticket category (the support rep can change it to something more appropriate) and the Open ticket status.
The IMAP channel adapter produces a MimeMessage, so you transform that into a DTO so the downstream chain can save it.
Choose a test account for your IMAP store, and, if you’re using GitHub code, change should-delete-messages from false to true on the IMAP inbound channel adapter.
Then start up the help desk app and send an email to the test account.
The channel adapter should see the email, grab it, delete it from the mailbox, and then turn it into a ticket.
You can see the ticket by viewing the ticket list in the help desk’s UI.
In this recipe, you learned that it’s easy to add support for inbound email to a Springenabled application.
This is useful because email is still a popular way to allow users to submit support requests and other communications.
In the following recipe, we’ll revisit the topic of confirmation emails, which you saw in chapter 8
This time you’ll use SI to send the confirmation email.
Generally, when users submit support tickets, you want to send them a confirmation message thanking them for their ticket and letting them know when they can expect to hear back from you.
Send the user a confirmation email when they submit a ticket.
Figure 13.14 shows the last step in the evolution of your integration environment.
This time you’re adding support for confirmation emails, which you send by way of SMTP.
This is the same as figure 13.1, but it’s reproduced here for your convenience.
As it happens, you can add confirmation emails without changing any app code.
Recall from recipe 13.4 that you connected the IMAP inbound channel adapter to a chain that your AMQP inbound channel adapter was already using for creating tickets.
Because they’re both using the same pipeline, you can modify that pipeline a bit to.
Figure 13.14 Adding outbound SMTP messaging to support confirmation emails.
Send confirmation messages over SMTP generate confirmation emails, regardless of whether the ticket came from the web form or an email.
This is the same as what you saw in recipe 8.2.16
Listing 13.18 Help desk’s beans-integration.xml, with support for confirmation emails.
Figure 13.15 Modifying the pipeline to support confirmation emails whenever somebody creates a new ticket.
You replace the original point-to-point channel C with a publish/subscribe (pub/ sub) channel.
The difference between them is that a point-to-point channel can have at most one consumer, whereas a pub/sub channel broadcasts messages to any number of consumers.
Here you want to continue broadcasting to the chain that saves the ticket, but you want to add a new consumer pipeline to generate confirmation emails.
The start of that new pipeline is the transformer at D.
Then you pass the emails along to an SMTP outbound channel adapter E, which sends the email.
Listing 13.19 TicketTransformer.java with a transform method for confirmation emails.
You use @Value to inject a couple of confirmation email parameters into the transformer at B.
The new transform method creates a confirmation email from a ticket DTO C.
You create the email D and then use the DTO to populate its fields E.
With respect to the description F, you hardcode the confirmation message, but in a more realistic example you would use a template engine (Velocity, FreeMarker, and so on) as you did in recipe 8.2
Start up the help desk and the portal, and try creating messages through the portal, help desk, and email interfaces.
In each case you should see the help desk generating confirmation emails.
Note that you’ll need to change the email addresses of the sample portal users to your own email address if you want to receive confirmation emails when submitting tickets involving those users.
This recipe demonstrated that it’s possible to perform integrations without having to modify the apps.
You added confirmation emails by replacing a point-to-point channel for ticket creation with a pub/sub channel and then attaching both the help desk service and the confirmation email pipeline to that channel.
Where you control the apps being integrated, it makes sense to consider combining integration logic with app modifications to eliminate redundancy and simplify integration.
In general, this means you’ll want to create abstract representations of key actions on the bus.
But to add a confirmation email, you had to represent that action in the bus and treat the actual ticket creation as just one flow out of the bus.
Ultimately, the services become implementation details to the logical representations on the bus, which makes the architecture and services easier to evolve over time.
Integration is an important concern in enterprise environments, where there is generally a bewildering array of both complementary and competing tools in place, often with little hope of long-term harmonization.
Integration becomes that harmonization— it provides a practical way to connect tools and their data to support higher-level process and workflow integration.
There are many approaches to integrating systems, and we’ve covered some of the databases can offer a simple and quick way to make data broadly available.
But this approach scales poorly, and so the next step is often to use web services to enhance decoupling.
Finally, domain- or even enterprise-level, broker-based messaging is a powerful way to increase decoupling even further, which becomes important as the number of collaborators in the integration grows.
Spring Integration in Action and RabbitMQ in Action, both published by Manning, are great places to start.
In the next and final chapter of the book, you’ll learn how to use Spring to create your own framework, complete with annotations and namespace configuration.
We’ll begin a quick overview of the pattern, then jump right into the parts that build your framework.
The code in this chapter is based on the open source Kite framework at https://github.com/springinpractice/kite.
Circuit-breaker overview Integration points between systems are a common source of production issues.
It’s common for problems with a service to create performance and availability issues for clients.
Similarly, it’s common for misbehaving clients to create performance and availability issues for the services they use.
Unavailable service makes client unavailable—Service unavailability often propagates across an integration point to render the client unavailable.
Slow service depletes client resources—When a service is merely slow rather than unavailable, client threads may spend a lot of time blocking on connections to the service, leaving fewer threads available for servicing new requests.
Client responds to slow service by hammering—Sometimes clients include aggressive retry logic.
When a service is having capacity issues, aggressive retrying only exacerbates the situation.
You can use the circuit-breaker pattern to prevent failure from propagating across integration points.
A software circuit breaker is like its counterpart in the physical world in that under normal conditions it’s closed, and requests (analogous to current) flow freely across the breaker.
But when the request failure rate crosses a given threshold, the breaker transitions into an open state for a period of time, during which client requests fail fast, protecting both the client and the service.
Figure 14.1 Fault propagation from service to client, and from client to service.
Figure 14.2 The breaker on the left is in the closed state, which is normal.
Besides the closed and open state, circuit breakers have a third state: half-open.
The breaker goes half-open after the open state’s timeout has elapsed.
A half-open breaker allows a single request to pass through.
If the request succeeds, the breaker resets itself back to the normal and healthy closed state; otherwise, the breaker trips again (it goes open) and waits for the next timeout.
You typically create multiple breakers, with any given breaker protecting all integrations against a specific resource.
For instance, if you have an application that calls two web services and two databases, you might create four distinct breakers.
All methods backended by a given resource go through a single breaker.
The first recipe shows how to implement a breaker using the template pattern.
The template pattern is a well-known means for factoring out repetitive boilerplate code, especially in cases where repetitive pre- and post-execution code is involved.
It works by placing the boilerplate code in a template class and the interesting code in a callback class.
The template class has a so-called template method that accepts the callback as an argument and executes the pre-execution boilerplate, the callback,
Figure 14.3 Circuit-breaker state diagram showing all three states and their transitions.
Figure 14.4 is a sequence diagram showing how the template pattern works.
In this recipe we’ll show how to implement a circuit breaker as a template.
Create a circuit breaker to protect against integration point faults.
From the background it should be clear that templates are ready-made for circuit breakers.
You’ll begin by creating a circuit-breaker template along the lines of JdbcTemplate, HibernateTemplate, and TransactionTemplate.
Spring makes liberal use of the template pattern, and because it’s a perfect fit for circuit breakers, you’ll use it too.
In this chapter all we’re worried about is circuit breakers, but we want to make it easy to expand this to other similar components.
We call such components guards because they guard integration points.2 So you’ll start by defining a simple interface for guards generally.
Other examples are rate-limiting throttles, concurrency throttles, and user blockers.
That will be useful when you expose the guards through JMX template here.
Because there’s a fair amount of code, we’ll break it into two pieces.
The next listing presents what is mostly breaker configuration code.
At B you name the class CircuitBreakerTemplate to keep with Spring’s template-naming convention.
The reason you care about the bean name is that circuit breakers are state machines, and they ought to log all state transitions because that’s important for monitoring and diagnostic purposes.
The name allows you to indicate exactly which breaker underwent a state transition.
One is the exception threshold C, which specifies how many consecutive exceptions it takes to trip the breaker.
The next parameter is the timeout D, which you’ve set to 30 seconds.
This is how much time must pass before an open breaker tries to reset itself.
The list of handled exceptions E indicates which exception classes cause the exception count to increment.
This gives you a way to focus attention on exceptions that indicate a problem with the underlying resource.
That concludes the examination of breaker configuration, but you’re not done with the breaker yet.
The meat of the circuit breaker is the state-management and transition logic.
You use a typesafe enum for the three circuit-breaker states B.
First you have the breaker state itself C, declared as volatile.
Without going into all the gory concurrency details, the idea is that you want threads to see each others’ state updates without creating a synchronization bottleneck, which would impact performance negatively.
The volatile keyword accomplishes this: it forces reads and writes to the variable to go all the way out to main memory (where they’re visible to all threads), but there’s no mutex.
For exceptionCount D you also want cross-thread visibility coupled with high performance, but you additionally need an atomic check/increment operation.
Although the expression exceptionCount++ looks atomic, when translated down to machine code.
Once again you need visibility and performance, but you don’t need an atomic check/set operation, so volatile is fine.
Instead, it uses the opportunity to check whether the breaker needs to transition from the state.
Normally, getter methods shouldn’t have side-effects like this, but this is arguably a case where it makes sense.
A successful action clears the exception count and returns the result.
Otherwise you increment the counter and trip if there are too many exceptions.
This is how the breaker protects both the client and the service: it prevents them from communicating at all.
If the breaker is half-open 1!, it gets one shot to perform the action.
If the action succeeds, then the breaker resets back to its closed state.
If the action fails, then the breaker trips again and must wait for the next timeout period to elapse before trying again.
See figure 14.5 for a sequence diagram illustrating how it works.
For completeness, here’s the CircuitOpenException class you reference from the template:
With modest effort, you’ve created a component that promises to reduce the production support responsibilities.
To accomplish that goal, you need to see how to apply the circuit breaker to the integration points.
To demo the circuit breaker, you’ll need to create a toy transactional app with a client and a flaky service.
The app will be a simple home page that calls a message service to get a couple of different kinds of messages: a message of the day (MotD) and a list of important messages.
After the app gets the messages from the service, it displays them to the end user on the home page.
Figure 14.6 shows the key elements of the app’s bean-dependency diagram.
Figure 14.6 HomeController uses a circuit breaker to protect against problems with the MessageService.
Because you want to see the circuit breaker trip every now and then, you need a way to make the message service slightly flaky.
The following listing is the component you’ll use to do that.
In the next listing you have the simple Message class that you’ll use for the message of the day and the important messages.
This is the message service implementation: it shows how to use the CircuitBreakerTemplate class.
At D you wrap the breaker around a method that gets the MotD.
The breaker decides whether to invoke the callback based on the breaker’s internal state.
You apply the same approach at E when getting the important messages.
As promised, home.jsp adopts defensive coding practices to ensure that the page remains available even if messages aren’t.
The final step is to take care of the app configuration.
You have a handful of configuration files to create to make the app work.
The beansservice.xml, beans-web.xml, and web.xml files are all fairly nondescript; they just set up the web app.
They’re in the code download if you want to see them.
Even though the app has only one breaker, an app that uses multiple resources.
Once the server starts up, point your browser to http://localhost:8080/sip/
You should see a simple Aggro’s Towne BBS home page.4 If you refresh the page several times, you should notice that periodically one or the other message is unavailable.
Occasional flakiness won’t cause the breaker to trip, but ongoing flakiness (four consecutive exceptions, per the configuration) will.
It may take a little patience before you see a trip.
If you like, hold down the reload/refresh key for your browser until the messages become unavailable.
On the console you should see that the circuit goes open.
Templates are a convenient way to avoid repeating boilerplate code everywhere, such as boilerplate code to apply circuit-breaker logic.
Template methods have the additional benefit of being flexible, as you can place whatever code you like inside the callback method.
Before we get to those, let’s plug a gap in the current implementation: there isn’t any way to control the breaker from a management console.
For various reasons, it’s sensible to provide a means by which your Network Operations Center or other operational staff can modify breaker timeouts and manually trip and reset circuit breakers through a management console.
You might be trying to troubleshoot a production incident, or you might want to relieve pressure on a database.
This recipe shows how to expose the circuit breaker as a JMX MBean so you can manage it through a JMX-enabled management console.
The first step is to add Spring JMX annotations to some of the classes.
The following listing shows the modification required on the AbstractGuard class.
Willie wrote a bulletin board system (BBS) called Aggro’s Towne when he was a kid, and this is his way of honoring its memory.
At B you import the various Spring JMX annotations you’ll use.
Then you annotate the breaker with @ManagedResource C to indicate that you want all breaker instances to be MBeans.
At D and elsewhere you annotate various properties with @ManagedAttribute, which allows you to view and set the property values through the JMX console.
Annotating a getter allows you to view the values, and annotating a setter allows them through the JMX console.
You set descriptions on the managed attributes and operations as well, to assist the console operator; the descriptions are typically displayed alongside the attributes and operations in the console.5
You’ll also need to update the beans-kite.xml configuration to include the context namespace, along with a <context:mbean-export> tag.
In addition to the annotations described here, there are @ManagedOperationParameter and aren’t using here.
The <context:mbean-export> tag B tells Spring to register any MBeans with whatever MBean server happens to be running.
In this case, the Maven POM includes the Jetty plug-in, so the configuration causes Spring to register the MBeans with Jetty’s MBean server.6 You add an ID to the breaker C so that it appears with the same ID in the JMX console.
Now let’s start the app and use JMX to manage it.
You start it in this manner so you can connect a local JMX console (like JConsole or Java VisualVM) to it.
For production use, you’d normally want a remote JMX console to avoid competing with the app for resources.
If you want to use the console that way, please consult the JMX documentation.7) For development, it’s fine to use a local console with authentication and SSL disabled.
With the app started, you’re ready to launch the JMX console.
The Java platform comes with a couple of JMX console options.
If you haven’t already done so, go to Tools > Plugins and install the VisualVMMBeans plug-in.
This will cause an MBeans tab to appear as an option, as shown in figure 14.7
VisualVM on the same host where you’re running your app, it should be under the Local node of the Applications hierarchy.
When running in a container without an MBean server, or when running in standalone mode, you’ll need to create your own MBean server.
See http://mng.bz/mFs0 for information on using JConsole, should you decide to use that.
In particular, try tripping and resetting the breaker using the Operations tab, and notice how it impacts the service when the browser requests it.
Figure 14.7 shows the JMX attribute view in Java VisualVM.
In addition to the attribute view, there are views for exposing operations (like.
We consider it critical because it’s generally wise to support manual overrides.
If, for example, some malicious user were to figure out how to induce exceptions on a service, then they could potentially create a denial of service using the breaker.
With JMX, you could potentially increase the exception threshold or even (in an extended implementation) disable tripping altogether.
And you can use the breaker as is to positive effect.
But we noted in the recipe 14.1 discussion that the template-based implementation is invasive.
You had to inject a circuit breaker into HomeController, and you also.
Supporting AOP-based configuration works, it’s less than desirable, partly due to the invasiveness, and partly because it’s potentially a lot of work to modify every integration point in an application.
In recipe 14.3 we’ll explore the first such option, which is AOP-based configuration.
One of the demerits of the template-based approach to circuit breakers is that you have to change the client code to effect the protection.
Although it’s useful to have the template option available, it’s also highly desirable to have a means by which you can install breakers without changing application code.
This recipe shows how to do that with Spring AOP.
Of necessity, we assume a basic working knowledge of Spring AOP.
As just discussed, you’ll put together some code to support declarative, AOP-based configuration.
Revert the client code to remove the programmatic template references.
You begin by building the circuit-breaker advice using the template you’ve already created.
Although the primary use case is the programmatic creation of circuit breakers by the library’s users, you can use it to develop further framework code.
Specifically, you’ll use it to implement interceptor-based advice using Spring AOP.
Recall from recipe 14.1 that you’re treating breakers more generically as guards.
This makes it easier to extend the framework to handle other components, such as rate limiters.10 Because it’s possible to apply multiple guards to an integration point (for example, you might want a breaker and a rate limiter at some integration point), you deal with guard lists here instead of dealing with single breakers or even single guards.
You’ll use GuardListInterceptor to apply guard lists at specified pointcuts.
You do that by wrapping the guards with individual interceptors and invoking them in reverse order.
Getting a little into the details, the GuardListSource interface requires that you provide a method and an optional class so the GuardListSource can determine the guard list.
The reason is that when you get to annotation-based configuration in recipe 14.5, you’ll want to look up the guard list using an annotation on the method.) You obtain the method and class from the MethodInvocation in the manner shown H.
If the method is static, then thisObj will be null, and the source object will get the guard list based on the method alone.
At I you get the breaker from the source, passing in the method and class you just derived.
The initial implementation returns a specified guard list every time, independently of the supplied method and class, as shown next.
The new interceptor and its support classes allow you to forgo the programmatic circuit-breaker approach you adopted in recipe 14.1
You’ll have to undo some of the work you did earlier and update the configuration as well.
Recall from recipe 14.1 that you had to inject a circuit breaker into MessageServiceImpl.
Finally you need to update beans-kite.xml to include the AOP-namespace configuration.
In the next listing you supplement the breaker definition from recipe 14.1 with the Spring AOP interceptor bean (representing advice in general AOP parlance), a pointcut, and a Spring AOP advisor (representing an aspect in general AOP parlance)
Because you’re using AOP, you begin by declaring the AOP namespace B and schema location.
The breaker definition is the same as before, but now you add a definition for the associated AOP advice, implemented as an interceptor C.
The interceptor definition includes a DefaultGuardListSource inner bean that makes the breaker available to the interceptor.
You want to define an aspect, which requires both a pointcut and an advice.
You already have the advice, so you need to create the pointcut and the aspect.
You define the pointcut at E using the AspectJ pointcut notation.
Then you define the aspect at F, which in Spring AOP you implement as an advisor, using both the advice and pointcut you created earlier.
We won’t get into all the details because you don’t need them here, but essentially.
In this case, autoproxying proxies the message service and applies the guard-list aspect to the MessageServiceImpl methods specified in the pointcut.
AOP-based circuit breakers are much more convenient and noninvasive than the purely programmatic approach you saw in recipe 14.1
It’s much easier to specify a pointcut than it is to go into a bunch of client methods and wrap then in callbacks and template methods.
In the next two recipes, you’ll learn how you can simplify configuration even further.
In recipe 14.4 you’ll create a custom namespace that allows you to use a domain-specific language to simplify the circuit-breaker configuration.
After that you’ll learn in recipe 14.5 how to add support for annotation-driven configuration, which tidies up the Spring configuration considerably.
Although Spring’s bean-based configuration provides a general configuration mechanism, it doesn’t always rank high on the usability front.
Configuring DAOs, service beans, and web MVC controllers is intuitive enough, but when you start adding infrastructural beans to the mix, it can quickly become unclear exactly which beans you need to place on the context in order to get everything to work.
Spring 2 introduced custom namespaces, which essentially allow you to accomplish configuration tasks without having to know exactly which beans are required behind the scenes.
Each custom namespace defines a domain-specific language (DSL) that simplifies configuration.
Moreover, the custom namespace tags hide the details of what’s going on, such as the fact that multiple beans were created, or that various postprocessors were created, and so forth.
This recipe explains how to create your own custom namespace in Spring.
For now we’ll keep it simple: you’ll create tags for the circuit breaker from recipe 14.1 and the.
These new tags correspond almost exactly to the raw bean definitions, which may leave you wondering what the point is.
Never fear; recipe 14.5 will expand the custom namespace in such a way that the benefit becomes clear.
Essentially, you need to do three things to implement a custom namespace:
Create an XML schema for the DSL ■ Create a NamespaceHandler and BeanDefinitionParsers to parse the custom.
To tell Spring where to find the XSD file, you create a properties file called spring.schemas inside the kite module’s src/main/resources/META-INF:
This is mapping an XSD file to two different URIs.
The file is a classpath resource and is thus relative to the kite module’s src/main/resources.
You have to escape the colon character because the colon is a permissible key/value separator in Java properties files, even though it seems as though everybody always uses the equals sign.
Besides the pointer to the XSD file, you also need a pointer to the NamespaceHandler.
This time it’s spring.handlers, also located in the src/main/resources/META-INF folder of the kite module:
With the two properties files you just created, Spring knows where to find the schema and NamespaceHandler for the custom schema.
You of course need to create those things, so let’s do that now.
For 13 chapters, you’ve managed to avoid creating an XML Schema, but the time has come.
The following listing shows the first half of the XSD that specifies the DSL.
You’re looking at some schema setup along with a circuit-breaker element definition.
In the setup, two of the namespaces you declare are Spring’s beans B and tool C namespaces.
At D you begin the definition of the new circuit-breaker custom tag you want to create.
You don’t have to define the id attribute explicitly because you declare the element to have the beans:identifiedType base type F, which automatically provides an id attribute of type xsd:ID.12
After that, you define a couple of attributes G corresponding to CircuitBreakerTemplate properties.
In addition to defining a custom tag for the circuit breaker, you want one for the guard list advice.
Spring IDE, as of version 2.3.1, doesn’t seem to do much with it other than display it as a tooltip when you hover over the circuit-breaker node in the bean-dependency graph.
Here you’re defining the schema for guard list advice B, which behind the scenes you’ve implemented as a Spring AOP interceptor.
As before, you use the tool namespace to indicate the bean class associated with this custom tag C.
The advice definition is fairly similar to the circuit-breaker definition, but notice at D that you’re defining a guards attribute even though the GuardListInterceptor doesn’t itself have a guards property.
You may recall from the previous recipe that the interceptor has a GuardListSource that it uses to get a breaker.
It’s the DefaultGuardListSource that has a guards property.) The idea is that when a user creates an advice using this custom tag, you want to wrap the referenced guards with a DefaultGuardListSource automatically so the user doesn’t have to do it.
You’ll learn how to pull that off in the next subsection, but for now it’s enough to notice that you can define the element in whatever way you think will be most convenient for the user, which is the main point of a DSL.
In addition to telling tools about the beans that specific tags generate, you can also provide information about custom tag attributes.
Another hint is that the expected type for this reference is com.springinpractice.ch14.kite.interceptor.GuardListInterceptor F.
Taken together, SpringSource Tool Suite uses these to offer content assistance, hyperlink navigation, hover information, and validation.
But you still need a way to parse the DSL into Spring beans.
The entry point for custom namespaces is the namespace handler.
This is where you register different bean-definition parsers that convert XML bean definitions into actual beans on the context.
A namespace handler will include a registration for each custom tag you define.
You create a KiteNamespaceHandler class that extends NamespaceHandlerSupport, which in turn implements NamespaceHandler.
To means by which to register bean-definition parsers (for top-level tags) and beandefinition decorators (for custom nested tags)
In this case you have only two registrations, both for parsers: one for guard-list-advice D and one for circuitbreaker E.
In a more full-fledged framework there could be many more, but here it’s just two for the moment.
To finish the namespace handler, you’ll need to create the two parsers you referenced from KiteNamespaceHandler.
Spring provides some helpful base implementations for implementing parser classes.
Table 14.1 offers high-level guidance on the best BeanDefinitionParser base class to use in a given situation.
The first parser will be CircuitBreakerParser, and it’s a parser for the circuitbreaker tag.
Supporting custom namespaces and because the tag attributes precisely match the bean properties, you can avail yourself of the AbstractSimpleBeanDefinitionParser base class.
All you have to do is tell Spring which class has the matching properties, so that’s façade into the custom namespace-handling capability, and nobody other than KiteNamespaceHandler needs to use or even see CircuitBreakerParser.
Again you’re creating a single bean, this time a GuardListInterceptor.
Here you’ll need to get slightly more sophisticated (and we do mean slightly) because the XSD has a guards attribute whereas the actual interceptor class has a source property.
The approach is similar to the AbstractSimpleBeanDefinitionParser approach, except that this time you need to build a DefaultGuardListSource around the guard list name in the DSL.
Appropriate when the tag’s attribute names exactly match the bean’s property names.
AbstractSingleBeanDefinitionParser Appropriate when the tag corresponds to a single bean, but the tag’s attribute names don’t exactly match the bean’s property names.
AbstractBeanDefinitionParser Appropriate when the tag needs to create more than one bean.
Takes more work to implement, but it’s more flexible than the other implementations.
Figure 14.10 presents a class diagram showing the relationship between the parsers you just implemented and the framework base classes that Spring provides.
Now you’ll update the sample app to use the guard framework enhancements.
In beans-kite.xml, you need to declare the namespace and its schema location, and you’ll replace the CircuitBreakerTemplate and GuardListInterceptor bean definitions with the custom tags you just created.
While you’re at it, let’s make the new namespace the default namespace for this.
Listing 14.26 is significantly cleaner, owing to the new custom namespace.
You declare the namespace B and schema location C just like you have to do with any namespace.
You make the kite namespace the default because this configuration file focuses specifically on Kite components.
You define the CircuitBreakerTemplate D and GuardListInterceptor E using custom tags that roughly resemble the explicit bean definitions they replace, although the advice definition conveniently hides the GuardListSource configuration.
Although custom namespaces can certainly get more complicated, the techniques you’ve just learned provide a solid foundation for further study.
Custom namespaces are a useful mechanism for limiting the complexity of XML-based configuration, which addresses a common criticism that Spring Framework friends and foes alike have leveled against Spring over the years.
But there is an even more powerful approach you can use to limit complexity, and that’s annotation-based configuration.
We generally prefer declarative configuration to programmatic configuration because it’s simpler and less invasive.
In recipe 14.3, you added support for declarative configuration based on AOP.
In this recipe we’ll explore the other major approach that Spring-based frameworks often support, which is declarative configuration based on annotations.
This recipe extends the infrastructure you’ve already created to add support for annotation-based configuration.
Annotations are popular in both Java in general and Spring in particular, and many framework users have come to expect support for annotation-based configuration.
Spring’s transaction support, for instance, uses annotations to specify transaction attributes such as isolation and rollback behavior.
For the circuit breaker, the annotation will target methods (not types), and it will reference the associated breaker by name.
The intent is that each separate resource has its own circuit breaker, and each method accessing that resource will use that breaker.
We’ll need to cover several steps, but the overall process is in line with what you’ve already done.
The first thing you’ll need, of course, is an annotation.
The annotation takes a single value, which is the name of the guard list guarding the annotated method.
Next you’ll create a new GuardListSource implementation to source guards from annotations, along with a corresponding pointcut and advisor.
In recipe 14.3 you created a GuardListInterceptor class whose job was in essence to wrap method calls with guard lists.
You could have injected guards directly into interceptors, but you instead created a GuardListSource abstraction to give you flexibility in sourcing the guards.
The original DefaultGuardListSource implementation held a guard list and returned it on demand, and so to use it you would need to create a separate interceptor for each guard list.
You’ll still use the GuardListSource abstraction, but the new implementation will use the @GuardedBy annotation to discover the guard list instead of directly referencing it.
In addition to enabling annotation-based configuration, this allows you to use a single interceptor across all guard lists instead of creating separate interceptors for each one.
Figure 14.11 The GuardListSource hierarchy, which includes two concrete implementations.
This source is BeanFactoryAware B so you can resolve the bean names embedded.
Supporting annotation-based configuration is, you want to look for annotations on implementing methods before examining the corresponding interface methods.
Instead you want to look for the annotation on the bridged method—that is, the method that the bridge method is bridging.
If the search for the annotation fails on the most specific method, you want to go back and check the passed method E.
Usually this means looking at the interface method to see if the annotation is there.
If there’s no annotation on the passed method, you return null.
The AnnotationGuardListSource gives the GuardListInterceptor a way to get a guard list according to the method being invoked.
But you’ll need to create a custom pointcut class as well.
You didn’t have to do that in recipe 14.3, but you have to do it here.
The reason has to do with Spring’s autoproxy mechanism, which you used implicitly in recipe 14.3 and which you’ll use again here.
In a nutshell, autoproxying figures out which advisors (aspects) apply to which beans by examining advisor pointcuts.
You have to give the autoproxying mechanism a pointcut that knows whether a given method has an associated guard list, indicating that a proxy must be created.
All you need is a pointcut with a reference to a GuardListSource.
The pointcut maintains a GuardListSource B for getting a guard-list reference.
The method matches the pointcut, in which case a proxy would be created.
Following a general Spring convention, you’ll create a new kite namespace element, <annotation-config>, to activate annotation-based configuration.
The parser, as you might expect, is much more involved than the element definition.
This makes sense, of course, because the DSL aims to hide complex configurations from the app developer.
The following listing shows the BeanDefinitionParser implementation responsible for parsing the new <annotation-config> element.
It’s packageprivate because only KiteNamespaceHandler needs to access and return a null bean definition.
This is related to the distinction between classand interface-based proxying.
Class-based proxying doesn’t use AOP, and so if you were to select class-based proxying, you wouldn’t want to load AOP-related classes.
By isolating the AOP references to an inner class, the class-based approach can avoid loading the AOP classes by not using the inner class.
See Spring’s AnnotationDrivenBeanDefinitionParser class (part of Spring’s transaction support) for an illustration showing how to do that.
Autoproxying needs to have some way to figure out how to order the different advisors it applies, whether those advisors are Kite advisors or some other type.
For example, you’d generally want guard-list advisors to wrap transaction advisors rather than the other way around.
To that end, the parser supports an order attribute that allows you to order Kite advisors relative to other advisors D.
At E you tell Spring to register the autoproxy creator if it hasn’t already been registered by some other framework code, like the aforementioned transaction support.
Otherwise, you create an annotation-based guard list source G, an interceptor H, a pointcut I, and an advisor J, using the following helper methods.
The AopAutoProxyConfigurer uses several helper methods to help keep the configurer’s logic clear.
The first helper creates a new definition, setting its source to the major application bean, and ROLE_SUPPORT, indicating a level of importance between a top-level application bean and a purely infrastructural bean that the user wouldn’t care about.)
In the cases of the source, interceptor, and pointcut beans, you don’t care what the instead of having to provide them yourself G.
Just a few more things to take care of to make the shiny new annotation-based configuration work.
Now you just need to update the client code and configuration.
For the client code, you add an annotation to the methods you want to protect.
But you must update the beans-kite.xml configuration to activate the annotations.
In the next listing you replace the explicit AOP configuration with the new <annotation-config> element you just created.
This configuration style is very much along the lines offered by Spring transaction support, Spring Integration, and so forth.
The <annotation-config> element causes the AnnotationConfigParser to kick in, thanks to the registration you added to KiteNamespaceHandler.
Then the parser creates the source, interceptor, pointcut, and advisor as you saw in listing 14.30
Annotation-based configuration is one of the key configuration styles in Spring, and now you know how to roll your own.
Although annotation-based configuration doesn’t have to involve a custom namespace, it typically does.
You normally define one or more custom annotations, along with a custom namespace, a parser, and bean definitions to handle converting configuration into actual annotation-processing machinery.
You may have noticed that the more sophisticated (from a framework development perspective) configuration approaches build on the simpler ones.
You started with a template and then built an AOP-based approach off of that by using the template from an interceptor.
Similarly, the annotation-based configuration uses annotations to decide where to apply interceptors.
Whether you use annotation-based configuration is largely a matter of style, and your preference may vary depending on what’s being configured.
You might feel, for instance, that it’s nice to be able to see your circuit-breaker annotations right where they’re being used.
Or you might instead prefer to manage such configuration centrally using the techniques from recipe 14.3
We’ve completed our foray into the world of implementing Spring-based frameworks.
We covered creating template methods, AOP- and annotation-based configuration, custom namespaces, and integration with SpringSource Tool Suite.
We also looked at JMX and some interesting concurrent programming concepts along the way.
Although creating a framework isn’t necessarily the kind of thing a developer does every day, sometimes it’s exactly what you want to do.
Spring provides such powerful frameworkbuilding infrastructure and idioms that it pays to know how to use them.
And even if you don’t plan to roll your own framework anytime soon, it can help you as a user of the Spring Framework to understand how things work under the hood.
This chapter brings us almost to the end of the book.
We hope you’ve enjoyed reading it as much as we’ve enjoyed writing it.
We welcome any comments, questions or thoughts you may have for us:
Manning author forum: www.manning.com/wheeler/ ■ Spring in Practice blog: http://springinpractice.com/
We look forward to hearing from you, and thanks for reading.
This appendix explains how this book’s sample code is organized and how to configure it.
These are areas where we had to come up with some kind of sensible scheme to support the needs of the book, and that scheme may not be completely transparent at first glance.
A.1 IDE and environment setup As regards the IDE, we used the Spring Tool Suite for the development of this book, which is essentially a branded version of Eclipse with additional support for Spring-based development.
Although we can’t speak to the details of what you’ll need when using standard Eclipse, IntelliJ IDEA, NetBeans, or other IDEs, in general you’ll need the following:
Maven support ■ Git support, if you plan to use Git to work with the code.
Spring-specific support (such as bean-dependency graphs, Spring AOP, configuration files, custom configuration views for Spring Integration, Spring Web Flow, and so on) isn’t strictly required, but you’ll find it very helpful.
The major Java IDEs all have Spring-specific support, either built in or as a plug-in.
In terms of your broader system environment, you’ll need to install Maven, and Git if you want to use it.
A.2 How the code is organized The book’s code is organized as a set of Git repositories managed at GitHub.
The projects are all Maven projects conforming to Maven’s conventions around internal 513
In addition, there are tags representing released versions of the recipe code.
One of the repositories is a top-level project, called sip-top, that factors out what’s common to the chapter repositories, such as dependencies and plug-ins.
It’s a multimodule project with support for Hibernate, JPA, DAOs, and web app development.
There is also a Spring Modules JCR repository that we use in chapter 12
From time to time we may release bug fixes or even enhancements.
These may appear as new tags on existing branches, or they may appear as entirely new branches if the changes diverge from what’s in the book.
In the event of changes that conflict with what’s in the book, we’ll indicate this in the branch and tag names using alt.
A.3 Getting the code The source code is available at https://github.com/springinpractice.
You have a couple of options for downloading the code.
The downloads are all Maven projects, so you should import your project as a Maven project.
This will give you more flexibility in working with the code.
Here’s how you can create a local clone of that fork:
This checks out the master branch, making it locally available.
If you want to switch to the branch for recipe 1, then go into the clone’s directory.
You can see a list of all the branches (local and remote) by typing.
Information about using Git and GitHub is beyond the scope of this book, but a great deal of high-quality Git documentation is available both online and in book form.
A.4 Building the code We use Maven for the projects in this book.
If you aren’t familiar with Maven, you’ll find it useful (and probably necessary) to look for a quick online tutorial to get you up to speed on the basics.
Most of the projects in this book are small, single-module web applications.
Most of the chapters don’t include unit tests, so even though mvn test is useful in general, we don’t use it much in this book.
Note however that we do have a recipe on unit tests and an entire chapter (chapter 10) on integration testing.
In those special cases, the recipes explain how to run the relevant tests.
For example, you’ll need to run mvn install in the sip-top project to make the packages it generates visible to the other projects, which all depend upon the sip-top packages.
A.5 Configuring the app The projects are set up to work with externalized configuration files.
That means you should be able to download the source code and place some configuration files in a location outside the project, and the app should run.
This decoupling keeps environment-specific configuration out of source control and makes it easier to refresh your project without losing your configuration.
It works by adding an external file path to the Jetty plug-in’s configuration.
This allows the plug-in to find jetty-env.xml (JNDI resource configurations, among other things) as well as environment-specific app configuration files that need to be on the classpath.
The sip-top project already handles this for you; see sip-top/pom.xml.
But you’ll need to provide a base path to allow the plug-in to find the various configuration files we just mentioned.
To do this, you’ll need to create a Maven settings.xml file in your local .m2 Maven repo, if you don’t already have one.
The .m2 folder is typically in the user directory (it may be hidden)
See the online Maven documentation for more information about settings.xml: http://maven.apache.org/settings.html.)
The /Users/williewheeler/projects/sip/conf directory is obviously specific to our example development environment; use whichever root folder you want to use.
Once you have this set up, the sample code will expect you to create chapterspecific folders in your root configuration folder.
To give you an idea what this looks like, here’s an example subset of configuration folders and files relative to the configuration root:
The Jetty plug-in configuration for any given project knows how to find the classpath resources (log4j.xml, various environment-specific properties files, and so on)
In addition, projects that use jetty-env.xml know to find it at the top of the chapterspecific folders.
Most of the recipes have a top-level folder called sample_conf that shows you which files you need to put in the chapter configuration folders.
Copy the contents of sample_conf into the chapter-specific folder before working through a chapter’s first recipe, preserving its internal structure.
As you progress through the chapter’s recipes, modify your configuration.
Running the app to reflect any additions to files in sample_conf.
You shouldn’t need to modify the files in sample_conf, and you shouldn’t need to copy them into the project.
A.6 Running the app Use the Maven Jetty plug-in to run the app.
If you want to clean the code before building and running it, you can type.
In most cases, you can then access the application by pointing your browser at http:// localhost:8080/sip/
There are some instances where that’s not true; in those cases, the book gives explicit instruction on which URLs to use.
Th e book starts with three carefully craft ed introductory.
And then, the core of the book takes you step-by-step through the important, practical techniques you will use no matter what type of application you’re building.
You’ll hone your Spring skills with examples on user accounts, security, NoSQL data stores, and application integration.
Along the way, you’ll explore Springbased approaches to domain-specifi c challenges like CRM, confi guration management, and site reliability.
What’s Inside Covers Spring 3 Successful outcomes with integration testing Dozens of web app techniques using Spring MVC Practical examples and real-world context How to work eff ectively with data.
Each technique highlights something new or interesting about Spring and focuses on that concept in detail.
Th is book assumes you have a good foundation in Java and Java EE.
Prior exposure to Spring Framework is helpful but not required.
Joshua White is a Solutions Architect in the fi nancial and health services industries.
To download their free eBook in PDF, ePub, and Kindle formats, owners of this book should visit manning.com/SpringinPractice.
Th is is the Spring introduction you’ve been waiting for.”—John Tyler, PROS Inc.
Spring in Practice brief contents contents preface acknowledgments about Spring about this book Roadmap Who should read this book? Code conventions and downloads Author Online About the authors.
