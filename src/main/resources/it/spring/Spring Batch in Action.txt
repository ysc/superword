The publisher offers discounts on this book when ordered in quantity.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the publisher.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in the book, and Manning Publications was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Recognizing the importance of preserving what has been written, it is Manning’s policy to have the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, Manning books are printed on paper that is at least 15 percent recycled and processed without the use of elemental chlorine.
Every IT project that needed offline processing found that they had to reinvent the basic features of deployment, processing patterns, and reliability.
Then, and to this day, these are the unavoidable but necessary features of many environments across a wide range of industries.
Who should read this book? For sure anyone who wants to learn how to use Spring Batch from the ground up.
Some knowledge of the Spring Framework and familiarity with Spring programming models and common practices are more or less mandatory, but these are common skills and easily obtained elsewhere.
Spring Batch isn’t all that difficult to get started with once you have basic Spring knowledge.
The highlights for me are the chapters on transactions and reliability in part 2 and the chapter on scalability.
These are two topics that come up again and again with clients and on the online open community forum.
I’m very glad for two reasons that someone else decided to write this book.
The second is on behalf of the readers: when the author of a framework writes a handbook for users, the temptation is to reveal all the secrets and inner workings of the implementation, which isn’t necessarily the best practical starting point for users.
The authors of this book are well qualified and present Spring Batch from the point of view of an expert user, which is the optimum point of view for readers because the authors have come to this knowledge by solving the problems that readers also experience while learning to use the framework.
In spite of this, they’re exciting to write, when you consider the large amount of data they process.
Batch jobs run every night, making it easy for millions of people to do things like banking, online shopping, querying billing information, and so on.
That was the case with Thierry, who was the first to work on the proposal for a Spring Batch book with Manning.
The funny thing is that we were still working on Manning’s Spring Dynamic Modules in Action book at the time.
Writing a book is a challenge, but you could consider writing two books at the same time a sign of madness.
Although Gary didn’t write any of the original material, he handled what is undoubtedly the more difficult task: editing and sometimes rewriting the source material of three French authors.
Gary is French, but don’t worry, he’s also American and has been living in the U.S.
The book doesn’t contain any trace of our French accents! Olivier was the last to join the team.
Fourteen chapters is a lot and another pair of hands was welcome.
We did our best to make this book as comprehensive and accessible as possible.
We hope you’ll benefit from our experience with Spring Batch and that our book will help you write fast, robust, and reliable batch jobs with this framework.
Michael Stephens first contacted us and helped us with the book proposal.
Publisher Marjan Bace gave us the opportunity to write the book and provided us with valuable advice about its structure.
Karen Tegtmeyer organized the reviews, which resulted in further improvements.
Last, but not least, we thank our development editor, Cynthia Kane, who helped us polish our writing and sharpen the book’s focus.
This book is about an open source project, so it would not exist were it not for the efforts of the people who spent their time creating and maintaining Spring Batch.
An open source project is also a community project, so thanks to everyone who contributed to the project by answering quesxviii.
This also helped us to learn more about how people use Spring Batch.
I’m eternally grateful to my wife Lori and my son Alexander for giving me the time to pursue a project like this one.
Along the way, I’ve studied and worked with truly exceptional individuals too numerous to name.
My father-in-law, Buddy Martin, deserves a special mention for providing wisdom and insights through great conversations and storytelling born of decades spent writing about sports (Go Gators!)
Finally, I thank my coauthors and all of the people at Manning for their support, professionalism, and great feedback.
Thanks to my mother, my father, and my sister for their love and support over the years.
And I would like to thank my wonderful girlfriend Maria for her patience and for giving me a chance to participate in this great adventure.
Batch applications involve reliably and efficiently processing large volumes of data to and from various data sources (files, databases, and so on)
Spring Batch is great at doing this and provides the necessary foundation to meet the stringent requirements of batch applications.
Spring is the framework of choice for a significant segment of the Enterprise Java development market.
Spring Batch leverages all the well-worn Spring techniques and components, like dependency injection, data access support, and transaction management.
Batch processing is a large topic and Spring Batch has a wide range of features.
We obviously focus on Spring Batch, but we also cover different, yet related, topics like schedulers.
Batch jobs aren’t islands, they’re integrated in the middle of complex systems, and we cover this aspect too.
That’s why chapter 11 discusses how Spring Batch can cohabit with technologies like REST and Spring Integration.
Again, we want to stick as close as possible to the reality of batch systems, and this is (one part of) our vision.
Because this is an In Action book, we provide code and configuration examples throughout, both to illustrate the concepts and to provide a template for successful operation.
Who should read this book? Our primary target audience for this book is Java developers and architects who want to write batch applications.
Experience with Spring is a plus, but not a requirement.
We strive to give the necessary pointers and reminders in dedicated sidebars.
Read this book even if you don’t know Spring—you can grab a copy of Manning’s Spring in Action, Third Edition, by Craig Walls to discover this wonderful technology.
For those familiar with Spring, basic knowledge of dependency injection, data access support, and transaction management is enough.
With this Spring background and this book, you’ll be Spring Batch-ing in a matter of minutes.
What if you don’t know Java and want to write batch applications? Well, think about learning Java to make your batch writing life easier.
The first part introduces the challenges presented by batch applications and how to use Spring Batch to addresses them.
The second part forms the core of the presentation of the Spring Batch feature set.
It exhaustively covers all of the scenarios you’ll meet writing real-life batch applications.
The third and final part covers advanced topics, including monitoring, scaling, and testing.
We also include appendixes covering the installation of a typical development environment for Spring Batch and the configuration of the Spring Batch Admin webbased administration console.
Chapter 1 discusses batch applications and gives an overview of Spring Batch features.
It also introduces Spring Batch using a hands-on approach, based on a realworld use case.
It’s a great place to start if you want to discover how to implement common batch scenarios with Spring Batch.
Chapter 2 covers the way Spring Batch structures the world of batch jobs.
We name and define each batch applications concept, using the domain language of batch applications.
With a term for each concept forming the vocabulary of batch jobs, you’ll be able to communicate clearly and easily with your team about your batch applications.
It explains in detail all the XML elements and annotations available to configure every aspect of your jobs.
Chapter 4 discusses launching batch jobs under different scenarios: from the command line, using a scheduler like cron, or from an HTTP request.
Chapter 5 covers reading data efficiently from different sources, using Spring Batch components.
It lists all the available components to write to databases and files, send emails, and so on.
Chapter 7 discusses an optional step between reading and writing: processing.
This is where you can embed business logic to transform or filter items.
Chapter 8 covers the Spring Batch built-in features that make jobs more robust: skipping incorrectly formatted lines from a flat file by using a couple of XML lines in your configuration, retrying operations transparently after a transient failure, and restarting a job exactly where it left off.
It explains how Spring Batch handles transactions, the how, when, and why of tweaking transactions, and useful transaction management patterns for batch applications.
Chapter 10 covers the way Spring Batch handles the flow of steps inside a job: linear versus nonlinear flows, sharing data between steps of a job, and interacting with the execution context.
Chapter 11 explores how a Spring Batch job can end up being in the middle of a complex enterprise integration application.
In this chapter, you’ll see how Spring Batch, Spring Integration, and Spring REST cohabit happily to meet real-world enterprise integration scenarios.
It covers the different strategies Spring Batch provides to parallelize the execution of your jobs on multiple threads or even multiple physical nodes.
Unit testing isolated components and testing a whole job execution are covered.
Code convention and downloads We’ve licensed the source code for the example applications in this book under the Apache Software Foundation License, version 2.0
Much of the source code shown in this book consists of fragments designed to illustrate the text.
When a complete segment of code is presented, it appears as a numbered listing; code annotations accompany some of the listings where further explanations of the code are needed.
When we present source code, we sometimes use a bold font to draw attention to specific elements.
In the text, we use Courier typeface to denote code (Java and XML) as well as Java methods, XML element names, and other source code identifiers:
A reference to a method in the text will generally not include the signature.
Note that there may be more than one form of the method call.
A reference to an XML element in the text can include the braces but not the attributes or closing tag, for example, <action>
This page provides information on registering, getting on the forum, the kind of help available, and the rules of conduct on the forum.
Manning’s commitment to our readers is to provide a venue where a meaningful dialogue between individual readers and between readers and the authors can take place.
It’s not a commitment to any specific amount of participation on the part of the authors, whose contribution to the Author Online forum remains voluntary (and unpaid)
We suggest you try asking them some challenging questions lest their interest stray! The Author Online forum and the archives of previous discussions will be accessible from the publisher’s website as long as the book is in print.
He’s a Spring addict and enthusiast and enjoys implementing any kind of applications and tools using it.
He is also the coauthor of some French books on these subjects and Spring Dynamic Modules in Action.
He recently joined Noelios Technologies, the company behind the Restlet framework, and lives in Brittany (France)
He lives in Florida with his wife, their son, and assorted golf clubs.
With over 12 years of experience, he develops complex business applications and high-traffic websites based on Java and web technologies.
The collection, which includes beautifully hand-colored copperplate engravings of costumes from around the world, has influenced theatrical costume design ever since it was published.
The diversity of the drawings in the Collection of the Dresses of Different Nations speaks vividly of the richness of the costumes presented on the London stage over 200 years ago.
The costumes, both historical and contemporaneous, offered a glimpse into the dress customs of people living in different times and in different countries, bringing them to life for London theater audiences.
Dress codes have changed in the last century and the diversity by region, so rich in the past, has faded away.
It’s now often hard to tell the inhabitant of one continent xxiv.
Perhaps, trying to view it optimistically, we’ve traded a cultural and visual diversity for a more varied personal life—or a more varied and interesting intellectual and technical life.
We at Manning celebrate the inventiveness, the initiative, and the fun of the computer business with book covers based on the rich diversity of regional and historical costumes brought back to life by pictures from collections such as this one.
Of course, you won’t be a Spring Batch expert by the end of this first part, but you’ll have a good foundation and understanding of all the features in Spring Batch.
Chapter 1 provides an overview of batch applications and Spring Batch.
To follow the In Action tradition, we also show you how to implement a real-world batch job with Spring Batch.
This introduction not only covers how Spring Batch handles the classical read-process-write pattern for large amounts of data but also shows you the techniques used to make a job more robust, like skipping invalid lines in a flat file.
Chapter 2 clearly defines the domain language used in batch applications and explains how Spring Batch captures the essence of batch applications.
Batch applications are a challenge to write, and that’s why Spring Batch was created: to make them easier to write but also faster, more robust, and reliable.
What are batch applications? Batch applications process large amounts of data without human intervention.
You’d opt to use batch applications to compute data for generating monthly financial statements, calculating statistics, and indexing files.
You’re about to discover more about batch applications in this chapter.
We also honor the In Action series by implementing a real-world Spring Batch job.
By the end of this first chapter, you’ll have an overview of what Spring Batch does, and you’ll be ready to implement your first job with Spring Batch.
Imagine you want to exchange data between two systems: you export data as files from system A and then import the data into a database on system B.
A batch application processes data automatically, so it must be robust and reliable because there is no human interaction to recover from an error.
The greater the volume of data a batch application must process, the longer it takes to complete.
This means you must also consider performance in your batch application because it’s often restricted to execute within a specific time window.
Based on this description, the requirements of a batch application are as follows:
Large data volume—Batch applications must be able to handle large volumes of data to import, export, or compute.
Automation—Batch applications must run without user interaction except for serious problem resolution.
Robustness—Batch applications must handle invalid data without crashing or aborting prematurely.
Reliability—Batch applications must keep track of what goes wrong and when (logging, notification)
Performance—Batch applications must perform well to finish processing in a dedicated time window or to avoid disturbing any other applications running simultaneously.
How batch applications fit in today’s software architectures Performing computations and exchanging data between applications are good examples of batch applications.
But are these types of processes relevant today? Computation-based processes are obviously relevant: every day, large and complex calculations take place to index billions of documents, using cutting-edge algorithms like MapReduce.
For data exchange, message-based solutions are also popular, having the advantage over batch applications of being (close to) real time.
Although messaging is a powerful design pattern, it imposes its own particular set of requirements in terms of application design and implementation.
Figure 1.1 A typical batch application: system A exports data to flat files, and system B uses a batch process to read the files into a database.
How does Spring Batch fit in the landscape of batch applications? The next section introduces the Spring Batch framework and its main features.
You’ll see how Spring Batch helps meet the requirements of batch applications by providing ready-to-use components and processing large amounts of data in an efficient manner.
The goal of the Spring Batch project is to provide an open source batch-oriented framework that effectively addresses the most common needs of batch applications.
The Spring Batch project was born in 2007 out of the collaboration of Accenture and SpringSource.
Accenture brought its experience gained from years of working on proprietary batch frameworks and SpringSource brought its technical expertise and the proven Spring programming model.
By using Spring Batch, you directly benefit from the best practices the framework enforces and implements.
You also benefit from the many off-the-shelf components for the most popular formats and technologies used in the software industry.
Table 1.2 lists the storage technologies that Spring Batch supports out of the box.
Spring Framework foundations Benefits from enterprise support, dependency injection, and aspectoriented programming.
Batch-oriented processing Enforces best practices when reading and writing data.
Ready-to-use components Provides components to address common batch scenarios (read and write data to and from databases and files)
Robustness and reliability Allows for declarative skipping and retry; enables restart after failure.
Note that batch jobs and messaging aren’t mutually exclusive solutions: you can use messaging to exchange data and still need batch applications to process the data with the same reliability and robustness requirements as the rest of your application stack.
As you can see in table 1.2, Spring Batch supports many technologies out of the box, making the framework quite versatile.
How does Spring Batch meet the requirements of robustness and reliability of batch applications?
Should a whole batch fail because of one badly formatted line? Not always.
The decision to skip an incorrect line or an incorrect item is declarative in Spring Batch.
The components then know where they left off and can restart processing at the right place.
Spring Batch isn’t a scheduler! Spring Batch drives batch jobs (we use the terms job, batch, and process interchangeably) but doesn’t provide advanced support to launch them according to a schedule.
Spring Batch leaves this task to dedicated schedulers like Quartz and cron.
A scheduler triggers the launching of Spring Batch jobs by accessing the Spring Batch runtime (like Quartz because it’s a Java solution) or by launching a dedicated JVM process (in the case of cron)
Sometimes a scheduler launches batch jobs in sequence: first job A, and then job B if A succeeded, or job C if A failed.
The scheduler can use the files generated by the jobs or exit codes to orchestrate the sequence.
Spring Batch can also orchestrate such sequences itself: Spring Batch jobs are made of steps, and you can easily configure the sequence by using Spring Batch XML (covered in chapter 10)
This is an area where Spring Batch and schedulers overlap.
We cover chunk processing later in this chapter, but here is the idea: a job reads and writes items in small chunks.
Chunk processing allows streaming data instead of loading all the data in memory.
By default, chunk processing is single threaded and usually performs well.
But some batch jobs need to execute faster, so Spring Batch provides ways to make chunk processing multithreaded and to distribute processing on multiple physical nodes.
Chapter 13 thoroughly discusses the scaling strategies in Spring Batch.
Let’s take a look at one of these strategies: partitioning.
Partitioning splits a step into substeps, each of which handles a specific portion of the data.
This implies that you know the structure of the input data and that you know in advance how to distribute data between substeps.
Distribution can take place by ranges of primary key values for database records or by directories for files.
The substeps can execute locally or remotely, and Spring Batch provides support for multithreaded substeps.
Figure 1.2 illustrates partitioning based on filenames: A through D, E through H, and so on, up to Y through Z.
Figure 1.2 Scaling by partitioning: a single step partitions records and autonomous substeps handle processing.
Some open source frameworks (Hadoop, GridGain, and Hazelcast, for example) have appeared in the last few years to deal with the burden of distributing units of work so that developers can focus on developing the computations themselves.
How does Spring Batch compare to these grid-computing frameworks? Spring Batch is a lightweight solution: all it needs is the Java runtime installed, whereas grid-computing frameworks need a more advanced infrastructure.
As an example, Hadoop usually works on top of its own distributed file system, HDFS (Hadoop Distributed File System)
In terms of features, Spring Batch provides a lot of support to work with flat files, XML files, and relational databases.
You now have a good overview of the most important features of Spring Batch and the benefits it brings to your applications.
Let’s move on to the more practical part of this chapter with the implementation of a real-world batch job using Spring Batch.
This section introduces a real application that we use throughout this book to illustrate the use of Spring Batch: an online store application.
This use case starts out small and simple but remains realistic in terms of technical requirements.
It not only demonstrates Spring Batch features but also illustrates how this use case fits into the enterprise landscape.
By implementing this use case using Spring Batch, you gain a practical understanding of the framework: how it implements efficient reading and writing of large volumes of data, when to use built-in components, when to implement your own components, how to configure a batch job with the Spring lightweight container, and much more.
By the end of this chapter, you’ll have a good overview of how Spring Batch works, and you’ll know exactly where to go in this book to find what you need for your batch applications.
The ACME Corporation wants to expand its business by selling its products on the web.
To do so, ACME chooses to build a dedicated online store application.
The system will process data every night to insert new products in the catalog or update existing products.
Figure 1.3 Thanks to this new application, anyone can buy ACME’s products online.
The system sends catalogs to a server where a batch process reads them and writes product records into the online store database.
That’s it for the big picture, but you should understand why ACME decided to build an online store in the first place and populate it using batch processes.
Why did ACME choose to build an online, web-based application to sell its products? As we mentioned, this is the best way for ACME to expand its business and to serve more customers.
Web applications are easy to deploy, easy to access, and can provide a great user experience.
The first version of the online store will provide a simple but efficient UI; ACME focuses on the catalog and transactions first, before providing more features and a more elaborate UI.
This precaution is rather drastic, but that’s how things are done at ACME.
Another reason for this architecture is that the catalog system’s API and data format don’t suit the needs of the online store application: ACME wants to show a summarized view of the catalog data to customers without overwhelming them with a complex catalog structure and supplying too many details.
You could get this summarized catalog view by using the catalog system’s API, but you’d need to make many calls, which would cause performance to suffer in the catalog system.
To summarize, a mismatch exists between the view of the data provided by the catalog system and the view of the data required by the online store application.
Therefore, an application needs to process the data before exposing it to customers through the online store.
The online store application scenario is a good example of two systems communicating to exchange data.
The online store application doesn’t need to expose live data because buyers can live with day-old catalog information.
Figure 1.4 Because ACME doesn’t want its internal catalog system to be directly accessible from the outside world, it doesn’t allow the two applications to communicate directly and exchange data.
Therefore, a nightly batch process updates the online store database, using flat files, as shown in figure 1.5
In figure 1.5, an ETL process creates the flat file to populate the online store database.
It extracts data from the catalog system and transforms it to produce the view expected by the online store application.
For the purpose of our discussion, this ETL process is a black box: it could be implemented with an ETL tool (like Talend) or even with another Spring Batch job.
We focus next on how the online store application reads and writes the catalog’s product information.
The online store application sells products out of a catalog, making the product a main domain concept.
The import product batch reads the product records from a flat file created by ACME and updates the online store application database accordingly.
Figure 1.6 illustrates that reading and writing products is at the core of this batch job, but it contains other steps as well.
The read-write step forms the core of the batch job, but as figure 1.6 shows, this isn’t the only step.
Figure 1.5 An extract, transform, and load (ETL) process extracts and transforms the catalog system data into a flat file, which ACME sends every night to a Spring Batch process.
This Spring Batch process is in charge of reading the flat file and importing the data into the online store database.
Extract, transform, and load (ETL) Briefly stated, ETL is a process in the database and data-warehousing world that performs the following steps:
Many products, both free and commercial, can help create ETL processes.
This is a bigger topic than we can address here, but it isn’t always as simple as these three steps.
Writing an ETL process can present its own set of challenges involving parallel processing, rerunnability, and recoverability.
The ETL community has developed its own set of best practices to meet these and other requirements.
The file is compressed to speed up transfer over the internet.
Reading and writing—The flat file is read line by line and then inserted into the database.
This batch process allows us to introduce the Spring Batch features displayed in table 1.3
Rather than describe each of Spring Batch’s features in the order in which they appear as batch job steps, we start with the core of the process: reading and writing the products.
Then we see how to decompress the incoming file before making the process more robust by validating the input parameters and choosing to skip invalid records to avoid the whole job failing on a single error.
Reading and writing the product catalog is at the core of the Spring Batch job.
Reading and writing is Spring Batch’s sweet spot: for the import product job, you only have to configure one Spring Batch component to read the content of the flat file, implement a simple interface for the writing component, and create a configuration file to handle the batch execution flow.
Table 1.3 lists the Spring Batch features introduced by the import catalog job.
Let’s start by using Spring Batch to implement the read-write use case.
Because read-write (and copy) scenarios are common in batch applications, Spring Batch provides specific support for this use case.
Spring Batch includes many ready-touse components to read from and write to data stores like files and databases.
Table 1.3 Spring Batch features introduced by the import catalog job.
Decompression Custom processing in a job (but not reading from a data store and writing to another)
Configuration Leveraging Spring’s lightweight container and Spring Batch’s namespace to wire up batch components.
Using the Spring Expression Language to make the configuration more flexible.
Figure 1.6 The Spring Batch job consists of the following steps: decompression and read-write.
Batch also includes a batch-oriented algorithm to handle the execution flow, called chunk processing.
Spring Batch handles read-write scenarios by managing an ItemReader and an ItemWriter.
Using chunk processing, Spring Batch collects items one at a time from the item reader into a configurable-sized chunk.
Spring Batch then sends the chunk to the item writer and goes back to using the item reader to create another chunk, and so on, until the input is exhausted.
Practically speaking, a large file won’t be loaded in memory; instead it’s streamed, which is more efficient in terms of memory consumption.
Chunk processing allows more flexibility to manage the data flow in a job.
Spring Batch also handles transactions and errors around read and write operations.
Spring Batch provides an optional processing step in chunk processing: a job can process (transform) read items before sending them to the ItemWriter.
The ability to process an item is useful when you don’t want to write an item as is.
The component that handles this transformation is an implementation of the ItemProcessor interface.
Because item processing in Spring Batch is optional, the illustration of chunk processing shown in figure 1.7 is still valid.
What can you do in an ItemProcessor? You can perform any transformations you need on an item before Spring Batch sends it to the ItemWriter.
This is where you implement the logic to transform the data from the input format into the format expected by the target system.
Spring Batch also lets you validate and filter input items.
If you return null from the ItemProcessor method process, processing for that item stops and Spring Batch won’t insert the item in the database.
The following listing shows the definition of the chunk-processing interfaces ItemReader, ItemProcessor, and ItemWriter.
Figure 1.7 In read-write scenarios, Spring Batch uses chunk processing.
Spring Batch reads items one by one from an ItemReader, collects the items in a chunk of a given size, and sends that chunk to an ItemWriter.
Chapter 7 covers the processing phase used to transform and filter items.
The next two subsections show how to configure the Spring Batch flat file ItemReader and how to write your own ItemWriter to handle writing products to the database.
Figure 1.8 Chunk processing combined with item processing: an item processor can transform input items before calling the item writer.
Spring Batch provides the FlatFileItemReader class to read records from a flat file.
You can kiss all your old boilerplate I/O code goodbye and focus on your data.
The input flat file format consists of a header line and one line per product record.
You may recognize this as the classic comma-separated value (CSV) format.
There’s nothing out of the ordinary in this flat file: for a given row, the format separates each column value from the next with a comma.
Spring Batch maps each row in the flat file to a Product domain object.
The Product class maps the different columns of the flat file.
Note the instance variable declarations for product attributes like id, name, price, and so on, in this snippet; the getter and setter methods are excluded for brevity:
Let’s now use the FlatFileItemReader to create Product objects out of the flat file.
The FlatFileItemReader class handles all the I/O for you: opening the file, streaming it by reading each line, and closing it.
The FlatFileItemReader class delegates the mapping between an input line and a domain object to an implementation of the LineMapper interface.
Reading and writing the product data called DefaultLineMapper, which delegates the mapping to other strategy interfaces.
That’s a lot of delegation, and it means you’ll have more to configure, but such is the price of reusability and flexibility.
You’ll be able to configure and use built-in Spring Batch components or provide your own implementations for more specific tasks.
A FieldSetMapper to transform the split line into a domain object.
You’ll soon see the whole Spring configuration in listing 1.2 (LineTokenizer is of particular interest), but next we focus on the FieldSetMapper implementation to create Product domain objects.
You use a FieldSetMapper to convert the line split by the LineTokenizer into a domain object.
Think of it as an equivalent to the JDBC ResultSet: it retrieves field values and performs conversions between String objects and richer objects like BigDecimal.
Figure 1.9 The FlatFileItemReader reads the flat file and delegates the mapping between a line and a domain object to a LineMapper.
The LineMapper implementation delegates the splitting of lines and the mapping between split lines and domain objects.
We leave Spring Batch to deal with all of the I/O plumbing and efficiently reading the flat file.
Where do these references come from? They’re part of the LineTokenizer configuration, so let’s study the Spring configuration for FlatFileItemReader.
The FlatFileItemReader can be configured like any Spring bean using an XML configuration file, as shown in the following listing.
In this example, the resource property defines the input file.
Because the first line of the input file contains headers, you ask Spring Batch to skip this line by setting the property linesToSkip B to 1
Reading and writing the product data define the name of each field.
Next up, to implement the database item writer, you need to do less configuration work but more Java coding.
To update the database with product data, you have to implement your own ItemWriter.
Each line of the flat file represents either a new product record or an existing one, so you must decide whether to send the database an insert or an update SQL statement.
Spring Batch creates the JdbcTemplate with a DataSource injected in the constructor B.
In the write method, you iterate over a chunk of products and first try to update an existing record C.
If the database tells you the update statement didn’t update any record, you know this record doesn’t exist, and you can insert it D.
That’s it for the implementation! Notice how simple it was to implement this ItemWriter because Spring Batch handles getting records from the ItemReader, creating chunks, managing transactions, and so on.
For the item writer to be configured as a Spring bean, it needs a DataSource, as shown in the following XML fragment:
You’ll configure the DataSource later, in a separate configuration file.
Now that you’ve created the two parts of the read-write step, you can assemble them in a Spring Batch job.
The step configuration can sit next to the declaration of the reader and writer beans, as shown in the following listing.
The configuration file starts with the usual declaration of the namespaces and associated prefixes: the Spring namespace and Spring Batch namespace with the batch prefix.
The Spring namespace is declared as the default namespace, so you don’t need to use a prefix to use its elements.
Unfortunately, this is inconvenient for the overall configuration because you must use the batch prefix for the batch job elements.
To make the configuration more readable, you can use a workaround in XML: when you start the job configuration XML element B, you specify a default XML namespace as an attribute of the job element.
The scope of this new default namespace is the job element and its child elements.
The chunk element C configures the chunk-processing step, in a step element, which is itself in a tasklet element.
In the chunk element, you refer to the reader and writer beans with the reader and writer attributes.
The values of these two attributes are the IDs previously defined in the reader and writer configuration.
Finally, D the commit-interval attribute is set to a chunk size of 100
You’re done with the copy portion of the batch process.
Spring Batch performs a lot of the work for you: it reads the products from the flat file and imports them into the database.
For the write operation, you only created the logic to insert and update products in the database.
Putting these components together is straightforward thanks to Spring’s lightweight container and the Spring Batch XML vocabulary.
Choosing a chunk size and commit interval First, the size of a chunk and the commit interval are the same thing! Second, there’s no definitive value to choose.
Too small a chunk size creates too many transactions, which is costly and makes the job run slowly.
The best value for the commit interval depends on many factors: data, processing, nature of the resources, and so on.
The commit interval is a parameter in Spring Batch, so don’t hesitate to change it to find the most appropriate value for your jobs.
As you’ve seen, Spring Batch provides a lot of help for this common use case.
The framework is even richer and more flexible because a batch process can contain any type of write operation.
Remember that the flat file is uploaded to the online store as a compressed archive.
You need to decompress this file before starting to read and write products.
Decompressing a file isn’t a read-write step, but Spring Batch is flexible enough to implement such a task as part of a job.
Before showing you how to decompress the input file, let’s explain why you must compress the products flat file.
The flat file containing the product data is compressed so you can upload it faster from ACME’s network to the provider that hosts the online store application.
Note that you could encrypt the file as well, ensuring that no one could read the product data if the file were intercepted during transfer.
The encryption could be done before the compression or as part of it.
In this case, assume that ACME and the hosting provider agreed on a secure transfer protocol, like Secure Copy (SCP is built on top of Secure Shell [SSH])
Now that you know why you compress the file, let’s see how to implement the decompression tasklet.
Spring Batch provides an extension point to handle processing in a batch process step: the Tasklet.
You implement a Tasklet that decompresses a ZIP archive into its source flat file.
The following listing shows the implementation of the DecompressTasklet class.
The DecompressTasklet class implements the Tasklet interface B, which has only one method, called execute.
The tasklet has three fields C, which represent the archive file, the name of the directory to which the file is decompressed, and the name of the output file.
These fields are set when you configure the tasklet with Spring.
In the execute method, you open a stream to the archive file D, create the target directory if it doesn’t exist E, and use the Java API to decompress the ZIP archive F.
Note that the FileUtils and IOUtils classes from the Apache Commons IO project are used to create the target directory and copy the ZIP entry content to the.
At G, you return the FINISHED constant from the RepeatStatus enumeration to notify Spring Batch that the tasklet finished.
Although the Tasklet interface is straightforward, its implementation includes a lot of code to deal with decompressing the file.
Let’s now see how to configure this tasklet with Spring.
The tasklet is configured as part of the job and consists of two changes in Spring: declare the tasklet as a Spring bean and inject it as a step in the job.
To do this, you must modify the configuration you wrote for reading and writing products, as shown in the following listing.
The configuration of a plain Tasklet is simpler than for a read-write step because you only need to point the Tasklet to the (decompression) bean B.
Only a data file and no metadata file in the ZIP archive? It’s common practice to have two files in a ZIP archive used for a batch job.
One file contains the data to import, and the other contains information about the data to import (date, identifier, and so on)
We wanted to keep things simple in our Spring Batch introduction, especially the tedious unzipping code, so our ZIP archive contains only a data file.
Let’s say the name of the unzipped file is made up of meaningful information such as the date and an identifier for the import.
Testing the batch process the job flow through the next attribute of the step element, which refers to the readWriteProducts step by ID.
Chapter 10 thoroughly covers how to control the flow of Spring Batch jobs and how to take different paths, depending on how a step ends, for example.
The tasklet element B refers to the decompressTasklet bean, declared atC.
If you find that the Tasklet bean is configured too rigidly in the Spring file (because the values are hardcoded), don’t worry: we’ll show you later in this chapter how to make these settings more dynamic.
You now have all the parts of the job implemented and configured: you can decompress the input archive, read the products from the decompressed flat file, and write them to the database.
You’re now about to see how to launch the job inside an integration test.
Batch applications are like any other applications: you should test them using a framework like JUnit.
Let’s test, then! This section covers how to write an integration test for a Spring Batch job.
You’ll also learn about the launching API in Spring Batch.
But don’t be too impatient—we need a couple of intermediary steps before writing the test: configuring a test infrastructure and showing you a trick to make the job configuration more flexible.
The next section is about setting up the test infrastructure: the ACME job needs a database to write to, and Spring Batch itself needs a couple of infrastructure components to launch jobs and maintain execution metadata.
Let’s see how to configure a lightweight test infrastructure to launch the test from an IDE.
Spring Batch needs infrastructure components configured in a Spring lightweight container.
These infrastructure components act as a lightweight runtime environment to run the batch process.
Setting up the batch infrastructure is a mandatory step for a batch application, which you need to do only once for all jobs living in the same Spring application context.
The jobs will use the same infrastructure components to run and to store their state.
These infrastructure components are the key to managing and monitoring jobs (chapter 12 covers how to monitor your Spring Batch jobs)
We don’t show tests systematically in this book; otherwise, half of the book would contain testing code! We truly believe in test-driven development, so we test all the source code with automated tests.
Download the source code, browse it, and read chapter 14 to discover more about testing techniques.
For this test, you use the volatile job repository implementation.
It’s perfect for testing and prototyping because it stores execution metadata in memory.
Chapter 2 covers how to set up a job repository that uses a database.
The following listing shows how to configure the test infrastructure.
This listing uses an open source in-memory database called H2; although it may look odd for an online application, it’s easy to deploy and you won’t have to install any database engine to work with the code samples in this chapter.
And remember, this is the testing configuration; the application can use a full-blown, persistent database in.
For a more traditional relational database management system (RDBMS) setup, you could change the data source configuration to use a database like PostgreSQL or Oracle.
Listing 1.7 also runs a SQL script on the database to create the product table and configures a JdbcTemplate to check the state of the database during the test.
This leads us to the following best practice: when configuring a Spring Batch application, the infrastructure and job configuration should be in separate files.
This allows you to swap out the infrastructure for different environments (test, development, staging, production) and still reuse the application (job, in our case) configuration files.
In a split application configuration, the infrastructure configuration file defines the job repository and data source beans; the job configuration file defines the job and depends on the job repository and data source beans.
For Spring to resolve the whole configuration properly, you must bootstrap the application context from both files.
You completed the infrastructure and job configuration in a flexible manner by splitting the configuration into an infrastructure file and a job file.
Next, you make the configuration more flexible by leveraging the Spring Expression Language (SpEL) to avoid hardcoding certain settings in Spring configuration files.
Remember that part of your job configuration is hardcoded in the Spring configuration files, such as all file location settings (in bold):
These settings aren’t flexible because they can change between environments (testing and production, for example) and because rolling files might be used for the incoming archive (meaning the filename would depend on the date)
How does a job refer to the job repository? You may have noticed that we say a job needs the job repository to run but we don’t make any reference to the job repository bean in the job configuration.
The XML step element can have its job-repository attribute refer to a job repository bean.
This attribute isn’t mandatory, because by default the job uses a jobRepository bean.
As long as you declare a jobRepository bean of type JobRepository, you don’t need to explicitly refer to it in your job configuration.
When launching a Spring Batch job, you can provide parameters, as in the following:
The good news is that you can refer to these parameters in your job configuration, which comes in handy for the DecompressTasklet and FlatFileItemReader beans, as shown in the following listing.
You’re done with the configuration: the job and infrastructure are ready, and part of the configuration can come from job parameters, which are set when you launch the job.
It’s time to write the test for your batch process.
You use good old JUnit to write the test, with some help from the Spring testing support.
The following listing shows the integration test for the job.
Listing 1.8 Referring to job parameters in the Spring configuration.
The test uses the Spring TestContext Framework, which creates a Spring application context during the test and lets you inject some Spring beans into the test (with the @Autowired annotation)
Chapter 14 is all about testing, so give it a read if you want to learn more about this topic.
This creates a consistent database environment for each @Test method.
At C, you launch the job with its parameters and check at D that the job correctly inserted the products from the test ZIP archive.
The test ZIP archive doesn’t have to contain thousands of records: it can be small so the test runs quickly.
You can now run the test with your favorite IDE (Eclipse, IDEA) or build tool (Maven, Ant)
Figure 1.10 shows the result of the test execution in Eclipse.
That’s it! You have a reliable integration test for your batch job.
Wasn’t it easy? Even if the job handles hundreds of thousands of records daily, you can test in an IDE in a couple of seconds.
The job works, great, but batch applications aren’t common pieces of software: they must be bulletproof.
The import product job isn’t robust yet: for example, it crashes abruptly if only a single line of the flat file is formatted incorrectly.
The good news is that Spring Batch can help make the job more robust by changing the configuration or by implementing simple interfaces.
Despite all its features, Spring Batch remains lightweight, making jobs easy to test.
Spring Batch’s features related to robustness are thoroughly covered in chapter 8
For now, we show you how to handle unexpected entries when you’re reading data.
By the end of this section, the import product job will be more robust and you’ll have a better understanding of how Spring Batch can help improve robustness in general.
On a good day, the import product job will decompress the input archive, read each line of the extracted flat file, send data to the database, and then exit successfully.
As you know, if something can go wrong, it will.
Perhaps this is acceptable behavior, but what if you can live with some invalid records? In this case, you could skip an invalid line and keep on chugging.
Spring Batch allows you to choose declaratively a skip policy when something goes wrong.
Let’s apply a skip policy to your job’s import step.
Suppose a line of the flat file hasn’t been generated correctly, like the price (in bold) of the third product in the following snippet:
The format of the price field of the third record is incorrect: is uses a comma instead of a period as the decimal separator.
Note that the comma is the field separator Spring Batch uses to tokenize input lines: the framework would see five fields where it expects only four.
Listing 1.10 Setting the skip policy when reading records from the flat file.
The skip-limit attribute B is set to tell Spring Batch to stop the job when the number of skipped records in the step exceeds this limit.
Your application can be tolerant, but not too tolerant! Then, the exception classes that trigger a skip are stated C.
You can now launch the job with an input file containing incorrectly formatted lines, and you’ll see that Spring Batch keeps on running the job as long as the number of skipped items doesn’t exceed the skip limit.
Assuming the ZIP archive contains incorrect lines, you can add a test method to your test, as shown in the following listing.
Note that this code doesn’t do any processing when something goes wrong, but you could choose to log that a line was incorrectly formatted.
Spring Batch also provides hooks to handle errors (see chapter 8)
The job executes quickly and efficiently, and it’s more robust and reacts accordingly to unexpected events such as invalid input records.
Your main tasks were to write a couple of Java classes and do some XML configuration.
That’s the philosophy: focus on the business logic and Spring Batch handles the rest.
Listing 1.11 Testing the job correctly skips incorrect lines with a new test method.
This chapter started with an overview of batch applications: handling large amounts of data automatically.
Meeting these requirements is tricky, and this is where Spring Batch comes in, by processing data efficiently thanks to its chunk-oriented approach and providing ready-to-use components for the most popular technologies and formats.
After the introduction of batch applications and Spring Batch, the import product job gave you a good overview of what the framework can do.
You used built-in components like the FlatFileItemReader class and implemented a database ItemWriter for business domain code.
Remember that Spring Batch handles the plumbing and lets you focus on the business code.
You also saw that Spring Batch isn’t limited to chunk processing when you implemented a Tasklet to decompress a ZIP file.
Configuring the job ended up being quite simple thanks to the Spring lightweight container and Spring Batch XML.
You even saw how to write an automated integration test.
Finally, you learned how to make the job more robust by dealing with invalid data.
You’re now ready to implement your first Spring Batch job.
With this chapter under your belt, you can jump to any other chapter in this book when you need information to use a specific Spring Batch feature: reading and writing components, writing bulletproof jobs with skip and restart, or making your jobs scale.
You can also continue on to chapter 2, where we present the Spring Batch vocabulary and the benefits it brings to your applications.
This introduction got you started with Spring Batch and gave you a good overview of the framework’s features.
It’s time to strengthen this newly acquired knowledge by diving into batch applications concepts and their associated vocabulary, or domain language.
Batch applications are complex entities that refer to many components, so in this chapter we use the Spring Batch domain language to analyze, model, and define the components of batch applications.
This vocabulary is a great communication tool for us in this book but also for you, your team, and your own batch applications.
We first explore Spring Batch’s services and built-in infrastructure components for batch applications: the job launcher and job repository.
The batch domain language dive into the heart of the batch process: the job.
Finally, we study how to model a Spring Batch job and how the framework handles job execution.
By the end of this chapter, you’ll know how Spring Batch models batch applications and what services the framework provides to implement and execute jobs.
All these concepts lay the foundation for efficient configuration and runtime behavior as well as for features like job restart.
These concepts form the starting point for you to unleash the power of Spring Batch.
In chapter 1, we used many technical terms without proper definitions.
We wrote this book with a gentle introduction and without overwhelming you with a large first helping of concepts and definitions.
We introduced Spring Batch from a practical point of view.
Now it’s time to step back and take a more formal approach.
In this section, we define the batch domain language: we pick apart the batch application ecosystem and define each of its elements.
Naming can be hard, but we use some analysis already provided by the Spring Batch project itself.
Let’s start by looking at the benefits of using a domain language for our batch applications.
Using well-defined terminology in your batch applications helps you model, enforce best practices, and communicate with others.
If there’s a word for something, it means the corresponding concept matters and is part of the world of batch applications.
By analyzing your business requirements and all the elements of the batch ecosystem, you find matches, which help you design your batch applications.
In our introduction to chunk processing in chapter 1, we identified the main components of a typical batch process: the reader, the processor, and the writer.
By using the chunk-processing pattern in your applications, you also structure your code with readers, processors, and writers.
The good news about chunk processing is that it’s a pattern well suited for batch applications, and it’s a best practice in terms of memory consumption and performance.
Another benefit of using the domain language is that by following the model it promotes, you’re more likely to enforce best practices.
That doesn’t mean you’ll end up with perfect batch applications, but at least you should avoid the most common pitfalls and benefit from years of experience in batch processing.
At the very least, you’ll have a common vocabulary to use with other people working on batch applications.
You’ll be able to switch projects or companies without having to learn a brand new vocabulary for the same concepts.
Now that you’re aware of the benefits of using a domain language to work with batch applications, let’s define some Spring Batch concepts.
In this subsection, we focus on the core components of Spring Batch applications, and the next subsection covers external components that communicate with Spring Batch applications.
The figure shows two kinds of Spring Batch components: infrastructure components and application components.
The infrastructure components are the job repository and the job launcher.
The application components in Spring Batch are the job and its constituent parts.
From the previous chapter you know that Spring Batch provides components like item readers and item writers you only need to configure and that it’s common to implement your own logic.
Writing batch jobs with Spring Batch is a combination of Spring configuration and Java programming.
Table 2.1 The main components of a Spring Batch application.
Job repository An infrastructure component that persists job execution metadata.
Step A phase in a job; a job is a sequence of steps.
Tasklet A transactional, potentially repeatable process occurring in a step.
Item A record read from or written to a data source.
Item reader A component responsible for reading items from a data source.
Item processor A component responsible for processing (transforming, validating, or filtering) a read item before it’s written.
Item writer A component responsible for writing a chunk to a data source.
The framework provides a job repository to store job metadata and a job launcher to launch jobs, and the application developer configures and implements jobs.
Going forward from this point, we use the terms listed in table 2.1
The remainder of this chapter describes the concepts behind these terms, but first we see how the components of a Spring Batch application interact with the outside world.
A batch application isn’t an island: it needs to interact with the outside world just like any enterprise application.
Figure 2.2 shows how a Spring Batch application interacts with the outside world.
You’ll always use the JobLauncher interface and JobParameters class, but the event can come from anywhere: a system scheduler like cron that runs periodically, a script that launches a Spring Batch process, an HTTP request to a web controller that launches the job, and so on.
Chapter 4 covers different scenarios to trigger the launch of your Spring Batch jobs.
Batch jobs are about processing data, and that’s why figure 2.2 shows Spring Batch communicating with data sources.
These data sources can be of any kind, the file system and a database being the most common, but a job can read and write messages to Java Message Service (JMS) queues as well.
In fact, the job repository stores job execution metadata in a database to provide Spring Batch reliable monitoring and restart features.
Note that figure 2.2 doesn’t show whether Spring Batch needs to run in a specific container.
Chapter 4 says more about this topic, but for now, you just need to know that Spring Batch can run anywhere the Spring Framework can run: in its own Java process, in a web container, in an application, or even in an Open Services Gateway initiative (OSGi) container.
The container depends on your requirements, and Spring Batch is flexible in this regard.
Now that you know more about Spring Batch’s core components and how they interact with each other and the outside world, let’s focus on the framework’s infrastructure components: the job launcher and the job repository.
Figure 2.2 A Spring Batch application interacts with systems like schedulers and data sources (databases, files, or JMS queues)
The Spring Batch infrastructure includes components that launch your batch jobs and store job execution metadata.
As a batch application developer, you don’t have to deal directly with these components, as they provide supporting roles to your applications, but you need to configure this infrastructure at least once in your Spring Batch application.
This section gives an overview of the job launcher, job repository, and their interactions, and then shows how to configure persistence of the job repository.
The Spring Batch infrastructure is complex, but you need to deal mainly with two components: the job launcher and the job repository.
These concepts match two straightforward Java interfaces: JobLauncher and JobRepository.
As figure 2.2 shows, the job launcher is the entry point to launch Spring Batch jobs: this is where the external world meets Spring Batch.
The run method accepts two parameters: Job, which is typically a Spring bean configured in Spring Batch XML, and JobParameters, which is usually created on the fly by the launching mechanism.
Who calls the job launcher? Your own Java program can use the job launcher to launch a job, but so can command-line programs or schedulers (like cron or the Javabased Quartz scheduler)
The job launcher encapsulates launching strategies such as executing a job synchronously or asynchronously.
Spring Batch provides one implementation of the JobLauncher interface: SimpleJobLauncher.
For now, it’s sufficient to know that the SimpleJobLauncher class only launches a job—it doesn’t create it but delegates this work to the job repository.
The job repository maintains all metadata related to job executions.
The JobRepository interface provides all the services to manage the batch job lifecycle: creation, update, and so on.
This explains the interactions in figure 2.2: the job launcher delegates job creation to the job repository, and a job calls the job repository during execution to store its current state.
This is useful to monitor how your job executions proceed and to restart a job exactly where it failed.
Note that the Spring Batch runtime handles all calls to the job repository, meaning that persistence of the job execution metadata is transparent to the application code.
What constitutes runtime metadata? It includes the list of executed steps; how many items Spring Batch read, wrote, or skipped; the duration of each step; and so forth.
We won’t list all metadata here; you’ll learn more when we study the anatomy of a job in section 2.3
Spring Batch provides two implementations of the JobRepository interface: one stores metadata in memory, which is useful for testing or when you don’t want monitoring or restart capabilities; the other stores metadata in a relational database.
Next, we see how to configure the Spring Batch infrastructure in a database.
Spring Batch provides a job repository implementation to store your job metadata in a database.
This allows you to monitor the execution of your batch processes and their results (success or failure)
Persistent metadata also makes it possible to restart a job exactly where it failed.
Configuring this persistent job repository helps illustrate the other concepts in this chapter, which are detailed in chapter 3
Spring Batch delivers the following to support persistent job repositories:
Let’s now see how to configure the database job repository.
All you have to do is create a database for Spring Batch and then execute the corresponding SQL script for your database engine.
The following listing shows how to configure a job repository in a database.
The job-repository XML element in the batch namespace creates a persistent job repository B.
To work properly, the persistent job repository needs a data source and a transaction manager.
A data source implementation C that holds a single JDBC connection and reuses it for each query is used because it’s convenient and good enough for a single-threaded application (like a batch process)
If you plan to use the data source in a concurrent application, then use a connection pool, like Apache Commons DBCP or c3p0
Using a persistent job repository doesn’t change much in the Spring Batch infrastructure configuration: you use the same job launcher implementation as in the in-memory configuration.
Now that the persistent job repository is ready, let’s take a closer look at it.
If you look at the job repository database, you see that the SQL script created six tables.
The latter class launches a job several times to populate the batch metadata tables.
Don’t worry if you see an exception in the console: we purposely fail one job execution.) We analyze the content of the job repository later in this chapter because it helps to explain how Spring Batch manages job execution, but if you’re impatient to see the content of the batch tables, read the next note.
The Console Server then provides the default username and password to log in.
Keep in mind that this is an in-memory database, so stopping the program will cause all data to be lost!
Figure 2.3 shows how you can use the Spring Batch Admin web application to view job executions.
Spring Batch Admin accesses the job repository tables to provide this functionality.
We now have a persistent job repository, which we use to illustrate forthcoming runtime concepts.
You have enough knowledge of the in-memory and persistent implementations of the job repository to compare them and see which best suits your needs.
Spring Batch users sometimes see the persistent job repository as a constraint because it means creating dedicated Spring Batch tables in a database.
They prefer to fall back on the in-memory job repository implementation, which is more flexible.
These users don’t always realize the consequences of using the in-memory implementation.
Let’s answer some frequently asked questions about the various job repository implementations.
We cover Spring Batch Admin in chapter 12, where we discuss Spring Batch application monitoring.
You’ll also see more of Spring Batch Admin in this chapter when you use it to browse batch execution metadata.
Figure 2.3 The Spring Batch Admin web application lists all job instances for a given job, in this case, import products.
Spring Batch Admin uses job metadata stored in a database to monitor job executions.
What are the benefits of the persistent job repository? The benefits are monitoring, restart, and clustering.
You can browse the batch execution metadata to monitor your batch executions.
When a job fails, the execution metadata is still available, and you can use it to restart the job where it left off.
The persistent job repository, thanks to the isolation the database provides, prevents launching the exact same job from multiple nodes at the same time.
Consider the persistent job repository as a safeguard against concurrency issues when creating batch entities in the database.
Does the persistent job repository add overhead? Compared to the in-memory job repository, yes.
Communicating with a potentially remote database is always more costly than speaking to local in-memory objects.
But the overhead is usually small compared to the actual business processing.
The benefits the persistent job repository brings to batch applications are worth the limited overhead!
Can I use a different database for the persistent job repository and my business data? Yes, but be careful with transaction management.
You can use the Java Transaction API (JTA) to make transactions span both databases: the batch tables and the business tables will always be synchronized, but you’ll add overhead because managing multiple transactional resources is more expensive than managing just one.
If transactions don’t span the two databases, batch execution metadata and business data can get unsynchronized on failure.
Data such as skipped items could then become inaccurate, or you could see problems on restart.
To make your life easier (and your jobs faster and reliable), store the batch metadata in the same database as the business data.
Let’s dive into the structural and runtime aspects of the core Spring Batch concept: the job.
The job is the central concept in a batch application: it’s the batch process itself.
A job has two aspects that we examine in this section: a static aspect used for job modeling and a dynamic aspect used for runtime job management.
Spring Batch also provides a strong runtime foundation to execute and dynamically manage jobs.
This foundation provides a reliable way to control which instance of a job Spring Batch executes and the ability to restart a job.
This section explains these two job aspects: static modeling and dynamic runtime.
Let’s delve into these concepts and see what they bring to your batch applications.
Recall from chapter 1 that the import products job consists of two steps: decompress the incoming archive and import the records from the expanded file into the database.
We could also add a cleanup step to delete the expanded file.
Figure 2.4 depicts this job and its three successive steps.
Decomposing a job into steps is cleaner from both a modeling and a pragmatic perspective because steps are easier to test and maintain than is one monolithic job.
Jobs can also reuse steps; for example, you can reuse the decompress step from the import products job in any job that needs to decompress an archive—you only need to change the configuration.
This version generates and sends a report to an administrator if the read-write step skipped records.
To decide which path a job takes, Spring Batch allows for control flow decisions based on the status of the previous step (completed, failed) or based on custom logic.
Figure 2.5 A Spring Batch job can be a nonlinear sequence of steps, like this version of the import products job, which sends a report if some records were skipped.
Figure 2.4 A Spring Batch job is a sequence of steps, such as this import products job, which includes three steps: decompress, readwrite, and cleanup.
Anatomy of a job (by checking the content of a database table, for example)
You can then create jobs with complex control flows that react appropriately to any kind of condition (missing files, skipped records, and so on)
Control flow brings flexibility and robustness to your jobs because you can choose the level of control complexity that best suits any given job.
The unpleasant alternative would be to split a big, monolithic job into a set of smaller jobs and try to orchestrate them with a scheduler using exit codes, files, or some other means.
You also benefit from a clear separation of concerns between processing (implemented in steps) and execution flow, configured declaratively or implemented in dedicated decision components.
You have less temptation to implement transition logic in steps and thus tightly couple steps with each other.
Spring Batch provides an XML vocabulary to configure steps within a job.
The following listing shows the code for the linear version of the import products job.
The next attribute of the step tag sets the execution flow, by pointing to the next step to execute.
Tags like tasklet or chunk can refer to Spring beans with appropriate attributes.
When a job is made of a linear sequence of steps, using the next attribute of the step elements is enough to connect the job steps.
The next listing shows the configuration for the nonlinear version of the import products job from figure 2.5
We introduce these concepts here only to illustrate the structure of a Spring Batch job.
Notice from the previous XML fragment that Spring Batch XML is expressive enough to allow job configuration to be human readable.
If your editor supports XML, you also benefit from code completion and code validation when editing your XML job configuration.
An integrated development environment like the Eclipse-based SpringSource Tool Suite also provides a graphical view of a job configuration, as shown in figure 2.6
To get this graph, open the corresponding XML file and select the BatchGraph tab at the bottom of the editor.
It also provides support for projects in the Spring portfolio like Spring Batch.
Now that you know that a Spring Batch job is a sequence of steps and that you can control job flow, let’s see what makes up a step.
Spring Batch defines the Step interface to embody the concept of a step and provides implementations like FlowStep, JobStep, PartitionStep, and TaskletStep.
The only implementation you care about as an application developer is TaskletStep, which delegates processing to a Tasklet object.
Java interface contains only one method, execute, to process some unit of work.
Creating a step consists of either writing a Tasklet implementation or using one provided by Spring Batch.
You implement your own Tasklet when you need to perform processing, such as decompressing files, calling a stored procedure, or deleting temporary files at the end of a job.
If your step follows the classic read-process-write batch pattern, use the Spring Batch XML chunk element to configure it as a chunk-processing step.
The chunk element allows your step to use chunks to efficiently read, process, and write data.
You now know that a job is a sequence of steps and that you can easily define this sequence in Spring Batch XML.
You implement steps with Tasklets, which are either chunk oriented or completely customized.
The tool displays a graph based on the job model defined in Spring Batch XML.
Because batch processes handle data automatically, being able to monitor what they’re doing or what they’ve done is a must.
When something goes wrong, you need to decide whether to restart a job from the beginning or from where it failed.
To do this, you need to strictly define the identity of a job run and reliably store everything the job does during its run.
This is a difficult task, but Spring Batch handles it all for you.
We defined a job as a batch process composed of a sequence of steps.
Spring Batch also includes the concepts of job instance and job execution, both related to the way the framework handles jobs at runtime.
Figure 2.7 illustrates the correspondence between a job, its instances, and their executions for two days of executions of the import products job.
Now that we’ve defined the relationship between job, job instance, and job execution, let’s see how to define a job instance in Spring Batch.
In Spring Batch, a job instance consists of a job and job parameters.
This is a simple yet powerful way to define a job instance, because you have full control over the job parameters, as shown in the following snippet:
As a Spring Batch developer, you must keep in mind how to uniquely define a job instance.
Table 2.2 Definitions for job, job instance, and job execution.
Job A batch process, or sequence of steps The import products job.
Job execution The execution of a job instance (with success or failure)
Figure 2.7 A job can have several job instances, which can have several job executions.
The import products job executes daily, so it should have one instance per day and one or more corresponding executions, depending on success or failure.
In our example, a job instance is temporal, as it refers to the day it was launched.
But you’re free to choose what parameters constitute your job instances thanks to job parameters: date, time, input files, or simple sequence counter.
What happens if you try to run the same job several times with the same parameters? It depends on the lifecycle of job instances and job executions.
Several rules apply to the lifecycle of a job instance and job execution:
When you launch a job for the first time, Spring Batch creates the corresponding job instance and a first job execution.
You can’t launch the execution of a job instance if a previous execution of the same instance has already completed successfully.
You can’t launch multiple executions of the same instance at the same time.
We hope that by now all these concepts are clear.
As an illustration, let’s perform runs of the import products job and analyze the job metadata that Spring Batch stores in the database.
The import products job introduced in chapter 1 is supposed to run once a day to import all the new and updated products from the catalog system.
To see how Spring Batch updates the job metadata in the persistent job repository previously configured, make the following sequence of runs:
Figure 2.8 shows the graphical interface with the job instances and the job repository created for this run.
The Start Date attribute indicates exactly when the job ran.
The instance is marked as COMPLETED and is the first and only execution to complete successfully.
You can also learn about the job instance because the job parameters appear in the table.
You get an exception because an execution already completed successfully, so you can’t launch another execution of the same instance.
You get an exception saying that nothing can be extracted from the archive (the archive is corrupted)
Step 1 Replace the corrupted archive with the correct file (the same as for the first run)
The first failed because of a corrupted archive, but the second completed successfully, thereby completing the job instance.
You just put into practice the concepts of job instance and job execution.
To do so, you used a persistent job repository, which allowed you to visualize job instances and executions.
In this example, job metadata illustrated the concepts, but you can also use this metadata for monitoring a production system.
This chapter is rich in Spring Batch concepts and terminology! Using a now welldefined vocabulary, you can paint a clear picture of your batch applications.
You learned how the Spring Batch framework models these concepts, an important requirement to understanding how to implement batch solutions.
You saw the static and dynamic aspects of jobs: static by modeling and configuring jobs with steps, dynamic through the job runtime handling of job instances and executions.
Restarting failed jobs is an important requirement for some batch applications, and you saw how Spring Batch implements this feature by storing job execution metadata; you also saw the possibilities and limitations of this mechanism.
With this picture of the Spring Batch model in mind, let’s move on to the next chapter and see how to configure our batch applications with the Spring lightweight container and Spring Batch XML.
By now, you have a good understanding of what Spring Batch is capable of, so it’s time to exhaustively cover the framework.
Chapter 3 is about configuration: how to configure jobs, steps in jobs, listeners, and infrastructure components like the job repository.
You can use this chapter as a reference for the XML syntax and the annotations you can use to configure Spring Batch.
You can also use this chapter to discover every single piece of Spring Batch configuration.
There are myriads of ways to launch batch jobs, depending on your requirements and the systems you’re working on.
Chapter 4 shows how to launch Spring Batch jobs from the command line and from HTTP requests.
It also shows you how to schedule the execution of jobs with the system scheduler cron and with a Java-based scheduler like Spring Scheduling.
Do you remember chunk-oriented processing? This is the way Spring Batch efficiently handles the classical read-process-write pattern in batch applications.
You’ll learn how to configure ready-to-use components to read from different data sources: JDBC, JPA, and Hibernate; flat and XML files; and JMS queues.
Again, Spring Batch provides off-the-shelf components to write data to the most popular resources.
Read this chapter to learn how to leverage batch updates to write to a database efficiently, send emails, and write to flat and XML files.
Chapter 7 covers the processing phase of a chunk-oriented step.
This is where you can embed business logic to transform read items before they’re written and where you can avoid sending an item to the writing phase through validation and filtering.
Chapter 8 covers Spring Batch built-in features for bulletproofing jobs, like skip, retry, and restart on failure.
Chapter 9 provides an in-depth presentation of transaction management in Spring Batch, as well as some transaction patterns you’ll definitely use in batch applications.
The chapter introduced configuring and implementing the structure of batch jobs and related entities with Spring Batch XML.
In this chapter, we continue the online store case study: reading products from files, processing products, and integrating products in the database.
Configuring this process serves as the backdrop for describing all of Spring Batch’s configuration capabilities.
After describing Spring Batch XML capabilities, we show how to configure batch jobs and related entities with this dedicated XML vocabulary.
We also look at configuring a repository for batch execution metadata.
In the last part of this chapter, we focus on advanced configuration topics and describe how to make configuration easier.
How should you read this chapter? You can use it as a reference for configuring Spring Batch jobs and either skip it or come back to it for a specific configuration need.
Or, you can read it in its entirety to get an overview of nearly all the features available in Spring Batch.
We say an “overview” because when you learn about configuring a skipping policy, for example, you won’t learn all the subtleties of this topic.
That’s why you’ll find information in dedicated sections in other chapters to drill down into difficult topics.
Like all projects in the Spring portfolio and the Spring framework itself, Spring Batch provides a dedicated XML vocabulary and namespace to configure its entities.
In this section, we describe how to use Spring Batch XML and the facilities it offers for batch configuration.
Without this vocabulary, we’d need intimate knowledge of Spring Batch internals and entities that make up the batch infrastructure, which can be tedious and complex to configure.
Like most components in the Spring portfolio, Spring Batch configuration is based on a dedicated Spring XML vocabulary and namespace.
By hiding internal Spring Batch implementation details, this vocabulary provides a simple way to configure core components like jobs and steps as well as the job repository used for job metadata (all described in chapter 2)
The vocabulary also provides simple ways to define and customize batch behaviors.
Before we get into the XML vocabulary and component capabilities, you need to know how use Spring Batch XML in Spring configuration files.
In the following listing, the batch namespace prefix is declared and used in child XML elements mixed with other namespace prefixes, such as the Spring namespace mapped to the default XML namespace.
Spring Batch uses the Spring standard mechanism to configure a custom XML namespace: the Spring Batch XML vocabulary is implemented in Spring Batch jars, automatically discovered, and handled by Spring.
In listing 3.1, because Spring XML Beans uses the default namespace, each Spring Batch XML element is qualified with the batch namespace prefix.
Note that a namespace prefix can be whatever you want; in our examples, we use the batch and beans prefixes by convention.
Listing 3.2 declares the Spring Batch namespace as the default namespace in the root XML element.
In this case, the elements without a prefix correspond to Spring Batch elements.
Using this configuration style, you don’t need to repeat using the Spring Batch namespace prefix for each Spring Batch XML element.
Listing 3.2 Using the Spring Batch namespace as the default namespace.
While this approach works for creating beans and injecting dependencies, it’s insufficient to define complex tasks.
The DTD bean and property mechanism can’t hide complex bean creation, which is a shortcoming in configuring advanced features like aspect-oriented programming (AOP) and security.
Spring 2.0 introduced a new, extensible, schema-based XML configuration system.
On the XML side, XML schemas describe the syntax, and on the Java side, corresponding namespace handlers encapsulate the bean creation logic.
The Spring framework provides namespaces for its modules (AOP, transaction, Java Message Service [JMS], and so on), and other Spring-based projects can benefit from the namespace extension mechanism to provide their own namespaces.
Each Spring portfolio project comes with one or more dedicated vocabularies and namespaces to provide the most natural and appropriate way to configure objects using modulespecific tags.
With a configuration defined using Spring Batch XML, you can now leverage all the facilities it provides.
In the next section, we focus on Spring Batch XML features and describe how to configure and use those capabilities.
You use this XML vocabulary to configure all batch entities described in chapter 2
Table 3.1 lists and describes the main tags in Spring Batch XML.
Spring Batch XML configures the structure of batches, but specific entities need to be configured using Spring features.
Spring Batch XML provides the ability to interact easily with standard Spring XML.
You can configure other entities like item readers and writers as simple beans and then reference them from entities configured with Spring Batch XML.
Figure 3.1 describes the possible interactions between the Spring Batch namespace and the Spring default namespace.
Some batch entities, such as a job, can refer to Spring beans defined with the beans vocabulary, such as item readers and writers.
Now that you’ve seen the capabilities provided for Spring Batch configuration, it’s time to dive into details.
In our case study, these capabilities let you configure a batch job and its steps.
As described in chapter 2, the central entities in Spring Batch are jobs and steps, which describe the details of batch processing.
The use case entails defining what the batch must do and how to organize its processing.
For our examples, we use the online store case study.
After defining the job, we progressively extend it by adding internal processing.
We focus here on how to configure the core entities of batch processes; we also examine their relationships at the configuration level.
Spring Batch XML makes the configuration of jobs and related entities easier.
You don’t need to configure Spring beans corresponding to internal Spring Batch objects; instead you can work at a higher level of abstraction, specific to Spring Batch.
Spring Batch XML configures batch components such as job, step, tasklet, and chunk, as well as their relationships.
Within Spring Batch XML, this hierarchy corresponds to nested XML elements with the ability to specify parameters at each level, as shown in the following listing.
For example, a job defined with the job element can contain one or more steps, which you define with step elements within the job element.
Similar types of configurations can be used for steps, tasklets, and chunks.
Chunk configuration Figure 3.2 The entity configuration hierarchy for batch processing.
The Spring configuration contains the job configuration, which contains the step configuration, which contains the tasklet configuration, which contains the chunk configuration.
For our case study, these nested elements are used to define each job, particularly the reading, processing, and writing logic.
Now that we’ve described high-level configuration concepts for Spring Batch entities, let’s examine configuration details using Spring Batch XML.
When implementing a batch application with Spring Batch, the top-level entity is the job, and it’s the first entity you configure when defining a batch process.
In the context of our case study, the job is a processing flow that imports and handles products for the web store.
To configure a job, you use the Spring Batch XML job element and the attributes listed in Table 3.2
The attributes parent and abstract deal with configuration inheritance in Spring Batch; for details, see section 3.4.4
Let’s focus here on the restartable, incrementer, and job-repository attributes.
This entity is required when trying to launch a batch job through the startNextInstance method of the JobOperator interface.
If true, this job is a parent job configuration for other jobs.
The restartable attribute specifies whether Spring Batch can restart a job.
The following snippet describes how to configure this behavior for a job:
The job-repository attribute is a bean identifier that specifies which job repository to use for the job.
The incrementer attribute provides a convenient way to create new job parameter values.
Note that the JobLauncher doesn’t need this feature because you must provide all parameter values explicitly.
The getNext method can use the parameters from the previous job instance to create new values.
Besides these attributes, the job element supports nested elements to configure listeners and validators.
You can configure Spring Batch to validate job parameters and check that all.
This class allows you to specify which parameters are required and which are optional.
The following listing describes how to configure and use this class in a job.
The requiredKeys and optionalKeys properties of the validator class are used to set these values.
Now let’s look at configuring job steps to define exactly what processing takes place in that job.
Here, we go down a level in the job configuration and describe what makes up a job: steps.
Don’t hesitate to refer back to figure 3.2 to view the relationships between all the batch entities.
Steps define the sequence of actions a job will take, one at a time.
In the online store use case, you receive products in a compressed file; the job decompresses the file before importing and saving the products in the database, as illustrated in figure 3.3
You configure a job step using the step element and the attributes listed in table 3.3
The attributes parent and abstract deal with configuration inheritance in Spring Batch; for details, see section 3.4.4
Configuring a step is simple because a step is a container for a tasklet, executed at a specific point in the flow of a batch process.
For our use case, you define what steps take place and in what order.
Product files come in as compressed data files to optimize file uploads.
A first step decompresses the data file, and a second step processes the data.
You always define the step element as a child element of the job element.
Spring Batch uses the id attribute to identify objects in a configuration.
This aspect is particularly important to use steps in a flow.
The next attribute of the step element is set to define which step to execute next.
In the preceding fragment, the step identified as decompress is executed before the step identified as readWrite.
Now that we’ve described the job and step elements, let’s continue our tour of Spring Batch core objects with tasklets and chunks, which define what happens in a step.
The tasklet and chunk are step elements used to specify processing.
To import products, you successively configure how to import product data, how to process products, and where to put products in the database, as illustrated in figure 3.4
If true, this step is a parent step configuration for other steps.
Figure 3.4 The import product tasklet configuration and chunk configuration define three steps: import, process, and store products.
A tasklet corresponds to a transactional, potentially repeatable process occurring in a step.
You can write your own tasklet class by implementing the Tasklet interface or use the tasklet implementations provided by Spring Batch.
Implementing your own tasklets is useful for specific tasks, such as decompressing an archive or cleaning a directory.
Spring Batch provides more generic tasklet implementations, such as to call system commands or to do chunk-oriented processing.
To configure a tasklet, you define a tasklet element within a step element.
Listing 3.5 shows how to use the last three attributes of the tasklet element, whatever the tasklet type.
Because you want to write the products from the compressed import file to a database, you must specify a transaction manager to handle transactions associated with inserting products in the database.
This listing also specifies additional parameters to define restart behavior.
You must use this attribute when implementing a custom tasklet.
By default, a tasklet is transactional, and the default value of the attribute is transactionManager.
We describe in section 3.2.5 how to control rollback for a step.
In the case of a custom tasklet, you can reference the Spring bean implementing the Tasklet interface with the ref attribute.
Spring Batch delegates processing to this class when executing the step.
In our use case, decompressing import files doesn’t correspond to processing that Spring Batch natively supports, so you need a custom tasklet to implement decompression.
The chunk child element of the tasklet element configures chunk processing.
Note that you don’t need to use the ref attribute of the tasklet element.
Configuring a tasklet can be simple, but to implement chunk processing, the configuration gets more complex because more objects are involved.
In chunk processing, Spring Batch reads data chunks from a source and transforms, validates, and then writes data chunks to a destination.
In the online store case study, this corresponds to importing products into the database.
To configure chunk objects, you use an additional level of configuration using the chunk element with the attributes listed in table 3.5
The first four attributes (reader, processor, writer, commit-interval) in table 3.5 are the most commonly used in chunk configuration.
These attributes define which entities are involved in processing chunks and the number of items to process before committing.
When the number of items read reaches the commit interval number, the entire corresponding chunk is written out through the item writer and the transaction is committed.
If processing reaches the skip limit, the next exception thrown on item processing (read, process, or write) causes the step to fail.
The attributes reader, processor, and writer B correspond to Spring bean identifiers defined in the configuration.
The commit-interval attribute C defines that Spring Batch will execute a database commit after processing each 100 elements.
Other attributes deal with configuring the skip limit, retry limit, and completion policy aspects of a chunk.
In listing 3.7, the skip-limit attribute configures the maximum number of items that Spring Batch can skip.
The cache-capacity attribute sets the cache capacity for retries, meaning the maximum number of items that can fail without being skipped or recovered.
We’ve described rather briefly how to configure skip, retry, and completion in steps.
We look at this topic in more detail in chapter 8, where we aim for batch robustness and define error handlers.
The last attributes correspond to more advanced configurations regarding transactions.
Most of the attributes described in table 3.5 have equivalent child elements to allow embedding beans in the chunk configuration.
These beans are anonymous and specially defined for the chunk.
Table 3.6 describes chunk children elements usable in this context.
The following listing describes how to rewrite listing 3.6 using child elements instead of attributes for the reader, processor, and writer.
You can configure other objects with child elements in chunks.
The following listing shows these elements specifying which exceptions will trigger an event (include child element) and which ones won’t (exclude child element)
We provide a short description of the feature and show how to configure it.
Streams provide the ability to save state between executions for step restarts.
The step needs to know which instance is a stream (by implementing the ItemStream interface)
Spring Batch automatically registers as streams everything specified in the.
By default, objects referenced using a reader, processor, and writer are automatically registered.
Note that it’s not the case when the step doesn’t directly reference entities.
That’s why these entities must be explicitly registered as streams, as illustrated in figure 3.5
If you use a composite item writer that isn’t a stream and that internally uses stream writers, the step doesn’t have references to those writers.
In this case, you must define explicitly the writers as streams for the step in order to avoid problems on restarts when errors occur.
The following listing describes how to configure this aspect using the streams child element of the chunk element.
Spring Batch automatically registers readers, processors, and writers if they implement the ItemStream interface.
Explicit registration is necessary if Spring Batch doesn’t know about the streams to register, such as the writers in the figure used through a composite writer.
In listing 3.10, you must register as streams B the item writers C involved in the composite item writer for the step using the streams element as a child of the chunk element.
The streams element then defines one or more stream elements—in this example, two stream elements B.
In this section, we described how to configure a batch job with Spring Batch.
We saw that transactions guarantee batch robustness and are involved at several levels in the configuration.
Because this is an important issue, we gather all configuration aspects related to transactions in the next section.
Transactions in Spring Batch are an important topic: transactions contribute to the robustness of batch processes and work in combination with chunk processing.
You configure transactions at different levels because transactions involve several types of objects.
In the online store use case, you validate a set of products during processing.
The first thing to configure is the Spring transaction manager because Spring Batch is based on the Spring framework and uses Spring’s transaction support.
Spring provides built-in transaction managers for common persistent technologies and frameworks.
Every Spring transaction manager must be configured using the factory provided by the framework to create connections and sessions.
In the case of JDBC, the factory is an instance of the DataSource interface.
Once you configure the transaction manager, other configuration elements can refer to it from different levels in the batch configuration, such as from the tasklet level.
The next snippet configures the transaction manager using the transactionmanager attribute:
Section 3.3 explains that you must also use a transaction manager to configure entities to interact with a persistent job repository.
Now that you know which Spring transaction manager to use, you can define how transactions are handled during processing.
That’s why Spring Batch provides a commit interval tied to the chunk size.
The commit-interval attribute configures this setting at the chunk level and ensures that Spring Batch executes a commit after processing a given number of items.
The following example sets the commit interval to 100 items:
Transactions have several attributes defining transactional behaviour, isolation, and timeout.
These attributes specify how transactions behave and can affect performance.
Because Spring Batch is based on Spring transactional support, configuring these attributes is generic and applies to all persistent technologies supported by the Spring framework.
The Spring transaction manager The Spring framework provides generic transactional support.
Spring provides implementations for a large range of persistent technologies and frameworks like JDBC, Java Persistence API (JPA), and so on.
Spring builds on this interface to implement standard transactional behavior and allows configuring transactions with Spring beans using AOP or annotations.
This generic support doesn’t need to know which persistence technology is used and is completely independent of it.
REPEATABLE_READ prevents dirty reads and nonrepeatable reads, but phantom reads can still occur because intermediate data can disappear between two reads; and SERIALIZABLE prevents dirty reads, nonrepeatable reads, and phantom reads, meaning that a transaction is completely isolated and no intermediate states can be seen.
Choosing the DEFAULT value leaves the choice of the isolation level to the database, which is a good choice in almost all cases.
Choosing REQUIRED implies that processing will use the current transaction if it exists and create a new one if it doesn’t.
Finally, the timeout attribute defines the timeout in seconds for the transaction.
If the timeout attribute is absent, the default timeout of the underlying system is used.
The first case is readers built on a transactional resource like a JMS queue.
Rollback and commit conventions in Spring and Java Enterprise Edition Java defines two types of exceptions: checked and unchecked.
A checked exception extends the Exception class, and a method must explicitly handle it in a try-catch block or declare it in its signature’s throws clause.
An unchecked exception extends the RuntimeException class, and a method doesn’t need to catch or declare it.
You commonly see checked exceptions used as business exceptions (recoverable) and unchecked exceptions as lower-level exceptions (unrecoverable by the business logic)
By default, in Java EE and Spring, commit and rollback are automatically triggered by exceptions.
If Spring catches a checked exception, a commit is executed.
If Spring catches an unchecked exception, a rollback is executed.
You can configure Spring’s transactional support to customize this behavior by setting which exceptions trigger commits and rollbacks.
As described throughout this section, configuring batch processes can involve many concepts and objects.
Spring Batch eases the configuration of core entities like job, step, tasklet, and chunk.
Spring Batch also lets you configure transaction behavior and define your own error handling.
The next section covers configuring the Spring Batch job repository to store batch execution data.
Along with the batch feature, the job repository is a key feature of the Spring Batch infrastructure because it provides information about batch processing.
In this section, we focus on configuring the job repository in Spring Batch XML.
The job repository is part of the more general topic concerning batch process monitoring.
Spring Batch provides the JobRepository interface for the batch infrastructure and job repository to interact with each other.
The interface provides all the methods required to interact with the repository.
We describe the job repository in detail in chapter 12
Spring Batch bases this class on a set of Data Access Objects (DAOs) used for dedicated interactions and data management.
Spring Batch provides two kinds of DAOs at this level:
You can use the in-memory DAO for tests, but you shouldn’t use it in production environments.
Configuring the job repository persistent DAO when you want to have robust batch processing with checks on startup.
Because the persistent DAO uses a database, you need additional information in the job configuration.
In this section, we configure the in-memory and persistent job repositories.
The first kind of job repository is the in-memory repository.
The persistent repository uses a Spring bean for configuration and requires a transaction manager.
Finally, the job repository is referenced from the job using the job-repository attribute of the job element.
The value of this attribute is the identifier of the job repository bean.
Configuring a persistent job repository isn’t too complicated, thanks to Spring Batch XML, which hides all the bean configuration details that would otherwise be required with Spring XML.
Our configuration uses the job-repository element and specifies the attributes listed in table 3.8
The following listing shows how to use the job-repository element and its attributes to configure a persistent job repository for a relational database.
This attribute is mandatory, and its default value is dataSource.
This attribute is mandatory, and its default value is transactionManager.
This attribute is mandatory, and its default value is SERIALIZABLE, which prevents accidental concurrent creation of the same job instance multiple times (REPEATABLE_READ would work as well)
This prefix allows identifying the tables used by the job repository from the tables used by the batch.
Use this attribute only with Oracle or if Spring Batch doesn’t detect the database type.
The job-repository element can then configure the persistent job repositoryC.
This element references the data source and transaction manager previously configured.
It also uses additional parameters to force the use of the SERIALIZABLE isolation level when creating new job executions and to identify Spring Batch tables with the BATCH_ prefix.
This job repository is then referenced from the job configuration D.
The job repository is an important part of the Spring Batch infrastructure because it records batch-processing information to track which jobs succeed and fail.
Although Spring Batch provides an in-memory job repository, you should use it only for tests.
In chapter 12, we monitor batch applications using the job repository.
This section focuses on advanced Spring Batch configurations that leverage the Spring Expression Language (SpEL), modularize configurations with inheritance, and use listeners.
The goal of the step scope is to link beans with steps within batches.
This mechanism allows instantiation of beans configured in Spring only when steps begin and allows you to specify configuration and parameters for a step.
If you use Spring Batch XML, the step scope is automatically registered for the current Spring container and is usable without additional configuration.
The job repository maintains batch metadata such as job instances and executions.
When creating these entities, the job repository also acts as a centralized safeguard: it prevents the creation of identical job instances when jobs launch concurrently.
The job repository relies on the transactional capabilities of the underlying database to achieve this synchronization.
Thanks to this safeguard, you can distribute Spring Batch on multiple nodes and be sure not to start the same instance twice due to a race condition.
Spring Batch XML, you must define the step scope with its StepScope class, as described in the following snippet:
Developers using custom Spring scopes may be surprised by this configuration.
The step scope is particularly useful and convenient when combined with SpEL to implement late binding of properties.
It handles cases when values can’t be known during development and configuration because they depend on the runtime execution context.
Spring Batch leverages SpEL to access entities associated with jobs and steps and to provide late binding in configurations.
The typical use case of late binding is to use.
A bean scope specifies how to create instances of the class for a given bean definition.
Spring provides scopes like singleton, prototype, request, and session but also allows custom scopes to be plugged in.
A custom scope implementation handles how an instance is served in a given Spring container.
For example, with the singleton scope, the same instance is always provided by Spring.
With the prototype scope, Spring always creates a new instance.
The scope attribute of the bean element defines the bean scope; for example:
SpEL was created to provide a single language for use across the whole Spring portfolio, but it’s not directly tied to Spring and can be used independently.
SpEL supports a large set of expression types, such as literal, class, property access, collection, assignment, and method invocation.
The power of this language is in its ability to use expressions to reference bean properties present in a particular context.
SpEL can resolve expressions not only in a properties file but also in beans managed in Spring application contexts.
Advanced configuration topics batch parameters specified at launch time in the batch configuration.
The values are unknown at development time when configuring batch processes.
Spring evaluates these values at runtime during the batch process execution.
Table 3.9 describes all entities available from the step scope.
With this approach, it’s now possible to specify property values filled in at launch time, as shown in the following listing.
In the context of the case study, this mechanism makes it possible to specify at batch startup the file to use to import product data.
You don’t need to hardcode the filename in the batch configuration, as illustrated in figure 3.6
Figure 3.6 Using the filename to import from the job configuration.
Spring Batch provides the ability to specify and use listeners at the job and step levels within batches.
This feature is particularly useful and powerful because Spring Batch can notify each level of batch processing, where you can plug in additional processing.
For example, in the online store case study, you can add a listener that Spring Batch calls when a batch fails, or you can use a listener to record which products Spring Batch skips because of errors, as shown in figure 3.7
We provide concrete examples of this feature in chapter 8
Table 3.10 describes the listener types provided by Spring Batch.
The job listener intercepts job execution and supports the before and after job execution events.
These events add processing before a job and after a job according to the completion type of a batch process.
They’re particularly useful to notify external systems of batch failures.
You configure a listener in a job configuration with the listeners element as a child of the job element.
The listeners element can configure several listeners by referencing Spring beans.
Figure 3.7 Notifications of lifecycle events and errors during job execution.
A JobExecution instance provides information regarding job execution status with BatchStatus constants.
To specify which methods do the listening, you use the BeforeJob and AfterJob annotations.
This listener class is a plain old Java object (POJO) and defines which methods to execute before the job starts with the BeforeJob annotation and after the job ends with the AfterJob annotation.
Steps also have a matching set of listeners to track processing during step execution.
You use step listeners to track item processing and to define error-handling logic.
StepListener acts as a parent to all step domain listeners.
Table 3.11 describes the step listeners provided by Spring Batch.
They’re respectively associated to the step and chunk and provide methods for before and after events.
The afterStep method triggered after step completion must return the status of the current step with an ExistStatus instance.
The ChunkListener interface provides methods called before and after the current chunk.
These methods have no parameter and return void, as described in the following snippet:
ItemReadListener Called before and after an item is read and when an exception occurs reading an item.
ItemWriteListener Called before and after an item is written and when an exception occurs writing an item.
SkipListener Called when a skip occurs while reading, processing, or writing an item.
Each interface provides three methods triggered before, after, and on error.
Each interface accepts as a parameter a single item from a list of handled entities for before and after methods.
For error-handling methods, Spring Batch passes the thrown exception as a parameter.
For item processing, these methods are beforeProcess, afterProcess, and onProcessError.
For the ItemReadListener interface, these methods are beforeRead, afterRead, and onReadError, as shown in the following snippet:
For the ItemWriteListener interface, these methods are beforeWrite, afterWrite, and onWriteError, as shown in the following snippet:
The last kind of interface in table 3.11 listens for skip events.
Spring Batch calls the SkipListener interface when processing skips an item.
The interface provides three methods corresponding to when the skip occurs: onSkipInRead during reading, onSkipInProcess during processing, and onSkipInWrite during writing.
You can also define listeners for all these events as annotated POJOs.
Spring Batch provides annotations corresponding to each method defined by the interfaces in table 3.11
Configuring listeners using annotations follows the same rules as the interface-based configurations described in the next section.
The following listing shows how to implement an annotation-based listener for step execution.
As is done for a job, configuring a step listener is done using a listeners element as a child of the tasklet element.
You can configure all kinds of step listeners at this level in the same manner.
Note that you can also specify several listeners at the same time.
Another type of listener provided by Spring Batch deals with robustness and provides notification when repeats and retries occur.
These listeners support the methods listed in table 3.12 and allow processing during repeat or retry.
The following snippet lists the RepeatListener interface called when repeating an item:
The following snippet lists the content of the RetryListener interface called when retrying an item:
Such listeners must be configured like step listeners using the listeners child element of the tasklet element, as described at the end of the previous section.
The next and last feature in our advanced configuration discussion is the Spring Batch inheritance feature used to modularize entity configurations.
As emphasized in section 3.2.2, Spring Batch XML provides facilities to ease configuration of batch jobs.
While this XML vocabulary improves Spring Batch configuration, duplication can remain, and that’s why the vocabulary supports configuration inheritance like Spring XML.
This feature is particularly useful when configuring similar jobs and steps.
Rather than duplicating XML fragments, Spring Batch allows you to define abstract entities to modularize configuration data.
In the online store case study, you define several jobs with their own steps.
As a best practice, you want to apply default values of the batch processes.
The default configuration parameters then apply to all child jobs and steps.
Modifying one parameter affects all children automatically, as shown in figure 3.8
You can use Spring Batch configuration inheritance at both the job and step levels.
Table 3.13 describes the abstract and parent configuration inheritance attributes.
As we’ve seen, configuration inheritance in Spring Batch is inspired by Spring and is based on the abstract and parent attributes.
Figure 3.8 Using configuration inheritance lets jobs inherit from an abstract job and steps inherit from an abstract step.
Configuration inheritance in Spring Configuration inheritance is a built-in feature of the default Spring XML vocabulary since version 2.0
Its aim is to allow modularizing configuration and to prevent configuration duplication.
It targets the same issue that Spring XML addresses: making configuration less complex.
Spring allows defining abstract bean definitions that don’t correspond to instances.
Such bean definitions are useful to modularize common bean properties and avoid duplication in Spring configurations.
Spring provides the abstract and parent attributes on the bean element.
A bean with the attribute abstract set to true defines a virtual bean, and using the parent attribute allows linking two beans in a parentchild relationship.
Inheriting from a parent bean means the child bean can use all attributes and properties from the parent bean.
The following fragment describes how to use the abstract and parent attributes:
Advanced configuration topics to define abstract batch entities that aren’t instantiated but are present in the configuration only to modularize other configuration elements.
A common use case is to define a parent step that modularizes common and default step parameters.
The following listing shows how to use configuration inheritance to configure step elements.
The parent step named parentStep includes a tasklet element, which also includes a chunk element.
You name the second step productStep and reference the previous step as its parent.
The productStep step has the same element hierarchy as its parent, which includes all elements and attributes.
In some cases, a parent defines attributes but children don’t, so Spring Batch adds the attributes to the child configurations.
In other cases, attributes are present in both parent and child steps, and the values of child elements override the values of parent elements.
An interesting feature of Spring Batch related to configuration inheritance is the ability to merge lists.
By default, Spring Batch doesn’t enable this feature; lists in the child element override lists in the parent element.
You can change this behavior by setting the merge attribute to true.
The following listing combines the list of listeners present in a parent job with the list in the child job.
The child element has all properties of its parent and can override them.
We specify the merge attribute B for the listeners element in the configuration of the child job.
In this case, Spring Batch merges the listeners of the parent and child jobs, and the resulting list contains listeners named globalListener and specificListener.
With configuration inheritance in Spring Batch, we close our advanced configuration section.
These features allow easier and more concise configurations of Spring Batch jobs.
Spring Batch provides facilities to ease configuration of batch processes.
This vocabulary can configure all batch entities described in chapter 2, such as jobs, tasklets, and chunks.
Spring Batch supports entity hierarchies in its XML configuration and closely interacts with Spring XML to use Spring beans.
All these features contribute to making batches more robust by providing support for commit intervals, skip handling, and retry, among other tasks.
Spring Batch provides support to configure access to the job repository used to store job execution data.
This feature relates to the more general topic of batch monitoring, and chapter 12 addresses it in detail.
Spring Batch XML also includes advanced features that make configuration flexible and convenient.
Features include the step scope and the ability to interact with the batch execution context using SpEL late binding of parameter values at runtime.
You can implement job, step, and chunk listeners with interfaces or by using annotations on POJOs.
Finally, we saw that Spring Batch provides modularization at the configuration level with the ability to use inheritance to eliminate configuration duplication.
Chapter 4 focuses on execution and describes the different ways to launch batch processes in real systems.
You must be eager to get your jobs up and running.
Launching a Spring Batch job is easy because the framework provides a Java-based API for this purpose.
However, how you call this API is another matter and depends on your system.
Perhaps you’ll use something simple like the cron scheduler to launch a Java program.
Alternatively, you may want to trigger your jobs manually from a web application.
Either way, we have you covered because this chapter discusses both scenarios.
If you’ve been reading this book from page one, you know the basics of Spring.
It’s time to launch your Spring Batch job! You’re about to see that launching a Spring Batch job is quite simple thanks to the Spring Batch launcher API.
But how you end up launching your batch jobs depends on many parameters, so we provide you with basic concepts and some guidelines.
By the end of this section, you’ll know where to look in this chapter to set up a launching environment for your jobs.
Here’s a shortened version of this interface (we removed the exceptions for brevity):
The JobLauncher and the Job you pass to the run method are Spring beans.
The call site typically builds the JobParameters argument on the fly.
The following snippet shows how to use the job launcher to start a job execution with two parameters:
A job parameter consists of a key and a value.
Spring Batch supports four types for job parameters: string, long, double, and date.
You can view an execution as an attempt to run a batch process.
If the notions of job, job instance, and job execution aren’t clear to you, please refer to chapter 2, which covers these concepts.
Spring Batch provides an implementation of JobLauncher, whose only mandatory dependency is a job repository.
The following snippet shows how to declare a job launcher with a persistent job repository:
As you can guess, this object represents the execution coming out of the run method.
The JobExecution interface provides the API to query the status of an execution: if it’s running, if it has finished, or if it has failed.
Because batch processes are often quite long to execute, Spring Batch offers both synchronous and asynchronous ways to launch jobs.
By default, the JobLauncher run method is synchronous: the caller waits until the job execution ends (successfully or not)
Synchronous launching is good in some cases: if you write a Java main program that a system scheduler like cron launches periodically, you want to exit the program only when the execution ends.
But imagine that an HTTP request triggers the launching of a job.
Writing a web controller that uses the job launcher to start Spring Batch jobs on HTTP requests is a handy way to integrate with external triggering systems.
What happens if the launch is synchronous? The batch process executes in the calling thread, monopolizing web container resources.
Submit many batch processes in this way and they’ll use up all the threads of the web container, making it unable to process any other requests.
Figure 4.2 shows how launching behaves when the job launcher is asynchronous.
The client waits until the job execution ends (successfully or not) before the job launcher returns the corresponding job execution object.
Synchronous execution can be problematic, for example, when the client is a controller from a web application.
To make the job launcher asynchronous, just provide it with an appropriate TaskExecutor, as shown in the following snippet:
In this example, we use a task executor with a thread pool of size 10
The executor reuses threads from its pool to launch job executions asynchronously.
Note the use of the executor XML element from the task namespace.
It’s now time to guide you through the launching solutions that this chapter covers.
This chapter covers many solutions to launch your Spring Batch jobs, and you’re unlikely to use them all in one project.
Many factors can lead you to choose a specific launching solution: launching frequency, number of jobs to launch, nature of the triggering event, type of job, duration of the job execution, and so on.
A straightforward way to launch a Spring Batch job is to use the command line, which spawns a new Java Virtual Machine (JVM) process for the execution, as figure 4.3 illustrates.
The triggering event can be a system scheduler like cron or even a human operator who knows when to launch the job.
Figure 4.2 The job launcher can use a task executor to launch job executions asynchronously.
The task executor handles the threading strategy, and the client has immediate access to the job execution object.
You’ll see that Spring Batch provides a generic command-line launcher that you can use to launch any job from the command line.
If you choose the scheduler option, you should also look at section 4.3.1, which covers cron.
Spawning a JVM process for each execution can be costly, especially if it opens new connections to a database or creates object-relational mapping contexts.
Such initializations are resource intensive, and you probably don’t want the associated costs if your jobs run every minute.
Another option is to embed Spring Batch into a container such that your Spring Batch environment is ready to run at any time and there’s no need to set up Spring Batch for each job execution.
You can also choose to embed a Java-based scheduler to start your jobs.
A web container is a popular way to embed a Spring Batch environment.
If you want to learn how to deploy Spring Batch in a web application, read section 4.4.1
Java-based schedulers also run in Spring, so read section 4.3.2 to learn about Spring scheduling support.
You can also have a mix of solutions: use cron because it’s a popular solution in your company and embed Spring Batch in a web application because it avoids costly recurring initializations.
The challenge here is to give cron access to the Spring Batch environment.
Figure 4.3 You can launch a Spring Batch job as a plain JVM process.
The triggering system can be a scheduler or a human operator.
This solution is simple but implies initializing the batch environment for each run.
Figure 4.5 An external system submits a job request to the container where the Spring Batch environment is deployed.
An example is a cron scheduler submitting an HTTP request to a web controller.
The web controller would use the job launcher API to start the job execution.
Figure 4.4 You can embed Spring Batch in a container along with a Java scheduler.
A web container is a good candidate because Spring integrates easily in web applications.
The list of launching solutions this chapter covers is by no means exhaustive.
The Spring Batch launcher API is simple to use, so you can imagine building other types of solutions—for example, event-driven with JMS or remote with Java Management Extension (JMX)
Using the command line is perhaps the most common way to launch a batch process.
Triggering the process can be done manually (by a human), but most of the time you’ll be using a system scheduler (cron on UNIX systems, for example) to trigger the launch.
Why? Because batch processes are launched at specific times (at night, on the last Sunday of the month, and so on)
We cover schedulers later; in this section, we focus on how to launch a batch process through the command line.
Because Spring Batch is a Java-based framework, launching a Spring Batch process means spawning a new JVM process for a class and using the Spring Batch launcher API in that class’s main method.
The Spring Batch launcher API is straightforward; the JobLauncher has one method—run—that takes a Job and a JobParameters argument, so writing a main method to launch a job is quick and easy.
This launcher should remove any need for custom command-line launchers because of its flexibility.
The file used to start the Spring application context; the file configures the Spring Batch infrastructure, jobs, and necessary components (data source, readers, writers, and so forth)
Job The name of the job to execute (refers to a Spring bean name)
Job parameters The job parameters to pass to the job launcher.
Exit code mapping A strategy to map the executed job exit status to a system exit status.
There’s little chance that your jobs won’t need any job parameters, especially if the job instance identity is relevant (to benefit from Spring Batch’s restart features, for instance), so let’s see how to specify job parameters to the command line job runner.
The second step is to create a neat layout on the file system such that the JVM can locate all the necessary Java classes and resources on the classpath.
We assume a lib directory contains all of these JAR files.
We refer to this directory with the classpath argument of the java program.
But how to gather these JAR files? The easiest way is to use a dependency manager, like Maven.
If you’re using Maven, the mvn package command packages your project as a JAR file in the target directory of your project.
This command copies all the dependencies you need in the target/dependency directory.
You can then gather all your JAR files in a common directory (the snippets of this chapter use a lib directory) to launch the job from the command line.
Recall that the import products job needs two parameters: the location of the input file and the current date.
The following snippet shows how to specify those parameters from the command line:
The syntax is simple: you specify job parameters after the name of the job, using the name=value syntax.
Remember that a job parameter can have a data type in Spring Batch.
The way parameters are defined in the previous snippet creates String–typed parameters.
Let’s now launch our job by passing in the date parameter as a real Date object:
Table 4.2 lists the different types of job parameters along with examples.
This command-line launcher is handy because it allows you to specify a Spring configuration file, the name of the job you want to start, and job parameters (with some advanced type conversion)
Let’s now see an advanced feature of the runner that you use when you need to set the system exit code returned by the launcher.
Use this feature if you want to run a series of jobs and choose precisely which job should follow a previous job.
The triggering system (a system scheduler, for example) can use this exit code to decide what to do next (see the sidebar on the use of exit codes)
For example, after the execution of job A, you want to run either job B or job C.
The scheduler decides on the basis of the exit code returned by job A.
But if your batch system relies on exit codes to organize the sequencing of your jobs, you’ll learn here how Spring Batch lets you easily choose which exit code to return from a job execution.
What is the exit code for a Spring Batch job? A job’s exit code is a property of the job’s exit status, which is itself part of the job execution returned by the job launcher.
Spring Batch provides an ExitStatus class, which includes an exit code typed as a String.
Don’t confuse BatchStatus (an enumeration) and ExitStatus (a simple string)! These are different concepts, even if, in most cases, the exit status is directly determined from the batch status.
Chapter 10 provides in-depth coverage of the batch status and exit status.
For now, just remember that, by default, Spring Batch gets the exit status from the batch status (either COMPLETED or FAILED) and that you can override this default behavior if you want to return a specific exit status.
Used for errors from the command-line job runner—for example, the runner couldn’t find the job in the Spring application context.
What’s the deal with exit codes? A system process always returns an integer exit code when it terminates.
As previously mentioned, system schedulers commonly trigger batch processes launched from the command line, and these schedulers can be interested in the exit code of the batch process.
That’s why the Spring Batch command-line launcher provides advanced support to map job exit statuses (string) with system exit codes (integer)
Figure 4.6 The command-line job runner uses an exit code mapper to translate the string exit status of a Spring Batch job into an integer system exit code.
You can override the defaults listed in table 4.3 if they don’t suit your needs.
How do you do that? Write an implementation of the ExitCodeMapper interface and declare a Spring bean of the corresponding type in the job’s Spring application context.
Let’s look at an example to illustrate overriding the default exit code mapper.
Remember, the goal is to use the exit code returned by a job to decide what to do next.
Imagine that this job (call it job A) deals with importing items from a file into a database.
The system scheduler you’re using runs job A and behaves as follows depending on the exit code returned by job A:
Your job as the developer of job A is to return the correct exit code such that the system scheduler uses it to decide what to do next.
To do so, you write an implementation of ExitCodeMapper to handle the exit code strategy and install it in job A.
The following listing shows the implementation of an ExitCodeMapper that honors this contract.
Note that the exitCode argument of the intValue method comes from the ExitStatus object of the job, which has a getExitCode() method.
Implementing an exit code mapper is straightforward: you get a String and return a matching integer.
Listing 4.1 Writing an ExitCodeMapper to map job and system exit codes.
Let’s assume here that you configured your job correctly to receive the appropriate exit status if the job skipped some items.
Now that the exit code mapper is implemented, you must declare it in the Spring configuration, as shown in the following snippet:
You now know how to launch Spring Batch jobs from the command line.
When using the command line to launch a batch job, you need someone or something to trigger this command line.
There are many ways to trigger batch jobs, and job schedulers are great tools to trigger jobs at specific times or periodically.
A job scheduler is a program in charge of periodically launching other programs, in our case, batch processes.
Imagine that you have a time frame between 2 a.m.
How would you do that? You can implement a solution yourself using a programming language like Java, but this is time consuming and error prone, and system utilities probably aren’t the focus of your business.
Alternatively, job schedulers are perfect for this work: triggering a program at a specific time, periodically or not.
Our goal here is to use several job schedulers to launch Spring Batch jobs.
We picked popular, mainstream, and free job schedulers to provide you with guidelines for choosing one over another, depending on the context of your applications.
Before we dive into the descriptions of each solution, table 4.4 lists the job schedulers we cover and their main characteristics.
The cron program is the de facto job scheduler on UNIX-like systems.
Cron enables launching commands or shell scripts periodically, using cron expressions.
Configuring cron is simple: you set up commands to launch and when to launch them in the crontab file.
The systemwide crontab file is stored in the /etc/ directory.
Figure 4.7 shows the structure of a line of the crontab file.
The command can be anything; in our case, it can be something we covered in section 4.2
The following snippet shows an entry to launch a job with Spring Batch’s command-line job launcher with the acogoluegnes user:
From the preceding snippet, you should recognize the structure of a cron entry (cron expression, user, and command)
The command is long: it must set the classpath, the Java class to launch, the Spring configuration file to use, the name of the job to launch, and the job parameters.
You can use any command in a crontab entry: Spring Batch’s command-line launcher or any other command to launch a job process.
Next is choosing when to trigger the command, which is where you use a cron expression.
If you’re new to cron, the start of the entry in the previous snippet must be puzzling: this is a cron expression, which says to launch the job every day at 4 a.m.
Cron expressions are to scheduling what regular expressions are to string matching.
Depending on your background with regular expressions, this assertion can be appealing or scary!
Table 4.4 Overview of the job schedulers covered in this section.
Cron A job scheduler available on UNIX-like systems; uses cron expressions to periodically launch commands or shell scripts.
Spring scheduler The Spring framework scheduler; configurable with XML or annotations, it supports cron expressions; available in Spring 3.0 and later.
Now that you know how to trigger Spring Batch jobs periodically with cron, let’s see some recommendations about the use of cron.
Is cron suited to launch your Spring Batch job? Remember, cron is a system scheduler: it spawns a new JVM process for each Spring Batch command-line launcher.
Imagine that you need to launch a job every night.
Cron triggers the command-line launcher, which creates a Spring application context before launching the job itself.
But imagine now that you need to launch another job that scans a directory for new files to import.
You set up cron to trigger this job every minute.
You can’t easily achieve this from the command line (hence with cron), but a Java scheduler like the Spring scheduler will do the trick.
Let’s now look at the second scheduling option from table 4.4: the Spring scheduler.
Do you want to schedule a job with a simple-to-deploy and yet powerful solution? Good news: Spring includes such a feature.
As of version 3.0, the Spring Framework offers a declarative way to schedule jobs without requiring extra dependencies for your Spring Batch jobs, because Spring Batch sits on top of Spring.
Spring’s lightweight scheduling provides features like cron expressions, customization of threading policy, and declarative configuration with XML or annotations.
The Spring scheduler needs a running Spring application context to work, so you typically embed it in a web application, but you can use any other managed environment, like an Open Services Gateway initiative (OSGi) container.
We cover how to embed Spring Batch in a web application in section 4.4.1
This is where you decide whether or not to use a thread pool.
This setup is optional, and Spring uses a single-threaded scheduler by default.
You can use XML or annotations on the target methods.
In our case, those methods use the Spring Batch API to launch jobs.
The next sections cover these steps, but let’s first see what kind of scheduling configuration Spring supports.
The fixed-rate and fixed-delay options are the simple options, depending on whether you want to launch job executions independently (fixed rate) or depending on the completion time of the previous execution (fixed delay)
The next sections show you the use of the fixed-rate option with both XML and annotations; remember that you can use the attributes in table 4.5 for fixed rate or cron.
You can declare this bean using the task namespace prefix:
Spring uses the default single-threaded scheduler as soon as you declare scheduled tasks.
Even though Spring uses reasonable defaults, declaring a scheduler explicitly is good practice because it reminds you that an infrastructure bean takes care of the actual scheduling.
It also serves as a reminder that you can tweak this scheduler to use a thread pool:
Fixed rate fixed-rate fixedRate Launches periodically, using the start time of the previous task to measure the interval.
Fixed delay fixed-delay fixedDelay Launches periodically, using the completion time of the previous task to measure the interval.
Multiple threads are useful when you need to schedule multiple jobs and their launch times overlap.
You don’t want some jobs to wait because the single thread of your scheduler is busy launching another job.
Now that the scheduler’s ready, let’s schedule a job using XML.
Imagine you have the following Java code that launches your Spring Batch job, and you want Spring to execute this code periodically:
It also elides the creation of job parameters, as job parameters are job specific.
Most of the time, you’ll be using a timestamp or a sequence to change the job identity for each run.
Finally, exception handling is up to you: here, the launch method just propagates any exception that the job launcher throws.
You now need to tell Spring to call this code periodically.
Fortunately, you inherit from all of Spring’s configuration features: dependency injection and the task namespace to configure the scheduling.
You first declare the bean that launches the Spring Batch job.
For each task you schedule, you use the task:scheduled element and refer to the bean and the method to call, using the ref and method attributes, respectively.
This listing uses a fixed rate, but remember that you can also schedule with a fixed delay or a cron expression.
This allows switching the scheduling configuration between the development and production environments, for example.
The XML configuration is external to your code: when you look at your Java code, you have no idea a scheduler launches it periodically.
If you change the name of the Java method to launch periodically, you need to reflect this change in the XML configuration.
If you want the scheduling configuration to be closer to your code than in a separate XML file, then annotations are the way to go.
Spring lets you schedule your jobs by annotating Java methods.
The following snippet shows how to schedule a job with the Spring @Scheduled annotation:
When using the @Scheduled annotation, the Java class does part of the configuration itself.
The annotation solution is less flexible, though: the scheduling configuration is hardcoded and it works only on that code (you can’t annotate code you don’t control)
This ends the coverage of schedulers used to launch Spring Batch jobs.
You can use a system scheduler like cron to launch Spring Batch jobs, which spawns a plain Java process for each job.
But cron isn’t suited for all cases, especially if bootstrapping the Spring application context is resource intensive and the job is triggered every second, for example.
In such cases, use a Java-based scheduler, like Spring scheduler.
Remember, when using a Java scheduler, you already have a Spring Batch environment ready; you don’t need to spawn a new JVM process for every job (as you do with cron, for example)
You now have everything ready, assuming you found a container to run your application.
A popular way to embed a Spring Batch environment and a scheduler is to use a web application.
In the next section, we see how to embed Spring Batch in a web application.
Spring Batch is a lightweight framework that can live in a simple Spring application context.
Here, we look at configuring a Spring Batch environment in a web application.
This makes Spring Batch available at any time; there’s no need to spawn a dedicated Java process to launch a job.
We can also embed a Java scheduler in the same web application.
What about Quartz? Quartz is a Java-based job scheduler that you can integrate in any Java environment (standalone or Java Enterprise Edition)
We chose not to cover Quartz here because you can do pretty much the same thing with the built-in scheduling support in Spring.
You can refer to Spring reference documentation for more information.
Figure 4.8 illustrates that a Spring application context can be contained in a web application.
Note that the job beans can also use any available services, like data sources, data access objects, and business services.
Hosting Spring Batch in a web application is convenient, but what about pushing this architecture further and triggering jobs through HTTP requests? This is useful when an external system triggers jobs and that system cannot easily communicate with the Spring Batch environment.
But before we study how to use HTTP to trigger jobs, let’s see how to configure Spring Batch in a web application.
The application context is called the root application context of the web application.
You configure the servlet listener in the web.xml file of the web application, as shown in the following listing.
This file should contain the configuration of the Spring Batch infrastructure, the jobs, the scheduler (if any), and application services.
A best practice is to split up this configuration into multiple files.
Contains Spring Batch’s infrastructure and jobs, a Javabased scheduler, and Spring beans for the web application.
Figure 4.8 A web application can contain a Spring application context.
This Spring application context can host Spring Batch’s infrastructure (job launcher, job repository) and jobs.
The context can also host a Java-based scheduler (like Spring scheduler or Quartz) and any Spring beans related to the web application (data access objects, business services)
Launching from a web application and encourages reuse of configuration files.
Should you redefine all your jobs for integration testing? No, so define the jobs in a dedicated file and import this file from a master Spring file.
If you follow the configuration of the previous snippet, the structure of the web application on disk should be as follows:
In others, the triggering event doesn’t come from an embedded scheduler but from an external system.
Next, we use HTTP to let this external system get access to our Spring Batch environment.
Imagine that you deployed your Spring Batch environment in a web application, but a system scheduler is in charge of triggering your Spring Batch jobs.
A system scheduler like cron is easy to configure, and that might be what your administration team prefers to use.
But how can cron get access to Spring Batch, which is now in a web application? You can use a command that performs an HTTP request and schedule that.
Figure 4.9 Once Spring Batch is in a web application, you can use an embedded Java scheduler such as Spring scheduler to launch jobs periodically.
Figure 4.10 illustrates launching a Spring Batch job with an HTTP request.
To implement this architecture, you need a web controller that analyzes the HTTP.
We use Spring MVC to do that, but we could have used any other web framework.
We chose Spring MVC because it’s part of the Spring Framework, so it’s free to our Spring Batch application.
Spring MVC is part of the Spring Framework and provides a simple yet powerful way to write web applications or Representational State Transfer (REST) web services.
In Spring MVC, controllers are plain Java classes with some annotations.
Figure 4.10 Once Spring Batch is in a web application, you can add a web layer to launch Spring Batch jobs on incoming HTTP requests.
This solution is convenient when the triggering system is external to Spring Batch (like cron)
The @RequestMapping annotation tells Spring MVC which URL and which HTTP operation to bind to the launch method.
With the @RequestParam annotation on the job parameter B, you tell Spring MVC to pass the value of the job HTTP parameter to the method.
As you probably guessed, this parameter is the name of the job you want to launch.
At C, you extract HTTP parameters and convert them to job parameters.
At D, you use the job launcher to launch the job.
Finally, you may have noticed the jobRegistry property in the web controller in listing 4.4
The JobRegistry is a Spring Batch interface used to look up Job beans.
This is exactly what the launching controller does: from the job name passed in the request, it retrieves the corresponding Job bean.
You need to declare the job registry in the Spring application context, typically where you declare the Spring Batch infrastructure.
At the heart of Spring MVC is a servlet class, DispatcherServlet, which you declare in the web.xml file of your web application, as shown in the following listing.
A Spring MVC servlet creates its own Spring application context.
In this case, you create an sbia-servlet.xml file in the WEB-INF directory of the web application.
You must declare the web controller in this file, as shown in the following snippet:
The Spring application context of the Spring MVC servlet can see the beans from the root application context because they share a parentchild relationship, as figure 4.11 shows.
You can now launch your Spring Batch jobs with a simple HTTP request! You should use this launching mechanism when an external system triggers your jobs and that system doesn’t have direct access to your Spring Batch environment.
Otherwise, you can just deploy your Spring Batch environment in a web application and use an embedded Java-based scheduler to trigger your jobs.
Remember, you can use Spring Batch wherever you can use the Spring Framework, and web applications are no exception.
We covered a lot of information on triggering and launching Spring Batch jobs.
By now, you should know which solution to adopt for your batch system.
Next, we learn how to stop all of these jobs.
We started many jobs in this chapter, but how do we stop them? Stopping a job is unfortunate because it means that something went wrong.
If everything is okay, a job execution ends by itself without any external intervention.
When it comes to stopping job executions, we distinguish two points of view.
The operator monitors batch processes but doesn’t know much about Spring Batch.
When something goes wrong, the operator receives an alert and stops a job execution, by using a JMX console, for example.
Figure 4.11 The web controller is defined in the servlet’s application context.
The root application context defines the job registry and the job launcher.
Because the two application contexts share a parent-child relationship, you can inject beans from the root application context into the web controller.
The developer writes Spring Batch jobs and knows that under certain circumstances, a job should be stopped.
Spring Batch provides techniques to stop a job for both the operator and the developer.
Spring Batch provides the JobOperator interface to perform such an operation.
The following snippet shows how to stop a job execution through a JobOperator:
We focus here on the way to use JobOperator for stopping job executions.
The steps are simple: the job operator returns the identifiers of the running job executions for a given job name.
You then ask the job operator to send a stop message to an execution using an execution ID.
Another way to call job operator methods is to provide a user interface in your application that lets an administrator stop any job execution.
You can create this user interface yourself, or you can use Spring Batch Admin, the web administration application introduced in chapter 2
Now that you know how to use the job operator, let’s see how to configure it.
The job operator isn’t automatically available; you need to declare it in your Spring configuration.
The following listing shows the Spring configuration required to declare the job operator.
Figure 4.12 You can expose the job operator bean to JMX and then call its methods remotely from a JMX client like JConsole.
An operator can learn about the Spring Batch runtime and stop or restart jobs.
The job operator has four dependencies: the job repository, job launcher, job registry, and job explorer.
By now, you’re used to seeing the job repository and the job launcher, as they’re essential parts of the Spring Batch infrastructure.
You need to declare the job registry and the job explorer only for specific tasks, and configuring the job operator is one.
As a bonus, the following configuration exposes the job operator to JMX.
You can now explain to your administration team how to stop a job execution.
But a member of the administration team might tell you that a job execution doesn’t stop.
The next subsection explains what happens when you request to stop a job execution.
When we showed the job operator in action, you may have found this line intriguing:
The job operator returns a Boolean when you request to stop a job execution.
This Boolean value tells you whether the stop message was sent successfully.
A stop message? When you call the stop method on a job operator, there’s no guarantee that the execution immediately stops after the call.
Why? In Java, you can’t stop code from executing immediately.
When does job execution stop after you request it? Let’s imagine some business code is executing when you send the stop message.
If the code detects the thread interruption, it can choose to end processing by throwing an exception or returning immediately.
As soon as the business code finishes and Spring Batch gets control again, the framework stops the job execution.
This means that the execution will stop only after the code finishes.
If the code is in the middle of a long processing sequence, the execution can take a long time to stop.
Stopping in the middle of a chunk-oriented step shouldn’t be a problem: Spring Batch drives all the processing in this case, so the execution should stop quickly (unless some custom reader, processor, or writer takes a long time to execute)
But if you write a custom tasklet whose processing is long, you should consider checking for thread interruption.
Understanding the stop message is a first step toward the developer’s point of view, so let’s now see how to stop a job execution from application code.
We saw that an administrator can use the job operator to stop a job execution, but sometimes stopping the execution from within the job itself is necessary.
Imagine you’re indexing your product catalog with a Spring Batch job.
The online store application can work with some unindexed products, but the job execution shouldn’t overlap with periods of high activity, so it shouldn’t run after 8 a.m.
You can check the time in various places in the job and decide to stop the execution after 8 a.m.
The first way to stop execution is to throw an exception.
This works all the time, unless you configured the job to skip some exceptions in a chunk-oriented step!
The second and preferred way to stop execution is to set a stop flag in the step execution object.
As soon as Spring Batch gets control of the processing, it stops the job execution.
The next topic to cover is how to get access to the StepExecution object from a job.
Getting access to the StepExecution depends on whether you’re working directly with a tasklet or in a chunk-oriented step.
A tasklet has direct access to the StepExecution through the step context, itself in the chunk context.
The following listing shows a tasklet that processes items, checks a stop condition, and sets the stop flag accordingly.
The stop condition could be any business decision, such as the time restriction mentioned previously.
Setting the stop flag in a tasklet is straightforward; let’s now see how to do this in a chunk-oriented step.
Remember how a chunk-oriented step works: Spring Batch drives the flow and lets you plug in your business logic or reuse off-the-shelf components to read, process, or write items.
If you look at the ItemReader, ItemProcessor, and ItemWriter interfaces, you won’t see a StepExecution.
You access the StepExecution to stop the execution using listeners.
These components should focus on their processing to enforce separation of concerns.
Chapter 3 covers the configuration of listeners, but we give you enough background here to use them for stopping jobs.
The idea of a listener is to react to the lifecycle events of a step.
You register a listener on a step by using annotations or implementing interfaces, and Spring Batch calls corresponding methods throughout the lifecycle of that step.
What lifecycle events can you listen for? A lot of them: step start; after each read, processed, or written item; step end, and so on.
The following listing shows a listener that keeps a reference to the StepExecution and checks a stopping condition after each read item.
Listing 4.8 An annotated listener to stop a job execution.
The real work is to implement the stopping condition, which is a business decision (the body of the stopConditionsMet method in our example)
The following listing shows how to register the listener on the chunk-oriented step.
This concludes the coverage of stopping a Spring Batch job.
You saw how to stop a job execution from the operator’s point of view.
You configure a job operator bean that you can expose to JMX and call the appropriate sequence of methods to stop a specific job execution.
Don’t forget that stopping an execution is only a request message: your code must be aware of this message if you want the execution to stop quickly.
As soon as Spring Batch gets control of the processing, it does its best to stop the execution gracefully.
Finally, remember that you can choose to stop the execution from within your business code.
Combining scheduling and stopping jobs Scheduling isn’t only for starting jobs; you can also schedule stopping your jobs.
If a job runs at night but must stop at 6 a.m., you can schedule a task to send a stop signal.
By doing so, you won’t embed the stop logic in your job.
Launching Spring Batch jobs is easy, and we covered the most common scenarios you’re likely to encounter in batch systems.
You can also choose to embed Spring Batch in a web application combined with a Java scheduler.
The generic command-line launcher-plus-cron solution is good for jobs that don’t run with a high frequency.
For example, you shouldn’t use this solution when the batch environment initialization is costly and the batch job runs every 30 seconds.
If you want your batch environment ready all the time, embed your Spring Batch environment in a web application.
Once your batch environment is in a web application, also embed a Java scheduler to start your jobs.
If the triggering event comes from an external system that doesn’t have direct access to Spring Batch, use an HTTP request to trigger the execution.
You should take this message into account in your code, but you can also count on Spring Batch to stop gracefully when it retakes control of the flow.
It’s now time to go back to the heart of Spring Batch: chunk-oriented processing.
The next three chapters cover the three corresponding phases of chunk processing: reading, writing, and processing.
In the previous two chapters, we concentrated on configuring and launching batch jobs.
It’s now time to dig into the features at the heart of batch processes.
As described in chapter 2, Spring Batch provides types for batch processes based on the concepts of job and step.
Chunk-oriented processing allows jobs to implement efficiently the most common batch processing tasks: reading, processing, and writing.
We focus here on the first step of this process, reading.
We describe general concepts and types implemented by Spring Batch.
These built-in types are the foundation used to support the most common use cases.
Spring Batch can use different data sources as input to batch processes.
In some cases, the Spring Batch built-in implementations aren’t enough, and it’s necessary to create custom implementations.
Because Spring Batch is open source, implementing and extending core types for reading is easily achievable.
Another thing to keep in mind is that reading data is part of the general processing performed by the chunk tasklet.
That’s why built-in implementations implicitly provide integration with the execution context to store current state.
The stored data is particularly useful to handle errors and restart batch processes.
We concentrate here on the data-reading capabilities of Spring Batch and leave chapter 8 to cover in detail these other aspects.
We use our case study to describe concrete use cases taken from the real world.
We explain how to import product data from different kinds of input, with different formats, and how to create data objects.
In this section, we introduce key concepts and types related to reading data in Spring Batch.
These concepts are the foundation for the Spring Batch reading feature.
This feature and its related types operate within the chunk tasklet, as illustrated in figure 5.1
This chapter focuses on the first part of chunk processing.
At this level, the first key type is the ItemReader interface that provides a contract for reading data.
This interface supports generics and contains a read method that returns the next element read:
If you’ve toured the Spring Batch documentation, you’ve noticed that readers implement another key interface: ItemStream.
The ItemStream interface is important because it allows interaction with the execution context of the batch process to store and restore state.
In this chapter, we concentrate on the ItemReader, and we discuss state management in chapter 8
At this point, it’s only necessary to know that readers can save state to properly handle errors and restart.
Reads Writes Figure 5.1 A chunk tasklet reads, processes, and writes data.
The following snippet describes the content of the ItemStream interface.
The open and close methods open and close the stream.
The update method allows updating the state of the batch process:
You can create your own implementations of the ItemReader and ItemStream interfaces, but Spring Batch provides implementations for common data sources to batch processes.
Throughout this chapter, we use our case study as the background story and describe how to import product data into the online store using different data sources.
The first data source we describe to input data in batch processes is files.
A file contains a set of data to integrate into an information system.
Each type of file has its own syntax and data structure.
Each structure in the file identifies a different data element.
To configure a file type in Spring Batch, we must define its format.
Figure 5.2 illustrates the batch process inputs in our case study.
Look at each box in figure 5.2 and see how Spring Batch handles that format.
Flat files are pure data files and contain little or no metadata information.
Some flat file formats, such as comma-separate value (CSV), may contain one header line as the first line that names columns.
In general, though, the file provider defines the file format.
This information can consist of field lengths or correspond to a separator splitting data fields.
Configuring Spring Batch to handle flat files corresponds to defining the file format to map file records to data objects.
The item reader for flat files is responsible for identifying records in the file and then creating data objects from these records, as shown in figure 5.3
Figure 5.2 The supported file formats in the case study are separator-based text, fixed length-based text, and JSON.
The ItemReader implementation for flat files is the FlatFileItemReader class.
Several other types work in conjunction with the FlatFileItemReader to identify data fields from file lines and to create data objects, as pictured in figure 5.4
The LineMapper interface is responsible for extracting data from lines.
The DefaultLineMapper class is the default and most commonly used implementation of the LineMapper interface.
Two additional interfaces related to the DefaultLineMapper class come into play.
The DefaultLineMapper class holds a LineTokenizer responsible for splitting data lines into tokens and a FieldSetMapper to create data objects from tokens.
The item reader first identifies records, and then creates data objects.
Figure 5.4 Classes and interfaces involved in reading and parsing flat files.
These interfaces are all involved when configuring the FlatFileItemReader class for a file format.
You’ll use these interfaces and their corresponding implementations to handle various file formats in this chapter.
This section on flat files introduced all concepts and types related to the item reader for flat files to import data files as objects.
In the next section, we describe the general configuration of a FlatFileItemReader bean in Spring Batch as well as implementations for record-separator policies and line mappers.
We then explain how to handle delimited, fixed-length, and JSON file formats and describe advanced concepts to support records split over several lines and heterogonous records.
The FlatFileItemReader class is configured as a Spring bean and accepts the properties described in table 5.2
Table 5.1 Interfaces for flat file processing with the FlatFileItemReader class.
FieldSetMapper Creates a data object from tokens; invoked by the DefaultLineMapper class, the default implementation of the LineMapper interface.
LineTokenizer Splits a data line into tokens; invoked by the DefaultLineMapper class, the default implementation of the LineMapper interface.
When a line begins with one of these prefixes, Spring Batch ignores that line.
You use this type of configuration for the online store use case with flat files.
You can use standard Spring facilities to locate the resource.
Specifying the value 1 for the linesToSkip property means that the reader doesn’t consider the first line as a data line and that the line will be skipped.
In the context of the use case, this line corresponds to the file header describing the record fields.
Finally, the code specifies how to create a product object from a data record using the lineMapper property.
To lighten the listing, we elided the beans corresponding to the two last entities but we detail them next.
Implementations can support continuation markers and unbalanced quotes at line ends.
Spring Batch provides several implementations of this interface, described in table 5.3
Configuring record separation policy classes can be simple because their default constructors cover the most common cases.
Another important interface present as a property of the FlatFileItemReader class is the LineMapper interface.
Spring Batch provides several implementations of this interface for different use cases and file formats, as described in table 5.4
The configuration of this class is shown in the following XML fragment:
DefaultLineMapper The default implementation tokenizes lines and maps items to objects.
JsonLineMapper Supports the JSON format for records and extracts data to a map for each record.
For each line type, a line tokenizer and a field-set mapper must be configured.
JavaScript Object Notation (JSON) JSON is an open and lightweight text-based standard designed for human-readable data interchange.
It provides a way to structure text data using braces and brackets.
This technology is similar to XML but requires about 30% fewer characters.
The JSON format is commonly associated with JavaScript because the language uses it to perform I/O for data structures and objects.
The JSON format is language independent, but you usually see it used in Asynchronous JavaScript + XML (AJAX)-styled web applications.
The DefaultLineMapper class is the most commonly used implementation because it handles files with implicit structures using separators or fixed-length fields.
In our use case, we accept several data formats for incoming data.
We describe next how to configure Spring Batch to handle data structures based on separators and fixedlength fields.
The most commonly used implementation of the LineMapper interface is the DefaultLineMapper class.
Parses a line to extract fields using an implementation of the LineTokenizer interface.
Creates data objects from fields using an implementation of the FieldSetMapper interface.
Figure 5.5 illustrates how the LineTokenizer and FieldSetMapper interfaces described in the preceding list interact within the DefaultLineMapper.
The lineTokenizer and fieldSetMapper properties configure the DefaultLineMapper class’s LineTokenizer and FieldSetMapper.
This example uses bean references (using the ref attribute), but you could also use an inner bean.
The lineTokenizer property is set to an instance of LineTokenizer.
The fieldSetMapper property is set to an instance of FieldSetMapper.
The LineTokenizer parses record lines, and the FieldSetMapper creates objects from field sets.
It’s now time to use DefaultLineMapper in our case study.
The FieldSetMapper implementation remains the same; it creates objects from a FieldSet.
The LineTokenizer implementation depends on the record format and provides the contract to create a field set from lines in a data file, as defined here:
A FieldSet is returned by the tokenize method, which parses the record.
A FieldSet contains all extracted record fields and offers several methods to manage them.
Table 5.5 lists the Spring Batch implementations of the LineTokenizer interface for different ways to delimit fields in data lines.
The first example contains product records that use the comma character to separate fields, as shown in the following example:
The names property defines the names of the fields, and the delimiter property defines the field delimiter; for example:
The delimiter property specifies the character used to separate fields in a data file— in this case, the comma: ","
When the tokenizer extracts a field from the data line, the corresponding name is associated with it so it’s possible to get the field value by name.
Another supported data structure in the case study is fixed-length fields.
Note that such a data structure is potentially larger because all fields must have the same length.
For this example, table 5.6 describes the lengths of each field.
The properties respectively define the names of fields and the column ranges used to identify them:
This property accepts a list of Range instances, but you can also configure it using a comma-separated string of ranges.
Now that you’ve configured extracting fields from data lines, it’s time to specify how to create data objects from these fields.
This process isn’t specific to a particular LineMapper but relates to the field set structure.
The mapFieldSet method implements the mapping where a LineTokenizer implementation has created the FieldSet instance.
Table 5.7 lists the Spring Batch implementations of this interface for different ways to handle fields and create data objects.
Before using a FieldSetMapper, you must implement the bean to receive the data.
In the case study, as you import product data, the bean is a plain old Java object (POJO) that contains the id, name, description, and price properties, as shown in the following listing.
Useful if you need to work directly with the field set.
Spring built-in PropertyEditor support Spring provides support to configure properties as strings.
The PropertyEditor interface describes how to convert strings to beans, and vice versa.
For nonstring type properties, Spring automatically tries to convert to the appropriate type using the registered property editors.
They come from the JavaBean specification and can be seen as a to/from string conversion service.
Spring Batch uses this mechanism to register its own property editors to make some types easier to configure.
To map the FieldSet instance to the Product data bean, you implement a FieldSetMapper to populate Product instances from a FieldSet instance.
The following listing shows the FieldSetMapper implementation that creates Product instances.
In the mapFieldSet method implementation, an uninitialized instance of a Product is first created.
The mapFieldSet method then uses the FieldSet instance and its various read methods.
The FieldSet class provides multiple read methods, one for each primitive Java type, plus String, Date, and BigDecimal.
Each read method takes a field name or field index as a parameter; some methods also provide an argument for a default value.
This class makes working with field sets easier because you don’t have to write a custom FieldSetMapper.
You specify a data template for the bean using the prototypeBeanName property, where the value is the bean name for this template.
You must configure the corresponding bean with the prototype scope.
When a Product is instantiated, its properties are set using field set data.
The bean property names and field names must match exactly for the mapping to take place.
Another interesting data format that Spring Batch supports is JSON.
In the next section, we describe how to implement and configure processing to support JSONformatted data in our case study.
The following listing shows the JSON content of a data file corresponding to the data presented in the previous section.
This is the last format for flat files in our case study.
Configuring the JsonLineMapper class is simple because line parsing is built into the class, and each FieldSet maps to a java.util.Map.
No additional types are required to configure the class, as shown in the following XML fragment:
Using a JsonLineMapper, you get a list of Map instances containing all data from the JSON structure.
If you were to convert this processing to code, you’d have a listing similar to the following.
Using the JsonLineMapper class is convenient to get data as Map objects, but it’s perhaps not exactly what you need.
At this point in the case study, you want to support several input data formats homogenously.
For every type of format, data must come through as Product instances.
You need to create an additional class implementing the LineMapper interface to wrap a JsonLineMapper.
We’ve described all the supported formats for flat files in Spring Batch.
Before moving on to XML data input support in Spring Batch, we explain how to handle records spread over multiple lines and how to support several record types within the same data file.
When an input source spreads records over several lines, a custom implementation is required to specify the conditions that delimit records.
The first line provides general data such as product identifier and name, and the second line includes additional information like the description and price.
In this case, the implementation of the isEndOfRecord method needs to detect if the line starts with a product identifier.
The following listing implements this format and assumes that no unbalanced quotes and continuation markers are present.
To determine if the current line is the end of the product record, you check if the string contains three commas B because a valid product must have four properties separated by commas (if a valid product required five properties, getCommaCount would be set to check for four commas)
To close the topic of reading from flat files, the following section describes heterogonous record handling within the same file.
Records present in flat files may not always be uniform.
Each record still corresponds to one line, including support for unbalanced quotes and a continuation character, but can correspond to different data records.
In our case study, this corresponds to having several product types with different data in the same file.
The following file example contains mobile phone records as before and new book records:
In this data file example, lines beginning with PRM correspond to mobile phones (product-mobile), and lines beginning with PRB to books (product-book)
In this case, you use polymorphism to create a basic product class and subclasses for specific types of products, mobile phones and books, as illustrated in figure 5.6
Because the data file mixes different types of products, you must define rules to detect the product type for a given line.
The prefix of the product identifier is used here: an identifier beginning with PRM is a mobile phone, and one with PRB is a book.
The following listing describes how to configure the class as a bean to handle a multiproduct data file.
The first property, tokenizers B, registers all LineTokenizers in a map.
The map keys contain patterns that select a tokenizer for a given line.
The wildcard “*” can be used as a map key.
This section ends our description of flat file support in Spring Batch.
This support is powerful, flexible, and can handle varied data formats.
To complete our presentation of using files as input, we look at XML.
The main difference between XML and flat files is that Java provides support for XML, which Spring Batch can use.
The MobilePhoneProduct and BookProduct classes inherit from the Product class.
As opposed to flat files, the Java runtime provides supports for XML.
Java can process XML input using different techniques, including using a Streaming API for XML (StAX) parser.
StAX is a standard XML-processing API that streams XML data to your application.
StAX is particularly suitable to batch processing because streaming is a principal Spring Batch feature used to provide the best possible performance and memory consumption profile.
Using XML alone in Java applications (and object-oriented applications) has limitations because of a mismatch between the XML and object-oriented models.
To address this limitation and provide efficient conversion between XML and objects, the Spring framework includes the Object/XML Mapping framework (aka OXM or O/X Mapper)
Spring OXM provides generic components called marshaller and unmarshaller to convert, respectively, objects to XML, and vice versa, as shown in figure 5.7
In addition to the Marshaller and Unmarshaller interfaces, Spring OXM supports the object-to-XML mapping libraries listed in table 5.8
Before diving into Spring Batch support for XML files, let’s describe the XML vocabulary used for products in our case study.
The following listing shows the contents of the file after conversion, as described in the section 5.2.3
This file format is the last supported import format in our case study.
Batch performance and XML Not all XML parsing approaches are suitable for obtaining the best performance for batch processes.
For example, DOM (Document Object Model) loads all content in memory, and SAX (Simple API for XML) implements event-driven parsing.
These two approaches aren’t suitable and efficient in the context of batch processing because they don’t support streaming.
Each product corresponds to a product XML element B under the root products element.
Every product has four XML children elements for identifier, name, description, and price.
Because of its reliance on Spring OXM, it’s independent of a parser implementation.
Because the property is of type Resource, you can use Spring to load the resource.
Table 5.10 lists most common built-in implementations of the Resource interface.
The unmarshaller property points to the bean definition used to convert XML to objects.
This marshaller uses Castor through the Spring OXM class CastorMarshaller, which implements both the Marshaller and Unmarshaller interfaces.
Before tackling databases as input sources, we describe how to handle a file set with item readers.
This approach is particularly useful for handling files in a directory.
In our case study, this corresponds to product data files sent using FTP or Secure Copy (SCP) to an input directory.
Input can enter an application as a set of files, not only as a single file or resource.
For example, files can periodically arrive via FTP or SCP in a dedicated input directory.
In this case, the application doesn’t know in advance the exact filename, but the names will follow a pattern that you can express as a regular expression.
A dedicated multiresource reader accepts several resources as input and delegates processing to individual resource readers.
This class is powerful because it leverages Spring resource support to easily configure multiple resources with simple patterns from different sources such as the file system or class path.
A file expression defines which files the reader uses as input:
Figure 5.8 How Spring Batch reads data from multiple files.
A multiresource reader delegates to a resource reader that reads files from an input directory.
Reading from relational databases the item reader used to process data.
This item reader is configured as a bean in the Spring configuration and must be designed to handle resources.
Spring Batch provides broad, flexible, and extensible support for flat files and XML.
You can configure Spring Batch with the most common data file formats and integrate custom types.
Next, we look at another input source for batch processes, relational databases.
In this case, data to import come from database tables.
Spring Batch provides two approaches for batch processing to stream data from databases: JDBC and Object-Relational Mapping (ORM)
We first see how Spring Batch leverages JDBC to read data from relational databases.
JDBC, in principle at least, keeps things simple and independent from databases’ implementations.
But it doesn’t provide a complete solution to handle the specifics of each SQL dialect.
Spring uses JDBC but hides the JDBC plumbing and error-prone code and leaves the application to contain business-specific code.
Spring Batch bases its database support on the Spring JDBC layer and hides its use by managing request calls and transactions.
In a batch job, you need to configure how to set request parameters and handle results.
Spring Batch also bases its database support on the Spring RowMapper interface and JDBC PreparedStatement interface.
In the next sections, we look at different reading techniques based on JDBC and supported by Spring Batch.
If you face performance issues when dealing with multiple input files, Spring Batch has built-in support to parallelize processing.
Spring Batch can process each file on its own thread and calls this technique partitioning.
In this approach, Spring Batch leaves the responsibility of reading data to the JDBC ResultSet interface.
This interface is the object representation of a database cursor, which allows browsing result data of a SELECT statement.
In this case, the result set integrates mechanisms to stream data.
With this approach, Spring Batch executes only one request and retrieves result data progressively using JDBC with data batches, as shown in figure 5.10
Spring Batch relies on JDBC configuration and optimizations to perform efficiently.
The properties specify the data source to access the database (dataSource), the SQL SELECT statement to execute to get data (sql), and the class to map.
A databasespecific JDBC driver implements communication with the database system.
In our use case, the SQL SELECT returns all the products to import from the product table and uses the ProductRowMapper class to convert data in the ResultSet to instances of the Product class.
If false, the cursor operates on its own connection and won’t participate in any transactions started for the rest of the step processing.
Next, you set the RowMapper implementation for the product mapper to a bean reference.
Finally, you declare the product row mapper to use the ProductRowMapper class.
The following listing defines a RowMapper implementation called ProductRowMapper, which is used in all our JDBC examples.
The mapRow method B is a factory method that creates Product instances based on a given JDBC ResultSet and row number.
The property value is a class that works directly on the PreparedStatement instance managed internally by Spring to set the parameters.
In this listing, you get the subset of products for the names that start with “Samsung.”
You can specify the maximum number of rows to retrieve through the maxRows property.
The fetchSize property allows a reader to transparently retrieve data in fixed-sized groups.
The following XML fragment describes how to configure these two properties:
You don’t always define SQL statements (here, a SELECT) outside the database in a configuration file; instead, a stored procedure can define the SQL to execute.
This feature is supported by JDBC and Spring, and Spring Batch provides an ItemReader implementation for stored procedures using the cursor-based approach.
In our case study, we have a stored procedure called sp_product that returns a product result set.
By default, the procedureName property configures a stored procedure that returns a ResultSet instance.
When the SQL code executed in the database is a stored function call, the function property must be set to true.
If a ref cursor in an out parameter returns data, then the cursorRefPosition property must be set to the position of the cursor in the output parameter list, as described in the following XML fragment:
In this example, you define parameters for a stored procedure.
The procedure has one output parameter that corresponds to a cursor B.
The Spring Batch cursor-based technique relies on JDBC and leverages streaming results using JDBC’s own ResultSet.
This mechanism allows retrieving data in batches and is useful with large data sets, as is often the case in batch processing.
Spring Batch also allows you to control data set retrieval using data pages, which we see next.
Instead of leaving JDBC to manage the data retrieval, Spring Batch allows you to handle this process using paging.
Spring Batch dynamically builds requests to execute based on a sort key to delimit data for a page.
To retrieve each page, Spring Batch executes one request to retrieve the corresponding data.
Figure 5.11 Using JDBC batch-based fetching to provide input data to an ItemReader by pages with fixed size.
Choosing between cursor-based and page-based item readers Why does Spring Batch provide two ways to read from a database? The reason is that there’s no one-size-fits-all solution.
Cursor-based readers issue one query to the database and stream the data to avoid consuming too much memory.
Cursor-based readers rely on the cursor implementation of the database and of the JDBC driver.
Depending on your database engine and on the driver, cursor-based readers can work well.
Page-based readers work well with an appropriate page size (see the sidebar on page size)
The trick is to find the best page size for your use case.
With Spring Batch, switching from cursor- to page-based item readers is a matter of configuration and doesn’t affect your application code.
The generated SQL query limits the number of retrieved rows to the page size specified by the pageSize property.
Finally, you set the RowMapper that creates business domain objects from ResultSet objects.
For this example, the product RowMapper from listing 5.12 is reused.
In our use case, we always need to have a SQL query returning products for cursorbased data retrieval.
This query uses the product table and returns the id, name,
Choosing a page size There’s no definitive value for the page-size setting.
Remember, the point of paging is to avoid consuming too much memory, so large pages aren’t good.
The good news is that the page size is a parameter in Spring Batch, so you can test multiple values and see which works best for your job.
The returned class is then responsible for generating SQL paging queries.
The query pattern is as follows: the first query is simple, using.
If you omit this property, the value is determined directly using the database through the specified data source.
Note that it’s unnecessary to specify this field if the database type is set.
The Spring FactoryBean classes When using a constructor (with the new keyword) to create beans doesn’t fit your needs, use a Spring FactoryBean.
A FactoryBean provides a level of indirection from the target bean by acting as a factory.
After the Spring BeanFactory (be careful, the names are similar) initializes the factory bean, the target bean instances are obtained through the getObject method.
For the next pages, queries include additional clauses to specify the beginning of the page using the specified sort key:
As we’ve seen in this section, Spring Batch provides sophisticated integration with JDBC to support batch processes.
As for nonbatch Java applications, we must explicitly define SQL queries to execute.
Next, we see the solutions Spring Batch provides for using ORM with batch applications.
In traditional Java and Java EE applications, ORM is commonly used to interact with relational databases.
The goal of ORM is to handle the mismatch between the relational database model and the object-oriented model.
As it does for JDBC, Spring provides supports for ORM.
We don’t describe here how Spring provides this support because Spring Batch does a good job of hiding it.
Next, we focus on solutions (which are similar to JDBC) that Spring Batch provides to use ORM with batch processes efficiently.
Reading with ORM cursors implies that code responsible for managing domain classes doesn’t use a first-level cache.
Is ORM the right tool for batch applications? ORM works great for online applications but can be difficult to deal with in batch applications.
In Spring Batch, reading takes place in a separate transaction from processing and writing (this is a constraint of cursor- and page-based readers)
This works well in normal cases, but when Murphy’s law kicks in, the combination of a failure, the separate transaction, and lazy loading is explosive.
A solution is to apply the driving query pattern: the reader reads only item identifiers (using JDBC cursors or paging), and the processor uses the ORM tool to load the corresponding objects.
The goal is to have the ORM tool use the same transaction as the writer.
Reading from relational databases called StatelessSession that provides the same methods as the classic Session but without caching and checking dirty state.
You first define a model class mapping to a relational database entity.
For the online store case study, you define a Product class to map the product database table using Hibernate annotations, as shown in the following listing.
The Entity annotation on the Product class specifies that the class maps to the product table.
The Id and Column annotations map class properties to table columns using the name in the annotation values.
Using ORM cursors is similar to JDBC, shown in figure 5.10
The only difference is that ORM is an additional layer on top of JDBC.
As emphasized at the beginning of this section, only Hibernate supports this approach.
For other ORM providers, Spring Batch provides a paging mechanism similar to the one described for JDBC.
For example, the Java Persistence API (JPA) technology doesn’t provide cacheless support.
In this case, paging is the natural solution because ORM caches only a page of objects in memory.
Spring Batch performs data retrieval by successively executing several queries, as shown in figure 5.11
The only difference is that ORM is an additional layer on top of JDBC.
Properties are almost the same as those described in table 5.15 with the addition of the pageSize property to specify the number of items per data page.
Note that for the JpaPagingReader class, the useStateless property doesn’t apply and the queryProvider property is of type JpaQueryProvider.
As you can see, configuring paging is similar to configuring cursors, and properties are generally the same; here you set the factory for the ORM and the query.
This section closes the description of relational databases as input sources using JDBC directly and through ORM with cursors and paging.
Spring Batch integrates mechanisms to guarantee performance and memory consumption when using ORM for batch processes.
In the next section, we focus on other input sources for importing data.
Files and databases are the main data sources used as input for batch processes, but they aren’t the only ones.
You may want to reuse services provided by existing applications or integrate with an information system with asynchronous and event-driven features.
Spring Batch provides implementations of the ItemReader interface for such cases, which this section examines.
Reusability of existing services is a key concept of modern applications.
This avoids reinventing the wheel, provides robustness, and saves time.
Batch processes can integrate in existing systems that already provide entities to read data.
Its aim is to provide a standardized layer so that ORM tools are implementations of this specification.
The specification describes how to map managed entities to database tables and an API to interact with databases.
New features are the ability to use this technology outside an Enterprise JavaBeans (EJB) container and to use local transactions instead of global transactions with JTA.
Figure 5.12 describes different patterns for batch processes to read data from existing entities.
To implement these patterns, Spring Batch provides the ItemReaderAdapter class, which makes it possible to see an existing entity as an ItemReader.
The ItemReaderAdapter class holds the bean and method to delegate data retrieval.
The only constraints at this level are that the delegated method must not have parameters and that it returns the same type as the read method of the ItemReader interface.
For this reason, it isn’t possible to use the target service directly, and you must implement an adapter class for the service.
The ItemReader adapter retrieves elements one by one, which isn’t the case for services because they usually return a set of elements.
Figure 5.12 Reusing methods of existing entities and services to get data to provide as input for batch processes.
Products are then retrieved one by one with the getProduct method C using the product list initially loaded.
The next listing shows how to configure this mechanism for a POJO configured in Spring to reuse the ProductService entity managing products.
Having configured the ProductService as a bean in the Spring configuration, you can reference it as the target object for the ItemReader adapter through the targetObject property B.
You then specify which method to use to get product data with the targetMethod property C.
The remote-slsb XML element configures a remote EJB3 proxy as a bean, which transparently provides a delegated business implementation.
For our case study, the EJB3 corresponds to a remote service that manages products.
The following snippet shows how to use a remote EJB3 with an ItemReader adapter:
For a remote EJB3 service, the configuration of the ItemReader adapter remains the same.
For the productService bean, the configuration changes and the remote-slsb element’s jndi-name property is set to the name in the JNDI entry for the EJB3 session.
Be aware that the target entity is entirely responsible for importing data in this case, and there’s no possible interaction with the Spring Batch execution context.
In fact, existing entities aren’t linked to Spring Batch mechanisms and objects.
You also need to check that the use of the target entity in a batch process performs efficiently.
Before describing advanced issues regarding importing data with Spring Batch, we see how to implement and configure importing data using message-oriented middleware (MOM) and JMS.
The JMS specification tackles application messaging by providing a generic framework to send and receive messages synchronously and asynchronously.
In the context of batch processes, this makes it possible to handle incoming data automatically.
The JmsItemReader class reads data directly from a JMS destination (queue or topic)
In the case study, the import job receives products as payload from JMS messages read from a JMS queue.
The following listing shows how to configure reading from a JMS destination using a JmsItemReader.
If JMS is event driven, why use it in batch applications? One benefit of JMS is notification of new messages queued on destinations.
Sometimes, you don’t want to process messages as soon as they arrive because their processing is costly and you want to postpone this processing to reduce load on the server (which is busy doing something else at the moment)
A batch job can consume JMS messages while throttling processing.
You can choose to trigger the job when appropriate (every 10 minutes or at night, for example)
The message-driven and batch approaches can work together: you can enable JMS listeners when your servers aren’t too busy and disable them when there’s too much load.
You’re also free to launch a dequeuing batch job whenever you want.
You first define the data object type contained in JMS messages B.
You then configure how to interact with the JMS provider through Spring’s JMS support and its JmsTemplate class.
To configure a JmsTemplate, you specify the JMS connection factory and destination.
This template must then be set in the item reader using the jmsTemplate property C.
In this chapter, we described all built-in capabilities of Spring Batch used to import data from different input sources.
Spring Batch supports several file formats, relational databases, MOM, and reusing existing services.
In some cases, the ItemReader implementations provided by Spring Batch aren’t enough, and you must implement custom readers, which we discuss next.
If Spring Batch ItemReader implementations don’t suit your needs, you can provide your own implementations.
We don’t describe interacting with the execution context here because it’s covered in chapter 8
Imagine that you want to handle all files present in a directory at batch startup.
The list of files in the directory is loaded when the item reader is instantiated.
For each read, you return the first list element after removing it from the list.
Chapter 9 includes guidelines to properly deal with JMS transactions.
As a custom item reader, this class implements the ItemReader interface.
Because the ItemReader interface supports generics, you specify the associated type for the class (File)
In the constructor, you initialize the list of files in the given directory.
Spring Batch calls this method until it returns null, indicating that the method returned all files in the list, one at a time.
When you create a custom reader in Spring Batch, you implement the ItemReader interface.
The read method performs all read processing, which returns elements one by one.
Reading data from batch processes is the first step of a chunk-based tasklet.
Spring Batch provides support for this step with the generic ItemReader and ItemStream interfaces.
Spring Batch implements these interfaces for common technologies used in batch processes to import data.
Using our case study as an example, we described how to read data from various types of flat files and XML files.
We also described how to get data from a database, how to integrate with existing services, and how to interact with a MOM like JMS.
We briefly mentioned that reading is involved in the complete batch process execution.
This aspect is fundamental to restart batch processes and avoid reading data again when things go wrong later in processing and writing.
We didn’t go into details about reading, but we deal with this issue in chapter 8
In chapter 5, we learned how to read data with Spring Batch.
In this chapter, we focus on another core feature of the batch process: writing.
Reading input items takes place at the beginning of a chunk-oriented step and writing items takes place at the end.
In Spring Batch, a writer is responsible for data persistence.
We use our case study to illustrate how to write data into flat files, XML files, and relational databases using both JDBC and ORM (Object Relational Mapping)
Spring Batch provides various implementations out of the box for these targets, but it may be necessary to create your own writer implementations, which we demonstrate.
If you already have legacy services that produce or save data, Spring Batch can delegate to and reuse these services.
We also learn how to write to a Java Message Service (JMS) queue and send emails.
Here we look at core concepts of writing with Spring Batch, particularly writing in a chunk-oriented tasklet.
In a chunk-oriented tasklet, an ItemReader reads input data, an ItemProcessor (optionally) processes it, and an ItemWriter writes it.
Spring Batch provides the plumbing to aggregate reading and passing the data to the writer.
The interface ItemWriter represents the writer, which is the counterpart to the ItemReader interface.
The ItemWriter interface defines a single method called write, which saves output data.
Most writers have the ability to write a set of items all at once, which is why the writer takes a list of items as a parameter.
After writing items, the writer can flush before the process continues.
For files, writers flush the underlying stream to guarantee that it passes all bytes to the operating system.
It’s the responsibility of each writer implementation to deal with flushing if applicable.
After all items are processed, Spring Batch commits the current transaction.
With built-in JDBC writers, Spring Batch uses batch updates to send all SQL statements in one operation to get the best performance.
The following snippet shows you how to configure the number of items to write for a transaction; Spring Batch commits the transaction for each chunk:
The commit-interval attribute on the chunk element defines the chunk size.
Spring Batch is smart enough to avoid loading all data in memory.
As you’ll see later, you can also create your own ItemWriter implementations.
Figure 6.1 A chunk-oriented tasklet implements the chunk-oriented processing pattern in Spring Batch.
Spring Batch provides item writers that write files in various formats: delimited text, fixed-field widths, and XML.
We discuss these formats using our case study as an example.
In our application, an item reader first reads Products from a flat file.
We can then use different writers to produce output files in different formats.
The flat file format is one of the earliest formats used to store information.
A flat file is a plain text file that holds records, usually one per line.
Fields compose a record, and a separator, usually a comma or tab character, may delimit each field.
Alternatively, each field can be of a predetermined fixed length, where spaces pad values to get each field to its desired length.
Additionally, a flat file may include an optional header and footer line, which can define additional metadata or add comments.
The following snippet shows a comma-delimited flat file for two product records with fields for ID, name, description, and price:
In the next fixed-length flat file example, each field has a fixed length with no separator; it also has footer and header lines.
What are batch updates? Batch updates are good: they make insert and update statements execute efficiently.
By providing a list of items to the item writer, Spring Batch facilitates batch updates, but that’s not enough: you need to send the batch updates correctly.
How do you do that? The Spring Framework provides everything you need: look at the batchUpdate method in the JdbcTemplate and the BatchSqlUpdate class.
Figure 6.2 Spring Batch supports writing to multiple file formats thanks to the various item writer implementations it provides.
Spring Batch writes a flat file in the following steps:
Figure 6.3 shows the process of writing a flat file.
A FlatFileItemWriter implements the ItemStream interface and follows the stream lifecycle.
Both the header and footer are optional in flat files.
A FlatFileItemWriter uses a LineAggregator to transform each item into a String.
If you use domain-specific classes, you may need to convert each object into a more complex.
Figure 6.3 Spring Batch extracts and aggregates fields for each item when writing to a flat file.
The framework also handles writing a header and footer to the file (both are optional)
Figure 6.4 The interfaces and classes involved in writing items to a flat file with the FlatFileItemReader.
The FlatFileItemWriter also provides optional callbacks for the header and the footer of a flat file.
These interfaces and classes all play a part in writing flat files, whatever the format.
But how does Spring Batch know how and what to write in a flat file? We start to answer this question next, with the configuration of a FlatFileItemWriter.
The FlatFileItemWriter class is the starting point for writing flat files.
You define a FlatFileItemWriter as a bean in an XML Spring context.
The following listing shows a minimal FlatFileItemWriter configuration with lineAggregator and resource properties.
The resource property contains a URL pointing to the file to write.
The FlatFileItemWriter class also has a required property named LineAggregator.
The purpose of the LineAggregator interface is to create the String representation of an item.
Spring Batch provides the LineAggregator implementations listed in table 6.3
We introduced the LineAggregator interface in the previous section, but here we look at it in more detail:
The interface is simple and focuses on converting an object to a String.
The aggregate method extracts fields from the given item using a FieldExtractor and then aggregates them into a String.
The FieldExtractor’s only job is to convert an object into an array of its parts.
To see an illustration of these objects, see figure 6.4
Implementers of the FieldExtractor interface extract the bean information you want to write to a file:
Listing 6.2 writes to a delimited file to demonstrate the use of a FieldExtractor, but don’t worry about the details; we get to that in the next section.
The extractor returns the input if it’s a FieldSet, the result of toArray() on a Collection, or the result (as an array) of values()on a Map, and wraps other types of objects in a one-element array.
The following snippet shows the result of running this example:
The output file contains one line per product and is the result of calling toString() on each Product object.
It’s not sexy, but it’s efficient, even if you don’t have access to all the object properties or if you can’t control the column order or formatting.
This extractor takes an array of property names, reflectively calls the getters on a source item object, and returns an array of values.
For each product, the field extractor calls the getter (via reflection) of each property in the list and creates a new value array.
The following snippet shows that you control the properties and their order:
Next, you get even more control over the output by adding computed fields.
Here, you add BEGIN and END at the beginning and the end of each array and compute a tax field.
The following snippet shows the result of using the product field extractor:
Remember to use a FieldExtractor to control exactly what you want to write to a file.
The following sections explore how to write delimited and fixed-length fields and how to add headers and footers.
A character, typically a comma or a tab, separates each field value from the next.
You also set the delimiter attribute to separate each field with the given string, which by default is the comma character.
The following snippet shows the result of running this example:
In a fixed-width file, fixed-length fields make up each record.
Spaces may pad each field value to get to the desired field length.
The Formatter class, inspired by C’s printf function, allows developers to specify text representations of data for strings, numbers, dates, and time.
This formatter is used by PrintStream’s printf(String format, Object ...
To format these fields, you set the format property to %-9s%6.2f%-30s.
The format string looks complicated, so let’s take a closer look:
Figure 6.5 shows the mapping between bean property, format expression, and output line.
The following snippet shows the result of running this example:
The optional argument_index field is a decimal integer indicating the position of the argument in the argument list.
The optional flags field is a set of characters that modify the output format.
The optional width field is a nonnegative decimal integer indicating the minimum number of characters to write.
Figure 6.5 Mapping between bean property, format expression, and output line.
The required conversion field is a character indicating how to format the argument.
The set of valid conversions for a given argument depends on the argument’s data type.
Table 6.5 lists examples of using a Formatter through System.printf()
Next, let’s see how to deal with heterogeneous items before moving on to adding headers and footers.
In chapter 5, you saw how to read a flat file containing a mix of different product types.
Here, you learn how to do the same thing for writing.
First, you write a flat file with a LineAggregator based on the product type.
When the import job reads a product file, you create a MobilePhoneProduct or a BookProduct depending on the prefix of the product ID.
To write products to a file, the writer receives these same two types of.
Table 6.5 Examples of using a Formatter through System.printf() (continued)
Figure 6.6 The domain model of the batch application uses several classes.
The flat file item writer can use a custom LineAggregator to delegate aggregation to dedicated LineAggregators (one for each product subclass)
A FlatFileItemWriter uses a LineAggregator to get a String representation of each item.
To configure multiple LineAggregators, you inject a Map whose key is the Product’s Class and the value an instance of LineAggregator.
The aggregate method uses this Map to find the right LineAggregator and then calls its aggregate method.
The following listing shows how to configure multiple LineAggregators with a Map.
The following snippet shows the result of running this example:
Depending of the type of Product, the last column is either a publisher or manufacturer, each using a dedicated formatter.
You can see how Spring Batch gives you the ability to customize a writer’s output.
Another common feature is to add a footer and a header to a flat file, which is what you do in the next section.
A header is a text record that Spring Batch inserts before item processing.
A footer is a text record added at the end of the file after item processing.
In this example, you write a simple description in the header, and you write the item count and the Step execution time in the footer.
Spring Batch passes a Writer to the writeHeader method B to output the header, in this case, a simple description.
In the next listing, you configure the header and footer of a FlatFileItemWriter.
Listing 6.11 Configuring header and footer callbacks for a flat file.
To configure your footer and header, you create header and footer callback beans B.
You set the footer D and the header E as properties of the FlatFileItemWriter.
The following snippet shows the result of running this example:
The first line is the header, which tells who generated the file.
The last two lines make up the footer, which includes the number of items written and how long the operation took.
We saw Spring Batch write different types of flat file formats: delimited and fixed width.
Next, we work with a different type of file, XML.
A Marshaller is a generic interface provided by the Spring Object/XML Mapping1 module to convert objects to XML, and vice versa.
The following listing shows the XML output for our case study.
This XML document has a root element named products followed by a header child element.
After all the products, the document has a footer and ends by closing the root element.
This writer uses a Marshaller to convert each item to XML and then writes them to an XML file using StAX (Streaming API for XML)
In this example, you use the XStreamMarshaller class because it’s simple to configure.
You now know all you need to implement this use case.
You’ve already chosen a Marshaller class, and you know how to set the root tag element with the rootTagName property.
To write the header and footer, you implement the StaxWriterCallback interface and its write method.
For the header element, you want to create an attribute called generated, as illustrated by the following snippet:
Because you’re now in the StAX world, you create an XMLEvent object using the XMLEventFactory class.
The write method starts by creating an instance of XMLEventFactory to help you create an element and an attribute with the current date.
The footer is like the header, and you implement the same interface.
With the XMLEventFactory class, you create an XML character for the writeCount element B.
Spring Batch injects the StepExecution C to give you access to the writeCount value.
This Spring context contains a job definition with a special listener, the footer callback, which Spring Batch injects with a StepExecution.
Finally, you define an XStream Marshaller C with an alias map to serialize Product objects to XML product elements.
This XML products document contains elements for a footer, a header, and for each product.
Spring Batch provides a mechanism to write file sets instead of a single file (see figure 6.7)
It’s useful if you want to create files with a maximum number of items.
Figure 6.7 The multiresource item writer rolls over files after writing a given number of items.
This creates multiple small files instead a single large file.
By default, a multiresource writer suffixes the output filenames with an index.
This section ends our discussion on writing flat files and XML files.
We saw how to write delimited and fixed-width flat files.
We also discussed how to write XML files using the Spring OXM Marshaller interface.
The next section focuses on how to write items into a relational database.
In the Java world, you access relational databases with JDBC or an ORM tool.
Our use case for this section is to write Product objects into a relational database.
Figure 6.8 Sending a batch of SQL statements to a relational database is more efficient than sending one query at a time.
The SQL batch size is equal to the commit interval configured in the chunkoriented tasklet.
Sending SQL statements to a database in a SQL batch is faster than sending them one at a time.
At runtime, the SQL parameter name called name (defined with :name in the SQL statement) is set.
This is the best and fastest way to insert data in a relational database.
In this listing, you set a value for each SQL parameter B from bean properties.
Here, you use the JDBC API directly to set each SQL statement parameter.
Note that this statement uses ? positional parameter markers instead of named parameters.
The next section explores ORM tools, another way to interact with a relational database.
In our case study, you use Hibernate to persist the Product objects to a table in the database.
You first annotate the Product domain class with database mapping annotations, as described in the following listing.
The Entity annotation on the Product class specifies that the class maps to the product table in the database.
The Id and Column annotations map instance variables to table columns using the database column name in the annotation values.
In this implementation, Spring Batch checks if each item is a Hibernate entity and calls saveOrUpdate.
The writer flushes the Hibernate session to synchronize the object model with the database.
If a Hibernate entity calls saveOrUpdate and isn’t already in the session, Hibernate executes a SQL SELECT to find the object in the database.
If Hibernate finds the object, it executes a SQL UPDATE; if not, it executes a SQL INSERT.
Hibernate uses fetch strategies to determine how and when to retrieve objects.
It uses fetch modes to fetch associations with an outer join, a SELECT or sub-SELECT, and for lazy or eager loading.
This may create overhead compared to JDBC where you can control SQL statements directly.
In the next sections, we focus on other targets like JMS and email senders.
Spring Batch supports other writers for other targets such as existing business services and JMS and email senders.
To avoid reinventing the wheel, it’s a good idea to reuse existing business services.
After discussing reuse, we see how to send an item in a JMS message and via email.
If you want to reuse existing services to implement an ItemWriter, Spring Batch provides helper classes.
The first helper class is the ItemWriterAdapter class used to delegate writing to another service.
Let’s say you have an existing ProductService bean, which has a write(Product) method:
The first step is to wire the existing bean to the adapter:
You configure an ItemWriterAdapter with a targetObject, the service, and a targetMethod, the target method of the service.
If the service method is more complex, Spring Batch provides another ItemWriter to extract properties from an item bean.
Let’s imagine another ProductService class with a write method that takes multiple arguments:
Before describing custom writer implementations, we implement and configure writing data using a message-oriented middleware (MOM) broker like JMS.
A MOM broker allows you to send messages asynchronously to other applications (see figure 6.9)
For example, a writer can send products to a billing information system.
Spring Batch includes the JmsItemWriter class, which you can use without writing any Java code, only XML configuration, as shown in the following listing.
Figure 6.9 An application puts messages on a JMS queue with a JMS item writer.
Applications often use JMS to communicate with each other in a decoupled and asynchronous way.
You first configure the connection parameters to the JMS Server and set the JMS queue name where the application will send JMS messages.
You also create a Spring JmsTemplate to create JMS messages more easily.
Spring Batch makes it easy to send items to other applications through a MOM broker.
Before implementing a custom writer, we see how an item writer sends email messages.
For this use case, you have a file (it could also be a database table) containing information about new users, and you want to send each a welcome email message.
The following snippet lists the content of the customer flat file:
Figure 6.10 illustrates a Spring Batch application that sends an email message for each customer in the input file.
The job reads the flat file and creates a Customer object for each input line.
After that, you use an ItemProcessor to convert each Customer to a SimpleMailMessage (a Spring support class,) as described in the following listing.
Figure 6.10 Sending an email message for each customer in an input file.
Because Spring Batch’s email item writer only takes care of sending email, it’s common practice to use an item processor to convert read items into ready-to-be-sent SimpleMailMessage or MimeMessage objects.
You implement an ItemProcessor to create new SimpleMailMessage objects from Customer objects B.
The process method takes a Customer item, creates a new email message, and sets the message fields From, To, Subject, and Body.
First, you create a JavaMailSenderImpl B, a bean to send mail required for the writer.
You configure a chunk with a processor to convert Customer items to SimpleMailMessage objects C.
In this section, we saw how to send messages to a mail server.
The next section focuses on implementing your own item writers.
If none of the built-in Spring Batch ItemWriters matches your requirements, you can create your own.
For example, you may want to create your own JDBC item writer.
The configuration injects a JdbcTemplate to take advantage of Spring JDBC.
The write method updates or inserts a row in the product table.
If the UPDATE statement returns zero rows affected, then you insert a new row in the table.
Imagine the system crashes when a job is writing to a file.
On a restart, if the reader can start reading where it left off, the writer should also be able to resume its writing exactly where it was interrupted.
You should care about restartability on the writer side mainly for file-based writers (good news: the implementations Spring Batch provides are restartable)
Database-based writers are usually automatically restartable: they just write the items the reader pushes.
You’ve seen how to use the item writers provided by Spring Batch and how to implement your own item writer.
In this section, you learn more about Spring Batch item writers.
To implement a complex job, it may be necessary to create something more complex than a custom item writer.
You may need to chain writers to write to different targets.
You may also need to choose between several writers, depending on the item.
Spring Batch can configure only a single item writer for a chunk-oriented tasklet, and sometimes you need multiple writers for the same chunk.
A composite wraps a set of objects and presents them as a single object.
This composite calls each item writer in the configured order.
This technique allows the job to write items to multiple files in different formats.
This technique also allows the job to write items in both files and relational databases, all from one composite item writer.
The next section discusses a more complex topic: how to route an item to a specific item writer.
In this section, we discuss a more complex use case: how to route items to specific item writers on the basis of some criteria.
In the following input file, the column called OPERATION represents an operation to execute: C to create, U to update, and D to delete:
Figure 6.11 A composite item writer delegates writing to a list of item writers.
Use this pattern to send items to several targets, like a database and a file.
For this use case, you want to route each product item to an item writer, depending on the value of a Product’s operation.
The operation determines whether to create, update, or delete a product in a relational database.
One of the building blocks Spring Batch provides to implement this type of use case is an interface called Classifier.
You implement a Classifier to map an input object of type C to another object of type T:
This Classifier takes an object and returns an ItemWriter for objects of the same type.
Now that you have some good building blocks, let’s see how to use these types for our use case.
First, you create the router, which is a Classifier, to return the operation value for a given Product:
You can create a class that implements the Classifier interface or use the @Classifier annotation to return the product operation value.
Figure 6.12 Routing a Product item to a specific writer.
This router maps Product operations to an item writer that does the work D.
You can also route domain objects to specific item writers.
Spring Batch can indeed deal with some complex writing use cases.
Writing data is the last step in a chunk-oriented tasklet.
Spring Batch offers a simple interface, the ItemWriter, to implement this task and provides implementations for different delimited and fixed-width flat files, XML files, JDBC, and ORM.
We studied how to write items reusing legacy services, how to write items to a JMS queue, and how to send emails for items.
We also saw how to implement a custom item writer.
Finally, we discussed advanced writing techniques, the use of the composite pattern, and how to route a domain object to an item writer on the basis of various criteria.
In chapter 7, you learn how to process data, the middle section of a chunkoriented step.
You learned that Spring Batch enforces best practices to optimize I/O and provides many ready-to-use components.
This is important for batch applications because exchanging data between systems is common.
Batch applications aren’t limited to I/O; they also have business logic to carry on: enforcing business rules before sending items to a database, transforming data from a source representation to one expected by a target data store, and so on.
In Spring Batch applications, you embed this business logic in the processing phase: after you read an item but before you write it.
After explaining item processing and its configuration in Spring Batch, we show you how to use item processors to modify, filter, and validate items.
For validation, we examine two techniques: programmatic validation in Java and validation through configuration files using a validation language and validation annotations.
Finally, we cover how to chain item processors following the composite design pattern.
By the end of this chapter, you’ll know exactly where and how to write the business logic for your batch application.
You’ll also learn about advanced topics, such as the distinction between filtering and skipping items.
Spring Batch provides a convenient way to handle a large number of records: the chunk-oriented step.
So far, we’ve covered the read and write phases of the chunkoriented step; this section explains how to add a processing phase.
It also avoids tangling your business code in the reading and writing phases (input and output)
We’ll see what kind of business logic the processing phase can handle, how to configure an item processor in a chunk-oriented step, and the item processor implementations delivered with Spring Batch.
Recall that a chunk-oriented step includes a reading component (to read items one by one) and a writing component (to handle writing several items in one chunk)
The two previous chapters covered how to read and write items from different kinds of data stores and in various formats.
Spring Batch can insert an optional processing component between the reading and writing phases.
Figure 7.1 illustrates where item processing takes place in a chunk-oriented step.
When a chunk-oriented step contains no processing phase, items read are sent asis to the writer, and Spring Batch takes care of aggregating items in chunks.
Now, imagine that an application can’t allow writing items as-is because some kind of processing must be applied to the items first.
Let’s add a new business requirement to the online store example: you want to apply discounts to products before the job imports.
Figure 7.1 Spring Batch allows insertion of an optional processing phase between the reading and writing phases of a chunk-oriented step.
The processing phase usually contains some business logic implemented as an item processor.
To do so, you must modify the products imported from the flat file in the item-processing phase.
The processing phase is a good place for business logic.
Table 7.1 lists the categories of business logic that can take place in the itemprocessing phase.
The processing phase is an interesting link between the reading and writing phase.
Let’s start with the basic configuration of an item processor in Spring Batch.
Spring Batch defines the item-processing contract with the ItemProcessor interface as follows:
The ItemProcessor interface uses two type arguments, I and O:
Spring Batch passes a read item of type I to the process method.
The type I must be compatible with the item reader type.
The process method returns an item of type O, which Spring Batch in turn sends to the item writer, also of a type compatible with O.
You define the concrete types I and O in your ItemProcessor implementation.
If the process method returns null, Spring Batch won’t send the item to the writer, as defined by the filtering contract (filtering is different from skipping; more on this later)
The following listing shows how to implement a filtering ItemProcessor.
Table 7.1 Categories of business logic in the item-processing phase.
Transformation The item processor transforms read items before sending them to the writer.
The item processor can change the state of the read item or create a new object.
In the latter case, written items may not be of the same type as read items.
Filtering The item processor decides whether to send each read item to the writer.
No transformation—The processor receives a Product object and returns a Product object.
The filtering logic is simple: if an item ID’s last character is an even digit, the filter accepts the item.
Our ItemProcessor example isn’t useful beyond showing you how to configure item processing in a chunk-oriented step.
The following listing shows how to configure this item processor.
Listing 7.2 Configuring an item processor in a chunk-oriented step.
Adding an item processor is straightforward with the Spring Framework and Spring Batch XML: you write a Spring bean that implements the ItemProcessor interface and then refer to it in your chunk-oriented step configuration with the processor attribute of a chunk element.
Now that you know the basics of item processing in Spring Batch, let’s see what the framework offers in terms of ready-to-use ItemProcessor implementations.
As you saw in the previous section, implementing an ItemProcessor is simple, and it’s usually what you end up doing to implement business logic.
Nevertheless, Spring Batch provides implementations of ItemProcessors that can come in handy; table 7.2 lists these implementations.
You’ll have opportunities to use these ItemProcessor implementations later in the chapter.
For now, let’s dive into the details of transforming items.
Transforming read items and then writing them out is the typical use case for an item processor.
In Spring Batch, we distinguish two kinds of transformation: changing the state of the read item, and producing a new object based on the read item.
In the latter case, the object the processor returns can be of a different type than the incoming item.
We illustrate both kinds of transformation with our online store application.
Imagine that the application is successful and that other companies ask ACME to add their products to the online catalog; for this service, ACME takes a percentage of each partner’s product sold.
With this use case in mind, let’s first explore transforming the state of read items.
In our scenario, the model of the ACME product and of each partner product is similar, but some modifications must be made to all partners’ imported products before they’re written to the database.
These modifications require some custom business logic, so you embed this logic in a dedicated application component.
The model of the ACME product and of each partner product is similar, but each partner maintains its own product IDs.
Figure 7.2 shows that the item reader, processor, and writer of the chunk-oriented step all use the same type of object.
The custom mapping between partner IDs and online store IDs takes place in the PartnerIdMapper class, as shown in the following listing.
Listing 7.3 Mapping partner IDs with store IDs in a business component.
Figure 7.2 In the processing phase of a chunk-oriented step, you can choose to only change the state of read items.
In this case, the item reader, processor, and writer all use the same type of object (illustrated by the small squares)
To perform the product ID mapping, you search the product ID for the online store in a mapping database table, using the partner ID and product ID in the partner namespace as the criteria.
In an alternative implementation, the PartnerIdMapper class could generate the ID on the fly and store it if it didn’t find it in the mapping table.
This is important because this class implements the business logic, and you don’t want to couple it tightly to the Spring Batch infrastructure.
You implement a dedicated ItemProcessor with a plug-in slot for a PartnerIdMapper, as shown in the following snippet:
Now you need to wire these components together in the configuration of a chunkoriented step, as shown in the following listing.
Listing 7.4 Configuring the dedicated item processor to map product IDs.
That’s it! You configured processing that converts the IDs of the incoming products into the IDs that the online store uses.
You can achieve the same goal without this extra custom class.
Sometimes an existing business component is similar to a Spring Batch interface like ItemProcessor, but because it doesn’t implement the interface, the framework can’t call it directly.
All you end up doing is a bit of Spring configuration.
This reminds us that it’s a best practice to have your business logic implemented in POJOs.
We’re done looking at a processing phase that changes the state of read items.
We use such processing when read items are of the same type as written items but need some sort modification, such as the ID conversion for imported products.
The next section covers a processing phase that produces a different type of object from the read item.
As ACME finds more partners, it must deal with different product lines as well as with mismatches between the partners’ product models and its own product model.
You still base the import on an input flat file, but ACME needs a processing phase to transform the partners’ products into products that fit in the online store database.
The processing phase of your chunk-oriented step transforms PartnerProduct objects read by the ItemReader into Product objects that the ItemWriter writes into the online store database.
This is a case where the reader, the processor, and the writer don’t manipulate the same kind of objects at every step, as shown in figure 7.3
You can find a simple implementation in the source code for this book.
What we need to do now is to plug this business logic into a Spring Batch job.
Figure 7.3 The item processor of a chunk-oriented step can produce objects of a different type (represented by circles) than the read items (squares)
The item writer then receives and handles these new objects.
Most of the time, online store applications don’t have a static structure for the products in their catalog: they use a metamodel configured with the structure of the products and a generic engine that uses this metamodel to display products dynamically.
For example, a metamodel for products in a book category could have fields for author, title, publication date, and so on.
We could imagine that the ACME online application uses such a metamodel but that the partners don’t.
Such a mapper would rely heavily on the metamodel to do its job.
Note that the actual argument types of the generic ItemProcessor interface now take two different types: PartnerProduct (for the input type) and Product (for the output type)
Now that we have our ItemProcessor, let’s see its configuration in the following listing.
Listing 7.6 A dedicated item processor to call the partner product mapper.
Listing 7.7 Configuring an item processor to map partner products to store products.
Thanks to the product mapping that takes place during the processing phase of this step, you can transform the partner product representation to the product representation expected by the online store application.
If you need to perform a different conversion for another partner, you only need to implement another item processor and reuse the item reader and writer.
Before ending this section on using the item-processing phase to modify or transform read items, let’s see how to use an ItemProcessor to implement a common pattern in batch applications: the driving query pattern.
The driving query pattern is an optimization pattern used with databases.
Execute one query to load the IDs of the items you want to work with.
Execute queries to retrieve a database row for each item.
This seems counterintuitive, but using this pattern can end up being faster than loading the whole content of each object in one single query.
Some database engines tend to use pessimistic locking strategies on large, cursorbased result sets.
This can lead to poor performance or even deadlocks if applications other than the batch application access the same tables.
The trick is to use a driving query to select the IDs and then load complete objects one by one.
A single query can prevent the database from handling large datasets.
Spring Batch can easily match and implement the driving query pattern in a chunkoriented step:
The driving query pattern and ORM tools The driving query pattern works nicely with object-relational mapping (ORM) tools.
A simple JDBC item reader reads IDs, and a custom item processor uses an ORM tool to load the objects.
Figure 7.4 illustrates the driving query pattern in Spring Batch.
Let’s use the driving query pattern in our online application.
Imagine the application features a search engine whose index needs to be updated from time to time (a complete re-indexing is rare because it takes too long)
The indexing batch job consists of selecting recently updated products and updating the index accordingly.
Because the online store is running during the indexing, you want to avoid locking overhead, so you shouldn’t load large datasets; therefore, the driving query pattern is a good match.
You use a Spring Batch cursor-based JDBC item reader to retrieve the product IDs.
The product table contains an update_timestamp column updated each time a product row changes.
Who performs the update? The application, a database trigger, or a persistence layer like Hibernate can perform the update.
We use the update_timestamp column to select the products that the database must index.
Listing 7.8 Configuring an item reader to execute the driving query.
Figure 7.4 The driving query pattern implemented in Spring Batch.
The item processor receives the IDs and loads the objects.
The item writer then receives these objects to, for example, write a file or update the database or an index.
You use the Spring Expression Language to get the date query parameter from the job parameters.
That’s it! You configured your item reader to execute the driving query.
Note that you didn’t write any Java code: you configured only existing components provided by Spring Batch and the Spring Framework.
Next, let’s see how to load the products from their IDs within an item processor.
You need to load a product based on its ID.
This is a simple operation, and a data access object (DAO) used in the online store application already implements this feature.
Listing 7.9 Implementing a DAO to load a product from its ID.
You use a ProductRowMapper to map a JDBC result set to Product objects.
You can use RowMapper implementations in a JDBC-based data access layer for your online applications.
You can also use a RowMapper with a Spring Batch JDBC-based item reader.
What you need to do now is connect your data access logic with Spring Batch.
This is what the item processor in the following listing does.
The configuration of a chunk-oriented step using the driving query pattern is like any other chunk-oriented step, except that it needs to have an item processor set.
The following listing shows this configuration (we elided the reader and writer details)
Listing 7.10 Implementing an item processor to call the DAO.
The implementation of the driving query pattern with Spring Batch ends this section.
We covered how to use the processing phase of a chunk-oriented step as a way to modify read items and create new items.
The next section covers how to use the processing phase as a way to filter read items before sending them to an item writer.
The processing phase of a chunk-oriented step not only can modify read items but also can filter them.
Imagine reading a flat file containing some products that belong in the database and some that don’t.
For example, some products don’t belong to any of the categories of items sold in the store, some products are already in the database, and so on.
You can use an item processor to decide whether to send a read item to the item writer, as shown in figure 7.5
We cover how to implement a typical filtering item processor and how to filter using validation.
We implement programmatic validation, but we also use declarative validation using integration between Spring Batch and the Spring Modules project.
First, let’s learn more about the filtering contract in the item-processing phase.
The basic contract for filtering in an item processor is simple: if the item processor’s process method returns null, the read item won’t go to the item writer.
It implements logic to decide whether to send a read item to the item writer.
Filtering and validating items defines the main contract, but there are subtleties; let’s look at the filtering rules for item processors:
If the process method returns null, Spring Batch filters out the item and it won’t go to the item writer.
The basic contract for filtering is clear, but we must point out the distinction between filtering and skipping:
Filtering means that Spring Batch shouldn’t write a given record.
For example, the format of a phone number is invalid.
You can easily look up this information using a tool like Spring Batch Admin or by consulting the corresponding database table.
The last detail of the filtering contract we need to examine is that an item processor can filter items by returning null for some items, but it can also modify read items, like any other item processor.
You shouldn’t mix filtering and transformation in a sameitem processor (separation of concerns), but it’s your right to do so!
Now that you know all about the filtering contract, let’s see how to implement a filtering item processor.
Let’s look back at the import products job from chapter 1 and see in which circumstances it could use filtering.
Remember that this job consists of reading a flat file containing product records and creating or updating the database accordingly.
You get into trouble if you execute the import job while the online store application hits the database: updating products from the job locks database rows and makes the online store less responsive.
Nevertheless, you want the database to be as up to date as possible.
A good compromise is to read the flat file and create new product records, but discard updating existing products.
You can update existing products later, in a separate job, when there’s less traffic in the online store.
Best practice: separate filtering and transformation If your application needs to both filter items and transform items, then follow the separation of concerns pattern by using two item processors: one to filter and one to transform.
You meet this requirement by inserting a filtering item processor between the reading of the flat file and the writing to the database.
This item processor checks the existence of the record in the database and discards it if it already exists.
Figure 7.6 illustrates how the import products job works with a filtering phase.
The following listing shows the implementation of the filtering item processor.
Figure 7.6 The filtering item processor discards products that are already in the database.
This item writer only inserts new records and doesn’t interfere with the online application.
A different job updates existing records when there’s less traffic in the online store.
This would make the filtering more dynamic and could eliminate the need to have two distinct jobs (one for inserts only and one for inserts and updates)
The following listing shows the configuration of the import products job with the filtering item processor.
The item processor implemented here is a typical case of using the item processor phase as a filter.
The item processor receives valid items from the item reader and decides which items to pass to the item writer.
Let’s now see another case where you can use item processing to prevent read items from reaching the item writer: validation.
Because validation is business logic, the standard location to enforce validation rules is in the item-processing phase of a chunk-oriented step.
A common practice in Spring Batch is for an item processor to perform validation checks on read items and decide whether to send the items to the item writer.
As an example, let’s see how to validate the price of imported products and check that prices aren’t negative numbers (products with a negative price shouldn’t reach the database—you don’t want to credit your.
Should you consider an item that fails the validation check filtered or skipped? Skipping is semantically closer to a validation failure, but this remains questionable, and the business requirements usually lead to the correct answer.
A validation failure should lead to a skipped or filtered item, but what you care about is that the item writer doesn’t receive the item in question.
Remember that the corresponding step-execution metadata stored in the job repository is distinct (skip and filter count), and this distinction can be relevant for some use cases.
If you want to enforce validation rules in your item processor, use the following semantics for validation failure in the item processor’s process method:
What kind of validation can an item processor perform? You can do almost anything: state validation of the read object, consistency check with other data, and so forth.
In the import products job, for example, you can check that the price of products from the flat file is positive.
A well-formatted negative price would pass the reading phase (no parsing exception), but you shouldn’t write the product to the database.
The job of the item processor is to enforce this check and discard invalid products.
It delegates validation to an implementation of the Spring Batch Validator interface.
It has a filter flag that can be set to false to throw an exception (skip) or true to return null (filter) if the validation fails.
Let’s say you want to check that a product doesn’t have a negative price.
The following snippet shows how to implement this feature as a Validator:
This validator isn’t rocket science, but as you configure it with Spring, it benefits from the ability to use dependency injection to, for example, access the database through a Spring JDBC template.
The configuration for this validating item processor example has some interesting aspects, so let’s examine the following listing.
This implies the configuration of a skip strategy if you don’t want to fail the whole job execution in case of a validation failure.
Writing dedicated validator classes can be overkill and result in overall code bloat.
An alternative is to make the validation declarative: instead of coding the validation in Java, you implement it with a dedicated validation language in the configuration file.
The Spring Modules project provides a simple yet powerful validation language: Valang (for va-lidation lang-uage)
You can easily integrate Valang with Spring Batch to write your validation rules without Java code.
For example, to verify that the product price isn’t negative, you write the following rule in Valang (assuming the evaluation context is a Product object):
We don’t cover this syntax here; our point is to show how to integrate Valang rules within a validating item processor.1
Because Valang isn’t Java code, we use the Spring configuration to implement validating the product price, as shown in the following listing.
Listing 7.15 Embedding validation logic in the configuration with Valang.
The valang property of ValangValidator accepts one or more validation rules (we used only one in the example)
We explicitly set the validating item processor to skip mode (the filter property is false), so we need to set up a skip strategy to avoid failing the job if the validation fails.
Valang works great and allows you to embed validation rules directly in your Spring configuration files.
But what if you want to reuse your validation rules in different contexts, such as in your batch jobs and in a web application? You can do this with Valang, but the Bean Validation standard also offers a widespread and effective solution.
It’s common to use the same validation constraints in multiple places: when an administrator updates the product catalog manually, they shouldn’t be able to enter a negative price.
If you really want to avoid products with a negative price, you can also validate the objects when they’re about to be persisted in the database.
Spring Batch provides a level of abstraction with its Validator interface and an implementation (SpringValidator) that uses the Spring Validator interface.
Both Validator interfaces are potential extension points for your own implementations.
It can also provide a level of indirection between Spring Batch and your favorite validation framework.
Spring Batch provides one implementation of Validator: SpringValidator, which plugs in the Spring Framework’s validation mechanism.
This can look confusing, but the Spring Batch team didn’t want to directly tie Spring Batch’s validation system to Spring’s.
Filtering and validating items execute code before storing an object in the database.
Finally, you also want to avoid negative prices when you import products with a Spring Batch job.
The idea behind Bean Validation is simple but powerful: embed validation constraints with annotations on the classes you want to validate and enforce them anywhere you need to.
The good news is that many frameworks support Bean Validation out of the box: they validate incoming objects transparently for you.
That’s the case with all the frameworks we just mentioned (Spring MVC, JSF, and JPA)
Wouldn’t it be nice to reuse all your Bean Validation constraints in your Spring Batch jobs? Let’s see how to do this.
Let’s start with the Product class, which now contains the constraint for negative prices.
The following listing shows the Product class with the Bean Validation annotations on the getter method for price.
The idea is to use Java classes as the definitive repository for validation constraints.
As soon as you have access to a Java object, you can validate it, because it contains its own validation rules.
You express Bean Validation constraints with Java annotations, but you can also do so in XML.
Bean Validation is becoming increasingly popular, and many frameworks integrate support for it.
The reference implementation for Bean Validation is the Hibernate Validator project, which is at the origin of the standard.
The validation constraints specify that the price can’t be null or negative.
These constraints are simple, but Bean Validation includes constraints that are more advanced and lets you also define your own.
To use Bean Validation in your Spring Batch jobs, you only have to define a custom Validator that enforces the validation constraints on incoming items, as shown in the following listing.
The validator is straightforward to implement thanks to the Bean Validation API.
You can inject an instance of the validator in the validator item processor that Spring Batch provides, as shown in the following snippet:
Using Bean Validation is particularly relevant if you use exactly the same classes in your batch jobs and in other applications.
Many frameworks support Bean Validation, from the web layer to the persistence layer, and using it offers the best opportunity to reuse your validation constraints.
The integration between Spring Batch and the Bean Validation standard ends our coverage of the use of the item-processing phase for validation.
Remember that you must follow strict rules if you don’t want to confuse skip with filter when validating items.
Let’s see now how to apply the composite pattern to chain item processors.
As we stated at the beginning of this chapter, the item-processing phase of a chunkoriented step is a good place to embed business logic.
Assuming that each item processor in your application implements one single business rule (this is simplistic but enough to illustrate our point), how could you enforce several business rules in the item-processing phase of a single step? Moreover, recall that you can insert only a single item processor between an item reader and an item writer.
The solution is to apply the composite pattern by using a composite item processor that maintains a list of item processors (the delegates)
The composite item processor delegates the calls to all the members in its list, one after the next.
Figure 7.8 illustrates the model of a composite item processor.
When using a composite item processor, the delegates should form a typecompatible chain: the type of object an item processor returns must be compatible with the type of object the next item processor expects.
Figure 7.8 Using a composite item processor allows item processors to be chained in order to apply a succession of business rules, transformations, or validations.
In section 7.2, we covered the transformation of read items, where we distinguished two types of transformations:
Changing the state of the read item—We mapped the product ID in the partner’s namespace to the product ID in the ACME system.
Producing another object from the read item—We produced instances of the Product class from PartnerProduct objects (created by the item reader)
What do you do if your import job needs to do both? You use two item processors: the first item processor reads raw PartnerProduct objects from the flat file and transforms them into Product objects, and then the second item processor maps partner product IDs to ACME IDs.
Listing 7.18 Chaining item processors with the composite item processor.
Figure 7.9 Applying the composite item processor pattern to the import products job.
The first delegate item processor converts partner product objects into online store product objects.
The second delegate item processor maps partner IDs with ACME IDs.
This example shows the power of the composite pattern applied to building a processing chain: you didn’t modify your two existing item processors, you reused them as is.
Spring Batch encourages separation of concerns by isolating business logic in reusable item processors.
Spring Batch isn’t only about reading and writing data: in a chunk-oriented step, you can insert an item processor between the item reader and the item writer to perform any kind of operation.
The typical job of an item processor is to implement business logic.
For example, an item processor can convert read items into other kinds of objects Spring Batch sends to the item writer.
Because batch applications often exchange data between two systems, going from one representation to another falls into the domain of item processors.
Spring Batch defines another contract in the processing phase: filtering.
For example, if items already exist in the target data store, the application shouldn’t insert.
You can filter items such that they’ll never get to the writing phase.
We made a clear distinction between filtering items and skipping items.
This distinction became even more relevant when we covered validation.
We saw that we can isolate validation rules in dedicated validator components, and we used this feature to plug in two declarative validation frameworks, Valang and Bean Validation.
This chapter about data processing ends our coverage of the three phases of a chunk-oriented step: reading, processing, and writing.
You now have all the information necessary to write efficient batch applications with Spring Batch.
Chapter 8 introduces you to techniques used to make batch applications more robust, and you’ll see that chunk-oriented processing plays an important role.
Previous chapters showed how Spring Batch helps to read, process, and write data efficiently and without much Java code, thanks to ready-to-use I/O components.
It’s time to deal with the automatic aspect of batch jobs.
Batch jobs operate over long periods, at night, for example, and without human intervention.
Even if a batch job can send an email to an operator when something’s wrong, it’s on its own most of the time.
A batch job isn’t automatic if it fails each time something goes wrong; it needs to be able to handle errors correctly and not crash abruptly.
Perhaps you know how frustrating it is to sit at your desk in the morning and see that some nightly jobs have crashed because of a missing comma in an input file.
This chapter explains techniques to make your batch jobs more robust and reliable when errors occur during processing.
By the end of this chapter, you’ll know how to build bulletproof batch jobs and be confident that your batch jobs will succeed.
The first section of this chapter explains how a batch job should behave when errors or edge cases emerge during processing.
Spring Batch has built-in support, its skip and retry features, to handle errors when a job is executing.
Skip and retry are about avoiding crashes, but crashes are inevitable, so Spring Batch also supports restarting a job after a failed execution.
It won’t fail abruptly, either, for a major problem like a constraint violation in the database.
Before reviewing some guidelines on the design of a robust job, let’s consider some requirements that a job must meet.
A bulletproof batch job should meet the following general requirements:
Robust—The job should fail only for fatal exceptions and should recover gracefully from any nonfatal exception.
As software developers, we can’t do anything about a power cut, but we can properly handle incorrectly formatted lines or a missing input file.
A job can skip as many incorrectly formatted lines as it wants, but it should log to record what didn’t make it in the database and allow someone to do something about it.
Restartable—In case of an abrupt failure, the job should be able to restart properly.
Depending on the use case, the job could restart exactly where it left off or even forbid a restart because it would process the same data again.
Good news: Spring Batch provides all the features to meet these requirements! You can activate these features through configuration or by plugging in your own code through extension points (to log errors, for example)
A tool like Spring Batch isn’t enough to write a bulletproof job: you also need to design the job properly before leveraging the tool.
To make your batch jobs bulletproof, you first need to think about failure scenarios.
What can go wrong in this batch job? Anything can happen, but the nature of the operations in a job helps to narrow the failure scenarios.
Many things can go wrong: the archive can be corrupt (if it’s there!), the OS might not allow the process to write in the working directory, some lines in the files may be incorrectly formatted, and the list goes on.
Once you’ve identified failure scenarios, you must think about how to deal with them.
If there’s no ZIP archive at the beginning of the execution, there’s not much the job can do, but that’s no reason to fail abruptly.
Spring Batch has built-in support for error handling, but that doesn’t mean you can make batch jobs bulletproof by setting some magical attribute in an XML configuration file (even if sometimes that’s the case)
Rather, it means that Spring Batch provides infrastructure and deals with tedious plumbing, but you must always know what you’re doing: when and why to use Spring Batch error handling.
That’s what makes batch programming interesting! Let’s now see how to deal with errors in Spring Batch.
Unless you control your batch jobs as Neo controls the Matrix, you’ll always end up getting errors in your batch applications.
Spring Batch includes three features to deal with errors: skip, retry, and restart.
Skip For nonfatal exceptions Keeps processing for an incorrect item.
Retry For transient exceptions Makes new attempts on an operation for a transient failure.
Restart After an execution failure Restarts a job instance where the last execution failed.
Testing failure scenarios Remember that Spring Batch is a lightweight framework.
It means you can easily test failure scenarios in integration tests.
You can simulate many failure scenarios thanks to testing techniques like mock objects, for example.
The features listed in table 8.1 are independent from each other: you can use one without the others, or you can combine them.
Remember that skip and retry are about avoiding a crash on an error, whereas restart is useful, when a job has crashed, to restart it where it left off.
Skipping allows for moving processing along to the next line in an input file if the current line is in an incorrect format.
If the job doesn’t process a line, perhaps you can live without it and the job can process the remaining lines in the file.
Retry attempts an operation several times: the operation can fail at first, but another attempt can succeed.
Retry isn’t useful for errors like badly formatted input lines; it’s useful for transient errors, such as concurrency errors.
Skip and retry contribute to making job executions more robust because they deal with error handling during processing.
Restart is useful after a failure, when the execution of a job crashes.
Instead of starting the job from scratch, Spring Batch allows for restarting it exactly where the failed execution left off.
Restarting can avoid potential corruption of the data in case of reprocessing.
Restarting can also save a lot of time if the failed execution was close to the end.
Before covering each feature, let’s see how skip, retry, and restart can apply to our import products job.
Recall our import products job: the core of the job reads a flat file containing one product description per line and updates the online store database accordingly.
Here is how skip, retry, and restart could apply to this job.
You don’t want to stop the job execution because of a couple of bad lines: this could mean losing an unknown amount of updates and inserts.
You can tell Spring Batch to skip the line that caused the item reader to throw an exception on a formatting error.
Retry—Because some products are already in the database, the flat file data is used to update the products (description, price, and so on)
Even if the job runs during periods of low activity in the online store, users sometimes access the updated products, causing the database to lock the corresponding rows.
The database throws a concurrency exception when the job tries to update a product in a locked row, but retrying the update again a few milliseconds later works.
The job fails as soon as you reach 10 skipped products, as defined in the configuration.
An operator will analyze the input file and correct it before restarting the import.
Spring Batch can restart the job on the line that caused the failed execution.
The import products job is robust and reliable thanks to Spring Batch.
Let’s study the roles of skip, retry, and restart individually.
Sometimes errors aren’t fatal: a job execution shouldn’t stop when something goes wrong.
In the online store application, when importing products from a flat file, should you stop the job execution because one line is in an incorrect format? You could stop the whole execution, but the job wouldn’t insert the subsequent lines from the file, which means fewer products in the catalog and less money coming in! A better solution is to skip the incorrectly formatted line and move on to the next line.
Whether or not to skip items in a chunk-oriented step is a business decision.
The good news is that Spring Batch makes the decision of skipping a matter of configuration; it has no impact on the application code.
Let’s see how to tell Spring Batch to skip items and then how to tune the skip policy.
Recall that the import products job reads products from a flat file and then inserts them into the database.
It would be a shame to stop the whole execution for a couple of incorrect lines in a file containing thousands or even tens of thousands of lines.
You can tell Spring Batch to skip incorrect lines by specifying which exceptions it should ignore.
You can specify several exception classes (with several include elements)
When using the include element, you specify not only one class of exception to skip but also all the subclasses of the exception.
Listing 8.1 Configuring exceptions to skip in a chunk-oriented step.
Note also in listing 8.1 the use of the skip-limit attribute, which sets the maximum number of items to skip in the step before failing the execution.
Skipping is useful, but skipping too many items can signify that the input file is corrupt.
As soon as Spring Batch exceeds the skip limit, it stops processing and fails the execution.
When you declare an exception to skip, you must specify a skip limit.
The include element skips a whole exception hierarchy, but what if you don’t want to skip all the subclasses of the specified exception? In this case, you use the exclude element.
Specifying exceptions to skip and a skip limit is straightforward and fits most cases.
Can you avoid using a skip limit and import as many items as possible? Yes.
When importing products in the online store, you could process the entire input file, no matter how many lines are incorrect and skipped.
As you log these skipped lines, you can correct them and import them the next day.
Spring Batch gives you full control over the skip behavior by specifying a skip policy.
Figure 8.1 The include element specifies an exception class and all its subclasses.
If you want to exclude part of the hierarchy, use the exclude element.
The exclude element also works transitively, as it excludes a class and its subclasses.
Let’s say you know exactly on which exceptions you want to skip items, but you don’t care about the number of skipped items.
You can implement your own skip policy, as shown in the following listing.
Once you implement your own skip policy and you declare it as a Spring bean, you can plug it into a step by using the skip-policy attribute, as shown in the following listing.
Listing 8.2 Implementing a skip policy with no skip limit.
Figure 8.2 When skip is on, Spring Batch asks a skip policy whether it should skip an exception thrown by an item reader, processor, or writer.
The skip policy’s decision can depend on the type of the exception and on the number of skipped items so far in the step.
Table 8.2 lists the skip policy implementations Spring Batch provides.
Don’t hesitate to look them up before implementing your own.
You typically use the default skip policy if you care about the total number of skipped items and you don’t want to exceed a given limit.
If you don’t care about the number of skipped items, you can implement your own skip policy and easily plug it into a chunk-oriented step.
Listing 8.3 Plugging in a skip policy in a chunk-oriented step.
How Spring Batch drives chunks with skipped items We focused on skipping items during the reading phase, but the skip configuration also applies to the processing and writing phases of a chunk-oriented step.
Spring Batch doesn’t drive a chunk-oriented step the same way when a skippable exception is thrown in the reading, processing, or writing phase.
Skipping incorrect items makes a job more robust, but you might want to keep track of these items.
Let’s see how Spring Batch lets you do that with a skip listener.
Once you have the skipped items in a file or in a database, you can deal with them: correct the input file, do some manual processing to deal with the error, and so on.
The point is to have a record of what went wrong!
Spring Batch provides the SkipListener interface to listen to skipped items:
You can implement a skip listener and plug it into a step, as figure 8.4 shows.
Spring Batch calls the appropriate method on the listener when it skips an item.
When an item processor throws a skippable exception, Spring Batch rolls back the transaction of the current chunk and resubmits the read items to the item processor, except for the one that triggered the skippable exception in the previous run.
Figure 8.3 shows what Spring Batch does when the item writer throws a skippable exception.
Because the framework doesn’t know which item threw the exception, it reprocesses each item in the chunk one by one, in its own transaction.
Figure 8.3 When a writer throws a skippable exception, Spring Batch can’t know which item triggered the exception.
Spring Batch then rolls back the transaction and processes the chunk item by item.
Note that Spring Batch doesn’t read the items again, by default, because it maintains a chunk-scoped cache.
There’s one more solution: using annotations on a simple class (no interface, no abstract class)
Next, you use the annotation solution with @OnSkipInRead to skip items during the reading phase.
The following listing shows the skip listener, which logs the incorrect line to a database.
Whenever a chunk-oriented step throws a skippable exception, Spring Batch calls the listener accordingly.
A listener can then log the skipped item for later processing.
Once you implement the skip listener, you need to register it.
The following listing shows how to register the skip listener on a step, using the listeners element in the tasklet element.
A couple of details are worth mentioning in the skip listener configuration:
Only one skip listener was registered in the example, but you can have as many as you want.
Spring Batch is smart enough to figure out the listener type.
The example used the generic listener element to register the skip listener.
Spring Batch detects that it is a skip listener (Spring Batch provides many different kinds of listeners)
When does Spring Batch call a skip listener method? Just after the item reader, processor, or writer throws the to-be-skipped exception, you may think.
Spring Batch postpones the call to skip listeners until right before committing the transaction for the chunk.
Why is that? Because something wrong can happen after Spring Batch skips an item, and Spring Batch could then roll back the transaction.
Later on, something goes wrong during the writing phase of the same chunk, and Spring Batch rolls back the transaction and could even fail the job execution.
You wouldn’t want to log the skipped item during the reading phase, because Spring Batch rolled back the whole chunk! That’s why Spring Batch calls skip listeners just before the commit of the chunk, when it’s almost certain nothing unexpected could happen.
We’re done with skipping, a feature Spring Batch provides to make jobs more robust when errors aren’t fatal.
By default, an exception in a chunk-oriented step causes the step to fail.
You can skip the exception if you don’t want to fail the whole step.
Skipping works well for deterministic exceptions, such as an incorrect line in a flat file.
Do you give up and start watching the clown show, or do you try to dial the number again? Maybe the connection will be better on the second attempt or in a couple of minutes.
Transient errors happen all the time in the real world when using the phone or online conference tools like Skype.
You usually retry several times after a failure before giving up and trying later if the call doesn’t go through.
What are transient exceptions in batch applications? Concurrency exceptions are a typical example.
If a batch job tries to update a row that another process holds a lock on, the database can cause an error.
Retrying the operation immediately can be successful, because the other process may have released the lock in the meantime.
You can configure Spring Batch to retry operations transparently when they throw exceptions, without any impact on the application code.
Because transient failures cause these exceptions, we call them retryable exceptions.
It happened while one of the authors was working on this book!
Notice the retry-limit attribute, used to specify how many times Spring Batch should retry an operation.
Just as for skipping, you can include a complete exception hierarchy with the include element and exclude some specific exceptions with the exclude element.
The following snippet illustrates the use of the exclude element for retry:
In the preceding snippet, you tell Spring Batch to retry when Spring throws transient exceptions unless the exceptions are related to pessimistic locking.
Spring Batch only retries the item processing and item writing phases.
By default, a retryable exception triggers a rollback, so you should be careful because retrying too many times for too many items can degrade performance.
You should use retryable exception only for exceptions that are nondeterministic, not for exceptions related to format or constraint violations, which are typically deterministic.
Figure 8.5 Spring Batch configured to retry exceptions: the include tag includes an exception class and all its subclasses.
By using the exclude tag, you specify a part of the hierarchy that Spring Batch shouldn’t retry.
Here, Spring Batch retries any transient exception except pessimistic locking exceptions.
Figure 8.6 Spring Batch retries only for exceptions thrown during item processing or item writing.
Retry triggers a rollback, so retrying is costly: don’t abuse it! Note that Spring Batch doesn’t read the items again, by default, because it maintains a chunk-scoped cache.
You can combine retry with skip: a job retries an unsuccessful operation several times and then skips it.
Remember that once Spring Batch reaches the retry limit, the exception causes the step to exit and, by default, fail.
Use combined retry and skip when you don’t want a persisting transient error to fail a step.
The following listing shows how to combine retry and skip.
Automatic retry in a chunk-oriented step can make jobs more robust.
It’s a shame to fail a step because of an unstable network, when retrying a few milliseconds later could have worked.
You now know about the default retry configuration in Spring Batch, and this should be enough for most cases.
The next section explores how to control retry by setting a retry policy.
By default, Spring Batch lets you configure retryable exceptions and the retry count.
Sometimes, retry is more complex: some exceptions deserve more attempts than others do, or you may want to keep retrying as long as the operation doesn’t exceed a.
Override equals() and hashCode() when using retry In a chunk-oriented step, Spring Batch handles retry on the item processing and writing phases.
By default, a retry implies a rollback, so Spring Batch must restore the context of retried operations across transactions.
It needs to track items closely to know which item could have triggered the retry.
Remember that Spring Batch can’t always know which item triggers an exception during the writing phase, because an item writer handles a list of items.
Spring Batch relies on the identity of items to track them, so for Spring Batch retry to work correctly, you should override the equals and hashCode methods of your items’ classes—by using a database identifier, for example.
Spring Batch delegates the decision to retry or not to a retry policy.
Table 8.3 lists the RetryPolicy implementations included in Spring Batch.
You can use these implementations or implement your own retry policy for specific needs.
Let’s see how to set a retry policy with an example.
Imagine you want to use retry on concurrent exceptions, but you have several kinds of concurrent exceptions to deal with and you don’t want the same retry behavior for all of them.
Spring Batch should retry all generic concurrent exceptions three times, whereas it should retry the deadlock concurrent exceptions five times, which is more aggressive.
Listing 8.8 Using a retry policy for different behavior with concurrent exceptions.
Listing 8.8 shows that setting a retry policy allows for flexible retry behavior: the number of retries can be different, depending on the kind of exceptions thrown during processing.
Listening to retries also helps you learn about the causes of retries.
Spring Batch provides the RetryListener interface to react to any retried operation.
A retry listener can be useful to log retried operations and to gather information.
Once you know more about transient failures, you’re more likely to change the system to avoid them in subsequent executions (remember, retried operations always degrade performance)
Listing 8.9 Implementing a retry listener to log retried operations.
The retry listener uses the SLF4J logging framework to log the exception the operation throws.
It could also use JDBC to log the error to a database.
The following listing registers the listener in the step, using the retry-listeners XML element.
What can you do if you need to retry in your own code, for example, in a tasklet?
Imagine you use a web service in a custom tasklet to retrieve data that a subsequent step will then use.
A call to a web service can cause transient failures, so being able to retry this call would make the tasklet more robust.
You can benefit from Spring Batch’s retry feature in a tasklet, with the RetryOperations interface and its RetryTemplate implementation.
The online store uses a tasklet to retrieve the latest discounts from a web service.
The discount data is small enough to keep in memory for later use in the next step.
The DiscountService interface hides the call to the web service.
The following listing shows the tasklet that retrieves the discounts (the setter methods are omitted for brevity)
The tasklet uses a RetryTemplate to retry in case of failure.
Note how the RetryTemplate is configured with a RetryPolicy directly in the tasklet.
You could have also defined a RetryOperations property in the tasklet and used Spring to inject a RetryTemplate bean as a dependency.
Thanks to the RetryTemplate, you shouldn’t fear transient failures on the web service call anymore.
Use of the RetryTemplate is simple, but the retry logic is hardcoded in the tasklet.
Let’s go further to see how to remove the retry logic from the application code.
Can you remove all the retry logic from the tasklet? It would make it easier to test, because the tasklet would be free of any retry code and the tasklet could focus on its core logic.
Furthermore, a unit test wouldn’t necessarily deal with all retry cases.
By using this interceptor, the tasklet can use a DiscountService object directly.
The interceptor delegates calls to the real DiscountService and handles the retry logic.
No more dependency on the RetryTemplate in the tasklet—the code.
Retrying on error becomes simpler! The following listing shows the new version of the tasklet, which doesn’t handle retries anymore.
If you want to keep the tasklet this simple, you need the magic of AOP to handle the retry transparently.
This proxy handles the retry logic thanks to the retry interceptor.
The following listing shows the Spring configuration for transparent, AOP-based retry.
Aspect-oriented programming (AOP) Aspect-oriented programming is a programming paradigm that allows modularizing crosscutting concerns.
The idea of AOP is to remove crosscutting concerns from an application’s main logic and implement them in dedicated units called aspects.
Typical crosscutting concerns are transaction management, logging, security, and retry.
The Spring Framework provides first-class support for AOP with its interceptor-based approach: Spring intercepts application code and calls aspect code to address crosscutting concerns.
Thanks to AOP, boilerplate code doesn’t clutter the application code, and code aspects address crosscutting concerns in their own units, which also prevents code scattering.
That’s it! Not only should you no longer fear transient failures when calling the web service, but the calling tasklet doesn’t even know that there’s some retry logic on the DiscountService.
In addition, retry support isn’t limited to batch applications: you can use it in a web application whenever a call is subject to transient failures.
Spring Batch allows for transparent, configurable retry, which lets you decouple the application code from any retry logic.
Retry is useful for transient, nondeterministic errors, like concurrency errors.
The default behavior is to retry on given exception classes until Spring Batch reaches the retry limit.
Note that you can also control the retry behavior by plugging in a retry policy.
Skip and retry help prevent job failures; they make jobs more robust.
Thanks to skip and retry, you’ll have fewer red-light screens in the morning.
Okay, your job is running, there are some transient errors, retry comes to the rescue, but these errors aren’t that transient after all.
More errors come up, and the job finally reaches the skip limit.
Can’t developers honor the exchange format you spent weeks to establish?
There’s still hope, because Spring Batch lets you restart a job exactly where it left off.
This is useful if the job was running for hours and was getting close to the end when it failed.
Figure 8.7 illustrates a new execution of the import products job that continues processing where the previous execution failed.
How does Spring Batch know where to restart a job execution? It maintains metadata for each job execution.
If you want to benefit from restart with Spring Batch, you need a persistent implementation for the job repository.
This enables restart across job executions, even if these executions aren’t part of the same Java process.
Chapter 2 shows how to configure a persistent job repository and illustrates a simple restart scenario.
It also discusses Spring Batch metadata and job executions, as figure 8.8 shows.
Spring Batch has a default behavior for restart, but because there’s no one-size-fitsall solution for batch jobs, it provides hooks to control exactly how a restarted job execution should behave.
Figure 8.7 If a job fails in the middle of processing, Spring Batch can restart it exactly where it left off.
Figure 8.8 Restart is possible thanks to batch metadata that Spring Batch maintains during job executions.
This seems obvious but has the following implications: you must provide the job launcher with the job and the exact same job parameters as the failed execution you want to restart.
Using Spring Batch terminology: when using restart, you start a new job execution of an existing, uncompleted job instance.
You can start a new execution for any failed job instance.
You can disable restart for a specific job, but you need to disable it explicitly.
A job restarts exactly where the last execution left off.
You can restart a job as many times as you want.
You can override the defaults, and Spring Batch lets you change the restart behavior.
Table 8.4 summarizes the restart settings available in the configuration of a job.
The defaults for these settings match the default behavior we described.
Let’s learn more about restart options in Spring Batch by covering some typical scenarios.
When a job is sensitive and you want to examine each failed execution closely, preventing restarts is useful.
After all, a commandline mistake or an improperly configured scheduler can easily restart a job execution.
Forbid restart on jobs that are unable to restart with correct semantics.
Forbidding an accidental restart can prevent a job from processing data again, potentially corrupting a database.
To disable restart, set the attribute restartable to false on the job element:
If you’re worried that you’ll forget that, set the restartable flag explicitly on all your jobs.
Restart is a nice feature, so let’s assume now that our jobs are restartable and explore more scenarios.
Remember that the import products job consists of two steps: the first decompresses a ZIP archive, and the second reads products from the decompressed file and writes into the database.
Imagine the first step succeeds and the second step fails after several chunks.
There’s no definitive answer to such questions; this is a business decision, which determines how to handle failed executions.
When is skipping the first, already completed step a good choice? If you check the Spring Batch logs and fix the decompressed input file after the failure, restarting directly on the second step is the way to go.
The chunk-oriented step should complete correctly, or at least not fail for the same reason.
If you stick to this scenario, you have nothing to do: skipping already completed steps is the default restart behavior.
Let’s consider now re-executing the first, already completed step on restart.
When the batch operator sees that the execution failed during the second step, their reaction may be to send the log to the creator of the archive and tell them to provide a correct one.
In this case, you should restart the import for this specific job instance with a new archive, so re-executing the first step to decompress the new archive makes sense.
The second step would then execute and restart exactly on the line where it left off (as long as its item reader can do so and assuming the input has no lines removed, moved, or added)
Figure 8.9 Spring Batch lets you choose if it should re-execute already completed steps on restart.
Restarting a job is like many things: don’t do it too often.
Repeatedly restarting the same job instance can mean there’s something wrong and you should simply give up with this job instance.
That’s where the restart limit comes in: you can set the number of times a step can be started for the same job instance.
If you reach this limit, Spring Batch forbids any new execution of the step for the same job instance.
You set the start limit at the step level, using the start-limit attribute on the tasklet element.
The following snippet shows how to set a start limit on the second step of the import products job:
Let’s see a scenario where the start limit is useful.
You launch a first execution of the import products job.
The first decompression step succeeds, but the second step fails after a while.
This second execution starts directly where the second step left off.
The first step completed, and you didn’t ask to execute it again on restart.) The second execution also fails, and a third execution fails as well.
The whole job execution fails and the job instance never completes.
You need to move on and create a new job instance.
Spring Batch can restart a job exactly at the step where it left off.
Can you push restart further and restart a chunk-oriented step exactly on the item where it failed?
When a job execution fails in the middle of a chunk-oriented step and has already processed a large amount of items, you probably don’t want to reprocess these items again on restart.
Reprocessing wastes time and can duplicate data or transactions, which could have dramatic side effects.
Spring Batch can restart a chunk-oriented step exactly on the chunk where the step failed, as shown in figure 8.10, where the item reader restarts on the same input line where the previous execution failed.
The item reader drives a chunk-oriented step and provides the items to process and write.
The item reader is in charge when it comes to restarting a chunk-oriented step.
Again, the item reader knows where it failed in the previous execution thanks to metadata stored in the step execution context.
There’s no magic here: the item reader must track what it’s doing and use this information in case of failure.
Imagine a file item writer that directly moves to the end of the written file on a restart.
A restartable item reader can increment a counter for each item it reads and store the value of the counter each time a chunk is committed.
In case of failure and on restart, the item reader queries the step execution context for the value of the counter.
Spring Batch helps by storing the step execution context between executions, but the item reader must implement the restart logic.
After all, Spring Batch has no idea what the item reader is doing: reading lines from a flat file, reading rows from a database, and so on.
Most of the item readers that Spring Batch provides are restartable.
You should always carefully read the Javadoc of the item reader you’re using to know how it behaves on restart.
How the item reader implements its restart behavior can also have important.
Figure 8.10 A chunk-oriented step can restart exactly where it left off.
The figure shows an item reader that restarts on the line where the previous execution failed (it assumes the line has been corrected)
To do so, the item reader uses batch metadata to store its state.
For example, an item reader may not be thread-safe because of its restart behavior, which prevents multithreading in the reading phase.
Chapter 13 covers how to scale Spring Batch jobs and the impacts of multithreading on a chunkoriented step.
What if you write your own item reader and you want it to be restartable? You must not only read the items but also access the step execution context to store the counter and query it in case of failure.
Let’s take an example where an item reader returns the files in a directory.
This item reader implements the ItemStream interface to store its state periodically in the step execution context.
Listing 8.14 Implementing ItemStream to make an item reader restartable.
The reader implements both the ItemReader (read method) and ItemStream (open, update, and close methods) interfaces.
The code at B initializes the file array to read files, where you would typically set it from a Spring configuration file.
You sort the file array because the order matters on a restart and the File.listFiles method doesn’t guarantee any specific order in the resulting array.
When the step begins, Spring Batch calls the open method first, in which you initialize the counter C.
You retrieve the counter value from the execution context, with a defined key.
On the first execution, there’s no value for this key, so the counter value is zero.
On a restart, you get the last value stored in the previous execution.
This allows you to start exactly where you left off.
In the read method, you increment the counter D and return the corresponding file from the file array.
Spring Batch calls the update method just before saving the execution context.
In update, you have a chance to store the state of the reader, the value of the counter E.
ItemStream provides the close method to clean up any resources the reader has opened (like a file stream if the reader reads from a file)
You leave the method empty, as you have nothing to close.
Listing 8.14 shows you the secret to restarting in a chunk-oriented step.
ItemStream is one kind of listener that Spring Batch provides: you can use the interface for item processors, writers, and on plain steps, not only chunk-oriented steps.
To enable restart, ItemStream defines a convenient contract to store the state of a reader at key points in a chunk-oriented step.
Note that Spring Batch automatically registers an item reader that implements ItemStream.
Remember that Spring Batch can restart a job instance where the last execution left off thanks to the metadata it stores in the job repository.
Spring Batch has reasonable defaults for restart, but you can override them to re-execute an already completed step or limit the number of executions of a step.
Restarting in the middle of a chunk-oriented step is also possible if the item reader stores its state periodically in the execution context.
To use this feature, it’s best to implement the ItemStream interface.
Remember that restart makes sense only when a job execution fails.
You can configure restart to prevent reprocessing, potentially avoiding data corruption issues.
Restart also avoids wasting time and processes the remaining steps of a failed job execution.
Skip and retry are techniques to use before relying on restart.
Skip and retry allow jobs to handle errors safely and prevent abrupt failures.
Congratulations for getting through this chapter! You’re now ready to make your Spring Batch jobs bulletproof.
Spring Batch has built-in support to make jobs more robust and reliable.
Spring Batch jobs can meet the requirements of reliability, robustness, and traceability, which are essential for automatic processing of large amounts of data.
This chapter covered a lot of material, but we can summarize this material as follows:
Don’t hesitate to write tests to simulate these scenarios and check that your jobs behave correctly.
Many Spring Batch components are already restartable, and you can implement restartability by using the execution context.
Disable restart on a job that could corrupt data on a restart.
Another key point to consider when you want to implement bulletproof jobs is transaction management.
Proper transaction management is essential to a batch application because an error during processing can corrupt a database.
In such cases, the application can trigger a rollback to put the database back in a consistent state.
The next chapter covers transactions in Spring Batch applications and is the natural transition after the coverage of the bulletproofing techniques in this chapter.
Because an error can occur at any time during batch processing, a job needs to know if it should roll back the current transaction to avoid leaving data in an inconsistent state or if it can commit the transaction to persist changes.
Once we show the transaction management defaults in Spring Batch, section 9.3 explains why and how to override them.
It also shows you how to avoid common pitfalls related to using declarative transactions and transactional readers.
Section 9.4 covers patterns that help you tackle tricky transaction scenarios in batch applications.
Why should you read this chapter? Spring Batch has reasonable defaults for simple jobs.
To implement complex jobs, you need to know more about transaction management.
That’s what this chapter is about: explaining how Spring Batch handles transactions and providing you with guidelines and ready-to-use solutions to deal with challenging jobs.
Transactions make the interactions between an application and a data store reliable.
When writing batch applications, you need to know exactly what you’re doing with transactions, because they affect the robustness of your batch jobs and even their performance.
With a transaction, you can safely interact with a data store.
An interaction consists of one or more operations—SQL statements if the data store is a database.
Safely” means that the transaction is atomic, consistent, isolated, and durable.
We commonly refer to these kinds of transactions as having ACID properties.
In most applications, you choose how to drive transactions by using programmatic transaction demarcations or declarative transaction management (as the Spring Framework provides)
It’s not the same in a Spring Batch job: Spring Batch drives the flow and the transactions.
Batch applications don’t follow the requestresponse flow of typical web applications; this makes transaction management in batch jobs more complicated.
The next section explains the default Spring Batch behavior that drives transactions.
This means that Spring Batch will never use only one transaction for a whole job (unless the job has a single step)
Remember that you’re likely to implement a Spring Batch job in one of two ways: using a tasklet or using a chunk-oriented step.
Let’s see how Spring Batch handles transactions in both cases.
This differs from the usual read-process-write behavior that Spring Batch’s chunk-oriented step handles well.
Here are cases where you can use a tasklet: launching a system command, compressing files in a ZIP archive, decompressing a ZIP archive, digitally signing a file, uploading a file to a remote FTP server, and so on.
By default, the execute method of a tasklet is transactional.
Each invocation of execute takes place in its own transaction.
As we mentioned, each execute invocation takes place in its own transaction.
To summarize, a tasklet is a potentially repeatable transactional operation.
Let’s now see how Spring Batch handles transactions in a chunk-oriented step.
A chunk-oriented step follows the common read-process-write behavior for a large number of items.
You know by now that you can set the chunk size.
Transaction management depends on the chunk size: Spring Batch uses a transaction for each chunk.
One transaction per item isn’t an appropriate solution because it doesn’t perform well for a large number of items.
Robust—An error affects only the current chunk, not all items.
When does Spring Batch roll back a transaction in a chunk? Any exception thrown from the item processor or the item writer triggers a rollback.
This isn’t the case for an exception thrown from the item reader.
This behavior applies regardless of the retry and skip configuration.
You can have transaction management in a step; you can also have transaction management around a step.
Remember that you can plug in listeners to jobs and step executions, to log skipped items, for example.
If logging to a database, for example, logging needs proper transaction management to avoid losing data or logging the wrong information.
Spring Batch provides many types of listeners to respond to events in a batch job.
When Spring Batch skips items from an input file, you may want to log them.
How does Spring Batch handle transactions in these listeners? Well, it depends (the worst answer a software developer can get)
There’s no strict rule on whether or not a listener method is transactional; you always need to consider each specific case.
Here’s one piece of advice: always check the Javadoc (you’re in luck; the Spring Batch developers documented their source code well)
If we take the ChunkListener as an example, its Javadoc states that Spring Batch executes its beforeChunk method in the chunk transaction but its afterChunk method out of the chunk transaction.
Therefore, if you use a transaction resource such as a database in a ChunkListener’s afterChunk method, you should handle the transaction yourself, using the Spring Framework’s transaction support.
Spring Batch also includes listeners to listen to phases for item reading, processing, and writing.
Spring Batch calls these listeners before and after each phase and when an error occurs.
The error callback is transactional, but it happens in a transaction that Spring Batch is about to roll back.
Therefore, if you want to log the error to a database, you should handle the transaction yourself and use the REQUIRES_NEW propagation level.
This allows the logging transaction to be independent from the chunk and the transaction to be rolled back.
Now that we’ve completed this overview of transaction management in Spring Batch jobs, let’s study how to tune transactions during job executions.
Transaction configuration attributes like the isolation level is common in batch applications because it can provide better performance.
Spring Batch uses reasonable defaults for transaction management, but you can’t use these defaults for all batch jobs.
This section explains why and how to override these defaults and how to avoid common pitfalls.
This allows you to have transaction attributes for a specific chunk different from the default attributes provided by a data source (which are commonly REQUIRED for the propagation level and READ_COMMITED for the isolation level)
If you want to learn more about this topic, please see Spring in Action by Craig Walls (Manning Publications, 2011)
Most of the time, default transaction attributes are fine, so when would you need to override these defaults? It depends on the use case.
Some batch jobs can work concurrently with online applications, for example.
The isolation level dictates the visibility rules between ongoing, concurrent transactions.
When a batch job works concurrently with an online application, increasing the isolation level can ensure that the batch job and the online application properly read and update data, but at the cost of lower performance.
Alternatively, a batch job can be the only process working on the data, so decreasing the isolation level can result in faster processing than with the default isolation level.
The following snippet shows how to set the isolation level to the lowest level, READ_UNCOMMITED:
READ_COMMITTED A transaction sees only committed changes from other transactions.
REPEATABLE_READ A transaction can read identical values from a field multiple times.
In this snippet, you ask the database to provide the lowest isolation guarantee, but because the batch job is the only one working on the data, you don’t care about concurrent access.
That’s it for transaction attributes: override them only when you must.
Spring provides declarative transaction management: you say what you want to be transactional and Spring demarcates transactions for you.
You can configure transactions using the @Transactional annotation or XML.
This is convenient for online applications, like web applications: application code doesn’t depend on transaction management because Spring adds it transparently at runtime.
If at any time Spring Batch calls application code annotated with @Transactional, the transaction for this code uses the transaction managed by Spring Batch.
The following are guidelines to avoid conflict between Spring Batch–managed and Spring-managed transactions:
Figure 9.1 Be careful when using Spring’s declarative transaction in a Spring Batch job.
Depending on the transaction attributes, the Spring-managed transaction can participate (or not) with the Spring Batch–managed transaction.
Be careful using propagation levels if declarative transactions are on—If you call transactional classes from a Spring Batch job, Spring’s transaction propagation can interfere with the Spring Batch transaction because of the propagation level.
The REQUIRES_NEW propagation level could typically cause problems because the application code runs in its own transaction, independent of the Spring Batch transaction.
One of your best friends in online applications can become your worst enemy in offline applications! Let’s now meet another friend, the transactional reader.
For example, Spring Batch buffers read items for a chunk so that, in case of a retryable error during writing, it can roll back the transaction and get the read items from its cache to submit to the writer instead of reading them again from the item reader.
This behavior works perfectly if you read items from a data source like a database: Spring Batch reads a record, that’s it.
The transaction rollback has no effect on the record read by Spring Batch: the database doesn’t care.
The story isn’t the same with a Java Message Service (JMS) queue.
You not only read a message from a queue, you dequeue it: you read a message and remove it from the queue at the same time.
In message-oriented middleware (MOM) and JMS terms, you also say that you consume a message.
When there’s a transaction rollback, JMS returns the read messages to the queue.
If the processing of the messages failed, the messages must stay on the queue.
In the case of a JMS reader, buffering the read items is a bad idea: if a rollback occurs, the messages go back to the queue, Spring Batch then resubmits the items to the writer using its cache, and the writing succeeds.
This is a bad combination: the processing succeeded but the messages are still on the queue, ready for Spring Batch to read and to trigger the processing…again!
Figure 9.2 illustrates the difference between a nontransactional and a transactional reader.
The nontransactional reader can read from a database (the database doesn’t care about clients reading it)
The transactional reader gets items from the data source and puts them back in case of an error.
The cache Spring Batch maintains for read items prevents the transactional reader from getting items again after a failure, so you should disable it when the reader is transactional.
Spring Batch sends messages back to the queue on a rollback and reads them again if it attempts to process the chunk again.
The default value is true, which implies always re-executing the processor before sending items to the writer.
In a chunk-oriented step, Spring Batch rolls back a chunk transaction if an error occurs in the item processor or in the item writer.
By default, Spring Batch maintains a cache of read items for retries.
You must disable this cache when the reader is transactional, so Spring Batch can read the items again in case of a rollback.
A JMS item reader is an example of a transactional reader because reading a message from a JMS queue removes it from the queue.
A database reader is a nontransactional reader, because reading rows from a database doesn’t modify the database.
Transaction management patterns could have corrupted the state of the transaction, so a rollback ensures data isn’t in an inconsistent state.
Sometimes you’re sure that a specific error didn’t corrupt the transaction, so Spring Batch can retry the operation or skip the item.
You now know a lot about transaction management in Spring Batch and related configuration options.
The next section explores transaction management patterns for real-world batch scenarios.
This section covers commons challenges related to transaction management in batch applications.
We look at guidelines and patterns using the Spring Framework and Spring Batch to overcome these challenges.
By the end of this section, you’ll have a clear understanding of global transactions (transactions spanning multiple resources)
We also see how to deal with global transactions when a database and a JMS queue are involved.
Applications sometimes need to perform transactional operations spanning multiple resources.
We call these types of transactions global or distributed transactions.
For example, such resources can be two databases, or a database and a JMS queue.
Such transactional operations must meet the ACID properties we previously listed.
In the case of two databases and the classic money transfer example, imagine that the credited account is in a first database and the debited account in a second database.
For a database and a JMS queue, the reception of the message and its processing must be atomic.
We don’t want to lose the message if the processing fails.
Figure 9.3 shows an application that uses transactions over multiples resources.
Global transactions are different from local transactions, where only one resource is involved and the application directly communicates with the resource to demarcate transactions, as shown in figure 9.4
You should strive to use local transactions as much as possible because they’re simple to set up, reliable, and fast.
Always consider if you need a JMS queue or a second database in your application.
The Spring Framework provides support for local transactions for various data access technologies such as JDBC, Hibernate, Java Persistence API (JPA), and JMS.
Global transactions are too difficult for an application to deal with, so an application relies on a dedicated component called a transaction manager.
This transaction manager component implements a special protocol called XA.
In this case, a third-party component handles the transactions, so we call such transactions managed transactions.
In Java, to perform global transactions using the XA protocol, we need the following:
The system must enforce ACID properties on all participating resources.
Figure 9.4 Local transactions between an application and a resource.
The application directly communicates with the resource to demarcate transactions.
Try to use local transactions as much as possible: they’re fast, simple, and reliable.
A JTA transaction manager—It implements the Java Transaction API (JTA) specification, which requires the implementation of the XA protocol.
Such a transaction manager is included in a Java EE application server or is available as a standalone component.
XA-aware drivers—The resources must provide XA-compliant drivers so the transaction manager can communicate with the resources using the XA protocol.
Thanks to these interfaces, the JTA transaction manager can enlist the resources in distributed transactions, as shown in figure 9.5
All Java EE application servers include a JTA transaction manager (Glassfish, JBoss, WebSphere, and so on; forgive us if we don’t list the others)
You can plug in a standalone transaction manager in a web container like Tomcat and Jetty to provide JTA transactions.
You can also use a standalone JTA transaction manager in a standalone process, like a batch application.
Figure 9.5 shows an application using a JTA transaction manager to demarcate transactions spanning multiple resources.
If you want to use global transactions, the database or the JMS provider you’re using must have an XA-compliant driver available.
Most of the popular databases and JMS providers have XA drivers.
Figure 9.5 An application can use a JTA transaction manager to handle global transactions.
The resources must provide XA drivers to communicate with the transaction manager using the XA protocol.
Second, some implementations (transaction managers and XA drivers) remain buggy.
Third, XA is inherently slower than local transactions because the strong transactional guarantees it provides imply some overhead (the transaction manager and the resources need to maintain precise logs of what they’re doing, for instance)
If your jobs are running inside a Java EE application server, consider using that server’s transaction manager.
We’re not saying that using JTA for global transactions is a bad solution.
It provides strong guarantees, but they come at a price.
Depending on the context and resources involved, other techniques are viable alternatives to XA; they involve coding and usually perform better than XA.
We examine the following two patterns: the shared resource transaction pattern when two databases are involved, and the best effort pattern when a database and a JMS queue are involved.
You can use both in batch applications by leveraging the Spring Framework and Spring Batch.
For example, two JDBC DataSources can point to the same database instance.
Using Oracle terminology, we say that you refer to schema B from schema A by using the same connection.
You also need to define synonyms in schema A for schema B’s tables.
This enables real global transactions using the same mechanism as for local transactions.
The overhead is a little more than for true local transactions but less than with XA.
Figure 9.6 shows a use case of the shared resource pattern, where a database schema contains tables for a first application and Spring Batch’s tables.
A Spring Batch job executes against both applications’ tables, but using only one connection, with the use of synonyms.
Applying the shared resource transaction pattern can have some limitations, depending on the database engine.
Figure 9.6 Use the shared resource transaction pattern when a common resource hosts the transactional resources.
In this example, two Oracle database schemas exist in the same database instance.
The first schema refers to the second schema’s tables using synonyms.
Transaction management patterns example, you may need to change some application or configuration code to add a schema prefix to refer explicitly to the correct schema.
Even when this pattern applies in a specific context, it generally provides better throughput and needs less configuration than an XA solution.
Let’s now see the best effort pattern, which applies to a database and a JMS queue.
Reading messages from a JMS queue and processing them in a database is a common scenario for a batch application.
For example, our online store could accumulate orders in a JMS queue and read them periodically to update its inventory.
This solution allows for full control over the processing of messages, including postponing processing to periods when the system isn’t under heavy load.
Note that this example solution doesn’t exclude processing the messages as they’re arriving by plugging in a queue listener.
Figure 9.7 illustrates a chunk-oriented step that reads from a queue and updates a database during the writing phase.
What can go wrong? Let’s look at the two cases:
Losing the message—The application receives a message, acknowledges it, but fails to process it.
The message is no longer on the queue, and there’s been no processing in the database: the message is lost.
Receiving and processing the same message twice—The application receives a message and processes it, but the acknowledgment fails.
The JMS broker delivers the message again, and the application processes it again.
The shared resource transaction pattern for batch metadata Here’s an example of the shared resource transaction pattern applied to Spring Batch.
People are sometimes reluctant to host the batch execution metadata in the same database as the business data (they don’t want to mix infrastructure and business concerns, which makes sense)
Therefore, Spring Batch must span transactions over two databases for the execution metadata and the business data to ensure proper counts of skipped items, retries, and so on.
You can use the shared resource transaction pattern to host batch execution metadata and business data in different databases.
The pattern keeps your batch metadata and business data separate and properly synchronized, and you can stick to local transactions.
Figure 9.7 The best effort pattern can apply when reading from a JMS queue and writing to a database.
Back to the inventory update example: losing messages means that orders arrive but the application doesn’t update the inventory.
The inventory ends up with more products than it should.
Perhaps the company won’t be able to provide customers with their ordered items.
Processing orders multiple times means that the inventory runs out of products faster.
Perhaps you’ll lose orders because customers won’t buy items that aren’t virtually in stock.
Perhaps the company will ask to resupply its stock when it doesn’t need to.
All these scenarios could put the company in a world of hurt.
We could use XA to avoid both problems, but remember that we can do without XA sometimes.
To avoid losing messages, Spring synchronizes the local JMS transaction with the database transaction.
Spring commits the JMS transaction immediately after the commit of the database transaction.
Spring does the synchronization transparently as long as you use the correct settings.
This synchronization is a Spring feature; you can use it in any kind of application, and Spring Batch jobs are no exception.
Figure 9.8 shows how Spring synchronizes a local JMS transaction with a chunk transaction.
To benefit from transaction synchronization, you need to tell Spring to use a local JMS transaction with a JmsTemplate to receive messages.
Listing 9.2 sets up a JmsTemplate and a JMS item reader to use a local JMS transaction and so benefits from the automatic transaction synchronization feature.
Note that the sessionTransacted flag is set to true in the JmsTemplate, which instructs Spring to use a local JMS transaction.
Once you acknowledge a message, the JMS provider removes it from the queue.
This last acknowledgment mode is faster than the auto acknowledgment mode but can lead to duplicate messages.
When using a local JMS transaction for acknowledgment, you start a transaction in a JMS session, receive one or more messages, process the messages, and commit the transaction.
The commit tells the JMS broker to remove the messages from the queue.
Remember that it’s not only thanks to local JMS transactions that you avoid losing messages; it’s also due to the transaction synchronization that Spring performs transparently.
The JmsTemplate uses the AUTO_ACKNOWLEDGE mode by default, but don’t use this default; set the sessionTransacted flag to true to use local JMS transactions.
Remember that the acknowledgment mode and the use of a JMS transaction are exclusive: you choose one or the other (JMS brokers usually ignore the acknowledgment mode when you ask for a transacted JMS session)
Synchronizing the commit of the local JMS transaction with the database transaction commit ensures that the application acknowledges the message only if processing is successful.
Spring automatically synchronizes the local JMS transaction commit with the commit of an ongoing transaction (the chunk transaction in the context of a Spring Batch job)
Does the best effort pattern apply only to JMS? You can apply the best effort pattern to any resources that have transaction-like behavior.
Spring Batch uses the best effort pattern when writing files.
But the best effort pattern isn’t perfect, and the next subsection covers its shortcomings.
The JMS item reader reads messages, the item writer processes the chunk, and Spring Batch commits the chunk transaction.
The JMS transaction is then committed because it’s synchronized with the chunk transaction commit.
What happens if the JMS transaction commit fails because of a network failure? Remember what a JMS transaction rollback means for the JMS broker: the application says the processing of the messages failed.
The JMS broker then puts back the messages read during the transaction on the queue.
The messages are then ready to be read and processed again.
Figure 9.9 illustrates this failure scenario, where the best effort pattern shows its limitation.
The best effort pattern isn’t bulletproof because of the small window it leaves open between the commit of the two transactions.
You won’t lose messages, thanks to the best effort pattern, but you still need to deal with duplicate messages.
Let’s now see two solutions to deal with duplicate messages.
It does this by synchronizing the flush with the database commit (it’s the same when synchronizing a JMS commit with a database commit)
The flush is the file equivalent of the transaction commit in a database.
Even if such duplicate messages are rare, they can corrupt data because of repeated processing.
When you use the best effort pattern, you need to avoid processing duplicate messages.
This is easily doable, but you need some extra code in your application.
Tracking messages during processing—The tracking mechanism can be a dedicated database table that flags messages as processed.
Detecting previously processed messages and filtering them out—The application must perform this check before processing by using a tracking system.
You have everything you need to build such a tracking system in Spring Batch.
In a chunk-oriented step, the item writer processes messages by updating the database and takes care of tracking by adding a row in the tracking table.
An item processor is in charge of filtering out duplicate, already-processed messages by checking the tracking table.
Remember that an item processor can transform read items before Spring Batch passes them to an item writer, but it can also filter out items by returning null.
Figure 9.10 shows a chunk-oriented step that reads JMS messages, filters out duplicate messages, and implements processing in the writing phase.
Let’s get back to the inventory example to see how to implement the detection of duplicate messages.
The online store accumulates orders in a JMS queue, and a batch job reads the messages to update the inventory table.
A JMS message contains an Order, which itself contains a list of OrderItems.
The following listing shows the definition of the Order and OrderItem classes.
Figure 9.10 Detecting duplicate messages and filtering them out with an item processor in a chunk-oriented job.
The item writer must track whether a processor processed each message.
The best effort pattern combined with this filtering technique prevents a processor from processing duplicate messages.
You now know what kind of objects you’re dealing with.
Let’s look at the processing by implementing the corresponding item writer.
Listing 9.4 Processing and tracking orders in an item writer.
Then, the writer tracks that the order has been processed C.
To track the processing, the system uses a dedicated database table to store the order ID D.
If you don’t have access to a unique ID for your custom processing, use the JMS message ID.
This can be useful to purge the table from time to time.
You now have the first part of your mechanism to detect duplicate messages.
The second part detects redelivered messages and checks to see if the job has already processed a redelivered message.
The following listing shows the item processor code that detects and filters out duplicate messages.
When a JMS broker redelivers a message, it sets the message object’s redelivered flag to true.
You use this flag B to avoid querying the database, an optimization.
If the message isn’t a redelivery, you let it go to the writer.
In the case of a redelivered message, you check C whether you’ve already processed the message.
Listing 9.5 Detecting and filtering out duplicate messages with an item processor.
Transaction management patterns of querying the tracking table to see if it contains the order ID D.
The detection of a duplicate is simple and cheap, and duplicate messages are rare.
This solution also performs better than the equivalent using XA.
The following listing shows the relevant portion of the job configuration (we skipped the infrastructure configuration for brevity)
This configuration is typical for a chunk-oriented step, but it contains a couple of subtleties.
This flag should always be set to true for a JMS item reader.
Remember that you need the JMS message in the item processor to check the redelivered flag.
Because you want to use the best effort pattern, you use local JMS transactions with the JMS template for message acknowledgment D.
That’s it; you detect duplicate messages with your filtering item processor.
By also using the best effort pattern, you enforce atomicity in your global transaction without using XA.
This solution is straightforward to implement thanks to Spring Batch and.
Next, we see how to deal with duplicate messages without any extra code.
In the inventory update example, you want to avoid duplicate messages because you can’t afford to process messages multiple times.
You need this functionality because processing removes ordered items from the inventory.
What if processing a message multiple times is harmless? When an application can apply an operation multiple times without changing the result, we say it’s idempotent.
It means that we don’t care about duplicate messages! Always think about idempotency when designing a system: idempotent operations can make a system much simpler and more robust.
Let’s see an example of an idempotent operation in the online store application.
The online store keeps track of the state of orders to inform customers of their orders.
A batch job reads messages from the shipped order queue and updates the online store database accordingly.
The processing is simple: it consists only of setting the shipped flag to true.
The following listing shows the item writer in charge of updating shipped orders.
Figure 9.11 When performing an idempotent operation on the reception of a message, there’s no need to detect duplicate messages.
The best effort pattern combined with an idempotent operation is an acceptable solution.
You can kiss goodbye any tracking system and filtering item processors.
You now know how to implement global transactions in Spring Batch.
A use case must meet specific conditions to apply these patterns successfully, and it’s up to you to design your system in a manner suitable to apply these patterns.
We saw how the shared transaction resource pattern applies when a transaction spans two schemas that belong to the same database instance.
You must detect duplicate messages by using a tracking system if the message processing isn’t idempotent.
You don’t have to worry if the processing is idempotent.
Transaction management in batch application holds no secrets for you anymore!
Transaction management is a key part of job robustness and reliability.
Because errors happen, you need to know how Spring Batch handles transactions, figure out when a failure can corrupt data, and learn to use appropriate settings.
Remember the following about transaction management in batch applications, and don’t hesitate to go back to the corresponding sections for more details:
A tasklet is transactional; Spring Batch creates and commits a transaction for each chunk in a chunkoriented step.
When a batch application interacts with more than one transactional resource and these interactions must be globally coordinated, use JTA or one of the patterns we’ve discussed.
The alternative techniques for handling global transactions without JTA work only in specific contexts and can add extra logic to the application.
With the previous chapter on bulletproofing jobs and this chapter on transaction management, you now know the techniques to write truly bulletproof jobs.
In a chunkoriented step, Spring Batch can skip exceptions to avoid failing a whole step.
When the step reaches the skip limit, Spring Batch fails the step.
Does Spring Batch cause the failure of the whole job? By default, yes, but you can override this behavior.
Instead of failing the whole job immediately, for example, you can execute a tasklet to create and email a report on the execution of the job to an operator.
If you’re interested in discovering how you can choose between different paths for steps in a Spring Batch job, please continue on to the next chapter, which covers how Spring Batch handles the execution of steps inside a job.
The previous chapters provide enough information to write complete batch applications using Spring Batch.
This final part guides you through advanced techniques and scenarios to make your batch architecture even more powerful.
If your batch jobs are made of complex, nonlinear flows of steps, chapter 10 is definitely for you.
It shows you how to decide which step to execute next when a step ends.
It also explains how to transmit data between steps and interact with a Spring Batch execution context.
Chapter 11 takes a Spring Batch job on a tour around the world of enterprise integration.
Don’t think batch jobs are isolated pieces of software running alone at night.
In this chapter, a batch job meets exciting technologies like REST and Spring Integration to cover a real-world enterprise integration scenario with Spring technologies.
Chapter 13 presents techniques to make your job execution scale.
You’ll discover which strategies Spring Batch provides to parallelize job executions on multiple threads and even on multiple nodes, thanks to JMS and Spring Integration.
You can test pretty much everything in a Spring Batch job: readers, writers, converters, listeners, and so on.
This chapter shows you why and how to test your Spring Batch jobs using unit and integration tests.
Following part 3 are two appendixes: one to set up your development with the SpringSource Tool Suite and Maven, and another to configure the Spring Batch Admin web console.
These chapters also covered error handling during processing by skipping errors or retrying operations transparently.
A job consists of steps, and execution refers to the sequence of steps that run when a job starts.
In complex jobs, execution can take multiple paths, and the sequence is no longer linear.
Figure 10.1 shows the structure of a simple, linear job and a more complex, nonlinear job.
Because batch jobs must run automatically, without human intervention, we need a way to configure a step sequence so that a job knows which step to execute next.
In an ideal world, steps are independent of each other: they don’t need to share data at runtime.
This happens when the execution of a first step has no impact on Controlling execution.
A job is simpler to implement when it’s made of independent steps.
You can easily test the steps independently, and you can even think about reusing them in another job.
Having only independent steps isn’t always possible: a time will come when a step needs data computed in a previous step.
If your jobs require complex flows or the need to share data between steps, this chapter helps you to fulfill these requirements.
You’re on your way to learning how to master complex execution scenarios in your batch applications.
The next section gives a detailed overview of the features covered in this chapter.
We base this overview on a real-world scenario: an advanced version of the import products job for our online store application.
In chapter 1, we introduced our online store example application, whose main job is to import products into a database.
The job is simple: a first step decompresses a ZIP archive, and a chunk-oriented step reads the extracted file in order to import it in the database.
Even though the job is simple, it’s realistic; but not all jobs are that simple.
Figure 10.2 shows an advanced version of our import products job.
This new version of the job is more demanding in terms of features: it has a more complex flow, one step requires data computed in a previous step, and the job execution can end after the first step if there’s no ZIP to download (whereas a job usually completes with the end of the last step)
Table 10.1 gives a detailed view of this new version of the import products job and lists the corresponding Spring Batch features used to fulfill its requirements.
Job A, on the left, has a simple linear flow.
Job B, on the right, isn’t linear because there are multiple execution paths.
That’s many new requirements! Now, those are common requirements in batch applications, and, good news, Spring Batch has everything in its toolbox to fulfill them.
This chapter focuses on job execution, so don’t be surprised if you don’t find details on step implementations (tasklets, readers, writers, and so on)
Now that you have an overview of the new features of our advanced version of the import products jobs, let’s start with creating nonlinear flows.
Table 10.1 Requirements and features of the new version of the import products job.
Nonlinear flow If the job skips lines in the read or write step or if the step fails, a report must be generated by a dedicated step.
Sharing data A step extracts metadata, and another step needs it to track the import.
Reuse of configuration Part of the job configuration should be reusable as-is by other jobs.
Early completion If the job didn’t download an archive, the job should complete immediately.
Figure 10.2 In this advanced version of the import products job, the flow of steps isn’t linear anymore and requires more complex features: job execution can end immediately after the first step, steps need to share data, and part of the configuration must be reusable by other jobs.
Not all jobs are linear: their steps don’t always execute one after the other in a simple sequence.
Jobs can take multiple paths: depending on the result of a step, you can choose to execute one step or another.
In our new version of the import products job, the job executes an optional step to generate a report if something goes wrong in the read-write step.
This section covers how Spring Batch lets you configure such nonlinear jobs.
By the end of this section, you’ll know how to escape the simple linear job scenario and create complex flows for your jobs.
Until now, we’ve focused on jobs with linear flows: the steps execute one after another in a linear fashion.
For linear flows, only the next attribute in the step element needs to be set and must indicate which step is next, as shown in the following snippet:
Note that the last step in a job doesn’t need a next attribute: step completion indicates the end of the job execution.
How would we define a nonlinear flow? Imagine that you don’t want the job to fail when the read-write step fails; instead, you want to generate a report and then execute a cleanup step.
To configure a nonlinear flow like the one in figure 10.3, use the nested next element in the step.
The condition of the transition is set with the attribute on.
The to attribute points to the next step to execute, as shown in the following listing.
If the read-write step fails, the execution shouldn’t end; instead, the job should generate a report.
For all other cases, the execution proceeds directly to the cleanup step.
What is the value of the on attribute in this example? It matches the exit status of the step (we’ll see more on the exit status concept in the upcoming subsection)
You can use exact values in the on attribute and special characters to define patterns.
Table 10.2 shows examples of exact values and the special characters Spring Batch accepts.
Note that Spring Batch is smart enough to order transitions from the most to the least specific automatically.
This means the order of the next tags in the configuration doesn’t matter; you can define transitions with wildcards first (less specific) and transitions with exact values last (more specific)
If the * matches the FAILED exit status (because there’s no more specific match), the next step is executed even if the current step fails.
Perhaps this isn’t what you want; you may want to fail the job execution when a step fails.
When using conditional transitions, you must handle failed steps yourself.
Refer to section 10.5 to see how to cause a job execution to fail in a transition decision.
You can now configure conditional execution by matching the exit status of a step with the next step to execute.
What exactly is the exit status, and what kind of value can it take?
Matches exactly one character C?T (matches CAT but not COUNT)
Spring Batch uses two concepts to represent the status of an execution: the batch status and the exit status.
Both step execution and job execution have their own batch and exit statuses property.
The batch status describes the status of the execution of a job or a step.
The exit status represents the status of the job/step once the execution is finished.
The difference between both statuses isn’t obvious, mainly because Spring Batch defines both as a status.
Table 10.3 lists other differences between the batch status and exit status.
To see the values a batch status can take, look at the BatchStatus enumeration; example values are COMPLETED, STARTED, FAILED, and STOPPED.
ExitStatus values roughly match those of the BatchStatus, but because it’s a class, you can define your own instances.
What’s the point of defining your own ExitStatus? You can define your own exit statuses to create a set of options to manage your job flows.
Remember that a transition bases its decision on the value of an exit status.
For complex conditional flows, you can use your own exit status instances to choose where to go next.
You aren’t limited to predefined values, and you can use exit statuses like COMPLETED WITH SKIPS, which carry more semantics than COMPLETED.
Let’s go back to the import products job and imagine you want to generate a report if the read-write step skips input lines.
You configure the transition decision using the next element and the on/to attributes, but there’s no ready-to-use exit status to express that the step skipped some items.
Wouldn’t it be nice to configure the transition as follows?
A batch status is persisted in the batch metadata as the overall status of a job or step execution.
An exit status is persisted in the batch metadata as the exit status of a job or step execution.
How can you get a COMPLETED WITH SKIPS exit status at the end of this step? The solution is to execute code immediately after the execution of the step.
This code checks the number of skipped items and returns the appropriate custom exit status.
You have two ways to do this; the first is by using a step execution listener.
Spring Batch lets the developer plug in listeners to react to events during job execution.
One of these listeners is the step execution listener: Spring Batch calls it before and after the execution of a step.
The after-execution callback is interesting in our case because it has access to the step execution object and can decide which exit status to return.
The job then uses this custom exit status to decide which step to execute next.
The following listing shows the step execution listener that chooses which exit status to return.
Listing 10.2 Choosing the exit status for a step with a step execution listener.
Figure 10.4 A nonlinear flow using a custom exit status.
Custom exit statuses carry more semantics than standard exit statuses (COMPLETED, FAILED, and so on), which is helpful in making complex flow decisions.
Figure 10.5 A step execution listener can change the exit status of a step.
The job can then use the exit status for the transition decision to the next step.
Because the listener has access to the StepExecution, it knows if Spring Batch skipped items during processing and decides which exit status to return.
Note that the listener returns the default exit status if there are no skipped items.
The following listing shows the corresponding XML configuration that plugs in the listener and specifies the transitions.
Listing 10.3 Conditional flow with custom exit status using a step execution listener.
That’s it! With a step execution listener, you can cause a step execution to return a custom exit status.
Using this exit status, you can configure which step the job should execute next.
Using a step execution listener is one solution to using custom exit statuses to control job flow, but Spring Batch also provides a dedicated component to determine the flow of a job: the job execution decider.
Contrary to a step execution listener, a job execution decider isn’t part of a step; it’s a dedicated component in the job flow, as illustrated in figure 10.6
Let’s look at our flow requirement to illustrate the use of a job execution decider: you want the job to transition to generate a report step if the read-write step skipped items.
Listing 10.4 Controlling job flow with a job execution decider.
Figure 10.6 A job execution decider is registered after a step to modify the step’s exit status.
The job then uses the exit status in its transition decision.
This data structure is roughly equivalent to a plain ExitStatus.
You register a job execution decider as a standalone component, in the flow of the job, as shown in the following listing.
Listing 10.5 Configuring a conditional flow with a job execution listener.
Listing 10.5 shows that a job execution decider has its own place in the definition of the flow.
The decider has an ID, and the step transitions to it by using this ID in its next attribute.
You achieve the same goal with a job execution decider as with a step execution listener: you set the exit status by querying the execution of the previous step.
By using transition decisions in the XML configuration, you can define which step to execute next.
Because you have two ways of achieving the same goal, let’s define some guidelines to help you decide when to use each solution.
Let’s be frank: the choice you’re about to make between a step execution listener and a job execution decider for your conditional flow won’t change your life.
Here are some considerations to help you choose the appropriate solution depending on your context:
Using the step execution listener affects the batch metadata of the step execution.
If you return a custom exit status in the step execution listener, Spring Batch persists it in the batch metadata.
Spring Batch doesn’t track anything about a job execution decider in the batch metadata.
The usefulness of a job execution decider is obvious and makes the configuration more readable.
As soon as you see a job execution decider in a job configuration, you know that there’s logic defining a conditional flow.
This isn’t as obvious with a step execution listener, which you can use for other purposes.
By using a job execution decider over a step execution listener, you follow the principle of least astonishment.
You know now how to control the flow of steps inside a job.
A job can base its transition decisions on the current execution, like skipped items in the previous step, or on anything available in the execution context.
The next section expands on job executions by exploring how steps can share data, so that a step can access and use data computed in any previous step.
In a Spring Batch job, each step has its own task to fulfill.
To know what it must do, a step can use job parameters.
For example, the decompress step uses a ZIP archive location parameter to know which file to decompress.
In this case, you know the required input for the step when you launch the job.
Sometimes, you can’t know the inputs for a step when the job starts because a previous step must compute these inputs.
In such circumstances, the calculating step and the receiving step must find a way to share data.
Imagine that the ZIP archive processed by the job contains not only a flat file with the products to import but also a text file that provides information about the import: the ID and date of the import, a list of input files in the archive, and a digest for each archived file used to validate it.
Figure 10.7 shows the job flow and the information that the two steps need to share.
The second step checks the integrity of the decompressed files using the import metadata.
Notice the track import step after the read-write step: it stores in the database confirmation that the job processed this particular import.
To do so, the track import step needs the import ID, which is read previously by the verify step.
This import ID is the data you want to share between steps.
We cover different techniques to share this import ID between the two steps.
This example is simple, but the principles remain identical with more complex data structures.
If you can’t make them independent, use the techniques presented here.
You should consider data sharing as a fallback pattern when you can’t build independent steps.
You can share data in batch applications in many ways, and you can implement them all using Spring Batch.
For example, you can use a database as a shared repository between steps: a step stores shared data in the database, and the receiving step reads them from the database.
Such a solution is straightforward to implement with Spring Batch using custom tasklets or chunk-oriented steps, but that’s not what we show here.
This section covers techniques that leverage special features of Spring Batch and of the Spring Framework.
Reads Figure 10.7 The verify step checks the integrity of the extracted files and extracts import metadata needed by the track import step.
Let’s start with using the Spring Batch execution context to share data.
You must know by now that Spring Batch maintains metadata on job executions.
This batch metadata allows for features like restart because the data is stored permanently in a database.
The batch developer can also use part of this batch metadata to store a job’s own data.
As soon as your application gains access to the execution context from a job artifact (tasklet, reader, processor, writer, or listener), you can use it to share data.
What exactly is the execution context? The class ExecutionContext represents the execution context, which acts as a map (of key-value pairs) but with an API better suited to batch applications.
Here’s an example of writing and reading data to and from ExecutionContext:
An execution context exists only as part of a job execution, and there are different kinds of execution contexts.
Spring Batch provides two kinds of execution contexts: the job execution context and the step execution context.
Figure 10.8 illustrates both kinds of execution contexts in a job execution.
Use a Spring Batch execution context as a container for user data.
A step writes to the execution context; then another step reads from the execution context.
A first step sets values in the holder; another step reads values from the holder.
Figure 10.8 A job execution has its own execution context.
Within a job execution, each step also has its own execution context.
How do you gain access to an execution context? You need a reference to the corresponding execution: a JobExecution if you want to access a job execution context, or a StepExecution if want to access a step execution context.
Nearly all Spring Batch artifacts can easily access either a JobExecution or a StepExecution.
The unlucky artifacts without this access are the item reader, processor, and writer.
If you need access to an execution context from an item reader, processor, or writer, you can implement a listener interface that provides more insight into the execution, such as an ItemStream.
The following listing shows an ItemReader that implements the ItemStream interface to gain access to the execution context.
Now that you have enough background information on execution contexts, let’s see the first technique used to exchange data between steps by using an execution context.
You can use the job execution context to share data between steps.
A first step writes data to the job execution context, and another step reads the data from the execution context.
In our example, the verify tasklet not only verifies the integrity of the decompressed files but also extracts the import metadata and stores the import ID in the job.
Listing 10.6 Implementing a listener interface to access the execution context.
A first step writes data in the job execution context for a subsequent step to read.
The following listing shows the verify tasklet code (setter methods are omitted for brevity)
You can see in listing 10.7 that the job execution context is accessible through the ChunkContext parameter of the tasklet’s execute method.
Try to use this kind of delegation in your tasklets because it avoids embedding too much business logic in the tasklet (a batch artifact) and promotes reusability of your business services.
This delegation also makes the tasklet easier to test because you can use a mock object for the BatchService in integration tests.
Listing 10.7 Writing data in the job execution context from a tasklet.
Figure 10.10 The succession of calls needed to access the job execution context from a tasklet.
Once the import ID is available in the job execution context, the track import tasklet can read and use it, as shown in the following listing.
When using the job execution context as a container to share data, the writing and reading components must agree on a key used to access the context map.
In our example, the key is the hardcoded String "importId", used mainly to make the code more readable and easier to understand.
The key could be configurable as a String property in both tasklets and set in the XML configuration.
Listing 10.8 Reading data from the job execution context in a tasklet.
Figure 10.11 A batch artifact such as a tasklet shouldn’t embed too much business logic.
Such delegation allows for better reusability of business logic and makes business components easier to test because they don’t depend on the batch environment.
Whatever way the job configures the key, be careful that another component using the same key for another purpose doesn’t erase the value.
Remember that the job execution context is global to the job: all batch artifacts can access it.
Giving steps access to the job execution context directly couples them quite tightly.
The next section covers a technique also based on the job execution context but provides more flexibility at the price of being more complex to configure.
The technique we show next is less straightforward than writing to and reading from the job execution context.
It requires that a step writes data in its own execution context and that a listener promotes the data to the job execution context, such that the data can be accessible to anyone.
Figure 10.12 illustrates this process; notice that a step listener promotes the data transparently.
The solution used to promote data from a step’s scope to a job’s scope consists of two parts: you programmatically make the data available in the step execution context and promote this data to the job execution context through configuration.
This solution looks complex, but it confines the data exposure to the step and makes the choice of exposing the data a configuration choice.
The communicating steps aren’t as tightly coupled with each other as in the pure job execution context solution.
In some cases, you need to share data between steps, and in other cases, you don’t need to share data.
Figure 10.12 Sharing data by writing into the step execution context and promoting data to the job execution context.
Going back to our example, the writing tasklet now writes in its own execution context, as shown in the following listing.
All you need to do is register the step listener on the writing step and set which key(s) you want to promote.
The following shows how to use the promotion step listener.
Listing 10.9 Writing data in the step execution context from a tasklet.
Listing 10.10 Promoting data from the step to the job execution context.
The receiving tasklet reads data from the job execution context, so it sees the data that the listener promoted.
Configuration drives the promotion approach more than the solution relying on purely using a job execution context.
Promotion provides more flexibility but needs the configuration of a listener.
The next solution is even more configuration driven and leaves the code less dependent on the execution context.
This approach consists of writing in the job execution context and then referring to the data from the XML configuration, using late binding.
The writing tasklet writes in the job execution context, so it remains the same as in listing 10.7
The receiving tasklet doesn’t read from any execution context: it reads its own property, as shown in the following listing.
The good news about this new version of the receiving tasklet is that it doesn’t depend on the Spring Batch runtime anymore.
This makes the tasklet easier to test: you don’t have to recreate the execution context; all you need to do is set the tasklet property.
This is a benefit of using dependency injection over a lookup mechanism.
Speaking of dependency injection, the responsibility of setting the property correctly now belongs to Spring, with help from the step scope and SpEL:
Listing 10.11 A tasklet reading data from its own property.
This implicit variable is available only for a step-scoped Spring bean running in a step! Spring Batch creates the instance at the last moment, when the step is about to run.
This solution, combining the job execution context to store data and SpEL, is elegant and frees part of the application code from a reference to the runtime environment.
As long as you can use late binding in a step scope, you should prefer this solution to the previous two.
But the step scope isn’t always available: imagine you want to share data between a step and a job execution decider (for a conditional flow)
The step scope isn’t available in the decider because it’s not part of a step.
You would need to use the job execution context in the decider code.
Okay, we’re not done yet with sharing data techniques, but we’re getting close to the end.
The next section covers yet another category of techniques to share data in a job.
It’s based on the definition of holder classes whose instances are used as Spring beans.
These techniques are more type-safe than the ones using execution contexts, but they require writing dedicated classes and rely on Spring for dependency injection.
Figure 10.13 illustrates the concept of a holder bean shared between steps.
The holder is empty when the Spring application context is created, a step writes data to the holder, and another step reads the data.
A job should clean the holder it uses once it’s done using it.
This warning doesn’t apply if you launch jobs from the command line, because you’ll have a different instance of the holder for each execution.
Remember the use case on sharing data: you extract some metadata from the verify step, and the job must make this metadata available to the track step.
Figure 10.13 Using a Spring bean as a holder to share data.
Spring injects the holder as a dependency into the batch artifacts that want to share data.
An artifact then writes data in the holder, and the receiving artifact reads the data from the holder.
Sharing data between steps the ImportMetadata class represents the metadata.
The ImportMetadata class contains only properties and corresponding getter and setter methods.
One of these properties is the one you’re interested in: the import ID.
The following snippet shows the holder code for the import metadata:
You now know enough to learn a first technique based on the use of a holder to share data between steps.
The second technique uses advanced features like SpEL and late binding.
When using a holder, the writing and reading components both refer to the holder.
Note that the listing includes the normally omitted setter methods to highlight their importance: Spring uses injection to set their values.
Now both tasklets depend only on the holder and no longer on the Spring Batch runtime.
The following listing shows the straightforward required configuration: it declares the beans and wires them together.
It has a couple of disadvantages: you need to create an extra holder class, and the batch artifacts depend on it.
The holder technique also introduces the risk of concurrent access to the holder (an execution could see the values from the previous execution)
Let’s see a variation on the holder technique, which gets rid of the dependency on the holder in the receiving tasklet.
Using this pattern, the writing tasklet fills in the holder as usual, but Spring creates and injects the receiving tasklet using late binding and SpEL.
By doing so, the receiving tasklet no longer depends on the holder: it depends on a String property, which is set by Spring when the tasklet is created, before its execution.
The tasklet is less tightly coupled to the sharing data pattern than it is using the previous technique because it depends only on a primitive type, not on a holder class.
The writing tasklet remains the same as in listing 10.12; it depends only on the holder.
The receiving tasklet reads the import ID from a property, like in listing 10.11
This one is tricky, so let’s study what is happening:
Spring creates beans, but the creation of the verify tasklet is deferred because it’s a step-scoped bean.
The verify tasklet is called and writes the import ID to the holder.
The job calls the track tasklet, and it uses the value of the importId property.
The key part of this technique is the creation of the step-scoped track tasklet: Spring initializes it at the last moment, once the holder already contains the import ID.
This last technique frees the receiving tasklet from any dependency on the holder.
This ends our coverage of patterns used to share data in a job.
We distinguish two main pattern categories: the first relies on Spring Batch’s notion of the execution context.
This works great but couples your batch components with the runtime environment.
The second group of patterns introduces data holder objects that batch artifacts use to share data.
This technique is more type-safe but requires the creation of dedicated holder classes.
Both techniques are roughly equivalent; choosing one over the other is a matter of preference.
Our preference goes to holders when we have enough energy to write holder classes and configure Spring beans.
We tend to use execution contexts when we’re tired or feel lazy.
It covers how to reuse parts of your flow configuration across jobs.
This feature comes in handy when some of your jobs share the same sequence of steps.
You can configure these common steps once and reuse the configuration across other jobs.
If your job configuration contains a lot of duplicate code, you should definitely read the next section to find ways to get rid of it!
Spring Batch is part of this quest when it comes to reusing batch components and job configurations.
Indeed, portions of a job configuration can be generic and reused across jobs.
In the advanced version of our import products job, before reading and writing products from a flat file, preliminary steps prepare the ground for the import: a first step downloads the ZIP archive, a second step decompresses the archive, and a third step checks the integrity of the decompressed files.
These preliminary steps are generic and could be useful to other jobs: it’s common for a batch job to import data from a file it has downloaded.
Figure 10.14 A flow of steps can be defined as a standalone entity so that other jobs can reuse it.
Jobs can then use it if they need to download, decompress, and verify an input file.
You don’t need to bother preparing a ZIP archive for the test!
You can push further the idea of externalized flow by reusing a whole job.
The following listing shows the preliminary steps of our advanced import job set up in a standalone job.
The core import job then refers to this job definition.
A job parameters extractor implements a translation strategy between the owning step execution and the job parameters to pass to the subjob.
The externalization of flows and jobs promotes the reusability of flow code and configurations.
Jobs return exit statuses, and the scheduler knows which job to execute next by using a mapping between exit statuses and jobs.
When using externalized jobs in a Spring Batch system, you orchestrate jobs by reusing job configurations.
With the externalization of flows and jobs, you now have a new way to orchestrate job executions.
The next section shows how to get even more control over a job execution by allowing the job execution to stop after a step.
If you want to learn how to control your jobs and to decide if a job should fail, stop, or complete, keep on reading!
Until now, our jobs end when you reach the last step, the processing throws an exception, or we stop the execution by sending an interrupt signal.
Table 10.5 summarizes these three alternatives, which are the default mechanisms to end a job execution.
The defaults shown in table 10.5 make sense, but a job flow isn’t always that simple.
An exception thrown in a step doesn’t always mean that the whole job execution must.
Interrupt signal STOPPED Interrupt signal sent from an admin console like Spring Batch Admin or programmatically.
Perhaps the job should take another path (section 10.2 shows how to do that with conditional flows)
Perhaps the step failure isn’t critical, so the application can consider the job execution complete.
In the advanced version of the import products job, the first step consists of downloading a ZIP archive.
To decide on the status of a job execution after the end of a step, Spring Batch provides three XML elements.
You use them, along with transition decisions, inside a step or decision element.
The end, fail, and stop elements have some special attributes of their own, but they all require an on attribute.
The value of the on attribute matches the value of the step exit status.
The following listing shows how to use the stop and fail elements after the download step in the import products job.
You can use an optional exit-code attribute to customize the ExitCode.
You can use an optional exit-code attribute to customize the ExitCode.
Spring Batch can restart the job instance after a manual operation, for instance.
Requires a restart attribute to specify on which step the execution should resume.
Listing 10.17 Choosing how to end a job after a step execution.
Figure 10.15 When a step ends, Spring Batch lets you choose if you want to complete, fail, or stop the job execution.
In the case of the import products job, if the job didn’t download a ZIP archive, it makes sense to consider the job execution complete.
Listing 10.17 shows that you choose to complete the job if there’s no archive, move on to the next step if the archive exists, and fail the job in all other cases.
These decisions are based on custom exit statuses set by a step listener after the end of the step.
You can use the end, fail, and stop elements for any exit statuses, and you could have stuck to exit statuses like COMPLETED and FAILED.
The end, fail, and stop elements bring a lot of flexibility to job flows.
Combined with conditional flows, which help you go beyond linear jobs, you can create job scenarios with complex step sequences.
You now know everything you need to create, control, and configure job execution flows; let’s wrap everything up.
This chapter covered patterns and techniques used to control precisely the execution of the steps inside your jobs.
Always keep things simple, but follow these guidelines when facing complex execution scenarios:
Control the sequence of steps using conditional flows—Spring Batch lets you choose declaratively which step to execute next on the basis of the exit status of the previous step.
Define custom exit statuses for complex flow decisions when you reach the limit of default exit statuses (COMPLETED, FAILED, and so on)
Use your own exit statuses with a step execution listener or a job execution decider.
Use the execution context or holder beans to exchange data between steps— when a step needs data computed by a previous step, use the execution context to make the data accessible between steps.
If you don’t want to depend on the batch runtime, inject holder beans inside the batch artifacts that need to share data.
You can externalize definitions of flows or of whole jobs and reuse them across jobs.
End the execution of a job declaratively after any step—use the end, fail, and stop elements to end a job with an appropriate batch status at the end of a step.
With all this new knowledge, you’re now ready to start one of the most exciting parts of this book.
Chapter 11 shows how Spring Batch fits in a real-world integration scenario.
Be prepared to leverage your Spring Batch skills but also to discover new content on enterprise integration styles, REST, and technologies like Spring Integration.
An example of Spring Batch helping applications communicate is a batch job processing files from one application and writing data into the database of another application.
The term enterprise integration refers to such communication techniques between applications.
Because batch processes are often central to enterprise integration projects, this chapter is dedicated to explaining how to use Spring Batch and other Spring-related technologies in such projects.
The online store application we use in this book is a typical enterprise integration project: the application imports product data from various systems (ACME and its partners) so that customers can browse and purchase products through the online store web interface.
Figure 11.1 illustrates how enterprise integration fits between the online store application and other systems.
What is enterprise integration? Enterprise integration is a broad topic, and trying to cover all of it would require a whole book, much less one chapter.
Instead of thinly covering as much as possible, this chapter uses a practical approach and implements a realistic enterprise integration scenario for the online store application.
By the end of this chapter, you’ll have an overview of enterprise integration and techniques used to tackle its challenges.
These techniques rely on technologies like Representational State Transfer (REST), messaging, and batch processes, and use frameworks like Spring MVC, Spring Integration, and Spring Batch.
All of the chapters in this book focus on Spring Batch, but this chapter isn’t only about Spring Batch: it covers other Spring frameworks.
The point is to show you how Spring Batch combines with other frameworks as a key component in real-world enterprise projects.
This section gives a more comprehensive and practical definition of enterprise integration, along with some use case examples.
This section also covers integration styles that deal with enterprise integration challenges and we’ll see what roles Spring Batch can play in this world.
Enterprise integration is a set of techniques and practices for connecting enterprise applications.
Most of the time, these applications don’t share the same data, don’t use the same technologies internally, or don’t even run on the same network.
Despite these differences, these applications need to communicate to fulfill business requirements.
It chooses a distinct application for each of its business functions and allows picking a best-of-breed solution for each situation (homemade applications and ready-to-use products)
Figure 11.1 The online store application uses enterprise integration techniques to import product data from other systems.
Let’s look at a couple of typical enterprise integration projects.
The first type of integration project we consider is the information portal (as named in the EIP book)
Imagine a user wants to check the status of an order.
This request can access data from multiple applications: the order management application, the customer directory, and the inventory application.
An information portal gathers all this information in one place so that the end user doesn’t have to access each application and track the necessary data spread over each application.
Applications often use the same data (like customers) but use their own data stores because they need a specific representation of the data.
For example, a shipping application doesn’t need the same customer data as a billing application.
The book focuses on solutions based on messaging and describes message-oriented patterns for enterprise integration.
We strongly recommend reading this book for anyone involved in enterprise integration projects.
We refer to it as the EIP book in this chapter.
In this example, the customer directory holds the reference data for anything related to customers.
The system replicates data from the customer directory to the shipping and billing data stores.
Figure 11.2 An information portal is a typical enterprise integration project.
It gathers data from several applications and makes it available in one place to the end user.
What is enterprise integration? Considering these two typical enterprise integration patterns, the information portal and data replication, presents us with the following challenges:
The list could go on; no two enterprise integration projects are the same, and you don’t know what applications you’ll need to integrate with your system.
Now that you have a better understanding of enterprise integration and the challenges it presents, let’s see some ways to deal with these issues.
We use four main approaches to solve enterprise integration challenges.
Per the EIP book, we refer to these as styles:
Each specific situation dictates which one to choose or even which combination of styles to use.
Table 11.1 lists some of the pros and cons of each integration style.
Table 11.2 lists the Spring technologies to use for each integration style.
Shared database Simple, transactional Can be slow, schema hard to evolve.
Remote procedure invocation Simple, can be fast Not interoperable, synchronous, tightly coupled.
Table 11.2 shows that Spring Batch matches the file transfer style.
Since this is a book about Spring Batch, let’s talk a bit more about this so we can understand how Spring Batch fits in enterprise integration.
By now, you know that Spring Batch is good at reading and writing large amounts of data in an efficient and reliable way.
You also know that Spring Batch has sophisticated support for files of any kind (text files, XML files)
This makes Spring Batch a perfect tool for file transfer integration.
Spring Batch can integrate applications using file transfer, database access, and much more.
This web application sells products from the ACME Corporation and its partners.
This is a typical example of file transfer integration, illustrated by figure 11.4
Unfortunately, the pace of imports isn’t fast enough for some partners: they would like to see their products available for sale as quickly as possible.
One solution is to accept small-to-midsized imports from clients and import them as quickly as possible (an hour between the submission and import is acceptable)
Figure 11.4 The online store application uses transfer file integration to synchronize its catalog with partner catalogs.
The corresponding batch process is the basis of our enterprise integration scenario.
This figure shows Spring Batch as one of many components in ACME’s enterprise integration scenario.
A single framework couldn’t fulfill all the tasks in this scenario by itself.
You need components other than Spring Batch to implement this scenario.
The Spring MVC web framework will help you receive import requests, and you’ll use techniques related to messaging integration to interact with the file system using Spring Integration.
Table 11.3 matches the use case tasks with Spring technologies.
As you can see from table 11.3, Spring Integration plays an important role in our scenario.
Spring Integration deals with messaging enterprise integration and deserves its own section, as described next.
Figure 11.5 A client submits products to import over HTTP.
Clients update the store catalog more frequently, but the application controls the frequency.
Spring Integration is a framework to facilitate messaging enterprise integration.
Spring Integration provides support for the enterprise integration patterns described in the EIP book and makes using these patterns easier for any Spring-based application.
Message-based integration is difficult, so let’s see how Spring Integration helps.
The Spring Integration framework aims at applying the Spring programming model to message-based enterprise integration applications.
Applications that communicate with messages are more decoupled and therefore easier to integrate.
For example, an ordering application doesn’t directly communicate with a billing application: it sends a message for an order, and the billing application handles the message when it sees fit.
Don’t confuse Spring Integration with an enterprise service bus (ESB)
Spring Integration is a framework that you embed in your applications, not a container that runs your application code.
Figure 11.6 illustrates how you can use Spring Integration in your applications.
Spring Integration provides support for enterprise integration patterns and simplifies message handling: routing, transforming, splitting, and aggregating are all operations you can easily perform on messages with Spring Integration.
Note that the Spring Integration messaging infrastructure is agnostic to any backing technology: messages can come from any source (a file dropped on the file system or a JMS queue,
It can be embedded in any Spring application and can integrate with external systems using built-in adapters for various technologies, such as HTTP, JMS, file systems, and RMI.
Spring Integration, a toolbox for enterprise integration for example), be routed through the application thanks to the messaging bus, and then be sent to any target.
This makes applications that use Spring Integration decoupled from their surrounding environment.
If the usefulness of Spring Integration is still in doubt, don’t worry: we’ll dive into some examples soon.
Because enterprise integration patterns are mainly message driven and Spring Integration supports messaging, you might think Spring Integration and Spring Batch wouldn’t need each other.
After all, the Spring Batch integration style is file transfer, not messaging.
But these two frameworks are complementary, and Spring developers who deal with enterprise integration should have both in their toolbox.
If frameworks had astrological signs, Spring Integration and Spring Batch would be compatible!
Spring Integration is complementary to Spring Batch: it adds the messaging integration style to the Spring Batch file transfer style.
An enterprise integration solution can use multiple integration styles, making Spring Integration and Spring Batch appealing solutions for the two most popular integration styles: messaging and file transfer.
Developers can implement solutions using these two frameworks, but Spring Batch can also use Spring Integration for internal tasks like retry and distributing chunks (in a step) on multiple nodes.
See chapter 13 to learn how to scale batch processes using these techniques.
The remainder of this chapter addresses the integration use case introduced in section 11.2.1, with Spring Integration playing an important role in creating a solution.
Because Spring Integration is a newcomer to this book, we give you a quick-start guide next.
Let’s see how to use the Spring Integration messaging system to launch Spring Batch jobs.
The goal of this quick-start is twofold: discovering Spring Integration and implementing a component to use in our use case.
The principle of the job launcher is simple: allow wrapping job launch requests in Spring Integration messages.
Such messages then trigger the launch of Spring Batch jobs, as shown in figure 11.7
Because Spring Integration provides adapters for many message sources (HTTP, file system, JMS, and so on), you can launch jobs using various technologies.
Let’s start by defining a job launch request with a Java class.
To launch a Spring Batch job, you need the name of the job and some (optional) job parameters.
The following listing shows our custom Java representation of a job launch request.
It doesn’t depend on the Spring Batch API: no one using the JobLaunchRequest class will know that you’re using Spring Batch at the end of the processing chain.
You now need to write the code to launch a Spring Batch job from a JobLaunchRequest.
The following listing shows the Java class in charge of adapting a JobLaunchRequest instance to the Spring Batch launching API (we elided Java imports for brevity)
Figure 11.8 illustrates the interaction of the job launching message handler with Spring Batch’s job registry and job launcher.
You now have the necessary classes to launch Spring Batch jobs in a generic manner.
Let’s create a simple job to illustrate our quick-start with Spring Integration.
Our sample job contains only a Tasklet that echoes its parameters to the console.
This job is simple but enough to make our Spring Integration quick-start complete.
Figure 11.8 The job launching message handler receives job launch request messages and uses Spring Batch’s job launcher to effectively launch jobs.
It retrieves job beans from the job registry— an infrastructure bean that Spring Batch provides.
Let’s now see the configuration of Spring Integration for this Tasklet.
For brevity’s sake, we elided the configuration of the batch infrastructure: the job launcher, job repository, and job registry.
In listing 11.4, you first define a job-requests message channel.
You use this channel to send messages containing JobLaunchRequest objects.
Spring Integration takes care of routing these messages to the service activator you defined.
Note that the Spring Integration service activator is two-way: it passes incoming messages to the application service and sends the object returned by the application service to an output channel.
This is because you use Java code to send messages.
The Spring Integration message bus runs in a plain Spring application context.
You only need to bootstrap a Spring application context, get the inbound message channel bean, and send messages using it, as shown in the following listing.
Figure 11.9 The flow of messages represented with enterprise integration pattern icons.
Job launch requests are sent and received by the service activator, which unwraps them from messages and calls the job launching message handler.
The service activator retrieves the job executions and sends them on a dedicated channel, where Spring Batch outputs them to the console.
The framework includes a namespace to define generic messaging components (channel, service activator, and so on) and namespaces for the different kinds of adapters (file system, JMS, RMI, and so forth)
Because namespace declarations are verbose (but easy to achieve with tools like SpringSource Tool Suite), we elide them in the listings and snippets for this chapter.
In listing 11.5, you create a JobLaunchRequest (to launch your echoJob job) and wrap the request object in a Spring Integration message.
Spring Integration provides the MessageBuilder class to create the message.
You use it to set the payload (or body) of the message with the job launch request.
Once you create the message, you can look up the job-requests channel from the application context and send the request.
If you execute the code from listing 11.5, you’ll see output like the following on the console:
The second line is the job execution displayed by the console channel adapter.
This means that your job launch with Spring Integration is a success!
This isn’t the case in listing 11.5 where you refer directly to the Spring Integration API.
But thanks to Spring Integration features (wrapping/ unwrapping, gateways), application components can work directly with application classes.
Spring Integration adapts the messaging infrastructure behavior to the application code.
You know the framework basics and saw the construction of a component you’ll reuse in the use case: a message handler to launch Spring Batch jobs.
Any Spring Integration message containing a JobLaunchRequest as its payload can trigger a job launch.
You do this because Spring Integration automatically unwraps the payload from the message.
The framework analyzes the signature of the message handler and is smart enough to call the handler’s method with the payload as the parameter.
With Spring Integration, message handlers can have flexible method signatures.
You now have many options to launch Spring Batch jobs.
Fasten your seat belt and get ready to receive import job submissions thanks to Spring MVC.
In figure 11.8, the first step in our use case is to receive job submissions for the import products job through HTTP.
A submission will contain a client-defined import identifier (to track the status of the import) and the products to import, all as an XML document such as this:
This question comes up quite often when exploring these frameworks, mainly because both provide support to connect to external resources: files (flat or XML), JMS queues, databases, and so on.
Remember that Spring Batch is good at bulk processing: it can efficiently and reliably read flat files with hundreds of thousands of lines, for example.
Spring Integration is more event driven: it can generate a message when it discovers a new file in a directory or on a remote FTP server.
Spring Integration also provides support to route and transform messages.
How can the frameworks work together? Spring Integration can work before Spring Batch to trigger an event that will start a job (dropping in a new input file)
Because this kind of job submission works for small-to-midsized files (tens or even hundreds of megabytes, but not more!), you can go from one large, nightly import to several smaller, more frequent imports.
Table 11.4 lists the tasks involved in handling job submissions and the corresponding technologies we use to implement them.
The first task uses Spring MVC to handle HTTP job submissions.
In our deployment, a Spring MVC controller handles job submissions coming in as HTTP requests.
The controller delegates tracking to a repository and routing to a Spring Integration gateway.
The Spring beans related to data access and Spring Integration are deployed in the root application context of the web application.
Table 11.4 Tasks and corresponding technologies to handle job submissions.
Copying job submissions to file system Spring Integration file system support.
You can write web applications using Spring MVC with features like validation, binding, and separation between request processing and view generation.
What is REST? REST stands for Representational State Transfer and is a style of architecture.
The most famous implementation of REST is the World Wide Web; the HTTP specification follows all REST semantics.
Servers and clients communicate through the transfer of representation of resources.
This structure is typical of web applications using Spring MVC as their web container.
The two application contexts share a parentchild relationship: the DispatcherServlet application context sees all the beans from the root application context, but not the other way around.
Figure 11.11 illustrates the two application contexts, their relationship, and the kinds of Spring beans they typically own.
The configuration has two steps: configuring the root application context and configuring the Spring MVC DispatcherServlet.
Table 11.5 displays the locations of Spring configuration files for both application contexts in our application.
Unless specified, all Spring configurations in the next snippets and listings take place in the configuration file for the root application context (you’ll see that there’s not much in the Spring MVC configuration)
Table 11.5 Location of Spring configuration files in the web application.
Figure 11.10 Job submissions come in as HTTP PUT requests.
The Spring MVC controller uses a repository to record each submission in a database.
The controller then sends submissions to the Spring Integration messaging infrastructure through a gateway.
Figure 11.11 In a web application, a DispatcherServlet (the heart of Spring MVC) has its own Spring application context, which can see beans from the root application context.
The scope of the root application context is the entire web application and contains beans for data access, business services, and all the beans from frameworks like Spring Batch and Spring Integration.
If you’re familiar with Spring MVC, the programming model for REST support is the same as for traditional, annotation-based Spring MVC applications.
If you’re new to Spring MVC, a REST controller is a POJO with Spring annotations.
But using a REST framework isn’t enough to be truly RESTful: you need to follow strict REST semantics, as defined by the HTTP specification.
Operations will be accessible on the /product-imports URL—REST is all about resources; our resource is an import products document.
The import content is in the body of the request—The body of the HTTP request contains the XML document describing the products to import.
The system can return a 409 status code if a client tries to submit the same import multiple times.
This makes job submissions idempotent—a client can resubmit an import if it didn’t get a response.
This is useful because the HTTP protocol doesn’t provide any guarantee of delivery.
If you think that fulfilling all these requirements is difficult, don’t worry: Spring MVC makes the implementation of the REST controller a breeze!
Clients must send an HTTP PUT to import products on a specific URL, which identifies the import as a resource.
Implementing the REST web controller by following the rules listed previously will make you a good citizen in the REST world.
You also get all the benefits of REST: the simplicity and reliability of the HTTP protocol and interoperability with large amounts of client technologies.
Spring MVC helps to enforce these rules through its REST support.
The following listing shows our job submission REST controller; this leverages some of the REST features of Spring MVC.
For the DispatcherServlet to pick up the REST controller, you annotate it with @Controller.
Because the controller is a Spring-managed bean, it can benefit from.
Listing 11.6 Receiving job submissions with a REST web controller.
In figure 11.8, we show two steps to handle submissions: first, we record the import request, and then we send it to the messaging infrastructure.
The system must track job submissions such that system administrators and clients can monitor the lifecycle of each import.
The following listing shows how each product import ID is stored in the database thanks to a JDBC-based repository.
Listing 11.7 shows the first version of the products import repository; we’ll improve it later to map a product import with its corresponding Spring Batch job instance.
Let’s now see the second step in handling the submission: the Spring Integration gateway.
The gateway is an enterprise integration pattern: it allows an application to access the messaging system transparently.
The XML content of the job submission goes on the Spring Integration messaging bus, but the REST controller doesn’t know anything about it.
It’s time to configure the components used to receive product import submissions.
If you guessed that we use Spring for this configuration, you’re correct.
The following XML snippet shows the configuration of the REST controller:
Let’s now see the configuration for the repository and gateway.
The repository and gateway are part of the root application context.
The following listing shows their configuration along with other infrastructure beans, such as the transaction manager.
Listing 11.8 Declaring the gateway and repository in the root application context.
Component scanning Spring can autodetect beans from classes annotated with specific Spring annotations.
This feature is called component scanning because Spring scans the classpath to discover candidate classes.
The annotations that trigger component scanning are stereotype annotations: they also denote the use of the class (web controller or business service, for example)
This allows creating your own stereotypes, also eligible for component scanning.
This is a plain Spring bean, not much to say.
This creates a Spring bean that you can inject in any other bean.
Remember that the gateway pattern allows application code to access the messaging system transparently.
This is a common requirement, and Spring Integration provides a dynamic implementation of this pattern, so you don’t need to write any Java code.
Because this is for testing purpose, you don’t wire content transfer to the file system (you’ll do that in a later example)
Finally, you configure a DataSource E and the transaction infrastructure F.
You’re done with the configuration, and although the system isn’t functional yet, you can easily test it.
The test checks that the REST controller receives a job submission correctly and sends it through the gateway such that the content of the request is output on the console.
All you have to do is send an HTTP PUT request with some content.
You create a products.xml file with some valid XML content and execute the following command:
This command sends an HTTP request with the content of the products.xml file in the body of the request.
As an alternative to curl, let’s see a Java solution using the Spring class RestTemplate.
Spring MVC not only provides REST support on the server side (see the REST controller), it also provides REST support on the client side.
The Spring class RestTemplate is used to consume RESTful web services.
In the following snippet, you use the REST template to submit a PUT request with a small XML document in its body:
The first parameter of the put method is the URL of the web service.
The second parameter is the body of the request (a string with some XML in this case)
You can then pass any number of additional parameters for the RestTemplate to bind in the requested URL (in this case, only one, the import ID)
You’re getting close to the end of handling a job submission.
Next, you need to do something with the content of the submission, namely, the XML document contained in the body of the request.
You’re going to copy the XML document to a file so that the system picks up the file later and submits it to Spring Batch.
You received a job submission and want to store the products to import in the file system.
Because you use a Spring Integration gateway, the content of the import is now.
Any other REST clients out there? There are graphical alternatives to curl and to the RestTemplate.
Firefox has a helpful plug-in called Poster for performing REST requests.
The rest-client (hosted on Google Code) project also provides a graphical Java-based application to test a variety of HTTP communications.
You could write a custom message handler to receive the content, create a file, and copy the content into it, but as a Java programmer, you know that file I/O in Java is cumbersome.
Moreover, writing the content of a message to the file system is a common requirement, so Spring Integration provides a ready-to-use file-writing channel adapter.
Figure 11.13 illustrates how the filewriting channel adapter works in the case of our import job submission.
Let’s see now how to configure the file-writing channel adapter.
Spring Integration provides a dedicated XML element to declare a file-writing channel adapter, as shown in the following XML fragment:
We should point out the default behavior of the file-writing channel adapter: it takes the String payload of incoming messages and uses it as the content of the output file.
This is a reasonable default, but the channel adapter needs some help to choose the name of the output file.
It delegates this job to a filename generator that you plug in using the filename-generator attribute.
This is an example of Spring Integration implementing boilerplate code (file I/O) and letting you plug in your business logic (filename generation)
Next, you create a custom implementation of a filename generator, which uses the import ID as part of the filename.
Figure 11.13 The file-writing channel adapter receives messages and copies their payload to the file system.
It delegates filename creation to a filename generator (an example of the strategy pattern)
You’re about to copy the XML document containing the job submission to a file, but you need to be careful about the name of this file, mainly to avoid collisions.
By using this ID in the name of the file, you should be able to avoid duplicate filenames.
Spring Integration allows plugging in a filename generation component in its file-writing channel adapter by implementing the FileNameGenerator interface.
The following listing shows the filename generator implementation, which uses the import ID to generate filenames.
A static method in the ProductImportUtils class encapsulates the import ID extraction.
This method contains some boilerplate XPath code to retrieve the value of the import-id attribute (this code isn’t worth showing here)
Clients can send HTTP PUT requests, and the system tracks each import submission and copies its content to the file system.
You can test the submission chain with curl or the RestTemplate class and see if the system creates the files in the target directory (assuming that you send a valid XML file with a products element and an import-id attribute)
Now that the import submissions are safely stored on the file system, you need to send them to a Spring Batch job for processing.
The next section covers how to achieve this with Spring Integration.
Our system handles job submissions by receiving them over HTTP and storing them on the file system in a specific target directory.
How should the system process these files? You can launch a Spring Batch job every hour to retrieve all files from the directory and import their contents in the database.
But what about reacting to a new import file in the directory? If you use an event-driven system, files can be imported as soon as they arrive in the directory.
Spring Integration provides a file message source that does exactly that: it polls a file system directory and creates a message for each new file created in this directory.
If you can notify the system when a new import file is available, you can trigger a Spring Batch job to import it.
You implemented a message-driven Spring Batch job launcher in the Spring Integration quick-start.
The only thing you need to add is functionality to adapt the message coming from the file system to a request message to launch a job.
In the rest of this section, we cover the configuration of the file-reading message source, the conversion between file messages and job launch messages, and the configuration of the Spring Batch import job.
The following snippet shows how to use it to poll our submission directory for new files every second (expressed as 1,000 milliseconds):
Figure 11.14 Triggering the import job when an import file is created in the submission directory.
The Spring Integration inbound file message source polls the submission directory and sends messages for each new file.
A custom message handler converts the file messages into job launch messages.
Our message-driven job launcher receives these messages to trigger Spring Batch jobs.
Every time the inbound file message source detects a new file in the submission directory, it sends a Spring Integration message with a File payload.
This message contains enough information to trigger the import job, but it needs some transformation before going into our generic job launcher.
We haven’t implemented the import job yet, but you can guess from figure 11.15 that its name will be importProducts and that it will need the import ID and the path to the import file as parameters.
The following snippet shows the Java class that does the conversion between the two types of messages:
You should now know enough about Spring Integration not to be surprised: we rely on Spring Integration to wrap and unwrap our application objects around and from messages automatically.
Figure 11.15 The conversion between a file message and a job launch request message.
The latter then goes to the generic job launcher to trigger the import job.
That’s it; the whole job submission chain is connected to the job launcher! Remember, we started from a REST request, went through the file system, and finally reused our message-driven job launcher.
At the end of this chain lies the import job itself, so let’s see some Spring Batch code to implement it.
The import job reads the input XML file and updates the database accordingly.
Nevertheless, you must implement a preliminary step: mapping the import with the job instance.
Spring Batch maintains metadata about everything it does, so by mapping an import with a job instance, you know the status of the import.
Figure 11.16 shows the two steps of the import job.
Let’s start at the beginning: the tasklet that maps the import with the job instance.
A custom tasklet is in charge of mapping the import with the job instance.
The product_import database table stores the mapping, so you only need to update it.
Listing 11.10 Updating the repository to map the import with the job instance.
The first step maps the import to the job instance, which gives you access to the import status.
The second step reads the XML file and updates the database.
Because a Spring Batch step can’t manipulate the repository, you implement the custom Tasklet shown in the following listing.
Listing 11.11 Tasklet to map the import with the job instance.
Let’s now see how to read products from XML and write them to the database.
The second step is a traditional read-write scenario, so the import isn’t a big deal to implement with Spring Batch.
You use Spring Batch’s XML support to read the XML file and do some configuration.
To update the database, you reuse the JDBC writer implemented in chapter 1
Recall that this writer only inserts or updates products in the database, depending on whether those products already exist.
You don’t write any new code for the writing phase, but you have to do some configuration work.
Spring Batch provides support to map XML fragments with Java objects.
The framework integrates with a Spring module called Spring OXM to do this job.
Spring OXM is mostly an abstraction that can plug into existing marshalling libraries.
We use Castor here, but XStream, JAXB 2, or XMLBeans would work as well.
Castor needs metadata to map an XML fragment to a Java class.
The metadata can be in an XML document, as shown in the following listing.
You can then declare a Spring OXM marshaller that points to this mapping file:
The Spring Batch XML reader needs a reference to this marshaller bean to create product Java objects from the incoming XML file.
Let’s put all of this together by configuring the job.
The following listing shows the job configuration, where you wire together the steps and their components: mapping tasklet, reader, and writer.
You configure the job with a plain Tasklet step and a chunk-oriented step.
You also use late binding with the step scope to refer to job parameters.
Don’t hesitate to refer to chapter 5 if you want a quick reminder of an XML reader configuration.
This time, this is it; you completed the import of products! You used enterprise integration techniques to make the update of the online application event driven.
Spring Batch lives at the end of this chain but fits nicely with REST and event-driven technologies like Spring MVC and Spring Integration.
Now that the import works, we can easily improve the system by adding monitoring features.
Next, we provide access to the status of imports through REST.
The online store application can now receive updates more frequently; you implemented this feature in the previous section.
But the system is a black box to the client that submits imports.
How can clients know that the application received and successfully processed the imports they submitted? It’s time to complement the REST interface of our new system by communicating the status of imports.
Remember the first step of the import job: mapping the import to the job instance in the database.
Our goal is for clients and system administrators to be able to access the status of any import job.
The REST controller is the entry point; it retrieves the information from the database thanks to an improved version of the JDBC repository.
Adding this feature requires modifying the JDBC repository, the REST controller, and some configuration in the Spring MVC servlet (but these modifications are minor)
The following listing shows the implementation of the ProductImport class.
Listing 11.14 Representing the status of an import in the ProductImport class.
Figure 11.17 A client can find out the status of an import from the REST controller.
The controller consults the repository, which uses system data and Spring Batch metadata to communicate the status of import jobs.
The ProductImport class is a POJO; its only goal is to encapsulate data.
You’ll see later that the system serializes instances of this class to XML and sends them to the client.
For now, you’re going to modify the JDBC repository to load ProductImport objects from the database, as shown in the following listing.
Listing 11.15 Retrieving the status of imports from the repository.
The Spring Batch JobExplorer interface provides easy access to batch metadata.
The repository uses it to retrieve job instances and job executions.
The repository’s get method contains the logic to communicate the status of an import.
It starts by checking the existence of the import the caller is querying B.
If the method finds no corresponding import, it throws an exception.
Once you know the import exists, you can start thinking about communicating its status.
At C, you define the default status—PENDING— which means the import exists but no corresponding job instance exists yet (the import file is in the submission directory, waiting for Spring Integration to pick it up)
At D, you try to find the corresponding job instance, and if it exists, you retrieve the status of its last execution.
The new version of the repository is a good example of using Spring Batch beans to access batch metadata.
Because the repository now depends on the job explorer, it requires a modification in its Spring configuration.
The following snippet shows how to declare the job explorer and how to inject it in the repository:
Now that the repository connects the application and Spring Batch, let’s see how to leverage this information in the web controller.
To communicate the status of an import, the REST web controller needs to follow the strict semantics illustrated back in figure 11.17:
The following listing shows how to fulfill these requirements with Spring MVC REST.
The web controller delegates the retrieval of the ProductImport to the repository.
If everything goes well, the system sends the ProductImport object to the responserendering mechanism.
Listing 11.16 Communicating the status of an import with the web controller.
The noImportFound method sends back a 404 (NOT FOUND) status code, following REST semantics.
But what happens to the ProductImport returned by the getProductImport method? You want the framework to serialize a ProductImport object to XML and send it back to the client in the response body.
Spring MVC can do that, but it needs some hints.
The first hint is the @ResponseBody annotation on the getProductImport method to trigger a special, REST-oriented rendering of the response in Spring MVC.
Spring MVC registers some default message converters, but you’ll need to override them if you want to convert the ProductImport object into XML properly.
Recall that in section 11.5.3, where Castor was used to import the XML products file into the database, you complemented the mapping information with instructions for the ProductImport class and plugged the Castor marshaller into Spring MVC.
The following listing shows how to provide the mapping information to Castor for converting a ProductImport object to XML.
Spring MVC needs to know about the Castor marshaller you defined earlier.
This means registering an XML-based message converter using the marshaller and plugging this message converter into Spring MVC’s HandlerAdapter.
The following listing shows the necessary configuration to make Spring MVC use the XML message converter.
Listing 11.17 Completing the Castor configuration to map the ProductImport class.
Read the corresponding sidebar to learn more details about Spring MVC, handler adapters, and message converters.
Testing the monitoring feature is easy: first, submit an import to the system (see the source code for this book)
Let’s assume you submit a partner1-1 import: you can use curl (or a web browser) to access it:
The response (formatted for readability) should look like the following:
Spring MVC, handler adapters, and message converters The DispatcherServlet class is at the heart of the Spring MVC framework.
It delegates work to infrastructure beans that include a handler adapter.
The DispatcherServlet has such a handler adapter set up by default, but you can override it by declaring your own in the Spring application context for the servlet.
Message converters registered on the handler adapter perform this serialization.
But how can you refer to this Castor marshaller bean? Remember, the root application context defines the marshaller, whose beans are all visible from the application context of a DispatcherServlet.
Congratulations, you completed the entire use case! This monitoring feature is the final touch on our enterprise integration system.
Clients can now interact and control everything in the system, from import submissions to job monitoring.
This was quite a journey! Spring Batch can do so much more than handle large amounts of data through nightly cron jobs.
Spring Batch is flexible enough to be part of an enterprise integration project that uses the messaging integration style.
The scenario used in this chapter illustrates how different systems can communicate using lightweight and effective techniques and stay loosely coupled.
Because of our new client submission architecture for imports to the online store, clients can update the catalog more frequently, in a simple way.
Implementing our integration use case involved technologies like REST, file manipulation, messaging, and batch processes.
We finished by using Spring Batch job metadata to communicate the status of imports to clients.
This monitoring feature rounds out our use case and, in chapter 12, you’ll discover more monitoring techniques.
Detecting job errors is more difficult because jobs don’t require user interaction during execution.
Detecting errors is also more challenging because applications generally execute batch jobs in the background.
In some cases, rerunning batch jobs is enough to solve problems, such as when a system like a database is temporarily down.
When errors are caused by bad data, running the batch job again won’t solve the problem.
Because batch jobs run mainly in the background, receiving notifications when errors occur is critical.
How do you solve this problem? You provide support in your application for batch monitoring and management.
Spring Batch makes it possible to monitor a job because it collects data on job and step executions and saves it to a Monitoring jobs.
In the real world, errors occur when running batch jobs.
Spring Batch provides classes to access the database and acts as the foundation for monitoring tools.
In this chapter, we cover what monitoring is and how to monitor job executions in Spring Batch.
We then go through the different ways to monitor batch job executions as supported by the job repository database.
In the scope of computer science, the word monitoring is used in different contexts: network, system, and website monitoring, for example.
In all cases, monitoring aims to detect problems automatically and notify administrators through dedicated consoles and applications.
This is a best practice used to detect errors that may occur in an application.
For Spring Batch applications, this means detecting errors in jobs and notifying administrators.
In this section, we show what monitoring looks like at a high level and see its function in the context of batch job executions.
Before we get into the benefits of monitoring, let’s first look at a high-level example by monitoring a web application.
The goal is to make sure the application is reachable and that it responds to requests within a given time limit.
For this use case, you can imagine a monitoring tool that checks that the web server process is up, the web application is available through HTTP, and the application responds to requests within a given time limit.
This tool displays its results through a web page that it updates periodically and notifies administrators via email when problems occur.
Figure 12.1 Monitoring a web application and checking its availability.
A monitoring tool has two main features: detection and notification.
The tool detects job execution errors and automatically sends notifications to application administrators depending on the severity of the problem.
A tool can send an alert using email, Short Message Service (SMS), instant messaging, and so on.
Monitoring an application warns you quickly of errors and provides information to allow you to diagnose and solve problems.
Let’s see how monitoring works with batch jobs in Spring Batch.
Monitoring is particularly useful and even essential in the context of batch jobs because applications execute jobs in the background without a user interface.
Monitoring is possible with Spring Batch because it records everything that happens during batch job executions in a job repository database.
Figure 12.2 shows interactions between batch job executions and monitoring tools using the job repository database.
In figure 12.2, you can see that Spring Batch stores everything that happens during job execution in the job repository database; tools also access the job repository.
Figure 12.2 Monitoring batch jobs using execution data from the job repository database.
The repository tracks not only the whole batch job but also its component steps.
This makes it possible to know precisely when and where errors occur.
You now know what monitoring is and what it looks like in general, as well as how it applies to batch jobs.
It’s time to get to the meat of the chapter by focusing on the job repository and detecting errors.
Next, we describe accessing the job repository database from Java applications using the Spring Batch API.
We also use Spring Batch Admin and JMX to monitor batch job executions.
When executing batch jobs, Spring Batch records information about what happens in the job repository database.
As described in the previous section, monitoring uses data from the job repository database managed by the Spring Batch infrastructure.
The first step in monitoring batch job executions is to understand the Spring Batch database schema.
You’ll then learn how to access the data using Spring Batch.
In this section, you use a database-based repository with JDBC.
You should use an in-memory repository only for tests, not in production environments.
Spring Batch provides a set of Data Access Objects (DAOs) that every class interacting with the persistent job repository uses internally.
First, we look at the job database schema used by Spring Batch for a persistent job repository.
Learning about this schema helps you understand how to monitor applications.
Spring Batch includes a built-in database schema for persistent job repositories but doesn’t provide automatic schema creation.
Spring Batch does, however, contain all the necessary scripts used to create this schema depending on your database.
Spring Batch uses JDBC to interact with the job repository database.
You use Spring’s JDBC facilities and its JDBC XML vocabulary to create the database structure, as in following snippet:
After configuring the data source to access the job repository, you specify the class path location of the H2 database script.
All SQL scripts create the tables described in figure 12.3
Notice that Spring Batch also provides cleanup scripts for dropping the tables.
Figure 12.4 shows the Spring Batch classes that carry the same data as the database tables.
These classes store everything that happens during batch job executions.
Because they provide an object-oriented representation of the tables they map, we use these classes to describe the schema.
Figure 12.3 Database schema for the Spring Batch job repository.
Table 12.1 provides a short description of each class involved in job executions.
To help you understand the roles these classes play, we describe how Spring Batch populates these objects when it executes a job.
When Spring Batch launches a job, it first checks to see if a job instance exists.
With this information, Spring Batch determines if it can launch the job.
Spring Batch then creates a job execution that includes information like the creation timestamp, start time, and a link to its job instance.
After Spring Batch executes each job step, the exit status is set on each step; the global exit status for the job is set after all steps are executed.
Figure 12.5 shows the interactions between a job’s lifecycle and the job repository.
You now know what happens during job execution and how Spring Batch populates data in the job repository.
It’s time to see what data is present in the job repository database after an execution of the imports products job from our case study.
StepExecution An execution of a step within a job execution.
Figure 12.5 Interactions between job and repository during batch execution.
Accessing batch execution data you may recall, the job consists of a single step: read products from a file and insert them in a database table.
You launch the job importProductsJob four times with different parameters.
Because you launched the job importProductsJob four times, you have four rows in the BATCH_JOB_INSTANCE table.
In addition to the start and end times of the execution, you can also see the status and exit codes and that only one execution completed successfully; the other three failed.
You can see which steps the job executed for each job execution, as described in figure 12.8
Because there’s only a single step in the importProductsJob job, you find four rows for the executions (one per execution) and see that only a single step ended successfully.
The exit codes of these step executions correspond to the exit codes of the job executions.
These database tables let you follow the execution of batch jobs and see what happens at both the job and step levels.
In the next section, we describe how to explore the job repository using Spring Batch facilities and how to detect problems in batch job executions.
In addition to providing data structures for job metadata, Spring Batch provides entities that provide easy access to the job repository.
In this section, we explore this subject and describe different ways to interact with the job repository.
Spring Batch defines the JobRepository interface for its internal infrastructure use when executing batch jobs.
Spring Batch uses a JobRepository to interact with the job repository during batch job execution.
Spring Batch also uses a JobRepository to check parameters when starting jobs and storing information corresponding to job and step executions.
You configure a JobRepository and reference it from the job configuration.
This section presents an overview of the services the JobRepository interface offers.
We describe how to use a JobRepository during batch execution, but we don’t use it for batch monitoring.
Spring Batch defines the JobRepository interface for use by the job infrastructure to populate the job repository with job and step execution data.
We don’t recommend using a JobRepository outside the Spring Batch infrastructure.
Fortunately, Spring Batch provides other objects for such use cases.
The first one we discuss, the JobExplorer interface, lets you explore the job repository content in read-only mode.
The Spring Batch infrastructure uses the JobRepository interface during batch job execution to store job and step data.
But JobRepository methods aren’t well suited to exploring the job repository.
To explore the job repository, Spring Batch provides a dedicated interface, the JobExplorer, which offers methods to get information about job and step executions for both running and completed jobs and steps.
The JobExplorer interface defines methods to get job execution, step execution, and job instance data, as described in the following listing.
As for the JobRepository interface, Spring Batch provides a built-in default implementation based on JDBC and previously described DAOs.
This factory class requires only a data source and a large object (LOB) handler for configuration.
This allows the explorer to configure DAOs to save execution contexts automatically.
Now that you’ve configured a JobExplorer entity, let’s see how you can use it.
These methods act as the foundation for the next section, which describes how to detect job execution problems.
Use the names returned by this method to find job instances.
The following snippet describes how to use the JobExplorer interface to find all currently running job instances.
The JobExplorer interface is the root interface used to browse data contained in the job repository.
The JobOperator interface let’s you interact with and control job executions using job metadata.
The JobOperator interface is similar to the JobExplorer interface but uses simpler types.
The JobOperator interface also includes methods to start and stop jobs.
Use this method with a job execution identifier from the getJobExecutions method.
The JobExplorer interface doesn’t have a method to return all step executions.
As you can see, the JobOperator interface is similar to the JobExplorer interface, but it uses String and Long identifiers instead of Spring Batch metadata objects.
Note the difference in the behavior of the getJobNames methods: the JobExplorer method looks at the repository and returns a sorted list of unique job instance names.
The JobOperator method returns the available job names you can launch with the start method.
The JobOperator interface is lightweight and particularly suitable for monitoring technologies like JMX, which we examine in section 12.5
Using what we’ve learned in this section, particularly the JobExplorer interface, we see next how to address real-life use cases monitoring batch jobs.
As emphasized early in this chapter, the primary goal of job monitoring is to find out if, when, and where something went wrong.
In this section, we present a practical example to detect problems and their causes.
A common use case in monitoring is detecting batch job failures.
You detect failed jobs by iterating over all job names and then finding job executions for job instances that end with a failed exit status.
The following listing shows how to implement this detection algorithm using the JobExplorer interface.
You use this code to iterate over the job names retrieved with the getJobNames method.
First, you use the job name to get job instances, in pages, using the getJobInstances method B from the JobExplorer interface.
You call this method until you get to the last page.
Using the job instance list, you then get the corresponding job executions with the getJobExecutions method C.
By checking the exit status of each job execution, you can find which jobs ended in failure.
You use the getExitStatus of the JobExecution class D to check the exit status.
Failures can occur at two levels during batch execution: job and step.
After identifying the failed job execution, you can collect errors at both the job execution and step execution levels.
These methods provide information on what went wrong during job executions.
The following listing describes how to query a failed job execution to get to the exceptions that caused the failures.
After checking the exit status to see if the job execution failed, you get a list of failure exceptions for this execution B.
The method returns an ExitStatus, which contains both the exit code and exit description.
Accessing batch execution data execution and get the corresponding failure exceptions C.
You add all exceptions to a List returned by the method.
Because Spring Batch doesn’t save these exceptions in the job repository, this information is reachable only from the same process that runs the job execution.
If you want to find the cause of a job execution failure after the job completes, you need to use the description of the exit status.
The following listing describes how to retrieve all failure descriptions for a job execution.
After using the exit status to check that the job execution failed, you get the exit status description for the failed execution B.
You then iterate over executed steps for the job execution and get the corresponding failure exit descriptions C.
You add all descriptions to a List returned by the method.
In some cases, it’s interesting to get execution information even if the executions are successful.
This makes it possible, for example, to detect whether or not skips are normal.
The getSkipCount method of the StepExecution class provides information on the number of processed skips during execution, as described in the following snippet:
Listing 12.6 Getting descriptions of problems that cause job execution failure.
By iterating over all executed steps for a job execution, the method checks if the step execution contains skips using the JobExecution getSkipCount method B.
If the returned value is greater than zero, the job contains at least one skip.
In section 12.5, we describe how to use the JobOperator methods to monitor job executions.
You can use the JobExplorer and JobOperator interfaces to monitor job executions.
You can also use tools to save time, receive notifications, and explore the job repository.
Next, we describe how to receive notifications using a listener.
You can use two approaches to monitor jobs: active and passive.
In this section, we show how to implement passive monitoring to send notifications when something goes wrong during batch job executions.
We base our implementation on the Spring Batch listener feature.
We also describe how to implement and configure a generic monitoring listener for any use case.
In this example, monitoring doesn’t use data from the job repository but uses in-memory objects for the current batch job execution.
When the listener receives failure notifications, you can then query the repository for more information.
The batch listener triggers notifications when failures occur during batch executions.
By using this interface with a notification listener, you keep the listener generic and configurable.
The job execution contains the job instance and failure exceptions.
With this interface defined, you can build a generic monitoring listener.
This listener uses the notifier when a job execution fails.
The following listing provides an implementation of such a listener using Spring Batch annotations.
When Spring Batch calls this listener after a job executes, if the status of the job execution is FAILED C, the listener calls the monitoring notifier with the current JobExecution instance as its parameter.
The following listing uses the listeners XML element to register the monitoring notifier in the importProductsJob job.
You use the listener XML element B to register the listener in the job with a bean reference.
You have implemented a generic framework to trigger notifications when failures occur; you can now implement some notifiers.
You start with a JavaMail notifier, which sends emails when the listener detects a problem.
Our first notification use case is sending emails when failures occur.
In this case, the application administrator receives emails containing error descriptions for failed jobs.
This implementation is based on the JavaMail API and the corresponding Spring support.
This example uses a Spring MailSender to send an email and a Spring SimpleMailMessage to build the message content.
In the notify method, you create an email message using the SimpleMailMessage class.
You set the recipient address and the plain text content created from data contained in the job execution B.
You then use the injected MailSender instance to send the message C.
To configure this notifier, you define a Spring JavaMail sender and a template for messages.
The following listing describes how to configure the email-based notifier.
You first define JavaMail entities using Spring support B and then inject them in the email-based notifier C.
This mechanism provides the ability to implement transparent bridges between technologies.
In the next section, we describe how to implement a monitoring notifier that creates and sends messages using Spring messaging.
Sending email messages is good, but it’s specific to a single messaging technology.
Using the generic messaging feature in Spring opens the door to using other technologies for failure notifications like Java Management Extension (JMX)
This feature is built in to the ApplicationContext and allows beans to send messages to each other.
This corresponds to implementing the listener pattern in the Spring container.
The Spring container itself uses this generic messaging framework to notify entities it manages of events like container initialization and finalization.
The following listing shows the implementation of a Spring messaging notifier.
As you can see, this mechanism is generic and dispatches events from batch jobs in the Spring configuration, but this isn’t all you can do.
We’ve now finished our overview of the API used to implement monitoring of batch job executions.
We focus next on higher-level tools to monitor batch jobs.
Spring Batch Admin is the monitoring web console that ships with Spring Batch.
It provides a quick and convenient way to explore the job repository using a web browser.
This approach falls into the active monitoring category because you need to use the tool to see if something went wrong; it doesn’t provide notifications.
Web monitoring with Spring Batch Admin tool briefly in chapter 2 to display batch execution results.
Figure 12.9 provides an overview of Spring Batch Admin and shows the tool running in a Java web server using Spring Batch execution data and JDBC-based DAOs to access the job repository.
Because Spring Batch Admin maps batch execution concepts and data into its UI, the information displayed will be familiar when using the tool.
You can find batch execution problems with the same concepts as when using the Spring Batch API.
In this section, we first provide an overview of the tool’s capabilities and then describe how to detect problems.
You can look at Spring Batch Admin as an application layer on top of the job explorer described in section 12.2.2
One feature of Spring Batch Admin is the ability to import job configuration files.
This feature makes it possible to manage executions (start, stop, and so on) directly through the web console.
This section concentrates on how to use the tool to access information on job executions.
You execute the importProductsJob job from our case study, which contains a step named readWrite.
Figure 12.12 shows that the job instance contains the execution you launched and that it completed successfully.
You can quickly see that the last execution completed successfully.
The last details you can get to are those of step executions.
You reach these pages by following the link on the job execution details page from figure 12.14
The Spring Batch Admin tool lets you quickly access information on batch job executions.
It provides a job list and the ability to drill down to details, all from a web browser.
This is a great tool for monitoring batch jobs remotely.
The main benefit is that you now have a way to detect failures.
In the previous section, the tables in the figures contained fields named Status and ExitCode, which tell you about the success and failure of job and step executions.
If you see FAILED, you can conclude that something went wrong during an execution.
As when using the job explorer, you look at the job and step details to know more about what failed.
As an example of finding a failure, we introduce a malformed field in the input file of the importProductsJob batch job of our case study.
Because the error occurs while importing the input file, you can’t see the details of this error at the job execution level.
You must look at the details of the corresponding step.
This is the readWrite step from our case study, the only step in the importProductsJob job.
When displaying the step details, you see the corresponding error in the exit message field.
This message corresponds to the exception thrown when trying to parse the malformed field.
Figure 12.19 shows the step execution details page, which displays information related to failures and skips.
Figure 12.17 Recent and current job executions containing a failed job execution.
Using tools like Spring Batch Admin layered on top of Spring Batch to access the job repository saves you a lot of time.
Spring Batch Admin uses the Spring Batch lowlevel API to provide a web console to monitor batch jobs.
Next, we use JMX, the Java technology dedicated to management, to monitor our application.
Today, many Java EE application servers and frameworks use JMX.
Applications that implement JMX provide access to information about application resources and provide administrative operations for live applications.
To understand how Spring Batch integrates with JMX, you must understand the JMX architecture.
The layered architecture of JMX exposes application resources as MBeans through the instrumentation layer.
The MBean server is responsible for handling resources exposed as remote resources through the distributed services layer.
Figure 12.19 Details of a step execution failure include a stack trace in the exit message.
Spring provides support to expose any POJO as an MBean.
Although exporting jobs is possible, you normally expose JobOperator entities as MBeans.
Note that using a JobExplorer through JMX isn’t great because it exposes complex objects, not simple ones like a JobOperator.
The JMX launcher configures and registers a JobOperator with JMX for running jobs asynchronously.
Clients include the standard Java jconsole application and any clients accessing entities remotely through JMX connectors.
As shown in the figure, the first step is to define the Spring Batch monitoring process that configures Spring Batch’s entities for JMX monitoring.
This means configuring the JobOperator entity and exporting it as an MBean in the JMX server through Spring’s JMX support.
Although not mandatory, you commonly define batch structures at this level to manage them (start, stop, and so on) directly through JMX via a JobOperator.
Next, we describe how to access and use entities present in the JMX server.
Based on these entities, we monitor batch jobs and their executions.
The main type in Spring JMX is the MBeanExporter class, which is responsible for transparently creating MBeans from POJOs and determining which fields and methods are reachable through JMX.
Because Spring JMX is out of the scope of this book, we focus only on how to use Spring JMX to export Spring Batch entities through JMX.
The first step is to define the bean corresponding to the JobOperator in the beans property of the MBeanExporter that defines beans to export and their corresponding JMX identifiers.
The second step uses the assembler property to determine which data to export through JMX.
The following listing shows how export a JobOperator bean through JMX.
When you reference the jobOperator bean in the beans property of the MBeanExporter bean, you specify its corresponding object name in JMX B.
In the case of the jobOperator bean, the domain is spring with two key-value property pairs.
The first property specifies that the object be relative to a batch.
For the previous JMX path, you specify use of the JobOperator interface.
The configuration doesn’t define an MBean server for the MBeanExporter bean.
The MBeanExporter bean can detect the current MBean server automatically from the JVM process.
For example, Spring uses the MBean server from the JVM process with no additional configuration.
You access JMX remotely because the MBean server doesn’t run on the same machine as the clients (by design)
Therefore, you must configure a JMX server connector, which the JMX specification defines to allow remote access to JMX agents.
In this case, JMX manages RMI transparently for remote access.
After defining the path of the connector in JMX with the objectName property, you specify the address to access the JMX server through RMI with the serviceUrl property.
At this point, you’ve configured everything to monitor Spring Batch jobs through JMX.
The next section focuses on how to explore batch execution data and find problems.
Starting with Java 5, JMX, an MBean server, and JConsole are built into the Java platform.
You launch JConsole using the jconsole command on the command line.
This syntax is a domain name and a colon, followed by a comma-separated list of key-value pairs.
JConsole provides monitoring information on the Java process, its memory usage, thread and class information, and MBeans.
For JMX configurations used to monitor batch jobs in JConsole, corresponding MBeans are located under the spring/batch tree node of the MBean explorer tree in the left pane, as shown in figure 12.22
In the case of the JobOperator MBean, these operations allow retrieving execution data, as shown in figure 12.23
Using these methods, you monitor and manage a Spring Batch job, its instances, and its failures.
First, you get job instances based on a job name, importProductsJob for the case study, as described in figure 12.24
Using these identifiers, you can retrieve identifiers corresponding to job executions with the getExecutions methods, as shown in figure 12.25
These two methods return a String containing information on executions, including the exit status and exit description, which you can use to detect errors and their causes.
Spring Batch integrates well with JMX, the Java standard management technology, for monitoring purposes.
Figure 12.24 Getting job instance identifiers for a job name.
Figure 12.25  Getting job execution identifiers for a job instance.
You can then access these objects remotely through a JMX console like JConsole to remotely execute operations, get execution information, and manage batch job executions.
Monitoring is an important aspect of working with batch jobs because it lets you see what happens during job executions and detect failures.
All through this chapter, we focused on which features and tools Spring Batch provides to monitor batch job executions and detect failures.
Spring Batch offers various ways to monitor batch job executions:
Directly browse the job repository database—The most basic approach is to browse history data directly in the job repository database using a SQL client and executing SQL requests.
Access execution history using the Spring Batch API—The JobExplorer and JobOperator interfaces implement a thin object-oriented layer on top of the job repository database.
Use Spring Batch Admin—Spring Batch provides the Spring Batch Admin web console used to monitor batch job executions based on the job repository database.
Monitor with JMX—You can use Spring Batch and JMX together to expose Spring Batch entities and access job execution history.
We don’t recommend using the Spring Batch API directly in most cases.
A better approach is to use high-level tools like Spring Batch Admin or JMX through a console like JConsole.
In chapter 13, we focus on advanced features of Spring Batch used to improve performance: scaling jobs and parallelizing executions.
Figure 12.27 Displaying summaries for all step executions of a job execution.
Now that you have some real batch jobs under your belt, you can test them for performance in a development or testing environment.
But what do you do when performance isn’t good enough?
You implement scaling and partitioning! Spring Batch provides several scaling techniques to improve performance without making code changes.
For partitioning, you implement code to divide work between a master and slave nodes.
We finish with guidelines for choosing the most efficient techniques to improve the performance of your batch job.
Before tackling scaling in Spring Batch, we describe what scaling is and how it can generally help improve the performance of your applications.
We then see how to apply scaling concepts in the context of batch jobs.
Spring Batch provides a scaling framework and various implementations to improve the performance of jobs and steps through configuration changes without modifying code.
Batch jobs are a bit particular with regard to scaling because they run in the background and don’t require user interaction.
For this reason, measuring the response time for user requests isn’t an applicable performance metric.
Batch jobs do have constraints on the time it takes to process an entire job.
Batch applications usually run at night and have a limited time window to complete.
The goal of scaling a batch job is to meet execution time requirements.
As with any application, you can tune the step and application algorithms.
This is the first step to consider, but processing can still take too much time even after such improvements.
Scaling is the capability of a system to increase total throughput under an increased load when you add resources (typically hardware)
You can consider several approaches to implement scaling for your applications:
Vertical scaling (scale up)—Getting a bigger, better, and faster machine that hosts the application to reach the desired performance.
Horizontal scaling (scale out)—Adding more processing nodes (or machines) to a system to handle load.
This approach aims to distribute processing remotely on several nodes.
With vertical scaling, you work at the computer and system levels to achieve what is also called local scaling.
Such an approach is particularly interesting if you want to leverage multicore or multiprocessor hardware, as illustrated in figure 13.1
Local scaling is suitable if processing implies a lot of I/O.
Horizontal scaling uses another approach by distributing processing over several nodes, as shown in figure 13.2
Figure 13.1 Vertical scaling (scaling up) migrates an application to more powerful hardware.
Scaling concepts scenario, computers don’t necessarily need to be as powerful as in the vertical approach.
Horizontal scaling commonly integrates mechanisms like load balancing, replication, and remote scaling.
Horizontal scaling can leverage grid and cloud computing in order to implement remote processing.
We now have a high-level view of the two techniques we use to improve batch job performance: horizontal and vertical scaling.
Next, we see how to implement horizontal and vertical scaling with minimum impact on applications.
As described in the early chapters of this book, Spring Batch offers a generic framework to support batch job concepts.
Job, Step, Tasklet, and Chunk are all domain objects in the batch job world.
Scaling in Spring Batch defines how to execute processing in parallel, locally, or on other machines.
Scaling takes place mainly at the step level, and you can use different strategies to define at which level you want to split processing.
You can choose to parallelize whole steps or only parts of their processing.
You can also define datasets and process them in parallel locally or remotely.
The best technique (or combination of techniques) is the one that allows your application to meet your performance expectations.
Figure 13.2 Horizontal scaling splits application processing on different nodes and requires load balancing.
Figure 13.3 Local scaling in a single process executes batch job steps in parallel.
You can implement batch job scaling through configuration by using the Spring Batch XML vocabulary for multithreading and parallel step execution.
For more advanced uses, you must configure steps with additional specialized objects.
Table 13.1 lists all scaling strategies provided by Spring Batch, shows if the strategy supports local or remote scaling, and describes its main feature.
Before exploring each scaling strategy provided by Spring Batch, let’s look at the features of local and remote scaling and the use of the Spring task executor.
As noted in table 13.1, Spring Batch supports both local and remote scaling.
Implementing scaling on a single machine uses multithreading through the Spring task executor abstraction that we describe in the next section.
Spring Batch natively supports this feature without any advanced configuration.
Remote scaling is more complex: it requires a remoting technology like Java Messaging Service (JMS) or GridGain, and you must plug in scaling to batch processing using Spring Batch hooks.
This allows you to remotely execute a step or process a chunk.
Remote scaling is more complex to configure and use but it provides higher scalability.
Partitioning step Local and remote Partitions data and splits up processing.
Figure 13.4 Remote scaling in more than one process executes batch job steps in parallel.
Spring Batch doesn’t provide implementations for remoting; it provides only the generic framework to plug in different service providers.
We look at Spring Batch Integration in the remote chunking and partitioning sections.
The Spring framework provides a Java 5–independent abstraction for using thread pools called the task executor.
This abstraction is identical to the concept of the executor introduced in Java 5 and uses the same contract.
This interface is used internally by Spring and its portfolio projects, but it can also be used for your own needs.
It specifies execution of Runnable code in a multithreaded environment.
The following snippet describes how to use the TaskExecutor interface in an application.
The first line creates the task executor, and the last line executes the task:
The following listing shows the SampleTask class that implements the Runnable interface and prints a message to the console from its run method.
The package uses hardware-level constructs to allow efficient use of concurrency in Java programs without resorting to native code.
The package provides classes and interfaces for collections (map, queue, list, and so on), executors (threads), synchronizers (semaphore), and timing.
It provides a simple contract and hides complexity in the implementations.
Each of these TaskExecutor implementations can be configured as a bean in the Spring configuration and injected in other Spring-powered plain old Java objects (POJOs)
It first defines a task executor bean and then specifies task executor properties:
The Spring TaskExecutor interface provides a uniform way to add concurrent processing to Spring applications.
Spring Batch uses a TaskExecutor to enable multithreading in batch jobs.
This feature is particularly useful to scale applications locally and enable parallel processing.
Practically speaking, when scaling locally, you declare a TaskExecutor bean and plug it into Spring Batch.
Now that you know the core concepts behind scaling, let’s see the Spring Batch techniques for implementing it.
For each technique, we describe its features, configuration, and when it applies.
By default, Spring Batch uses the same thread to execute a batch job from start to finish, meaning that everything runs sequentially.
This makes it possible to process chunks using several threads.
Supports a concurrency limit, which blocks any invocations that are over the limit until a slot is free.
You can use multithreading to avoid waiting on one object (reader, processor, or writer) to finish processing one chunk in order to process another.
Reading, processing, and writing chucks can take place in separate execution threads.
This technique may not improve performance and is useful only if multithreading is supported by the hardware.
For example, performance wouldn’t increase on a machine with one processor core and a job doing a huge amount of processing, but the technique would be more efficient for a job performing a lot of I/O.
Figure 13.5 illustrates how a step handles reading and writing using multiple threads.
There's no guarantee as to the item processing order, so you should consider the order random or undefined.
We look at this aspect of multithreading later in this section with an example.
We’re done with multithreaded step concepts, so let’s dive into configuration and usage.
Configuring a multithreaded step in Spring Batch is simple because it involves only specifying a task executor for the step’s tasklet.
Spring Batch then automatically enables multithreading for the step and uses the task executor to process chunks.
Spring Batch entities and thread safety Make sure you check the documentation of the readers and writers you use before configuring a step for multithreading.
Most of the built-in Spring Batch readers and writers aren’t thread-safe and therefore are unsafe for use in a multithreaded step.
If the Javadoc for a class doesn’t document thread safety, you need to look at the implementation to determine thread safety and make sure the code is stateless.
You can still work with thread-safe (stateless) readers and writers; see the Spring Batch parallelJobs example, which demonstrates using a progress indicator for reading items from a database.
Figure 13.5 A step reading and writing using multiple threads.
The XML tasklet element sets the task-executor attribute, which is used to specify a TaskExecutor implementation configured as a Spring bean.
Because understanding what happens when multithreading is involved is a bit difficult, let’s see how it works by running an import of 100 products.
You add trace statements in the reader and writer to see which thread executes read and write operations.
The following listing shows a portion of the console output.
The main consequence of this approach is that Spring Batch doesn’t read items sequentially; chunks may contain items that aren’t consecutive because threads read input data progressively and concurrently.
Each thread builds its own chunk using a reader and passes this chunk to the writer.
When a thread reaches the commit interval for a chunk,
Because you’re using stock Spring Batch readers and writers that aren’t thread-safe, you must read, process, and write items from the same thread.
Furthermore, out-of-order item processing must be supported for the application if you want to use this technique.
Listing 13.3 also shows that each chunk built by a reader on a thread contains the number of items specified in the commit interval, except for the last items.
When configured for multithreading, the tasklet element also accepts an additional attribute called the throttle-limit.
This attribute configures the level of thread concurrency and has a default value of 6
This is particularly useful to ensure that Spring Batch fully utilizes the thread pool.
You must check that this value is consistent with other pooling resources such as a data source or thread pool.
A thread pool might prevent the throttle limit from being reached.
Ensure the core pool size is larger than this limit.
The following listing uses the throttle-limit attribute to configure a multithreaded step.
This approach is particularly interesting to get several threads to process chunks in parallel and save execution time.
Multithreading also has its drawbacks, because it implies concurrent access of readers, processors, and writers.
Such issues can be problematic when the implementations aren’t thread-safe.
Spring Batch frees you from thread management in your code, but the nature of operating in a multithreaded environment means that you must be aware of its limitations and requirements.
This is a similar situation as with Java EE environments and servlets.
All objects shared by threads must be thread-safe to insure correct behavior.
The bad news here is that most Spring Batch readers and writers aren’t thread-safe.
The most problematic classes regarding thread safety in Spring Batch are ItemReader implementations because they commonly manage the state of processed data to make jobs restartable.
This class uses a JDBC ResultSet to read data and carries no thread-safety guarantee.
Listing 13.4 Setting the throttle limit of a multithreaded step.
We have solutions to work around these thread safety issues.
The first one is to implement a synchronizing delegator for the ItemReader interface that adds the synchronized keyword to the read method.
Reading is usually cheaper than writing, so synchronizing the reading isn’t that bad: one thread reads (quickly) a chunk and hands it off to another thread that handles the (time-consuming) writing.
The writing thread is busy for a while, at least long enough for the reading thread to read another chunk and for another thread to write the new chunk.
To summarize, threads won’t fight for reading, because they're busy writing.
The following listing shows how to implement a synchronized reader.
Thread safety Thread safety describes the behavior of code when several threads access it concurrently.
We say code (like a class) is thread-safe if you can use it in a multithreaded environment and it still produces correct results.
This mainly means that conflicts don’t arise when using its static and instance variables.
Accessing static and instance variables from several threads can cause problems, so this type of code usually isn’t thread-safe.
Such issues can also create additional problems during concurrent accesses of methods that use static variables without multithreading support.
If one thread sets an instance variable, it can cause problems for another thread reading or writing it.
First, you mark your product item reader’s read method B with the synchronized keyword and delegate processing to the delegate item reader.
Because the target reader can potentially implement the ItemStream interface to manage state, you also need to implement this interface and delegate to its corresponding methods C.
Another solution is to add finer synchronization to processing and handle state yourself.
After adding the synchronize keyword to the reader, you deactivate the Spring Batch step state management.
You configure the ItemReader bean with the saveState attribute for built-in Spring Batch item readers.
For custom implementations, you implement the update method from the ItemStream interface to do nothing if the class implements the ItemReader interface.
Because you manage state yourself, you can restart the job.
Let’s implement a thread-safe reader that applies the process indicator pattern.
To apply this pattern, you add a dedicated column to the input data table to track processed products.
For our use case, you use a column called processed from the Product table as the process indicator.
The first step is to implement a thread-safe item reader.
In this simple scenario, the item writer sets the processed indicator flag to true after writing the item, as shown in figure 13.6
Figure 13.6 Implementation of the process indicator pattern in a step.
The synchronized item reader uses the delegate property to reference the delegate item reader.
You then use the processed indicator column in the SQL statement to read data.
A processed value of false causes the database to return only unprocessed rows.
This is the other requirement to make the item reader thread-safe (with the synchronization of the read method)
But by doing that, you lose the reader’s restartability feature, because the item reader won’t know where it left off after a failure.
Luckily, the process indicator is there to enable restartability: the reader reads only unprocessed items.
The item writer then needs to flag the product as handled using the processed column and then write the item, as described in the following listing.
In the write method, each item in the loop is tagged as processed B by setting the processed column to true.
Parallelizing processing (single machine) processed column value set to true.
This technique allows managing state and makes the job restartable.
To go further and be nonintrusive, you add an item processor that manages the indicator column, as illustrated in figure 13.7
You can imagine that implementing your own state management is more difficult with files as input.
A common technique is to import data from a file into a dedicated staging database table.
The job then bases its data processing on this staging database table, using a parallelized step and the process indicator pattern.
We’ve begun to parallelize the processing using multithreading and focusing on chunk processing.
This is an interesting way to improve performance but holds limitations due to multithreading and thread-safety issues.
Let’s turn our attention to a new technique that also uses multithreading to parallel processing, but at the step level, and eliminates these types of problems.
We can now see that the key to scaling is to find a suitable technique to parallelize batch processing.
Spring Batch provides a convenient way to organize steps for parallel execution.
Spring Batch XML supports this feature directly at the configuration level.
We focus here on the capability of a job flow to split step processing.
This aspect is useful for scaling batch jobs because it allows executing several steps in parallel, as illustrated in figure 13.8 where Spring Batch executes dedicated steps in parallel to process products, books, and mobile phones.
A Spring Batch job can define a set of steps that execute in a specific order.
In chapter 10, we configure Spring Batch with advanced flows to control which steps to execute and in what order.
Spring Batch flow support provides the split element as a child of the job element.
The split element specifies parallel execution of its containing steps.
Figure 13.7 The process indicator pattern for a step using an ItemProcessor.
Configuring steps for parallel execution is simple and natural in Spring Batch XML.
In a split XML element, you add flows to define what to execute in parallel.
These flows can contain a single step or several steps with a specific order.
Because you can consider a split to be a step, it can have an identifier and be the target of the next attributes in steps.
A split can also define a next attribute to specify what to execute after all flows in the split end.
The following listing describes how to organize the steps in our case study to read books and mobile products in parallel.
Listing 13.8 defines a job with parallel steps named importProductsJob.
After receiving and decompressing product files, you process the files in parallel that correspond.
Remote chunking (multiple machines) to products for books and mobile phones.
For this task, you define a split element with the identifier readWrite B.
This split defines two flows with a single step for each flow and for each product type C.
Once these two steps end, you call the step moveProcessedFiles.
By default, parallel step execution uses a SyncTaskExecutor, but you can specify your own using the task-executor attribute on the split element, as described in the following listing.
Our first two scaling techniques use multithreading to parallelize processing of chunks and steps where all processing executes on the same machine.
In the next section, we use techniques to process jobs remotely, providing a higher level of scalability.
Let’s start with the remote chunking pattern, which executes chunks on several slave computers.
Therefore, if performance still isn’t suitable, you can consider using multiple machines to handle processing.
In this section, we describe remote chunking, our first Spring Batch scaling technique for batch processing on multiple machines.
Remote chunking separates data reading and processing between a master and multiple slave machines.
The master machine reads and dispatches data to slave machines.
The master machine reads data in a step and delegates chunk processing to slave machines through a remote communication mechanism like JMS.
Figure 13.9 provides an overview of remote chunking, the actors involved, and where processing takes place.
Because the master is responsible for reading data, remote chunking is relevant only if reading isn’t a bottleneck.
As you can see in figure 13.9, Spring Batch implements remote chunking through two core interfaces respectively implemented on the master and slave machines:
The ChunkProvider interface is responsible for returning chunks from an ItemReader.
The ChunkProcessor interface receives the chunks and is responsible for processing them in its process method.
Now that we know about the relevant mechanisms and actors used in remote chunking, it’s time for a concrete example.
If you look for additional remote chunking support in the Spring Batch distribution, you find nothing more.
Spring Batch only provides the extensible framework to make it possible to use such a mechanism in batch processing, but it doesn’t provide implementations.
In addition to the remote chunking framework in the Spring Batch core, the Spring Batch Admin project provides a module called Spring Batch Integration that includes a Spring Integration–
Figure 13.9 Remote chunking with a master machine reading and dispatching data to slave machines for processing.
This module provides facilities to implement remoting in Spring Batch and remote chunking using Spring Integration channels.
The major challenge in implementing remote chunking is to make the master and its slaves communicate reliably to exchange chunks for processing.
This leaves the door open for supporting other messaging technologies, such as Advanced Message Queuing Protocol (AMQP)
The remote chunking implementation based on Spring Integration isn’t in the Spring Batch distribution itself, but you can find it in the Spring Batch Admin distribution.
Chapter 11 covers the basics of Spring Integration in a real-world enterprise integration scenario.
If you’re in a hurry and are only interested in implementing remote chunking, you can move on directly to the next section, which describes remote chunking using channels.
A messaging channel is a communication medium between two applications using a message-oriented middleware (MOM) system, as described in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf (Addison-Wesley, 2004)
On one end, the application writes data on the channel, and on the other end, the application reads data from the channel.
A channel is a great communication medium for remote chunking.
It provides the abstraction to make communication between master and slaves independent from any technology for remotely processing chunks.
Moreover, channels implement reliable messaging, ensuring that no message is lost.
Figure 13.10 shows which mechanisms and entities are involved when implementing remote chunking with Spring Integration and Spring Batch.
We focus here on how to configure these machines using Spring Integration and how to make them communicate.
To keep things simple, we implement only one slave, but you can generalize this to several slaves.
Why does remote chunking need guaranteed delivery? With remote chunking, a master node sends chunks to slave nodes for processing.
In remote chunking, the master is responsible for reading input data and sending the corresponding chunks to slaves for processing.
Because you use Spring Integration channels, you configure channels for requests, replies, and the messaging gateway.
The following listing describes how to configure channels and the messaging gateway for a remote chunking master machine.
Listing 13.10 Configuring Spring Integration for a remote chunking master.
You configure the messaging gateway B to send and receive messages from the messaging middleware.
The gateway uses channels for requests and replies that you configure using Spring Integration XML C.
Now that you’ve configured your Spring Integration XML elements, let's see how to define Spring Batch entities from the Spring Batch Integration module to implement remote chunking for the master.
To achieve this, the class replaces the current chunk processor with one that writes chunks to a message channel.
The following listing describes how to configure a master for remote chunking.
You set the chunkWriter property to the chunk channel writer, and then reference the step defined with the stepChunk ID using the step property.
This step corresponds to the step implementing remote chunking for the batch job.
In this case, you add a service activator bean using Spring Integration.
You’ve configured the master to send chunks through a channel for remote processing.
In remote chunking, slaves process chunks remotely and can send data back to the master.
Slaves correspond to dedicated Spring applications that are channel listeners that receive messages, process content, and use the reply channel to notify the master.
At the slave level, you use more low-level objects because you communicate through JMS destinations, the underlying mechanism for channels.
The service activator is a JMS message listener that triggers processing for the chunk handler.
The following listing describes JMS listener definitions and the service activator configuration.
Listing 13.12 Configuring Spring Integration for a remote chunking slave.
You use a message listener container to receive messages from a JMS message queue and drive it to a POJO defined as a listener.
You set attributes on the listener-container element for a JMS connection factory, a transaction manager, and the acknowledgment type.
The listener element specifies how to route messages to the chunk handler.
The handler is responsible for triggering processing of the chunk processor for the received chunk.
For this reason, you must configure a chunk processor in the handler.
This handler knows how to distinguish between a processor that is fault tolerant, and one that is not.
If the processor is fault tolerant, then exceptions can be propagated on the assumption that there will be a rollback and the request will be re-delivered.
Configuring slaves requires defining the chunk handler that the listener calls when receiving messages from the requests destination.
Third-party tools like GridGain1 provide additional implementations for remote chunking.
In summary, remote chunking uses a master to send chunks to remote slaves for processing.
In the next section, we explore a flexible scaling Spring Batch technique called partitioning.
The last technique provided by Spring Batch for scaling is partitioning.
You use partitioning to implement scaling in a finer-grained manner.
At this level, you can also use multithreading and remoting techniques.
Figure 13.11 shows an overview of how partitioning works in Spring Batch and how it provides scaling.
The figure shows that partitioning takes place at the step level and divides processing into two main parts:
Data partitioning depends on the nature of the data: ranges of primary keys, the first letter of a product name, and so on.
A batch developer would likely implement their own partitioning logic in a Partitioner.
The Spring Batch Partitioner interface defines the contract for this feature.
It can be local with multithreading (or not) and even remote using technologies like Spring Integration.
A framework typically provides the way to handle execution, and Spring Batch provides a multithreaded implementation.
The Spring Batch PartitionHandler interface defines the contract for this feature.
We recommended this approach when you want to parallelize processing of data partitions and when you want to control how to create and handle these partitions.
Now that we've described the general concepts behind Spring Batch partitioning, it’s time to see how to implement and configure an example.
Figure 13.11 Partitioning splits input data processing into several step executions processed on the master or remote slave machines.
As described previously, to implement partitioning, you must define the splitting and processing of data.
Partitioning corresponds to creating several step executions for a step.
To configure partitioning, you use the partition element instead of the tasklet element in the step configuration.
The partition element partitions the target step configured using the step attribute, which eliminates any impact on implementations of step entities like readers, processors, and writers.
Additional settings are also available to configure the partitioner and handler.
Listing 13.14 describes a basic partitioning configuration for the readWriteProducts step of our case study.
The listing shows how to use the partition and handler elements and hides the configuration of additional entities like the partitioner.
We focus on these later when we describe the Spring Batch partitioning Service Provider Interface (SPI) in section 13.5.2
The partitioning configuration is located in the step instead of in its declaration.
You set additional properties on the partition with the partitioner attribute and the handler inner element.
The default value for the partition handler defined in the handler element is.
Why doesn’t partitioning need guaranteed delivery? Contrary to remote chunking, partitioning doesn’t need guaranteed delivery.
With partitioning, Spring Batch handles each partition in its own step execution.
On a restart after failure, Spring Batch re-creates the partitions and processes them again.
The step is now defined independently using the step that contains standard elements like tasklet and chunk.
Note that using the step attribute makes sense only for local partitioning, because it refers to a local step bean in the current Spring application context.
For remote partitioning, you usually set up the step name on the partition handler.
The step name then refers to a step bean in another Spring application context.
The configuration schema provides the ability to use any handler implementation with the handler attribute in the partition element, as described in the following listing.
Before dealing with the partitioning SPI, we emphasize an important and interesting aspect of partitioning: late binding is available with this feature.
The difference here is that late binding gives access to property values present in the current step execution.
If you split a step to handle each file in a multiresource reader separately, you can access the name of the current file using late binding, as described in the following snippet.
Each partition sets the filename property for the step execution.
Notice that the step is involved in a partitioned step:
In this section, we described how to use partitioning in step configurations.
We saw how easy implementation is and that there is no impact on the steps involved.
Spring Batch provides an open framework for partitioning that allows defining and implementing advanced and custom solutions.
It’s time to look at how things work under the hood and how to extend this support.
Spring Batch provides a complete SPI for this purpose, using the interfaces listed in table 13.3
These interfaces and classes are involved when partitioning steps, as described earlier in listing 13.14
When Spring Batch executes a partitioned step, it invokes the partition handler to process the partition.
The step then aggregates results and updates the step status.
Once the splitting is complete, the partition handler executes each step with a defined strategy.
As a batch job developer, you would typically write (or configure) only a Partitioner implementation to partition your data.
It’s now time to tackle different ways of using partition handlers and partitioners.
An implementation completely controls the execution of a partitioned StepExecution.
It doesn’t know how partitioning is implemented and doesn’t manage the aggregation of results.
The strategy is independent from the partition handler that executes each step.
The default implementation is the SimplePartitioner class, which creates empty step executions.
Figure 13.12 Partitioning SPI objects involved in partitioning and processing data for a partitioned step.
The partitioning classes give you the ability to scale steps on several processing nodes and enable strategies to increase performance.
Partitioning is particularly useful to distribute step processing on several computers.
In fact, the default Spring Batch PartitionHandler implementation uses multithreading to process steps.
Let’s take our case study as an example and use multithreading to import multiple product files concurrently.
Section 13.2 describes how to add multithreading for the whole step, but this approach can’t control which thread processes which data.
Partitioning provides this support by using dedicated threads to process all of the data for each file.
Configuring this strategy is simple because it’s similar to the generic strategy described earlier.
Figure 13.13 Using dedicated threads to process data when importing product files with partitioning.
Configuring this strategy follows the same rules as for using the partition element B.
The step attribute B specifies the step to partition, and the handler child element sets the partition handler B.
It also sets the keyName property D to specify the name of the current resource to use when adding a file in the step context attributes.
The last thing you must do is specify the resource file to process in the item reader using late binding.
Partitioning is most powerful when each created step execution has its own parameter values.
The following snippet describes how to specify the filename at the item reader level.
Remember to specify the step scope when using late binding!
The previous sections describe how to use the Spring Batch implementations of the partitioning SPI.
It’s also possible to use implementations from third-party tools like the Spring Batch Integration module or to implement custom classes.
Figure 13.14 shows which mechanisms and types are involved when implementing remote partitioning with Spring Integration support from Spring Batch.
The following listing describes how to configure this partition handler and set it on the partition.
Figure 13.14 Partitioning using Spring Integration: the master and slaves communicate using channels, a messaging gateway, and a listener.
The replyChannel property is set to the channel to listen to replies.
The stepName property is set to the step to execute on the slave.
As for remote chunking, a listener triggers processing on the slave.
The following listing describes how to configure a slave for remote partitioning.
The entry point for the slave is a Spring Integration service activator B that uses input and output channels to communicate with the remote partitioning step.
A step locator is in charge of finding this step.
Partitioning is flexible because of the partition handler, which implements a local or remote strategy to process partitioned data.
For this reason, developers don’t commonly implement custom classes; instead, customizations take place at the Partitioner level.
Spring Batch uses a Partitioner at the end of the partitioning process chain.
The partition method uses the given grid size to create a set of unique input values, such as a set of non-overlapping primary key ranges or a set of unique filenames.
Let’s now implement a custom strategy to partition data from a database table.
You first determine data ranges and then assign them to step executions.
Assume here that data is distributed uniformly in the table.
Fine-grained scaling with partitioning uses these values to define ranges based on the grid size specified.
The partition method assumes that the column is of integer type and queries the minimum and maximum values from the database B.
The method then creates as many execution contexts as specified by the targetSize count C and adds them to the partition Map D.
The method also sets the minValue and maxValue properties D in the context to identify the range for the current context.
We’ve seen throughout this chapter that Spring Batch provides several scaling patterns to improve performance.
The challenge is in choosing the right pattern for a given use case.
In the next section, we compare the pattern features and provide guidelines for choosing a pattern (or combination patterns) for different use cases.
In choosing the best pattern for a use case, you need to consider your batch jobs, overall application, and the whole information system.
In this section, we provide guidelines for choosing scaling patterns.
Table 13.4 summarizes the Spring Batch approaches used to implement scaling.
These patterns leverage multithreading, remoting, and partitioning to improve performance.
The first piece of advice we can give you about choosing a scaling techniques is, don’t do it! Implement your jobs traditionally and use the techniques in this chapter only if you face performance issues.
Keep it simple! Then, if your jobs take too long to execute, you can first consider using local scaling with multithreading if your hardware.
Multithreaded step Local A set of threads handles a step.
Because parallel steps must be strictly independent, this approach has no concurrency issues.
This is relevant if you have multicore or multiprocessor hardware.
For multithreaded steps, you must be extremely cautious, think about thread-safety, and batch job state.
Most of the classes involved in steps, like the built-in readers and writers, aren’t thread-safe, so you shouldn’t use them in a multithreaded environment.
You can also parallelize processing with multithreading using parallel steps and partitioning steps.
Parallel steps require organized processing in steps, which is generally a good approach.
With partitioning steps, you can leverage built-in classes to select sets of data to process in parallel.
For example, using one thread per file to import data is particularly convenient and efficient.
In a second round, if performance still doesn’t suit you, you can consider remoting and splitting batch processing on several machines.
On one hand, be aware that remote scaling introduces complexity in your batch jobs; use it only if you must.
On the other hand, these techniques provide high levels of scalability.
You can use two different Spring Batch techniques in this context: remote chunking and partitioning.
Remote chunking systematically sends chunks to slaves for remote processing, whereas partitioning creates data sets to send to slaves for remote processing.
Table 13.5 lists the pros and cons of both patterns.
For the remoting patterns, Spring Batch doesn’t provide implementations in its core distribution.
As you can see, Spring Batch provides a large solution set to implement batch process scaling.
The biggest challenge is in choosing the right patterns to improve performance.
Scaling in Spring Batch provides various solutions to enhance batch job performance with minimum impact on existing job implementations.
Spring Batch configuration files mostly implement scaling and can involve multithreading, parallel executions, partitioning, and remoting.
One approach is to use multithreaded steps, but you need to be cautious because this requires thread-safe code, and you must add specific state management code to keep batch jobs restartable.
Spring Batch also provides the ability to execute steps in parallel.
This requires proper organization of a batch process in steps.
Spring Batch provides advanced and highly scalable patterns and frameworks.
Remote chunking splits chunk processing on several computers to balance the processing load.
Partitioning provides a way to split up data for remote or multithreaded processing.
Scaling can be challenging to implement and is strongly linked to batch processing and the execution environment.
Spring Batch implements a set of patterns that you can use and combine to improve performance.
Chapter 14 covers an essential aspect of application development: testing.
This is particularly true for batch jobs because they mainly process data without user interaction and apply complex business rules.
Unit and functional testing gives us the confidence to maintain and grow applications.
Then, in the Spring Batch advanced concepts chapters, we explored exception handling, batch monitoring, and scaling.
We addressed all the important aspects of creating batch jobs.
The next step is to verify that an application works correctly.
Testing is a best practice in software development and is essential to batch applications.
This chapter explains testing concepts and implementing unit, integration, and functional tests.
This chapter demonstrates how to do all of this by using our case study as an example.
The goal of our batch application is to import products from a text file into a database.
You start by validating the job’s parameters: an input file and a report file.
Then, you read the product flat file and convert each line into a Product object.
In the core chapters of this book, we introduced Spring Batch concepts and you.
An item writer outputs products that don’t match these conditions to a reject file with the help of a StepListener.
You write product items with your own ProductItemWriter based on the SimpleJdbcTemplate.
If the job writes any product to the reject file, a statistics step generates a file that contains the average product price.
Let’s talk a bit about what testing is and how testing can improve the reliability and robustness of an application.
We look at different kinds of tests and then see how to implement them for our case study.
Software testing is a process used to ensure that the source code you write does what it’s supposed to do.
Let’s look at two main testing strategies: black-box and white-box testing.
Our testing strategy applies to all the components shown in the figure.
Black-box testing focuses on software features without relying on internal knowledge of the implementation.
This kind of testing shows that the application accepts input and produces output without errors; you base tests on requirements and functionality.
White-box testing focuses on implementation of software with internal knowledge of design, algorithms, and constraints.
Depending on what you want to test, you have two strategies to choose from: whitebox or black-box testing.
This chapter focuses on unit tests, integration tests, and functional tests.
A unit test should address a single point of functionality and should be fast, understandable (by a human), automatic (no human intervention), and isolated from external resources such as a database, file system, web service, and JMS broker.
With integration and functional tests, you can use external systems such as a database, which can even be an in-memory database to speed up tests.
All these tests must be automatic and require no human intervention.
Unit testing Tests a single software module (a component, a service, and so on) and requires detailed knowledge of implementation internals.
Integration testing Tests software modules to verify overall functionality and requires detailed knowledge of implementation internals.
Functional testing Focuses on functional requirements of an application by verifying that input is accepted and expected output is produced without errors.
System testing Tests all parts of an application Black box.
Acceptance testing Verifies that the application meets customer-specified requirements Black box.
Performance testing Checks whether the system meets performance requirements Black box.
Test-driven development (TDD) TDD is a software development process based on short iteration cycles and unit tests.
Using this process, you write an automated test first, ensure that the test fails, write the code, and then ensure that the test passes.
The benefits of the TDD process are that it drives the API and design, provides good code coverage, promotes safe code refactoring, and keeps developers focused on the task.
If you’re reading this book, you’re human and writing software.
A defect in a production environment may cause a malfunction of a part of the software, leading to a system failure with corrupted data.
For example, in an insurance company, this could result in an error to a client’s refund or a salesperson may be unable to create a new contract.
The later bugs are discovered, the more it costs to fix them.
During development, we wish to add features quickly without breaking existing ones.
They don’t know what the software is supposed to do or how it works.
Software testing helps developers understand existing source code by reading through test source code.
Software applications have grown in complexity and size, and testing in the development phase helps developers gain confidence and get feedback when a test succeeds or fails.
The benefits of testing are great: tests improve the quality, reliability, and robustness of software.
With all the necessary pieces in place, we’re ready to write some tests.
In this section, we introduce how to write basic unit tests with JUnit, and then we look at the Mockito mock framework.
Table 14.2 shows which of our sample unit tests require JUnit and a mock framework (the test class names refer to the source code from the book)
Figure 14.2 depicts what components we cover in our unit test examples.
This figure shows that we unit test all components except for the statistics step.
This is good news: Spring Batch artifacts (validator, reader, writer, and so on) are unit testable! Even if these artifacts are meant to be used in complex jobs that handle large amounts of data, they’re isolated enough from their runtime environment to be unit tested.
Table 14.3 shows our example test classes and the corresponding Spring Batch domain objects they cover.
JUnit is an open source unit testing framework and is the de facto standard for unit testing in Java.
Because we use annotations, our test class no longer needs to inherit from the JUnit TestCase class as it did in JUnit version 3
To create a test method, you write a public method with no return value, no arguments, and an @Test annotation.
At the start of a test run, JUnit runs methods with @Before annotations before each test method.
Use @Before methods to initialize new instances of test objects stored in instance variables.
These objects are available in each test method, and we call them fixtures.
JUnit runs methods with @After annotations after each test method to clean up fixtures or other resources as necessary.
Because some resources are expensive to initialize and manage, you can have JUnit call setup and tear down methods once per test run.
Use the @BeforeClass annotation to call a method once per test run before JUnit has run all test methods.
Use the @AfterClass annotation to call a method once per test run after JUnit has run all test methods.
Methods in the Assert class help you compare an expected value to an actual value.
You’ll mostly use methods in the Assert class prefixed with assert.
If an assert method contract is respected, the test continues; if not, the method throws an Error, and the test is stopped and marked as failed.
The following listing shows a bare-bones unit test with the annotations and some of the Assert methods described in table 14.4
Listing 14.1 Basic JUnit test case with annotations and assert methods.
In the method testMethod2, you use an assert method to verify that a string isn’t null.
In most of the tests cases in this chapter, we use the static import coding style, so we don’t need to prefix assert method calls with the Assert class name.
We can now move on and test some real code.
For this test scenario, you instantiate a Validator, prepare a Product fixture, and verify the result.
In this case, it’s easy to test a Validator, which has only one method, validate.
Listing 14.2 Testing a product validator for a non-null price.
In the first test method, testValidProduct, the validator checks that a product has a positive price.
In the method testPositivePrice C, you test a positive product price; this unit test validates the product, and the test method passes.
Listing 14.3 Testing a product validator for a positive price.
In this section, we touched on JUnit framework basics: how to use JUnit with example test cases and how to test validators from our batch application.
The next section introduces a mock framework to help you control the behavior of objects internal to our case study.
JUnit is a great tool dedicated to unit testing, but our objects aren’t as simple as in the previous example.
For a complex object, you only want to verify the behavior of the object, not the dependencies the object relies on.
To achieve this goal, you create mock objects for dependencies.
Mock objects are powerful for testing components in isolation and under total control.
A mock object is a fake object, dynamically (or automatically) generated for us, that you control: you define what a method returns, when a method throws an exception, and so on.
You can do all this in a few lines of code without writing new classes.
After executing tests, you can verify that execution reached a mock object, what methods the test caused to call, how many times, and so forth.
For this book, we chose Mockito1 as our mock framework, but many others are available.
With Mockito, you can easily create a mock object for a class or interface and control and validate mock objects with a fluent-styled2 API.
The following sample code, inspired by the Mockito documentation, shows how to create, manipulate, and verify a mock object:
This example checks that the code calls the methods add and clear only once by using the verify method and the call to times(1)
The methods prefixed with verify throw MockitoExceptions and fail a unit test when a call doesn’t meet expectations.
With Mockito, you can mock a concrete class and control most of the behavior of the mock object.
The following sample shows how to stub a LinkedList’s get method to return the string "first":
You can also create spy objects, based on real objects, and verify behavior, as shown in the following example:
In this case, you call the add method on the List<String> object; the benefit is to confirm that the add method has been called twice, once with the parameter one and once with the parameter two.
Remember that, in a unit test, we want to test only a single module or component of an application and only one object if possible.
In these unit tests, we use the whitebox testing strategy, and we don’t want to depend on other objects.
Mockito helps us achieve these goals by mocking dependent objects, which we then wire into our own objects.
In addition, we can control and verify how mock objects are used.
The following sections explore how to test batch applications using JUnit and Mockito.
In chapter 5, we use a FieldSetMapper to create an object from tokens with the help of a FieldSet.
The next listing tests our FieldSetMapper with and without Mockito.
Then the assert methods check that a product was created and correctly populated.
This is the standard JUnit style; you test expected values against actual values.
Using a mock object, you can check the mapFieldSet behavior in detail.
Note that the @Required annotation marks the property method setExcludeWriter as mandatory and causes the Spring context load to fail if it isn’t called.
If the job filters out a product, the afterProcess method has a null result argument value, and you write the Product item to a product reject file using the excludeWriter.
This implementation uses a FlatFileItemWriter as a reject file writer to maintain less source code.
The goals of the tests in this section are to avoid using the file system and to write tests that filter and don’t filter items.
In this listing, the method setUp populates the product items list fixture with one Product and mocks a FlatFileItemWriter to avoid using the file system.
The test checks that the listener invokes the write method on the exclude writer once.
In the next sections, we look at some of Mockito’s advanced features, like controlling values returned from a method, creating elaborate tests, and spying on objects.
The ImportValidator verifies that the product input file exists and that the statistics path parameter isn’t null.
For this unit test, you don’t want to depend on the file system, so you mock a ResourceLoader.
In the testJobParameters method, you verify the ImportValidator implementation by spying on JobParameters B.
You use spied objects because you want to control how many times the test calls getParameter and getString C.
This test checks only one case; a complete implementation would check all possible cases.
The next section tests an ItemWriter, a more complex example.
Our ProductItemWriter writes Product objects to a database via a SimpleJdbcTemplate.
If a product already exists in the database, the ProductItemWriter executes a SQL UPDATE.
If the product isn’t in the database, the ProductItemWriter first tries to execute an UPDATE, which fails, and then executes an INSERT, which succeeds.
For each product item, the writer creates a SqlParameterSource to bind its values into a SQL statement’s named parameters.
Let’s look at the item writer test, where you unit test everything with Spring Batch, JUnit, and Mockito.
The following listing shows the unit test for the item writer.
First, you set up your fixture objects in the setUp method where you also set up a mock object for a SimpleJdbcTemplate B and initialize it with its required dependencies.
The first test, testUpdateProduct, simulates at C that the UPDATE statement returns only one affected row, which means that the product already exists.
Using the eq and any methods, Mockito allows you to control method arguments for any instance of SqlParameterSource.
After that, you count how many times the test called each SQL statement D; you expected one UPDATE and zero INSERTs.
In the second test, testInsertProduct, if the UPDATE statement affects zero rows, the SimpleJdbcTemplate executes a SQL INSERT.
You expected one call to UPDATE and one call to INSERT.
You have now successfully tested a FieldSetMapper, an item listener, and an item writer, which are all key components of Spring Batch.
The ability to test these core components gives you the confidence to proceed with changing and growing the application.
In the previous sections, we introduced JUnit, a powerful unit testing framework, and the Mockito mocking framework.
We can’t use unit testing for most of our case study yet, but it won’t be long before we can.
Next, we look at techniques that allow you to test Spring Batch applications at all levels.
Some of these objects are too complex to create outside the Spring Batch infrastructure, like a unit test.
The Test module opens up Spring Batch domain objects to be tested.
It returns NEXT if the job writes any products, based on the number of written items for this execution, and COMPLETED otherwise.
You set the write count value to a positive value, call the decide method, and check that the result is NEXT.
In the next test method, you set the write count to zero and check that the result is COMPLETED.
Listing 14.12 Testing a simple tasklet with Spring Batch domain mocks.
In the CleanTaskletTest class, you create a ChunkContext B to test the Tasklet, which doesn’t require any dependencies.
We created unit tests for our case study with JUnit, Mockito, and Spring Batch mock domain objects.
These examples show you how to improve the reliability and robustness of batch applications.
In this section, we address another aspect of testing where we test software modules in realistic production conditions to validate overall functionality.
As a white-box strategy, integration testing is aware of internal implementation details.
This time, we use a real database, Spring, and Spring Batch application contexts, including batch job definitions.
Table 14.5 shows test class names and corresponding Spring Batch domain classes.
Figure 14.3 depicts what components we cover in our integration test examples.
To track what you’re testing, please refer to figure 14.3
We focus next on testing instances of Step, ItemReader, and ItemProcessor.
The Spring Framework provides support for integration testing with the Spring TestContext Framework, which lets you write tests for a framework like JUnit.
In JUnit, the @RunWith annotation sets a Runner class, which is responsible for launching tests, triggering events, and marking tests as successful or failed.
For integration testing, we use the following Spring TestContext Framework features:
The @DirtiesContext annotation indicates that the application context for a test is dirty and that TestContext should close it after each test.
You configure it as an in-memory database to avoid file system access.
The following snippet shows the Spring configuration of the DataSource bean:
The keyword mem indicates that H2 should work only in memory.
You now have the basic elements in place to begin integration testing of our batch job.
You load a Spring application context and set up an in-memory database based on SQL scripts.
The next step is to deal with Spring Batch components using a special scope.
In Spring Batch, you can configure components at runtime with a special scope named step and a late binding SpEL (Spring Expression Language) expression based on a step or a job execution:
It’s time to begin integration testing based on our case study and practice what we’ve learned.
In our case study, you use an ItemProcessor in the Product step.
Remember that you already tested each Validator separately in unit tests (see the previous section on unit testing)
For an integration test, you test the real processor chain, which the Spring context defines.
You can now validate your test scenarios based on various product price values E.
If a product price is positive, the ItemProcessor returns the same product object; otherwise it returns null.
Let’s continue with a more complex example: testing an ItemReader.
In this section, we describe two ways to test an ItemReader.
First, we see an example using the same technique as previously shown.
In the test in the following listing, you read data from a real file in the file system (but you could mock it)
The test checks the content of the first line of data and how many lines the file includes.
We start each test with the @Before setUp method B to open the stream with a new ExecutionContext.
We read a first line C and compare the expected ID value D.
The product file contains eight lines, which we verify by calling read eight times and checking that each call returns data.
Then, we expect the ninth call to read to return null E, indicating that we’ve reached the end of the file.
The Spring Batch Test StepScopeTestUtils method doInStepScope is the other way to test a Spring Batch component in a step scope for the ItemReader in our case.
You start the test by creating an anonymous implementation of the Callable interface B.
Then you open a stream with a new ExecutionContext, and while the reader reads items in a loop, you count lines as you read them C.
Again, you ensure that you have read eight lines D.
Table 14.6 shows functional test classes and corresponding Spring Batch domain objects.
Figure 14.4 shows what components we cover in our functional test examples.
Here, we focus on the functional requirements of an application, and remember.
Considering the job’s overall functionality, we provide input values and verify output values without concern for any implementation details.
This kind of testing gives you confidence in the correctness of a whole batch job.
Because this is a section on testing Step and Job objects, we validate the product and statistics step and the batch overall (see the test plan in figure 14.4)
Table 14.6 Functional testing plan and Spring Batch domain objects.
The following snippet shows how to launch only one step:
You can also launch a whole job with job parameters, as shown in the following snippet:
The AssertFile class from the Spring Batch Test module includes assert methods to compare File and Resource contents.
You use the AssertFile class to compare the content of the expected reference file with an actual exclude or statistics file.
Recall that JUnit uses the terms expected and actual in its APIs consistently, where the expected value comes first and the actual value second:
Using these new testing classes, we can now work on the Step test.
A step is a critical part of a job, so it’s important to test it in isolation.
Our test scenario in the following listing provides input data, launches the productsStep, ensures that the batch status is COMPLETED, counts how many lines the job has filtered and written, and finally compares the rejected products file with a reference file.
You expect the COMPLETED batch status for the job execution.
Then you retrieve the first step in the StepExecution, count filtered items, and count written items.
Finally, you count the products in the database and compare file contents from the exclude product file and the reference file.
The statistics step calculates the average price of products; there’s no code, only Spring Batch configuration, as shown in the following listing.
Listing 14.17 Compute and write the product average into a file from a step.
The following listing shows you another example by testing the step statisticStep and setting up data in a database.
Listing 14.18 Functional testing of a step with a database.
This test is similar to the product step test except that you set up data in a database.
You launch the step statisticStep and check that the file content is equal to the content of the reference file.
These examples show that it’s easy to test with Spring and Spring Batch.
What a difference from using a bash shell! With Spring Batch, applications are easy to write and test.
Stay with us: the final section tests a whole job.
We finish this chapter with The Big One: testing an entire job—an easy task with all that we’ve learned.
It’s now time to conclude our journey in the land of batch application testing.
In this chapter, you learned how to implement unit, integration, and functional tests for batch applications.
You looked at test concepts and why writing tests improves the reliability and robustness of applications.
We introduced the notion of test strategies like white-box and black-box testing.
You also learned how to write unit tests with JUnit and the Mockito mock framework, where you learned how to test your application’s behavior.
Then, you looked at the Spring TestContext Framework and the Spring Batch Test module, which helped a lot.
Finally, you created Step and Job tests to check overall functionality.
Based on our case study, you created many test cases to show that all aspects of a batch job can be tested.
Indeed, it has never been easier to create tests for batch jobs.
This is the final chapter; we hope you enjoyed this book.
The appendixes that follow show you how to configure your favorite IDEs and Maven for use with Spring Batch and how to set up Spring Batch Admin, the web administration console for Spring Batch.
This appendix shows you how to set up your development environment.
We begin with Apache Maven 3: installing Maven and using basic commands.
We then create a new Maven project from scratch and add the necessary dependencies for Spring Batch.
We also explore Spring Batch features from the SpringSource Tool Suite for Eclipse and learn how to set up a Spring Batch project quickly from a blank Maven project and use the Spring Batch development tools.
One of the benefits of Maven is to promote standards and conventions that accelerate the development cycle.
The POM contains general project information like name, version, dependencies, and plug-in descriptions.
We then create a new Maven project from scratch and configure it to use Spring Batch.
Maven is a Java program, so you need Java on your computer to be able to run it.
To check that your computer has the Java SDK (software development kit) installed and available, run the command java -version; you should see this output: 439
The java -version command shows you the Java version in detail.
At the time of this writing, the latest version of Apache Maven is 3.0.3
You can download Maven 3 from its official website: http://maven.apache.org.
You can check that you installed Maven properly by running the mvn –version command, which outputs information about your system:
The output you get will differ, but it ensures that Maven is correctly installed on your computer.
Now that you’ve installed Maven, let’s use it! But before we look at the Maven command line, let’s go over some concepts.
Maven uses the concept of a build lifecycle, where the three built-in build lifecycles are default, clean, and site.
Each build lifecycle consists of an ordered list of build phases.
For example, the default lifecycle has the following phases, in this order: validate, compile, test, package, integration-test, verify, install, and deploy.
When you run a given phase, all prior phases run before it in sequence.
For example, if you run install, phases from validate to verify run first.
A build phase consists of goals, where a goal represents a task smaller than a build phase.
For example, the Maven Clean plug-in contributes to the clean phase with a goal named clean.
Also, note that Maven uses two phases by default: clean and site.
Apache Maven 3 Thanks to its build lifecycles, phases, and goals, Maven commands are the same whatever the project.
Next, we look at the steps necessary to create a blank Maven project from scratch.
We’ll then add the necessary dependencies before importing the project in the SpringSource Tool Suite.
In this section, we create a blank Maven project, a necessary step before making the project Spring Batch–powered.
An archetype is a project template that contains model files like pom.xml and a standard directory tree.
To create a new project, you use the maven archetype plug-in.
This command generates an empty project based on the Maven file and directory layout.
Maven creates the project in the appA directory (the name of the project)
The project contains a couple of Java classes; you can delete them and create directories to make your project look like the following:
If you follow this structure, Maven will know automatically where to find Java classes, test classes, and so on.
You have your Maven project—this is great, but it’s empty.
If you were to develop some Spring Batch inside the project, Maven wouldn’t be able to compile your code.
This is because you need to add Spring Batch dependencies, so let’s see how to add them.
There’s nothing related to Spring Batch in your blank Maven project, so you need to add these dependencies inside the pom.xml file.
Maven downloads all dependencies from internet repositories, so you don’t have to worry about hunting the JAR files from multiple websites.
The following listing shows the content of the pom.xml file once you add the Spring Batch dependencies.
It’s more convenient to define a property once and then refer to it.
You also set up maven.compiler properties the for the Java compiler.
Note the use of the version property and the exclusion of the Apache Commons Logging dependency.
Commons Logging would come as a transitive dependency of the Spring Framework, and you don’t want it: that’s why you exclude it with the exclusion element (more on logging later)
Chapter 14 introduces you to this test module, but you can still test your Spring Batch jobs without it (import the dependency only if you need it)
With the scope element set to test, Maven adds the corresponding dependency only for compiling and running tests.
You now have the bare minimum to use Spring Batch, but you need to add some other dependencies to make the case study from chapter 1 work.
For the full story, look at the sidebar about logging; for the short story, add the dependencies listed in table A.3
As you can see from table A.3, we’re using Logback as the logging implementation.
The following listing shows the content of the logback-test.xml file.
Table A.2 Spring dependencies to add to the pom.xml file.
Table A.3 Logging dependencies to add to the pom.xml file.
We’re done with logging dependencies; let’s now configure dependencies that the case study uses.
The batch job uses Apache Commons IO to deal with decompressing the input ZIP archive and H2, the Java database.
We’re getting close to the end of the dependencies: next, we test dependencies! ADDING TEST DEPENDENCIES We use JUnit and the Spring TestContext Framework to write an integration test for our case study batch job.
Don’t forget to use the test scope for these dependencies! We’re done with the dependencies: your Maven project is ready to compile and launch Spring Batch jobs.
But are you going to use a text editor to write these jobs? Surely not.
Table A.4 Case study dependencies to add to the pom.xml file.
Table A.5 Test dependencies to add to the pom.xml file.
The focus of STS is to provide tools to help Spring developers create and manage Spring-based applications.
That’s why we use it, but you could use NetBeans or IntelliJ IDEA on a Spring Batch project as well.
It’s also worth mentioning that you can install STS’s tooling on top of an existing Eclipse installation (instead of downloading STS as a bundle) because it’s a set of plug-ins.
This section covers how to import a Maven project into STS and use the tooling to create a Spring Batch job.
You’ll end up with a ready-to-edit project, with configuration managed by the STS-Maven integration plug-in.
You can then browse to the project directory and click on Finish, as figure A.1 shows.
Once created, the project shows up in the Eclipse workspace with a Maven layout, as figure A.2 illustrates.
Let’s see how to create and edit a Spring configuration file.
This support includes a wizard to create a Spring file, namespace management, code completion (Ctrl-Space works in Spring XML files!), and much more.
You start by using a wizard to create a Spring configuration file.
From the package explorer, right-click the src/main/resources source directory and select New > Spring Bean Configuration File.
You can then choose the namespaces you want to include.
Include the batch namespace, as figure A.3 shows, and then click Finish.
Once the Spring configuration file is created, you can select and edit it, as shown in figure A.4
You’re now ready to create a Spring Batch job: use the batch namespace to declare the job, steps, your item reader and writer, and so on.
You can then visualize the Spring Batch job by choosing the Batch-Graph tab in the editor.
Figure A.3 When creating a Spring configuration file with the wizard, STS lets you choose which XML namespaces you want to include in the file declaration.
You can also change the namespaces once the wizard creates the file, on the Namespaces tab.
Figure A.4 The XML editor for Spring configuration files is arguably STS’s most useful tool for the Spring developer.
It provides validation, code completion, and graphical visualization for Spring Batch.
We introduced Maven, installed it, and learned its basic commands.
Using a simple Maven archetype, we created a blank project and added the necessary dependencies for Spring Batch development.
Finally, we saw how the STS for Eclipse facilitates Spring Batch application development.
We used STS to import the Maven project and to create and edit a Spring configuration file for a Spring Batch project.
You can also edit the job definition by dragging components from the left to the main editor area.
With it, you can monitor the execution of batch jobs and start and stop executions.
You can make Spring Batch Admin the only entry point in your batch infrastructure, because it provides all the features to manage and monitor the behavior of your jobs.
In addition, if you find a missing feature, you can implement it and contribute it back to the project, because Spring Batch Admin is an open source project.
Spring Batch Admin is a fullblown web application, but it remains easy to configure and deploy.
This appendix shows you how to deploy a Spring Batch Admin instance in a couple of minutes.
Even if your Spring Batch Admin instance runs on top of an embedded database, it will be helpful for you to discover the web console.
You’ll also see how to deploy Spring Batch Admin in its own web application and how to make it cohabit with other applications inside the same web application.
Spring Batch Admin uses Spring for its configuration, and you’ll see how to set it up to connect to your batch metadata.
If you’re already running a Spring Batch infrastructure, you’ll be able to browse a web view of it in a matter of minutes.
This appendix also covers more advanced configuration scenarios, like deploying job definitions and overriding infrastructure Spring beans in Spring Batch Admin.
If you download the Spring Batch Admin distribution from the download page, you’ll see the package is small.
Spring Batch Admin is a web frontend, so it runs inside a web application, but you can easily embed it in any web application.
You can also choose to let it run alone in a dedicated web application.
You have multiple deployment options, and we’ll start with the latter, as the distribution contains a sample project to build a web application with Spring Batch Admin in it.
This is the quickest way to have a Spring Batch Admin instance running.
We’ll then study the second deployment option before covering how to configure Spring Batch Admin in more detail.
We use the sample provided in the distribution, which uses Maven as the build tool.
You’ll need Maven 2 or greater installed on your computer.
Unzip the Spring Batch Admin distribution and open an OS command shell in the corresponding directory.
The first set of commands installs a parent POM inside your local Maven repository.
You can deploy this WAR file in any web container (like Jetty or Tomcat) or any application server (like Glassfish or JBoss)
Once Maven has deployed the archive, go the following URL to check the installation: http://localhost:8080/spring-batch-admin-sample-1.2.0.RELEASE/
The home page of your Spring Batch Admin instance is shown in figure B.1
From this home page, you can navigate through the application and discover its interface.
The Spring Batch Admin sample uses an embedded database and has some default jobs installed.
This is useful for a quick tryout of Spring Batch Admin and its main features.
You can also use the sample as a starting point for your own Spring Batch Admin installation and customize its Spring configuration files and web.xml file.
We’ll see more about the configuration of Spring Batch Admin in section B.4
Perhaps you already have a web application running and you would like to run Spring Batch Admin in this same application, next to your business application.
The next section shows how to embed Spring Batch Admin in an existing web application.
B.3 Embedding Spring Batch Admin in a web application Spring Batch Admin is a lightweight application: it consists of only two JAR files containing web controllers, Spring configuration files, images, and views.
Embedding Spring Batch Admin in a web application is as easy as including these two JARs and their dependencies and configuring the web.xml file.
Let’s see how to add Spring Batch Admin dependencies in a Maven project.
Spring Batch Admin has its own dependencies (Spring Batch, the Spring Framework, and so forth), but Maven pulls them in automatically.
Figure B.1 The home page of a Spring Batch Admin instance lists the services the application provides.
When adding Spring Batch Admin dependencies to an existing project, be careful of conflicts between dependencies.
Your project can have its own dependencies, so you may need more configuration than what listing B.1 provides.
This extra configuration can exclude transitive dependencies from Spring Batch Admin, or from your own project.
The easiest way to configure Spring Batch Admin in the web.xml of your web application is to use the sample’s web.xml file as inspiration.
A servlet listener to bootstrap the root application context—If your web application uses Spring, this listener is already there, so just add an entry for the Spring Batch Admin Spring configuration file.
Servlet filters to deal with HTTP headers in requests and responses—They’re useful mainly because Spring Batch Admin uses some RESTful-styled communication between the client and the server.
Be careful to avoid conflicts with your own application when configuring the URL mapping of Spring Batch Admin.
Once you have completed this configuration, you can package your web application with Maven and deploy it in a web container.
You’ll get the same result as in the previous section, but now Spring Batch Admin will be bundled inside your own application!
The next section covers how to configure Spring Batch Admin to plug in your own environment.
B.4 Configuring Spring Batch Admin Spring Batch Admin uses Spring for its configuration, and it’s straightforward to set up for your own environment.
We’ll see first how to plug Spring Batch Admin into the batch metadata.
You’ll be able to monitor the execution of your batch jobs, even if they don’t run in the same process as Spring Batch Admin.
Then, we’ll see how to add job configurations to Spring Batch Admin.
You’ll be able to monitor job executions and start and stop jobs from Spring Batch Admin.
We’ll finish with advanced settings used to integrate Spring Batch Admin more deeply with an existing application.
Spring Batch Admin needs only one thing to plug into batch metadata: how to connect to the database that hosts these metadata.
The following listing shows an example of such a properties file to connect to an H2 database.
In a Maven project, the file can be in the src/main/resources directory.
The file must contain the four usual database connection settings: driver class, URL, username, and password.
You can also specify the implementation of a value incrementer for Spring Batch to generate primary keys.
The configuration in listing B.2 tells Spring Batch not to run any SQL scripts on the target database.
This assumes the database contains all the batch metadata tables (remember, the scripts to create these tables are in the Spring Batch core JAR file)
Once you complete the database settings, you can package the application and deploy it.
The application will connect to the batch metadata, and you’ll be able to.
Configuring Spring Batch Admin monitor the execution of your jobs.
By configuring just the connection to the batch metadata, you can’t really use Spring Batch Admin as the only entry point to your batch infrastructure.
Spring Batch Admin scans a specific location to find Spring configuration files that define jobs.
Each configuration file must be self-contained: it must define a job and all the Spring beans the job depends on, except for infrastructure beans like the job repository, the data source, and the transaction manager.
Every bean defined in the root application context of the web application is visible to a job in a Spring configuration file.
That’s why job configurations can depend on such common beans and don’t need to define them.
Such a deployment is powerful: you can write your Spring Batch jobs in standalone projects and deploy them inside a Spring Batch Admin instance.
Delete these jobs if you don’t want them to appear in your Spring Batch Admin instance.
The Spring Batch Admin UI isn’t the only way to trigger a job execution: you can embed a Java scheduler like Quartz or Spring Scheduler inside the application and let jobs be kicked off periodically.
Look at chapter 4 for the various ways to use a Java scheduler with Spring Batch.
Once Spring Batch Admin connects to your batch metadata and can accept your new jobs, it gives you a great view of your batch infrastructure.
You can even go further through the configuration and change some Spring Batch Admin internals.
This is especially useful when Spring Batch Admin must cohabit with an existing business application in the same web application.
Spring Batch Admin includes configuration for Spring beans like the data source, the transaction manager, and the job repository.
Spring Batch Admin lets you configure parts of these beans, as we saw in section B.4.1 when we used a properties file for the.
Spring Batch Admin also lets you override part of its configuration.
This means you can define Spring beans that Spring Batch Admin will use in place of the default beans.
Imagine that your data source comes from an application server as a Java Naming and Directory Interface (JNDI) resource.
In this case, the database connection settings don’t make sense because you only need to perform a JNDI lookup.
You can define a data source bean and use Spring’s JNDI support for the lookup.
The bean definition must be loaded after Spring Batch Admin bean definitions.
To meet the first condition, you need to know the names of beans in the Spring Batch Admin configuration.
Table B.2 lists some of the beans that are likely to be overridden.
The Spring Batch Admin reference guide also provides the list of the beans you can override.
You now know about overriding infrastructure beans, which is the first condition to meet to change the configuration of Spring Batch Admin.
The second condition is to ensure that the definitions of the new beans are loaded after the Spring Batch Admin configuration.
This doesn’t mean you need an extra application context; it means you need to be careful about the order in which Spring configuration files are loaded.
Bean definitions override previous definitions with the same ID, so the order in which configuration files are loaded matters.
This a rule Spring enforces in its application context implementations.
To be sure your properly named beans override the default beans, you have two options (you can use one or the other or both):
Once both conditions are met (names for the beans and correct locations for the files), you’re ready to configure some the Spring Batch Admin infrastructure.
The following listing shows how to override the data source and the task executor to launch jobs.
By default, the task:executor element uses the Java 5 thread pool.
Being able to override some of the key components of Spring Batch Admin is powerful.
Spring Batch Admin can share any resource you use in your business application.
You can plug in a server-provided data source, as we did in listing B.3, where Spring looks up the data source through JNDI.
In listing B.3, we also defined our own thread-pooled task executor for the job launcher to use.
Note that you can also plug in a serverprovided thread pool like the JCA WorkManager or CommonJ (depending on what’s available in your application server)
Spring provides a bridge with a TaskExecutor implementation for CommonJ.
Spring Batch Admin can then use resources from the application server.
You have multiple deployment options with Spring Batch Admin: you can deploy it in its own web application or let it cohabit with other applications in the same web application.
The former is well suited to headless batch jobs if they’re not part of any web frontend.
The latter works nicely when you embed Spring Batch in a web application and run it as a business application frontend.
The first setup step is to plug Spring Batch Admin into your batch metadata to monitor the execution of your jobs.
You can then go further and use Spring Batch Admin to start and stop job executions.
You can also override the Spring Batch Admin infrastructure to plug in existing infrastructure components, like a data source or a thread pool provided by your server.
See DBCP database cursors, reading with 140–144 database item writers.
See XML externalizing job flow definitions 300–302 extract, transform, and load process.
Spring Batch is a framework for writing batch applications in Java.
And it uses Spring’s familiar programming model to simplify confi guration and implementation, so it’ll be comfortably familiar to most Java developers.
Spring Batch in Action is a thorough, in-depth guide to writing effi  cient batch applications.
Starting with the basics, it discusses the best practices of batch jobs along with details of the Spring Batch framework.
You’ll learn by working through dozens of practical, reusable examples in key areas like monitoring, tuning, enterprise integration, and automated testing.
What’s Inside Batch programming from the ground up Implementing data components Handling errors during batch processing Automating tedious tasks.
