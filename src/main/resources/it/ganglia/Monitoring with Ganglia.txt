Monitoring with Ganglia, the image of a Porpita pacifica, and related trade dress are trademarks of O’Reilly Media, Inc.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a trademark claim, the designations have been printed in caps or initial caps.
While every precaution has been taken in the preparation of this book, the publisher and authors assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
In 1999, I packed everything I owned into my car for a cross-country trip to begin my new job as Staff Researcher at the University of California, Berkeley Computer Science Department.
It was an optimistic time in my life and the country in general.
The economy was well into the dot-com boom and still a few years away from the dot-com bust.
When I arrived at Berkeley, the Network of Workstations (NOW) project was just coming to a close.
The NOW team had clustered together Sun workstations using Myrinet switches and specialized software to win RSA key-cracking challenges and break a number of sort benchmark records.
The success of NOW led to a following project, the Millennium Project, that aimed to support even larger clusters built on x86 hardware and distributed across the Berkeley campus.
Ganglia exists today because of the generous support by the NSF for the NPACI project and the Millennium Project.
Long-term investments in science and education benefit us all; in that spirit, all proceeds from the sales of this book will be donated to Scholarship America, a charity that to date has helped 1.7 million students follow their dreams of going to college.
Of course, the real story lies in the people behind the projects—people such as Berkeley Professor David Culler, who had the vision of building powerful clusters out of commodity hardware long before it was common industry practice.
David Culler’s cluster research attracted talented graduated students, including Brent Chun and Matt Welsh, as well as world-class technical staff such as Eric Fraser and Albert Goto.
Ganglia’s use of a lightweight multicast listen/announce protocol was influenced by Brent Chun’s early work building a scalable execution environment for clusters.
Matt Welsh is well known for his contributions to the Linux community and his expertise was invaluable to the broader teams and to me personally.
Eric Fraser was the ideal Millennium team lead who was able to attend meetings, balance competing priorities, and keep the team focused while still somehow finding time to make significant technical contributions.
His software allowed me to easily deploy and test Ganglia on large clusters and definitely contributed to the speed and quality of Ganglia development.
I consider myself very lucky to have worked with so many talented professors, students, and staff at Berkeley.
I spent five years at Berkeley, and my early work was split between NPACI and Millennium.
Looking back, I see how that split contributed to the way I designed and implemented Ganglia.
I should mention here that Federico Sacerdoti was heavily involved in the implementation of gmetad and wrote a nice academic paper2 highlighting the strength of its design.
The components of Ganglia complement each other to deliver a scalable monitoring system that can handle a variety of deployment scenarios.
In 2000, I open-sourced Ganglia and hosted the project from a Berkeley website.
You can still see the original website today using the Internet Archive’s Wayback Machine.
The dendrite would send periodic heartbeats as well as publish any significant /proc metric changes on a common multicast channel.
To collect the dendrite updates, you deployed a single instance of a daemon process, called an axon,
If you ran ganglia without any options, it would output the following help: $ ganglia.
As you can see from the help page, the first version of ganglia allowed you to query and sort by 21 different system metrics right out of the box.
At one time, they were! The output of the ganglia command made it very easy to embed it inside of scripts.
For example, the output from Example P-1 could be used to autogenerate an MPI machine file that contained the least-loaded machines in the cluster for load-balancing MPI jobs.
Ganglia also automatically removed hosts from the list that had stopped sending heartbeats to keep from scheduling jobs on dead machines.
The UI allowed for filtering by hostname and could limit the total number of hosts displayed.
Ganglia 1.0 ran only on Linux, whereas Ganglia today runs on dozens of platforms.
Ganglia 1.0 had no time-series support, whereas Ganglia today leverages the power.
Ganglia 1.0 was not extensible, whereas Ganglia today can publish custom metrics via Python and C modules or a simple command-line tool.
Ganglia 1.0 could only be used for monitoring a single cluster, whereas Ganglia today can been used to monitor hundreds of clusters distributed around the globe.
I just checked our download stats and Ganglia has been downloaded more than 880,000 times from our core website.
When you consider all the third-party sites that distribute Ganglia packages, I’m sure the overall downloads are well north of a million! Although the NSF and Berkeley deserve credit for getting Ganglia started, it’s the generous support of the open source community that has made Ganglia what it is today.
Over Ganglia’s history, we’ve had nearly 40 active committers and hundreds of people who have submitted patches and bug reports.
The authors and contributors on this book are all core contributors and power users who’ll provide you with the in-depth information on the features they’ve either written themselves or use every day.
Reflecting on the history and success of Ganglia, I’m filled with a lot of pride and only a tiny bit of regret.
I’d like to thank Michael Loukides, Meghan Blanchette, and the awesome team at O’Reilly for making this book a reality.
Conventions Used in This Book The following typographical conventions are used in this book: Italic.
Indicates new terms, URLs, email addresses, filenames, and file extensions.
Constant width Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.
Constant width bold Shows commands or other text that should be typed literally by the user.
Constant width italic Shows text that should be replaced with user-supplied values or by values determined by context.
Using Code Examples This book is here to help you get your job done.
In general, you may use the code in this book in your programs and documentation.
You do not need to contact us for permission unless you’re reproducing a significant portion of the code.
For example, writing a program that uses several chunks of code from this book does not require permission.
Selling or distributing a CD-ROM of examples from O’Reilly books does require permission.
Answering a question by citing this book and quoting example code does not require permission.
Incorporating a significant amount of example code from this book into your product’s documentation does require permission.
An attribution usually includes the title, author, publisher, and ISBN.
Technology professionals, software developers, web designers, and business and creative professionals use Safari Books Online as their primary resource for research, problem solving, learning, and certification training.
Safari Books Online offers a range of product mixes and pricing programs for organizations, government agencies, and individuals.
For more information about Safari Books Online, please visit us online.
How to Contact Us Please address comments and questions concerning this book to the publisher:
We have a web page for this book, where we list errata, examples, and any additional information.
For more information about our books, courses, conferences, and news, see our website at http://www.oreilly.com.
Find us on Facebook: http://facebook.com/oreilly Follow us on Twitter: http://twitter.com/oreillymedia Watch us on YouTube: http://www.youtube.com/oreillymedia.
If you’re reading this, odds are you have a problem to solve.
I won’t presume to guess the particulars, but I’m willing to bet that the authors of this book have shared your pain at one time or another, so if you’re in need of a monitoring and metrics collection engine, you’ve come to the right place.
We created Ganglia for the same reason you’ve picked up this book: we had a problem to solve.
If you’ve looked at other monitoring tools, or have already implemented a few, you’ll find that Ganglia is as powerful as it is conceptually and operationally different from any monitoring system you’re likely to have previously encountered.
It runs on every popular OS out there, scales easily to very large networks, and is resilient by design to node failures.
In the real world, Ganglia routinely provides near real-time monitoring and performance metrics data for computer networks that are simply too large for more traditional monitoring systems to handle, and it integrates seamlessly with any traditional monitoring systems you may happen to be using.
In this chapter, we’d like to introduce you to Ganglia and help you evaluate whether it’s a good fit for your environment.
It’s a Problem of Scale Say you have a lot of machines.
I’m not talking a few hundred, I mean metric oodles of servers, stacked floor to ceiling as far as the eye can see.
Servers so numerous that they put to shame swarms of locusts, outnumber the snowflakes in Siberia, and must be expressed in scientific notation, or as some multiple of Avogadro’s number.
It would also need to store, graph, and present that data quickly and efficiently.
This is the problem domain for which Ganglia was designed; to monitor and collect massive quantities of system metrics in near real time for Large installations.
Large installations are interesting because they force us to reinvent or at least reevaluate every problem we thought we’d already solved as systems administrators.
The prospect of firing up rsync or kludging together some Perl is altogether different when 20,000 hosts are involved.
As the machines become more numerous, we’re more likely to care about the efficiency of the polling protocol, we’re more likely to encounter exceptions, and we’re less likely to interact directly with every machine.
That’s not even mentioning the quadratic curve towards infinity that describes the odds of some subset of our hosts going offline as the total number grows.
I don’t mean to imply that Ganglia can’t be used in smaller networks—swarms of locusts would laugh at my own puny corporate network and I couldn’t live without Ganglia—but it’s important to understand the design characteristics from which Ganglia was derived, because as I mentioned, Ganglia operates quite differently from other monitoring systems because of them.
The most influential consideration shaping Ganglia’s design is certainly the problem of scale.
Hosts ARE the Monitoring System The problem of scale also changes how we think about systems management, sometimes in surprising or counterintuitive ways.
For example, an admin over 20,000 systems is far more likely to be running a configuration management engine such as Puppet/Chef or CFEngine and will therefore have fewer qualms about host-centric configuration.
The large installation administrator knows that he can make configuration changes to all of the hosts centrally.
Smaller installations instead tend to favor tools that minimize the necessity to configure individual hosts.
Large installation admin are rarely concerned about individual node failures.
Designs that incorporate single points of failure are generally to be avoided in large application frameworks where it can be safely assumed, given the sheer amount of hardware involved, that some percentage of nodes are always going to be on the fritz.
Smaller installations tend to favor monitoring tools that strictly define individual hosts centrally and alert on individual host failures.
This sort of behavior quickly becomes unwieldy and annoying in larger networks.
This tendency to centralize and strictly define the configuration begets a central daemon that sits somewhere on the network and polls every host every so often for status.
These systems are easy to use in small environments: just install the (usually bloated) agent on every system and.
A single daemon will always be capable of polling only so many hosts, and every host that gets added to the network increases the load on the monitoring server.
Large installations sometimes resort to installing several of these monitoring systems, often inventing novel ways to roll up and further centralize the data they collect.
The problem is that even using roll-up schemes, a central poller can poll an individual agent only so fast, and there’s only so much polling you can do before the network traffic becomes burdensome.
In the real world, central pollers usually operate on the order of minutes.
Ganglia, by comparison, was born at Berkeley, in an academic, Grid-computing culture.
The HPC-centric admin and engineers who designed it were used to thinking about massive, parallel applications, so even though the designers of other monitoring systems looked at tens of thousands of hosts and saw a problem, it was natural for the Berkeley engineers to see those same hosts as the solution.
Ganglia’s metric collection design mimics that of any well-designed parallel application.
Every individual host in the grid is an active participant, and together they cooperate, organically distributing the workload while avoiding serialization and single points of failure.
The data itself is replicated and dispersed throughout the Grid without incurring a measurable load on any of the nodes.
Ganglia’s protocols were carefully designed, optimizing at every opportunity to reduce overhead and achieve high performance.
This cooperative design means that every node added to the network only increases Ganglia’s polling capacity and that the monitoring system stops scaling only when your network stops growing.
Polling is separated from data storage and presentation, both of which may also be redundant.
All of this functionality is bought at the cost of a bit more per-host configuration than is employed by other, more traditional monitoring systems.
Redundancy Breeds Organization Large installations usually include quite a bit of machine redundancy.
Whether we’re talking about HPC compute nodes or web, application, or database servers, the thing that makes large installations large is usually the preponderance of hosts that are working on the same problem or performing the same function.
So even though there may be tens of thousands of hosts, they can be categorized into a few basic types, and a single configuration can be used on almost all hosts that have a type in common.
There are also likely to be groups of hosts set aside for a specific subset of a problem or perhaps an individual customer.
Ganglia assumes that your hosts are somewhat redundant, or at least that they can be organized meaningfully into groups.
Ganglia refers to a group of hosts as a “cluster,”
The term originally referred to HPC compute clusters, but Ganglia has no particular rules about what constitutes a cluster: hosts may be grouped by business purpose, subnet, or proximity to the Coke machine.
In the normal mode of operation, Ganglia clusters share a multicast address.
This shared multicast address defines the cluster members and enables them to share information about each other.
Clusters may use a unicast address instead, which is more compatible with various types of network hardware, and has performance benefits, at the cost of additional per-host configuration.
If you stick with multicast, though, the entire cluster may share the same configuration file, which means that in practice Ganglia admins have to manage only as many configuration files as there are clusters.
Is Ganglia Right for You? You now have enough of the story to evaluate Ganglia for your own needs.
You have a number of computers with general-purpose operating systems (e.g., not routers, switches, and the like) and you want near real-time performance information from them.
In fact, in cooperation with the sFlow agent, Ganglia may be used to monitor network gear such as routers and switches (see Chapter 8 for more information)
You aren’t averse to the idea of maintaining a config file on all of your hosts.
Your hosts can be (at least loosely) organized into groups.
Your operating system and network aren’t hostile to multicast and/or User Datagram Protocol (UDP)
If that sounds like your setup, then let’s take a closer look at Ganglia.
As depicted in Figure 1-1, Ganglia is architecturally composed of three daemons: gmond, gmetad, and gweb.
Operationally, each daemon is self-contained, needing only its own configuration file to operate; each will start and run happily in the absence of the other two.
Certain advanced features such as sFlow, zeromq, and Graphite support may belie the use of gmetad and/or gweb; see Chapter 3 for details.)
Given that every gmond host multicasts metrics to its cluster peers, it follows that every gmond host must also record the metrics it receives from its peers.
In fact, every node in a Ganglia cluster knows the current value of every metric recorded by every other node in the same cluster.
An XML-format dump of the entire cluster state can be requested by a remote poller from any single node in the cluster on port 8649
This design has positive consequences for the overall scalability and resiliency of the system.
Only one node per cluster needs to be polled to glean the entire cluster status, and no amount of individual node failure adversely affects the overall system.
Further, for the 200 necessary network connections the poller must make, every metric (CPU, disk, memory, network, etc.) on every individual cluster node is recorded instead of just the single CPU metric.
The recent addition of sFlow support to gmond (as described in Chapter 8) lightens the metric collection and polling load even further, enabling Ganglia to scale to cloud-sized networks.
What performs the actual work of polling gmond clusters and storing the metric data to disk for later use? The short answer is also the title of the next section: gmetad, but there is a longer and more involved answer that, like everything else we’ve talked about so far, is made possible by Ganglia’s unique design.
Given that gmond operates on its own, absent of any dependency on and ignorant of the policies or requirements of a centralized poller, consider that there could in fact be more than one poller.
Any number of external polling engines could conceivably interrogate any combination of.
Multiple polling engines could be used to further distribute and lighten the load associated with metrics collection in large networks, but the idea also introduces the intriguing possibility of special-purpose pollers that could translate and/or export the data for use in other systems.
As I write this, a couple of efforts along these lines are under way.
The first is actually a modification to gmetad that allows gmetad to act as a bridge between gmond and Graphite, a highly scalable data visualization tool.
The next is a project called gmond-zeromq, which listens to gmond broadcasts and exports data to a zeromq message bus.
It’s not because we think gmond is more efficient, scalable, and better designed than most agent software.
All of that is, of course, true, but the real reason the comparison pains us is that Ganglia’s architecture fundamentally alters the roles between traditional pollers and agents.
Instead of sitting around passively, waiting to be awakened by a monitoring server, gmond is always active, measuring, transmitting, and sharing.
This architecture allows for a much simpler poller design, entirely removing the need for the poller to know what services to poll from which hosts.
Such a poller needs only a list of hostnames that specifies at least one host per cluster.
The clusters will then inform the poller as to what metrics are available and will also provide their values.
Of course, the poller will probably want to store the data it gleans from the cluster nodes, and RRDtool is a popular solution for this sort of data storage.
This is fine for a few days of data, but it’s not optimal to store 8,640 data points per day for a year for every metric on every machine in the network.
This sort of data storage scheme lets us analyze recent data with great specificity while at the same time providing years of historical data in a few megabytes of disk space.
It has the added benefit of allocating all of the required disk space up front, giving us a very predictable capacity planning model.
It is a simple poller that, given a list of cluster nodes, will poll each cluster, writing whatever data values are returned for every metric on every host to individual round robin databases.
Indeed, gmetad could easily be replaced by a shell script that used netcat to glean the XML dump from various gmond nodes and then parse and write the data to RRDtool databases via command-line tools.
As of this writing, there is, in fact, already a Pythonbased replacement for gmetad, which adds a plug-in architecture, making it easier to write custom data-handling logic.
It includes interactive query functionality and may be polled by external monitoring systems via a simple text protocol on TCP port 8652
Finally, as mentioned in the previous section, gmetad is also capable of sending data to Graphite, a highly scalable data visualization engine.
You want graphs that make your data dance, brimming with timely, accurate data and contrasted, meaningful colors.
And not just pretty graphs, but a snazzy, welldesigned UI to go with them—a UI that is generous with the data, summarizing the status of the entire data center in just a few graphs while still providing quick, easy access to every combination of any individual metrics.
It should do this without demanding that you preconfigure anything, and it should encourage you to create your own graphs to explore and analyze your data in any way you can imagine.
If it seems like I’m reading your mind, it’s because the Ganglia authors are engineers like you, who designed Ganglia’s visualization UI, gweb, from their own notion of the ideal data visualization frontend.
Quite a bit of thought and real-world experience has gone into its creation, and we think you’ll find it a joy to work with.
It knows what hosts exist, and what metrics are available for those hosts, but it doesn’t make you click through hierarchal lists of metrics to see graphs; rather, it graphically summarizes the entire grid using graphs that combine metrics by cluster and provides sane click-throughs for increased specificity.
If you’re interested in something specific, you can specify a system name, or a regex or type-glob to combine various metrics from various hosts to create a custom graph of exactly what you want to see.
Before I move on, however, I should mention that gweb is a PHP program, which most people run under the Apache web server (although any web server with PHP or FastCGI support should do the trick)
It is usually installed on the same physical hardware as gmetad, because it needs access to the RRD databases created by the poller.
Installation details and specific software requirements are provided in Chapter 2
If you’re adept at any of those languages, you should appreciate the thorough documentation of gmond’s internals included herein, written by the implementor of gmond’s modular interface.
I personally wish that documentation of this quality had existed when I undertook to write my first gmond module.
Anyone who has spent any time on the Ganglia mailing lists will recognize the names of the authors of Chapter 6
Bernard and Daniel both made the mistake of answering one too many questions on the Ganglia-General list and have hence been tasked with writing a chapter on troubleshooting.
If you have a problem that isn’t covered in Chapter 6, odds are you’ll eventually get the answer you’re looking for from either Bernard or Daniel on the Ganglia lists.
Ganglia includes built-in functionality that enables it to integrate with both of these tools, each of which extend Ganglia’s functionality beyond what would otherwise be a limitation.
Finally, the chapter we’re all most excited to bring you is Chapter 9, wherein we’ve collected detailed descriptions of real-world Ganglia installs from several fascinating organizations.
Each case study highlights the varied and challenging monitoring requirements of the organization in question and goes on to describe the Ganglia configuration employed to satisfy them.
Any customizations, integration with external tools, and other interesting hurdles are also discussed.
The authors, all of whom are members of and contributors to the Ganglia community, undertook to write this book ourselves to make sure it was the book we would have wanted to read, and we sincerely hope it meets your needs.
If you’ve made it this far, it is assumed that you’ve decided to join the ranks of the Ganglia user base.
Congratulations! We’ll have your Ganglia-user conspiracy to conquer the world kit shipped immediately.
Until it arrives, feel free to read through this chapter, in which we show you how to install and configure the various Ganglia components.
In this chapter, we cover the installation and configuration of Ganglia 3.1.x for some of the most popular operating systems, but these instructions should apply to later versions as well.
Installing Ganglia As mentioned earlier, Ganglia is composed of three components: gmond, gmetad, and gweb.
In this first section, we’ll cover the installation and basic setup of each component.
It’s a lightweight service that must be installed on each node from which you want to have metrics collected.
This daemon performs the actual metrics collection on each host using a simple listen/announce protocol to share the data it gleans with its peer nodes in the cluster.
Using gmond, you can collect a lot of system metrics right out of the box, such as CPU, memory, disk, network, and data about active processes.
Ganglia packages are available for most Linux distributions, so if you are using the package manager shipped with your distribution.
The Ganglia components are available in a prepackaged binary format for most Linux distributions.
We’ll cover the two most popular types here: .deb- and .rpm-based systems.
You’ll find that some RPM-based distributions ship with Ganglia packages in the base repositories, and others require you to use special-purpose package repositories, such as the Red Hat project’s EPEL (Extra Packages for Enterprise Linux) repository.
If you’re using a RPM-based distro, you should search in your current repositories for the gmond package:
If the search fails, chances are that Ganglia is not shipped with your RPM distribution.
Red Hat users need to install Ganglia from the EPEL repository.
If you need to add the EPEL repository, be sure to take careful note of the distro version and architecture you are running and match it to that of the EPEL you’re adding.
Finally, to install gmond, type: user@host:# sudo yum install ganglia-gmond.
Refer to the following instructions, which work for the latest Mac OS X Lion.
For other versions of Mac OS X, the dependencies might vary.
Several dependencies must be satisfied before building and installing Ganglia on OS X.
Xcode is a collection of development tools, and an Integrated Development Environment (IDE) for OS X.
You will find Xcode at Apple’s developer tools website for download or on the MAC OS X installation disc.
MacPorts is a collection of build instructions for popular open source software for OS X.
It is architecturally identical to the venerable FreeBSD Ports system.
To install MacPorts, download the installation disk image from the MacPorts website.
If you’re using Snow Leopard, the download is located here.
For older versions, please refer here for documentation and download links.
Once MacPorts is installed and working properly, use it to install both libconfuse and pkconfig:
After satisfying the previously listed requirements, you are ready to proceed with the installation.
Change to the directory where the source file has been downloaded.
On Mac OS X 10.5+, you need to apply a patch so that gmond builds successfully.
For further details on the patch, please visit the website.
Download the patch file, copy it to the root of the build directory, and run the patch:
Assuming that you installed MacPorts under the default installation directory (/opt/ local), export MacPorts’ bin directory to your PATH and run the configure script, specifying the location of lib/ and include/ as options:
Convenient binary packages for Solaris are distributed in the OpenCSW collection.
Run the pkgutil tool on Solaris, and then use the tool to install the package:
The default location for the configuration files on Solaris (OpenCSW) is /etc/opt/csw/ ganglia.
You can now start and stop all the Ganglia processes using the normal SMF utility on Solaris, such as:
Because Ganglia is an open source project, it is possible to compile a runnable binary executable of the gmond agent on virtually any platform with a C compiler.
The Ganglia projects uses the autotools build system to detect the tools available on most Linux and UNIX-like environments and build the binaries.
The autotools build system is likely to have support for many other platforms that are not explicitly documented in this book.
Please start by reading the INSTALL file in the source tree, and also look online for tips about Ganglia or generic tips about using autotools projects in your environment.
It also provides a simple query mechanism for collecting specific information about groups of machines and supports hierarchical delegation, making possible the creation of federated monitoring domains.
The requirements for installing gmetad on Linux are nearly the same as gmond, except for the addition of RRDtool, which is required to store and display time-series data collected from other gmetad or gmond sources.
Once again, you are encouraged to take advantage of the prepackaged binaries available in the repository of your Linux distribution; we provide instructions for the two most popular formats next.
As mentioned in the earlier gmond installation section, an EPEL repository must be installed if the base repositories don’t provide gmetad.
There are only two functional differences between building gmond and gmetad on OS X.
First, gmetad has one additional software dependency (RRDtool), and second, you must include the --with-gmetad option to the configure script, because only gmond is built by the default Makefile.
Following is the list of requirements that must be satisfied before you can build gmetad on Mac OS X:
Once you have those sorted out, install the following packages to satisfy the requirements:
Once those packages have been installed, proceed with the Ganglia installation by downloading the latest Ganglia version.
Download the patch file and copy it to the root of the extracted Ganglia source tree, then apply it:
Assuming that you installed MacPorts under the default installation directory (/opt/ local)
Export MacPorts’ /bin directory to your PATH, and run the configure script, specifying the location of lib/ and include/ as options.
Convenient binary packages for Solaris are distributed in the OpenCSW collection.
Run the pkgutil tool on Solaris, and then use the tool to install the package:
The default location for the configuration files on Solaris (OpenCSW) is /etc/opt/csw/ ganglia.
You can now start and stop all the Ganglia processes using the normal SMF utility on Solaris, as in:
After collecting several different metrics in order to evaluate how our cluster is performing, we certainly need a visual representation, preferably using graphics in the Web.
Please see the “Demos” section here for live demos of the web frontend.
As of Ganglia 3.4.0, the web interface is a separate distribution tarball maintained in a separate source code repository.
The release cycle and version numbers of gweb are no.
Future versions of gweb may require a later version of gmond/gmetad.
It’s recommended to check the installation documentation for exact details whenever installing or upgrading gweb.
This book covers gweb versions 3.4.x and later, which may not be available to all distributions, requiring more work to get it installed.
If you are installing from the repositories, the installation is pretty straightforward.
Requirements will be automatically satisfied, and within a few commands you should be able to play with the web interface.
To install gweb on a Debian-based Linux distribution, execute the following command as either root or user with high privilege:
This command installs Apache and PHP 5 to satisfy its dependencies, in case you don’t have it already installed.
You might have to enable the PHP JSON module as well.
Once it’s downloaded, explode and edit Makefile to install gweb:
This means that gweb will be available to the user here.
The way to install gweb on a RPM-based distribution is very similar to installing gweb on a Debian-based distribution.
You also need to enable the JSON extension for PHP.
Once downloaded, explode and edit Makefile to install gweb 2:
If you need to install gweb on Mac OS X, you have to follow a slightly different approach than if you were installing in Linux.
Again, there isn’t any binary package for Mac OS X, leaving you with the option of downloading the source from the website.
Before downloading, you have to make sure that your Mac OS X has shipped with a few of the requirements.
First off, an HTTP server is required, and chances are good that your Mac OS X installation was shipped with Apache Web Server.
You can also install it via MacPorts, but this approach is not covered here.
To enable, edit the httpd.conf file and uncomment the line that loads the php5_module.
Now that you have satisfied the requirements, it’s time to download and install gweb 2
Once you have finished, change to the directory where the file is located and extract its content.
This next step really depends on how Apache Web Server is set up on your system.
You need to find out where Apache serves its pages from or, more specifically, its DocumentRoot.
Of course, the following location isn’t the only possibility, but for clarity’s sake, we will work with the default settings.
Insert the location of your Apache’s DocumentRoot and the name of the user that Apache runs.
This means that gweb will be available to the user here.
If no errors are shown, Ganglia Web is successfully installed.
Read the next sections to configure Ganglia prior to running it for the first time.
Convenient binary packages for Solaris are distributed in the OpenCSW collection.
Run the pkgutil tool on Solaris, and then use the tool to install the package:
The default location for the configuration files on Solaris (OpenCSW) is /etc/opt/csw/ ganglia.
You can now start and stop all the Ganglia processes using the normal SMF utility on Solaris, as in:
Configuring Ganglia The following subsections document the configuration specifics of each Ganglia component.
We would also like you to understand how the choice of a particular option may affect Ganglia deployment in your environment.
It interacts with the host operating system to obtain metrics and shares the metrics it collects with other hosts in the same cluster.
Every gmond instance in the cluster knows the value of every metric collected by every host in the same cluster and by default provides an XML-formatted dump of the entire cluster state to any client that connects to gmond’s port.
Of particular importance in this diagram is the disparate nature of the gmond daemon.
Internally, gmond’s sending and receiving halves are not linked (a fact that is emphasized in Figure 2-1 by the dashed vertical line)
Any local data captured by the metric modules are transmitted directly to the network by the sender, and the receiver’s internal database contains only metric data gleaned from the network.
This topology is adequate for most environments, but in some cases it is desirable to specify a few specific listeners rather than allowing every node to receive (and thereby waste CPU cycles to process) metrics from every other node.
More detail about this architecture is provided in Chapter 3
The deaf and mute parameters exist to allow some gmond nodes to act as special-purpose aggregators and relays for other gmond nodes.
Mute means that the node does not transmit; it will not even collect information about itself but will aggregate the metric data from other gmond daemons in the cluster.
Deaf means that the node does not receive any metrics from the network; it will not listen to state information from multicast peers, but if it is not muted, it will continue sending out its own metrics for any other node that does listen.
The use of multicast is not required in any topology.
The deaf/mute topology can be implemented using UDP unicast, which may be desirable when multicast is not practical or preferred (see Figure 2-3)
Further, it is possible to mix and match the deaf/mute, and default topologies to create a system architecture that better suits your environment.
At least one gmond instance must receive all the metrics from all nodes in the cluster.
Periodically, gmetad must poll the gmond instance that holds the entire cluster state.
In practice, however, nodes not configured with any multicast connectivity do not need to be deaf; it can be useful to configure such nodes to send metrics to themselves using the address 127.0.0.1 so that they will keep a record of their own metrics locally.
This makes it possible to make a TCP probe of any gmond for an XML about its own agent state while troubleshooting.
For a more thorough discussion of topology and scalability considerations, see Chapter 3
You can generate a default configuration file for gmond by running the following command:
The configuration file is composed of sections, enclosed in curly braces, that fall roughly into two logical categories.
The sections in the first category deal with host and cluster configuration; those in the second category deal with the specifics of metrics collection and scheduling.
Some may be defined in the configuration file multiple times; others must appear only once.
The include directive can be used to break up the gmond.conf file into multiple files for environments with large complex configurations.
The configuration file is parsed using libconfuse, a third-party API for configuration files.
In particular, boolean values can be set using yes, true, and on for a positive value and their opposites, no, false, and off for a negative value.
There are eight sections that deal with the configuration of the host itself.
The globals section configures the general characteristics of the daemon itself.
The following is the default globals section from Ganglia 3.3.1:
Set this value to false if you’re running gmond under a daemon manager such as daemontools.
A debug_level greater than zero will result in gmond running in the foreground and outputting debugging information.
It is not a good idea to change this value.
Mute” gmond nodes are only mute when it comes to other gmond daemons.
They still respond to queries from external pollers such as gmetad.
In large grids with thousands of nodes per cluster, or carefully optimized HPC grids, in which every CPU cycle spent on something other than the problem is a wasted cycle, “normal” compute nodes are often configured as deaf in order to minimize the overhead associated with aggregating cluster state.
In these instances, dedicated nodes are set aside to be mute.
In such a setup, the performance metrics of the mute nodes aren’t measured because those nodes aren’t a computationally relevant portion of the grid.
Their job is to aggregate, so their performance data would pollute that of the functional portion of the cluster.
This value might be useful if you are using your own frontend and would like to save some bandwidth.
Because messages may get lost in the network, gmond will consider the host as being down if it has not received any messages from it after four times this value.
This approach requires that gexecd be running on the host and the proper keys have been installed.
This directive by default is set to 0, which means that gmond will send the metadata packets only at startup and upon request from other gmond nodes running remotely.
If a new machine running gmond is added to a cluster, it needs to announce itself and inform all other nodes of the metrics that it currently supports.
In multicast mode, this isn’t a problem, because any node can request the metadata of all other nodes in the cluster.
However, in unicast mode, a resend interval must be established.
The interval value is the minimum number of seconds between resends.
If omitted, defaults to the value of the compile-time option: --with-moduledir.
This option, in turn, defaults to a subdirectory named Ganglia in the directory where libganglia will be installed.
To discover the default value in a particular gmond binary, generate a sample configuration file by running:
For example, in a 32-bit Intel-compatible Linux host, the default is usually at /usr/ lib/ganglia.
Each gmond daemon will report information about the cluster in which it resides using the attributes defined in the cluster section.
The default values are the string "unspecified"; the system is usable with the default values.
This section may appear only once in the configuration file.
The attributes in the cluster section directly correspond to the attributes in the CLUSTER tag in the XML output from gmond.
When the node is polled for an XML summary of cluster state, this name is inserted in the CLUSTER element.
The gmetad polling the node uses this value to name the directory where the cluster data RRD files are stored.
It supersedes a cluster name specified in the gmetad.conf configuration file.
The name attribute specified in the cluster section does place this host into a cluster.
The multicast address and the UDP port specify whether a host is on the cluster.
The name attribute acts justs as an identifier when polling.
The host section provides information about the host running this instance of gmond.
Clusters are defined by UDP communication channels, which is to say, that a cluster is nothing more than some number of gmond nodes that share the same send and/or receive channels.
By default, every node in a gmond cluster multicasts its own metric data to its peers via UDP and listens for similar UDP multicasts from its peers.
This is easy to set up and maintain: every node in the cluster shares the same multicast address, and new nodes are automatically discovered.
For this reason, any number of gmond send and receive channels may be configured to meet the needs of your particular environment.
Each configured send channel defines a new way that gmond will advertise its metrics, and each receive channel defines a way that gmond will receive metrics from other nodes.
Note that a gmond node should not be configured to contribute metrics to more than one Ganglia cluster, nor should you attempt to receive metrics for more than one cluster.
This option creates a multicast channel and is mutually exclusive with host.
This option creates a unicast channel and is mutually exclusive with mcast_join.
If it’s not set, port 8649 is used by default.
Setting this value to any value higher than necessary could result in metrics being transmitted across WAN connections to multiple sites or even out into the global Internet.
If you do not specify multicast attributes, gmond will create a unicast UDP server on the specified port.
Ganglia will not allow IPv6=>IPv4 mapping (for portability and security reasons)
Ganglia will not allow IPv6=>IPv4 mapping (for portability and security reasons)
This list allows you to specify addresses and address ranges from which gmond will accept or deny connections.
The syntax should be fairly self-explanatory to anyone with a passing familiarity with access control concepts.
The default attribute defines the default policy for the entire ACL.
Any number of access blocks may be specified that list hostnames or IP addresses and associate allow or deny actions to those addresses.
The mask attribute defines a subnet mask in CIDR notation, allowing you to specify address ranges instead of individual addresses.
Notice that in case of conflicting ACLs, the first match wins.
Originally targeted at embedded network hardware, sFlow collectors now exist for general-purpose operating systems as well as popular applications such as Tomcat, memcached, and the Apache Web Server.
Further information about sFlow interoperability is provided in Chapter 8
The modules section contains the parameters that are necessary to load a metric module.
Metric modules are dynamically loadable shared object files that extend the available metrics gmond is able to collect.
Much more information about extending gmond with modules can be found in Chapter 5
Each modules section must contain at least one module subsection.
The default configuration contains every module available in the default installation, so you should not have to change this section unless you’re adding new modules.
Alternatively, the name can be the name of the source file if the module has been implemented in an interpreted language such as Python.
If the enabled directive is not included in the module configuration, the enabled state will default to yes.
If a module that has been disabled contains a metric that is still listed as part of a collection group, gmond will produce a warning message but will continue to function normally by ignoring the metric.
If the value of path does not begin with a forward slash, the value will be appended to that of the module_path attribute from the globals section.
Multiple parameters can be passed to the module’s initialization function by including one or more param sections.
Each param section must be named and contain a value directive.
The collection_group entries specify the metrics that gmond will collect, as well as how often gmond will collect and broadcast them.
You may define as many collection groups as you wish.
Each collection group must contain at least one metric section.
These are logical groupings of metrics based on common collection intervals.
The groupings defined in gmond.conf do not affect the groupings used in the web interface, nor is it possible to use this mechanism to specify the names of the groups for the web interface.
This includes metrics such as the OS type or the number of CPUs installed in the system.
These metrics need to be collected only once at startup and are configured by setting the collect_once attribute to yes.
By using the name_match parameter instead of name, it is possible to use a single definition to configure multiple metrics that match a regular expression.
You can get a list of the available metric names by running gmond with an -m switch.
The units denoted by the value vary according to the metric module.
For CPU stats, for example, the value represents a percentage, and network stats interpret the value as a raw number of bytes.
Any time a value_threshold is surpassed by any single metric in a collection group, all metrics in that collection group are sent to every UDP receive channel.
By default, gmetad will collect and aggregate these metrics in RRD files, but it is possible to configure gmetad to forward metrics to external systems such as Graphite instead.
It also responds to interactive requests on tcp port 8652
The interactive facility allows simple subtree and summation views of the grid state XML tree.
The simplest topology is a single gmetad process polling one or more gmond instances, as illustrated in Figure 2-4
Redundancy/high availability is a common requirement and is easily implemented.
Figure 2-5 shows an example in which two (redundant) gmetads poll multiple gmonds in the same cluster.
In large installations in which there is an IO constraint, rrdcached acts as a buffer between gmetad and the RRD files, as illustrated in Figure 2-7
The gmetad.conf configuration file is composed of single-line attributes and their corresponding values.
Attribute names are case insensitive, but their values are not.
Some may be defined in the configuration file multiple times; others must appear only once.
Each data_source line describes either a gmond cluster or a gmetad grid from which this gmetad instance will collect information.
If gmetad detects that the data_source refers to a cluster, it will maintain a complete set of round robin databases for the data source.
If, however, gmetad detects that the data_source refers to a grid, it will maintain only summary RRDs.
Setting the scalable attribute to off overrides this behavior and forces gmetad to maintain a full set of RRD files for grid data sources.
The following examples, excerpted from the default configuration file, are valid data sources:
The first is a string that uniquely identifies the source.
The second field is a number that specifies the polling interval for the data_source in seconds.
The third is a space-separated list of hosts from which gmetad may poll the data.
The addresses may be specified as IP addresses or DNS hostnames and may optionally be suffixed by a colon followed by the port number where the gmond tcp_accept_channel is to be found.
If no port number is specified, gmetad will attempt to connect to tcp/8649
Two or three are usually sufficient to ensure that data is collected in the event of a node failure.
The following attributes affect the functioning of the gmetad daemon itself: gridname (text)
Used by other gmetad instances to locate graphs for this instance’s data sources.
Several attributes affect the creation and handling of RRD files.
The full details of an RRA specification are contained in the manpage for rrdcreate(1)
It is possible to export all the metrics collected by gmetad to Graphite, an external open source metrics storage and visualization tool, by setting the following attributes.
This setting is important because gmetad’s carbon sender is not threaded and will block waiting on a response from a down carbon daemon.
As mentioned previously, gmetad listens on TCP port 8652 (by default) for interactive queries.
The interactive query functionality enables client programs to get XML dumps of the state of only the portion of the Grid in which they’re interested.
Interactive queries are performed via a text protocol (similar to SMTP or HTTP)
Queries are hierarchal, and begin with a forward slash (/)
For example, the following query returns an XML dump of the entire grid state:
To narrow the query result, specify the name of a cluster: /cluster1
To narrow the query result further, specify the name of a host in the cluster: /cluster1/host1
Queries may be suffixed with a filter to modify the type of metric information returned by the query (as of this writing, summary is the only filter available)
For example, you can request only the summary metric data from cluster1 like so:
In fact, there is no need to change anything whatsoever in gweb’s default configuration file to get up and running with a fully functional web UI.
Although gweb itself requires no configuration to speak of, some web server configuration is necessary to get gweb up and running.
For further reading on the subject, we recommend further reading here.
In fact, this file overrides and extends the default configuration set in conf_default.php.
The file, as its name suggests, is itself a PHP script made up of variable assignments.
Unlike the other configuration files, assignments might span multiple lines.
Attribute names are themselves keys in gweb’s $conf data structure, so they are case sensitive, and look like PHP array assignments.
The following line, for example, informs gweb of the location of the RRDtool binary:
All attributes in the file are required, and some may be defined multiple times; others must appear only once.
Attributes in this category affect gweb’s functional parameters—its own home directory, for example, or the directories in which it will check for RRDs or templates.
These are rarely changed by the user, but a few of them bear mentioning.
Specifies the directory in which gweb will search for template files.
Templates are like a skin for the site that can alter its look and feel.
As described in the next chapter, users may specify custom report graphs in JSON format and place them in this directory, and they will appear in the UI.
As described in Chapter 7, various Nagios integration features may be set in gweb’s conf.php.
Collectively, these enable Nagios to query metric information from gweb instead of relying on remote execution systems such as Nagios Service Check Acceptor (NSCA) and Nagios Remote Plugin Executor (NRPE)
The config.php file defines numerous settings that modify the functional attributes of the graphs drawn in the UI.
For example, you may change the colors used to plot the values in the built-in load report graph and the default colors used in all the graphs and even define custom time ranges.
For more information on the authorization features in gweb, see Chapter 4
This approach requires you to install patched versions of whisper and the Graphite webapp on your gweb server.
Postinstallation Now that Ganglia is installed and configured, it’s time to get the various daemons started, verify that they’re functional, and ensure that they can talk to each other.
Starting Up the Processes Starting the processes in a specific order is not necessary; however, if the daemons are started in the order recommended here, there won’t be a delay waiting for metadata to be retransmitted to the UDP aggregator and users won’t get error pages or incomplete data from the web server:
If you’re using the UDP unicast topology, start the UDP aggregator nodes first.
This ensures that the aggregator nodes will be listening when the other nodes send their first metadata transmission.
Remember rrdcached If gmetad is configured to use rrdcached, it is essential for rrdcached to be running before gmetad is started.
Testing Your Installation gmond and gmetad both listen on TCP sockets for inbound connections.
To test whether gmond is operational on a given host, telnet to gmond’s TCP port:
In reply, gmond should output an XML dump of metric data.
If the gmond is deaf or mute, it may return a rather empty XML document, with just the CLUSTER tag.
A functioning gmetad will respond with an XML dump of metric data.
See Chapter 6 for a more comprehensive list of techniques for validating the state of the processes.
Firewalls Firewall problems are common with new Ganglia installations that span network subnets.
Here, we’ve collected the firewall requirements of the various daemons together to help you avoid interdaemon communication problems:
If the gmond hosts must traverse a firewall to talk to each other, allow udp/8649 in both directions.
For multicast, support for the IGMP protocol must also be enabled in the intermediate firewalls and routers.
If gmetad must traverse a firewall to reach some of the gmond nodes, allow tcp/8649 inbound to a few gmond nodes in each cluster.
These ports are used by gweb, which is usually installed on the same host as gmetad, so unless you’re using some of the advanced integration features, such as Nagios integration, or have custom scripts querying gmetad, you shouldn’t need any firewall ACLs for gmetad.
If your Ganglia installation uses sFlow collectors and the sFlow collectors must traverse a firewall to reach their gmond listener, allow inbound udp/6343 to the gmond listener.
Who Should Be Concerned About Scalability? Scalability is discussed early in this book because it needs to be factored in at the planning stage rather than later on when stability problems are observed in production.
Scalability is not just about purchasing enough disk capacity to store all the RRD files.
Particular effort is needed to calculate the input/output operations per second (IOPS) demands of the running gmetad server.
A few hours spent on these calculations early on can avoid many hours of frustration later.
This chapter is a must-read for enterprises of that size: without it, the default Ganglia installation will appear to be completely broken and may even flood the network with metric data, interfering with normal business operations.
If it’s set up correctly (with a custom configuration), the authors can confirm that Ganglia performs exceptionally well in such an environment.
In fact, the number of nodes is not the only factor that affects scalability.
In a default installation, Ganglia collects about 30 metrics from each node.
However, third-party metric modules can be used to collect more than 1,000 metrics per node, dramatically increasing the workload on the Ganglia architecture.
Administrators of moderate-sized networks pursuing such aggressive monitoring strategies need to consider scalability in just the same way as a large enterprise with a huge network does.
For a small installation of a few dozen hosts, this chapter is not required.
It is necessary to consider how the receiving nodes are affected as the network is scaled: Memory impact.
The more nodes and the more metrics per node, the greater the memory consumption on those gmond processes that are configured to receive metrics from other nodes.
The faster the rate of metrics arriving from the network, the more the CPU core is utilized.
The metric arrival rate depends on three things: the number of nodes, the number of metrics per node, and the rate at which the nodes are configured to transmit fresh values.
No special effort needs to be made and this section can be skipped.
If you have a large installation, it is important to read this section carefully before proceeding to configure gmetad.
If the entire RRD file fits into one block on disk (4,096 bytes, typically) then all modifications to the file can, at best, be made with a single IO request.
The actual amount of data stored for a single data point in the RRD file is 8 bytes.
Furthermore, for the operating system to make this write, it first has to have the entire page/block in RAM.
If it is not in RAM, there is a read IO before the write IO, a total of two IOs for one write.
Having enough physical RAM for the page cache to retain all the currently active blocks from each RRD can avoid this double IO problem, reducing the IO load by 50 percent.
If the RRD file is keeping data for an extended period of time, or keeping different types of data (both MIN and MAX values), it may span more than one block and multiple IOs are required when a single update occurs.
This issue can be compounded by the read-before-write problem just described.
The RRD file structure interleaves the data from different data sources but not from different consolidation functions (such as MIN and MAX values)
In RRDtool deployments that store data from multiple sources in a single RRD file, this means that all the values for a particular time interval are likely to be stored in the same disk block accessible with a single disk IO read or write operation.
However, this optimization is not of any benefit with Ganglia: because Ganglia supports a variable number of data sources for each host and RRD files are inherently static in structure, Ganglia creates a different RRD file for each metric, rather than creating multiple data sources within a single RRD file.
If more than one consolidation function is used (for example, MIN, MAX, and AVERAGE values are retained for a single data source), these values are not interleaved.
The RRD file contains a separate array for each function.
Theoretically, if the sample interval and retention periods were equivalent, such values could be interleaved, but this approach is not currently supported by RRDtool.
Although interleaving is significantly more efficient for writing, it has the slight disadvantage that reading/reporting on a single consolidation function for a single source has to read all the interleaved data, potentially multiplying the number of blocks that must be read.
In many situations, the write workload is much heavier than the reporting workload, so this read overhead of interleaving is far outweighed by the benefits of the strategy.
It is likely that the array of 15-minute samples is in the second or third disk block of the file, so it is an extra write IO operation, it requires extra space in the page cache, and it may not be a contiguous block suitable for a sequential read or write.
This may not seem like a major overhead for one write out of every 60, until you contemplate the possibility that every RRD file may simultaneously make that extra write IO operation at the same time, stressing the write IO system and page cache in order to handle the amount of IO that is processed for every other sample.
In fact, RRDtool’s designers anticipated this nightmare scenario, and every RRD file has a randomized time offset to ensure that the IO workload is spread out.
Nonetheless, it is still important to bear in mind the extra IO effort of accessing multiple regions in the RRD file.
Understanding the file format at this level is essential to anyone involved in an extreme Ganglia deployment.
RRDtool is an open source project with an active community.
In conclusion, when deciding how to configure the RRA parameters in gmetad.conf, it is essential to consider not only the disk space but also the extra IO demand created by every consolidation function and sample interval.
Acute IO Demand During gmetad Startup The first thing to be aware of is the startup phase.
If gmetad stops for many hours, then on the next startup, it has to fill in the gaps in the RRD files.
This is a common scenario if a server goes offline for a 12-hour weekend maintenance window, or if a server crashes at midnight and is started up again by staff arriving eight hours later.
If the RRD parameters are configured to retain data with short sample intervals for many hours, this situation implies a lot of gaps to be filled.
During the startup phase, gmetad and RRDtool are trying to write to RRD files from each cluster in parallel.
This means that the disk write activity is not sequential.
Consequently, if there is a truly huge number of RRD files and if the disk storage system does not cope well with a random access workload (e.g., if it is not an SSD or SAN), the startup phase may take many hours or may not even complete at all.
During this phase, no new data is collected or stored.
Sometimes, the only way to bypass this startup issue is to delete all the existing RRDs (losing all history) and start afresh.
Therefore, when planning for scalability of the storage system, this startup phase should be considered the worst-case scenario that the storage must support.
Decisions about the storage system, sample intervals, and retention periods must be carefully considered to ensure a smooth startup after any lengthy period of downtime.
If each RRD file spans more than one disk block, then storing an update to the file is likely to involve multiple nonsequential writes: one write to the header (first block), one write to each RRA, and one write to the file access time (directory-level metadata)
Moving to the next RRD file is likely to be nonsequential, as gmetad does not actively order the files or the writes in any particular way.
The gmetad processes runs a separate thread for each Ganglia cluster.
Reading to generate web graphs/reports Each time the user accesses a page in the web UI, the web server reads from every RRD that is needed (maybe 30 separate files for a host view) to create the necessary graphs.
Due to the multithreaded nature of the web browser and web server, the read workload is likely to be random access.
Reading for reporting If any batch reports are executed on an hourly or daily basis, the workload may be sequential or random access, depending on the amount of data extracted by the report and the way the reporting script is written.
Due to the relatively simple architecture of Ganglia and RRDtool, they are inherently very scalable and adaptable.
Keep the previous points in mind as you design your Ganglia deployment, but rest assured that the lightweight and efficient architecture has been scrutinized by many talented engineers and there’s not much to improve upon.
As long as the workload is relatively constant (a constant number of hosts in the network), the IO workload will also remain somewhat constant.
There are clear solutions for this type of workload, which are covered extensively in the rest of this chapter.
When you consider all of these IO demands together, it should become obvious that careful planning and real-time monitoring of the gmetad servers is needed.
Forecasting IO Workload Before committing to a large installation of Ganglia, it is desirable to forecast the IO workload.
Here is a brief outline of how to make an accurate forecast:
Decide on the exact RRA structure you require (sample intervals, retention periods, and functions such as MIN, MAX, or AVERAGE)
Create a logical volume big enough for one RRD file (4 MB should be fine)
The simulation should generate enough writes to span all the sample intervals.
Divide the IOPS count by the value of $PERIOD to find the actual IOPS rate for a system running in real time (the simulation runs faster than real time, so the rate reported by iostat is meaningless)
Multiply that by the anticipated number of hosts and metrics to estimate the real IO workload.
Testing the IO Subsystem The previous section described how to estimate the required IO throughput (measured in IOPS)
It is important to verify that the purchased disk system/SAN can support the maximum throughput demands.
In many corporate environments, there is a standard SAN solution for all users.
The SAN may or may not be configured to operate at the IO levels claimed by the manufacturer.
Existing applications on the SAN may already be using a significant portion of its IO capacity.
Therefore, it is necessary to run a test to verify the actual performance that the SAN can offer.
It is a good idea to run such testing several times during the day and night to observe whether the SAN provides consistent performance.
For example, the SAN may be slower on Sundays due to IO-heavy backup/replication tasks.
The only way to discover these patterns is to run a simulation every 15 minutes for a week.
The example presented here is for the flexible IO tester tool (man fio)
First, create a configuration file for the tool, a sample of which is shown here:
Do some important numbers on SSD drives, to gauge what kind of # performance you might get out of them.
Sequential read and write speeds are tested, these are expected to be # high.
Random reads should also be fast, random writes are where crap # drives are usually separated from the good drives.
Change to the actual mount point of a SAN filesystem.
Increasing this value could make the test span more disks in the SAN, which makes the test output more reliable, but it also makes the test take longer.
The output report (san_test.out) is created in the current directory.
In this sample, much of the output is truncated, so that only the rand-write job output is displayed.
This job gives the best simulation of gmetad/RRDtool write behavior.
The key point to look at is the IOPS throughput rate, iops=193 in the sample.
If you find that the IOPS capacity of the SAN is insufficient, consider options such as reducing the number of RRAs for different functions (e.g., keep only MAX and not MIN or AVERAGE), get a faster SAN, or review the list of suggestions in the next section.
This directory usually resides on the local disk of the server that runs gmetad.
If you are experiencing slowdowns on your gmetad servers, consider purchasing a RAID system and storing the RRD files there.
Alternatively, good experiences have been reported by users placing RRD files on SAN storage or solid-state drives (SSDs)
Use a disk controller with a large hardware write cache.
Furthermore, these often have a battery-backup or flash-backup option that is essential for reliability with such a large write cache.
Although this approach is very reliable (and necessary) in a client/server model, it is inappropriate for applications that have a heavy IO demand, particularly if the application is accessing multiple files.
When a user invokes a web view that contains many graphs, many different RRD files are accessed and atime is updated for every RRD file (metadata write operation)
If it is a cluster view and the files are from different hosts, then each metadata access is on a different directory, so a read operation actually involves a massive nonsequential write operation at the same time.
Therefore, enabling the noatime option can eliminate a huge amount of unnecessary IO and have a huge benefit.
Similar to the previous solution, one could create a RAM disk using the physical memory available on the gmetad server.
By default, half of the physical memory will be used by tmpfs.
Because anything that is stored on tmpfs is volatile, the downside of this solution is that you will need a method of regularly synchronizing the data on tmpfs to persistent disk.
The interval of backup should be determined by how much data you are willing to lose if the server crashes unexpectedly.
You’ll also need to set up some script to automatically preload the data stored on persistent disk back to tmpfs after each reboot prior to gmetad starting up.
Considering the size of your RRD files, the number of RRAs, sample intervals, and retention periods, you can estimate the rate of IOPS.
Use a tool such as the flexible IO tester (man fio) to verify that your block device supports the target IO rate.
Use a web proxy such as Squid to cache the graphs so that the same graphs are not constantly regenerated for multiple users.
Set up cron jobs to prepare static HTML reports at desired intervals.
Give users access to the static HTML reports rather than direct access to gweb.
When updating RRDs, rrdtool must read and write entire disk blocks rather than just writing out the bytes that change.
Consequently, having enough RAM available for the page cache to buffer all active disk blocks (at least one 4,096-byte block per RRD file) can avoid the need for rrdtool to read blocks from disk before the write IO is executed.
So far, this book has dealt with the collection of data.
Visualization of these data is the primary responsibility of a web-based application known as gweb.
This chapter is an introduction to gweb and its features.
Whether the job is understanding how a problem began in your cluster or convincing management that more hardware is required, a picture is worth a thousand data points.
These tabs allow you to easily jump right to the information you need.
The gweb Main Tab gweb’s navigation scheme is organized around Ganglia’s core concepts: grids, clusters, and nodes.
As you click deeper into the hierarchy, breadcrumb-style navigation links allow you to return to higher-level views.
Figure 4-1 shows how you can easily navigate to exactly the view of the data you want.
Grid View The grid view (Figure 4-2) provides the highest-level view available.
Grid graphs summarize data across all hosts known to a single gmetad process.
Grid view is the jumpingoff point for navigating into more details displays dealing with individual clusters and the hosts that compose those clusters:
Clicking on any grid-level summary graphs brings up the all time periods display.
They may be grouped by physical location, common workload, or any other criteria.
The top of the cluster view (Figure 4-3) displays summary graphs for the entire cluster.
A quick view of each individual host is further down the page.
Clicking on a cluster summary shows you that summary of a range of time periods.
Clicking on an individual host takes you to the host display.
The background color of the host graphs is determined by their one-minute load average.
The metric displayed for each host can be changed using the Metric select box near the top of the page.
The utilization heatmap provides an alternate display of the one-minute load averages.
This is a very quick way to get a feeling for how evenly balanced the workload is in the cluster at the present time.
When working with a cluster with thousands of nodes, or when using gweb over a slow network connection, loading a graph for each node in the cluster can take a significant amount of time.
Cluster view also provides an alternative display known as physical view (Figure 4-4), which is also very useful for large clusters.
Physical view is a compressed text-only display of all the nodes in a cluster.
By omitting images, this view can render much more quickly than the main cluster view.
Clicking on a hostname in physical view takes you to the node view for that host.
Grid, cluster, and host views allow you to specify the time span (Figure 4-5) you’d like to see.
You are free to define your own time spans as well via your conf.php file.
All of the built-in time ranges are relative to the current time, which makes it difficult to see (for example) five minutes of data from two days ago, which can be a very useful view to have when doing postmortem research on load spikes and other problems.
The time range interface allows manual entry of begin and end times and also supports zooming via mouse gestures.
In both cluster and host views, it is possible to click and drag on a graph to zoom in on a particular time frame (Figure 4-6)
The interaction causes the entire page to reload, using the desired time period.
Note that the resolution of the data displayed is limited by what is stored in the RRD database files.
After zooming, the time frame in use is reflected in the custom time frame display at the top of the page.
You can clear this by clicking clear and then go.
Host View Metrics from a single gmond process are displayed and summarized in the host view (Figure 4-7)
Summary graphs are displayed at the top, and individual metrics are grouped together lower down.
Host Overview contains textual information about the host, including any string metrics being reported by the host, such as last boot time or operating system and kernel version.
Raw graph data can be exported as CSV or JSON.
Events can be turned off and on selectively on all graphs or specific graphs.
Trend analysis can make predictions about future metric values based on past data.
Graph can be time-shifted to show overlay of previous period’s data.
Node view (Figure 4-8) is an alternative text-only display of some very basic information about a host, similar to the physical view provided at the cluster level.
This display shows the same graph over a variety of time periods: typically the last hour, day, week, month, and year.
Many of the options described for viewing individual metrics are also available for all time periods, include CSV and JSON export, interactive inspection, and event display.
The gweb Search Tab Search allows you to find hosts and metrics quickly.
Find a particular metric, which is especially useful if a metric is rare, such as out going_sms_queue.
Figure 4-9 shows how gweb search autocomplete allows you to find metrics across your entire deployment.
To use this feature, click on the Search tab and start typing in the search field.
Once you stop typing, a list of results will appear.
Click on any of the links and a new window will open that will take you directly to the result.
You can keep clicking on the results; for each result, a new window will open.
The gweb Views Tab Views are an arbitrary collection of metrics, host report graphs, or aggregate graphs.
They are intended to be a way for a user to specify things of which they want to have a single overview.
For example, a user might want to see a view that contains aggregate.
There are two ways to create/modify views: one is via the web GUI, and the other by programatically defining views using JSON.i Creating views using the GUI.
To create views click the Views tab, then click Create View.
Adding metrics to views using the GUI Click the plus sign above or below each metric or composite graph; a window will pop up in which you can select the view you want the metric to be added.
Those values will appear as vertical lines on the graph.
Figure 4-10 shows the UI for adding a metric to a view.
Defining views using JSON Views are stored as JSON files in the conf_dir directory.
You can change that by specifying an alternate directory in conf.php:
Here is an example definition of a view that will result with a view with three different graphs:
Table 4-1 lists the top-level attributes for the JSON view definition.
Each item can have the attributes listed in Table 4-2
Regex view allows you to specify regex to match hosts.
You can use metric or graph keys but not both.
This item needs a hash of regular expressions and a description.
Once you compose your graphs, it is often useful to validate JSON—for example, that you don’t have extra commas.
The gweb Aggregated Graphs Tab Aggregate graphs (Figure 4-11) allow you to create composite graphs combining different metrics.
At a minimum, you must supply a host regular expression and metric regular expression.
This is an extremely powerful feature, as it allows you to quickly and easily combine all sorts of metrics.
Figure 4-12 includes two aggregate graphs showing all metrics matching host regex of loc and metric regex of load.
Decompose Graphs Related to aggregate graphs are decompose graphs, which decompose aggregate graphs by taking each metric and putting it on a separate graph.
This feature is useful when you have many different metrics on an aggregate graph and colors are blending together.
The gweb Compare Hosts Tab The compare hosts feature allows you to compare hosts across all their matching metrics.
This feature is helpful when you want to observe why a particular host (or hosts) is behaving differently than other hosts.
They are useful in providing visual cues when certain events happen.
For example, you might want to overlay software deploys or backup jobs so that you can quickly associate change in behavior on certain graphs to an external event, as in Figure 4-13
In this example, we wanted to see how increased rrdcached write delay would affect our CPU wait IO percentage, so we added an event when we made the change.
Alternatively, you can overlay a timeline to indicate the duration of a particular event.
For example, Figure 4-14 shows the full timeline for a backup job.
By default, Ganglia stores event in a JSON hash that is stored in the events.json file.
It is also possible to use a different backend for events, which can be useful if you need to scale up to hundreds or thousands of events without incurring the processing penalty associated with JSON parsing.
This feature is configured with two configuration options in your conf_default.php file.
You should have PHP support for MySQL installed on your gweb server before attempting to configure this support.
Alternatively, you can add events through the web UI or the API.
To use it, invoke the URL along with key/value pairs that define events.
Key/value pairs can be supplied as either GET or POST arguments.
The full list of key/value pairs is provided in Table 4-3
Allowed options are now (uses current system time), UNIX timestamp, or any other well-formed date, as supported by PHP’s strtotime function.
The gweb Automatic Rotation Tab Automatic rotation is a feature intended for people in data centers who need to continuously rotate metrics to help spot early signs of trouble.
To activate it, click Automatic Rotation and then select the view you want rotated.
Metrics will be rotated until the browser window is closed.
You can change the view while the view is rotated; changes will be reflected within one full rotation.
Another powerful aspect of automatic rotation is that if you have multiple monitors, you can invoke different views to be rotated on different monitors.
The gweb Mobile Tab gweb mobile represents the Ganglia web interface optimized for mobile devices.
It is intended for any mobile browsers supported by the jQueryMobile toolkit.
The mobile view contains only a subset of features, including views optimized for a small screen, host view, and search.
Custom Composite Graphs Ganglia comes with a number of built-in composite graphs, such as a load report that shows current load, number of processes running, and number of CPUs; a CPU report that shows system CPU, user CPU, and wait IO CPU all on the same graph; and many others.
You can define your own composite graphs in two ways: PHP or JSON.
Defining graphs via PHP is more complex but gives you complete control over every aspect of the graph.
For typical use cases, JSON is definitely the easiest way to configure graphs.
For example, consider the following JSON snippet, which will create a composite graph that shows all load indexes as lines on one graph:
To use this snippet, save it as a file and put it in the graph.d subdirectory of your gweb installation.
The filename must contain _report.json in it to be considered by the web UI.
The first is a set of configurations for the overall report, and the second is a list of options for the specific data series that you wish to graph.
The configuration options passed to the report are shown in Table 4-4
Note that each series has its own instance of the different options.
If this value is not specified, it defaults to 2
Once you compose your graphs, it is often useful to validate JSON.
One example would be to verify that there are no extra commas, etc.
Other Features There are a number of features in gweb that are turned off by default or can be adjusted: Metric groups initially collapsed.
By default, when you click on a host view, all of the metric groups are expanded.
You can change this view so that only metric graph titles are shown and you have to click on the metric group to expand the view.
To make this collapsed view the default behavior, add the following setting to conf.php:
Strip domain name from hostname in graphs By default, the gweb interface will display fully qualified domain names (FQDN) in graphs.
If all your machines are on the same domain, you can strip the domain name by setting the strip_domainname option in conf.php:
Set default time period You can adjust the default time period shown by adjusting the following variable:
Authentication and Authorization Ganglia contains a simple authorization system to selectively allow or deny users access to certain parts of the gweb application.
We rely on the web server to provide authentication, so any Apache authentication system (htpasswd, LDAP, etc.) is supported.
Apache configuration is used for examples in this section, but the system works with any web server that can provide the required environment variables.
If you wish to enable or disable authorization, add the change to your conf.php file.
When a user successfully authenticates, a hash is generated from the username and a secret key and is stored in a cookie and made available to the rest of gweb.
If the secret key value becomes known, it is possible for an attacker to assume the identity of any user.
Users who have already logged in will need to log in again.
This variable is not needed on any other gweb page.)
This is a secret value used for hashing authenticated user names.
If login.php does not require authentication, the user will see an error message and no authorization will be allowed.
More information about configuring authentication in Apache can be found here.
Note that Apache need only provide authentication; authorization is provided by gweb configuration.
Sample configurations for other web servers such as Nginx and Lighttpd are available on the gweb wiki.
Access Controls The default access control setup has the following properties:
Admins may view all public and private clusters and edit configuration (views) for.
The GangliaAcl configuration property is based on the Zend_Acl property.
Note that there is no built-in distinction between a user and a group in Zend_Acl.
The system supports the configuration of hierarchical sets of ACL rules.
We implement user/group semantics by making all user roles children of the GangliaAcl::GUEST role, and all clusters children of GangliaAcl::ALL:
Users may also have other roles, but this one grants global view privileges to public clusters.)
GangliaAcl::ADMIN Admins may access all private clusters and edit configuration for any cluster.
GangliaAcl::VIEW This permission is granted to guests on all clusters, and then selectively denied for private clusters.
GangliaAcl::EDIT This permission is used to determine whether a user may update views and perform any other configuration tasks.
Actions Currently, we only support two actions, view and edit.
So one user may have view access to all clusters, but edit access to only one.
The usernames you use must be the ones provided by whatever authentication system you are using in Apache.
If you want to explicitly allow/deny access to certain clusters, you need to spell that out here.
All later examples assume you have this code to start with:
In this chapter, we describe the various ways in which the Ganglia monitoring environment can be extended.
Primarily, we discuss two ways in which to extend Ganglia, including the development and deployment of additional metric modules, and the use of a standalone utility called gmetric.
Either is suitable, and you should use whichever approach works best for your environment.
If you are someone who likes to get down and dirty in source code, developing a gmond module might be the way to go.
On the other hand, if you just need a quick way to introduce an additional metric in your environment, gmetric is the perfect utility.
The first step in monitoring any system through Ganglia is to install the gmond daemon on each machine.
Once installed and running, this daemon uses a simple listen/announce protocol via eXternal Data Representation (XDR) to collect and share monitoring state information with other gmond services within a cluster.
In the Ganglia monitoring world, a cluster is defined as a group of gmond services that are all listening and sharing data with each other.
A Ganglia cluster might consist of anything from a single gmond node to many nodes, all talking to one another.
In the default configuration, each node listens and talks on a single multicast channel.
As metric data is placed on the channel, every node in the cluster has an opportunity to retrieve and store the data in its own internal data store.
Being configured in this way allows all cluster nodes to act as a backup for one another.
Within the cluster, one gmond instance is usually designated as the primary node and is queried from time to time by the gmetad aggregating service.
However, because any cluster node could potentially act as the primary node, gmetad has the ability to quickly switch from one node to another in the event that the primary node goes down.
This means that within a cluster of gmond nodes, there is no weak link or single point of failure for.
If any node within the cluster goes down, there is always another one ready to step up and take its place.
There are two different modes in which gmond clusters can be configured.
The default mode, which was described previously, is the multicast mode in which each gmond node in a cluster is configured to listen for metric data as well as send its own data via a single multicast channel.
In multicast mode, each gmond node not only gathers metric data from the host on which it is installed but also stores the last metric values gathered by every other node in the cluster.
In this way, every node in a cluster is capable of acting as the primary node or reporting node for the gmetad aggregator in case of a failover situation.
Which node within the cluster is designated as the primary node is determined through the configuration of gmetad itself.
The gmetad configuration also determines which nodes will act as failover nodes in case the primary node goes down.
The ability for any gmond node to report metrics for the entire cluster makes Ganglia a very highly robust monitoring tool.
The second mode in which gmond can be configured is unicast mode.
Unlike multicast mode, unicast mode specifically declares one or more gmond instances as being the primary node or reporting node for the cluster.
The primary node’s job is to listen for metric data from all other leaf nodes in the cluster, store the latest metric values for each leaf node, and report those values to gmetad when queried.
The major difference between multicast mode and unicast mode is that most of the nodes in the cluster neither listen for, nor store metric data from, any other nodes in the cluster.
A gmond instance that is mute is only capable of listening for and storing data from other nodes in the cluster.
It is usually the mute nodes that are designated as the gmetad reporting nodes.
Another difference between unicast and multicast modes is that each gmond leaf node is configured to send its data via a UDP socket connection rather than a multicast channel.
At first glance, unicast mode would appear to be less robust, due to the fact that not every node in the cluster can act as a reporting node—and if the primary node failed, no metric values would be reported to gmetad.
However, because more than one instance of gmond can be designated as the primary node, redundancy can be achieved by configuring backup primary nodes and allowing each leaf node to send its metric data to both the primary node as well as any backup nodes.
One thing to keep in mind is that multicast mode and unicast mode are not necessarily mutually exclusive.
Both multicast and unicast can be used within the same cluster at the same time.
Also, the configuration of gmond can include any number of send and received channels, which allows the configuration of a gmond metric gathering and reporting cluster to be extremely flexible in order to best fit your needs.
Base Metrics From the very first release of Ganglia, gmond was designed to collect dozens of system metrics that included a series of CPU-, memory-, disk-, network-, and process-related values.
Prior to version 3.1 of Ganglia, the set of metrics that gmond was able to gather was fixed.
There was no way to extend this set of fixed metrics short of hacking the gmond source code, which limited Ganglia’s ability to expand and adapt.
However, there was a way to inject new metric values into the Ganglia monitoring system.
Using a very simple utility that shipped with the Ganglia monitoring system, called gmetric, additional metric values could be gathered and written to the same unicast and multicast channels on which each gmond agent listened.
Even though gmetric provided a simple way of injecting a new metric into the system, the reality was that gmond was still incapable of gathering anything outside of its hard-coded set of metrics.
This hard-coded set of metrics became known as the default or base metrics that most monitoring systems are used to gathering.
Beyond the base metrics, there are many other metrics that are provided through addition modules.
These modules, along with a description of the metrics that they provided, are listed in Appendix A.
One of the advantages of supporting a fixed set of metrics was that gmond could be built as a very simplistic self-contained metric gathering daemon.
However, the disadvantage was obvious: despite producing a very vital set of metrics in terms of determining system capacity and diagnosing system issues by means of historical trending, gmond was incapable of moving beyond this base set of metrics.
Of course, introducing the ability for gmond to expand would certainly increase its footprint and the risk of skewing the metrics.
But given the fact that expanding gmond would be done through a modular interface, the user would have the ability to determine gmond’s footprint through the configuration itself.
Weighing the potential increase in footprint against the need to monitor more than just the basic metrics, the decision was made to enhance gmond by providing it with a modular interface.
Extended Metrics With the introduction of Ganglia 3.1 came the ability to extend gmond through a newly developed modular interface.
Although there are many different ways in which a modular interface could have been implemented, the one chosen for gmond was very closely modeled after one originally developed for the Apache HTTP server.
Those familiar with the Apache Web Server may recognize one of its main features: the ability to extend functionality by adding modules to the server itself.
In fact, without modules, the Apache Web Server is almost useless.
By adding and configuring modules to the web server, its capabilities can be expanded in ways that for the most part, are taken for granted.
So rather than reinvent a new type of modular interface, why not just reuse a tried and true interface? Of course, the fact that gmond is built on top of the Apache Portability Runtime (APR) libraries made the Apache way of implementing a modular interface an obvious fit.
With the addition of the modular interface to gmond in version 3.1, gmond was no longer a single self-contained executable program.
Even the base metrics that were included as a fixed part of gmond were separated out and reimplemented as modules.
This meant that if desired, gmond’s footprint could be reduced even beyond the previous version by eliminating some of the base metrics as well.
By configuring a head node as mute, basically there is no need for the node to gather metrics because it wouldn’t have the ability to send them anyway.
In addition, the system administrator also has the flexibility to gather more than just basic system metrics.
Ultimately, with the introduction of the modular interface, if a metric can be acquired programatically, a metric module can be written to track and report it through Ganglia.
When Ganglia 3.1 was initially released, it not only included the modular interface with a set of base metric modules but also included some new modules that extended gmond’s metric gathering capabilities, such as TcpConn, which monitors TCP connection, and MultiCpu and MultiDisk for monitoring individual CPUs and disks, respectively.
In addition to adding new metrics to gmond, these modules as well as others were included as examples of how to build a C/C++ or Python Ganglia module.
Extending gmond with Modules Prior to the introduction of the modular interface, the gmetric utility, which will be discussed later in this chapter, was the only way to inject new metrics into the Ganglia monitoring system.
The idea behind introducing the modular interface in gmond was to allow metric gathering to take advantage of everything that gmond was already doing.
It was a way to configure and gather a series of metrics in exactly the same way as the core metrics were being gathered already.
By loading a metric gathering module into gmond, there was no need to set up cron or some other type of scheduling mechanism for each additional metric that you wanted to gather.
Of course with every new feature like this, there are trade-offs.
As of Ganglia 3.1, gmond would no longer be a single all-inclusive executable that could simply be copied to a system and run.
The new modular gmond required modules that are separate dynamically loadable modules.
Part of the transition from a single executable also included splitting out other components such as the Apache Portable Runtime (APR) library, which was previously being statically linked with gmond as well.
The result of this new architecture was the fact that gmond became a little more complex.
Rather than being a single executable, it was now an executable with library dependencies and loadable modules.
However, given the fact that gmond is now much more flexible and expandable, the trade-off was worth it.
In the current version of gmond, there are two types of pluggable modules, C/C++ and Python.
The advantages and disadvantages of each are, for the most part, the same advantages and disadvantages of the C/C++ languages versus the Python scripting language.
Obviously, the C programing language provides the developer with a much lower-level view of the system and the performance that comes with a precompiled.
At this level, the programmer would also have the ability to take full advantage of the C runtime and APR library functionality.
However, C does not have many of the conveniences of a scripting language such as Python.
The Python scripting language provides many conveniences that make writing a gmond module trivial.
Even a beginning Python programmer could have a gmond Python module up and running in a matter of just a few minutes.
The Python scripting language hides the complexity of compiled languages such as C but at the cost of a larger memory and processing footprint.
One advantage to gmond modular interface is that there is plenty of room for other types of modules as well.
As of the writing of this book, work is being done to allow modules to be written in Perl or PHP as well.
As mentioned previously, if you were to open the hood and take a peek at the gmond source code, and if you were familiar at all with the Apache HTTP server modules, you would probably notice a similarity.
The implementation of the gmond modular interface looks very similar to the modular interface used by Apache.
First, one of the major components of gmond is the APR library, a cross-platform interface intended to provide a set of APIs to common platform functionality in a common and predictable manner.
In other words, APR allows a software developer to take advantage of common platform features (that is, threading, memory management, networking, disk access, and so on) through a common set of APIs.
By building software such as gmond on top of APR, the software can run on multiple platforms without having to write a lot of specialized code for each supported platform.
Because gmond was built on APR, all of the APR APIs were already in place to allow gmond to load and call dynamically loadable modules.
In addition, there was already a tried and proven example of exactly how to do it with APR.
If you haven’t guessed already, APR plays a very significant role in gmond—everything from loading and calling dynamically loadable modules to network connections and memory management.
Although having a deep knowledge of APR is not a requirement when writing a C/C++ module for gmond, it would be a good idea to familiarize yourself with at least the memory management aspects of APR.
Interacting with APR memory management concepts and even some APIs may be necessary, as you will see in the following sections.
At this point, you might be wondering what the second reason is for modeling the gmond modular interface after the Apache HTTP server, as the first reason seemed sufficient.
Well, the second reason is that the Ganglia developer who implemented the modular interface also happened to be a member of the Apache Software Foundation and already had several years of experience working on APR and the Apache HTTP server.
So it just seemed like a good idea and a natural way to go.
As mentioned previously, the gmond modular interface was modeled after the same kind of modular interface that is used by the Apache HTTP server.
If you are already familiar with Apache server modules, writing a gmond module should feel very familiar as well.
If you aren’t familiar with this type of module, then read on.
Don’t worry—the Ganglia project has source code examples that you can reference and can also be used as a template for creating your own module.
Many of the code snippets used in the following sections were taken from the mod_example gmond metric module source code.
If you haven’t done so already, check out the source code for mod_example.
It is a great place to start after having decided to implement your own C/C++ gmond metric module.
The following sections go into each one of these module parts in a little more detail.
The mmodule structure defines everything that gmond needs to know about a module in order for gmond to be able to load the module, initialize it, and call each of the callback functions.
In addition, this structure also contains information that the metric module needs to know in order for it to function properly within the gmond environment.
In other words, the mmodule structure is the primary link and the initial point of data exchange between gmond and the corresponding metric module.
The mmodule structure for a typical metric module implementation might look something like this:
When defining the mmodule structure within your metric module, the first thing to notice about the structure is that it contains pointer references to each of the other four required parts of every gmond module.
The data that are referenced by these pointers provide gmond with the necessary information and entry points into the module.
The rest of the structure is filled in automatically by a C macro called STD_MMODULE_STUFF.
At this point, there is really no need to understand what this C macro is really doing.
But in case you have to know, it initializes to null several other internal elements of the mmodule structure and fills in a little bit of static information.
All of the elements that are initialized by the C macro will be filled in by gmond at runtime with vital information that the module needs in order to run properly.
Some of these elements include the module name, the initialization parameters, the portion of the gmond configuration.
Keep in mind that the data stored in this structure can be referenced and used by your module at any time.
The mmodule structure is defined in the header file gm_metric.h.
The name of the Ganglia_25metric structure does not seem to be very intuitive, especially as the purpose of this structure is to track the definitions of each of the metrics that a metric module supports.
Again, taking a look at an example, an array of Ganglia_25metric structures might look like this:
In the previous example, there are actually three array entries, but only two of them actually define metrics.
The third entry is simply a terminator and must exist in order for gmond to appropriately iterate through the metric definition array.
Taking a closer look at the data that each Ganglia_25metric entry provides, the elements within the structure include information such as the metric’s name, data type, metric units, description, and extra metric metadata.
For the most part, the elements of this structure match the parameter list of the gmetric utility that will be discussed in a later section.
The metric_init callback function is the first of three functions that must be defined and implemented in every gmond metric module.
By the name of this function, you can probably guess that its purpose is to perform any module initialization that may be required.
The metric_init function takes one parameter: a pointer to an APR memory pool.
We mentioned earlier that it would probably be a good idea to understand some of the memory management concepts of APR.
This is the point at which that knowledge will come in handy.
The following code snippet is an example of a typical metric_init callback function.
The implementation in this example reads the module initialization parameters that were specified in the gmond configuration for the module and it defines some extra metric metadata that will be attached to metric information as it passes through gmond and the rest of the Ganglia system.
As gmond loads each metric module, one of the first things that it does is allocate an APR memory pool specifically for the module.
Any data that needs to flow between the module and gmond must be allocated from this memory pool.
One of the first examples of this is the memory that will need to be allocated to hold the extra metric metadata that will be attached to the metrics themselves.
Fortunately, there are some helper C macros that will make sure that the memory allocation is done properly.
As mentioned previously, there were several elements of the mmodule structure that are initialized by the STD_MMODULE_STUFF macro but filled in at runtime by gmond.
At the time when gmond loads the metric module and just before it calls the metric_init function, gmond fills in the previously initialized elements of the mmodule structure.
What it means is that when your module sees the mmodule structure for the first time, all of its elements have been initialized and populated with vital data.
Part of this data includes the module parameters that were specified in the corresponding module block of the gmond configuration file.
There are actually two elements of the mmodule structure that can contain module parameter values.
This element is defined as a string pointer and will contain only a single string value.
The value of this element is determined by the configuration params (plural) directive within a module block.
This value can be any string value and can be formatted in any way required by the module.
The value of this parameter will be passed straight though to the module as a single string value.
The contents of this array are defined by one or more param (singular) directive blocks within corresponding module blocks of the gmond configuration file.
Each param block must include a name attribute and a value directive.
The name and value of each of the parameters will be included in the.
There are two different ways of passing parameters from a configuration file to a metric module merely for convenience.
If your module requires a simple string value, referencing the module_params string from the mmodule structure is much more convenient than iterating through an APR array of name/value pairs.
Additionally, as there is no restriction on the format of the string contained in the module_params element, you can actually format the string in any way you like and then allow your module to parse the string into multiple parameters.
Basically, whichever method of passing parameters to your module works best for you, do it that way.
There is one other aspect of metric module initialization that should be explained at this point: the definition or addition of extra module metadata.
Each metric that is gathered by gmond carries with it a set of metadata or attributes about the metric itself.
In previous versions of Ganglia, these metric attributes were fixed and could not be modified in any way.
These attributes included the metric name, data type, description, units, and various other data such as the domain name or IP address of the host from which the metric was gathered.
Because gmond can be expanded through the module interface, it is only fair that the metadata or metric attributes also be allowed to expand.
As part of the module initialization, extra attributes can be added to each metric definition.
A few of the standard extended attributes include the group or metric category that the metric belongs to, spoofing host, and spoofing IP address if the gmond module is gathering metrics from a remote machine.
However, the extra metric metadata is not restricted to these extra attributes.
Any data can be defined and set as extra metadata in a metric definition.
Defining the extra metadata for a metric definition includes adding a key/value pair to an APR array of metadata elements.
Because adding an element to an APR array includes allocating memory from an APR memory pool as well as calling the appropriate APR array functions, C macros have been defined to help make this functionality a little easier to deal with.
The first macro allocates the APR array and requires as the last parameter the reference to the APR memory pool that was passed into the metric_init callback function.
The second macro adds a new metadata name/value pair to the array by calling the appropriate APR array functions.
Because the extra metadata becomes part of the metric definition, this data can be referenced by your module at any time.
If extra metadata was set that helps to identify a metric at the time that the module metric_handler function is called, this data could be referenced by accessing the mmodule structure.
But keep in mind that because the extra metadata is attached to the metric itself, this data will also be passed through gmetad to the web frontend allowing the Ganglia web frontend to better identify and display metric information.
The purpose of this function is to allow your module to clean up any memory allocations or threads, close any files, or simply tidy up any loose ends before the module is unloaded.
Although many metric modules will have no need for this function, it still must be implemented and it will still be called on shutdown.
The metric_cleanup function does not take any parameters, but at the time that it is called, all of the data that is stored in the mmodule structure is still valid and can still be referenced.
Following is an example of a typical metric_cleanup callback function:
Finally, the metric_handler function is the last of the three functions that every metric module must implement.
It is the callback function that actually does all of the work.
It is called every time that gmond determines that your module needs to provide a new metric value that corresponds to any one of the metric definitions your module supports.
By using this metric definition index, your module can easily identify the metric that should be gathered, do the work necessary to gather the last metric value, and then finally return the value back to gmond.
This single callback function must be implemented in such a way that it is able to gather metric values for any of the metrics supported by your module.
Outside of the five parts that are required of every metric module, your module is free to do anything else that may be required in order to gather metric values.
Keep in mind that your module must also be well behaved.
In other words, every time that gmond calls a metric module callback function, it expects the function to return a value in as short of a time as possible.
If the metrics that your module supports take a significant time to gather, you might consider starting your own gathering thread within your module and caching the metric values instead.
This way when gmond asks for a metric, our module can simply return the latest metric value from its cache.
The configuration of a gmond C/C++ module is a matter of telling gmond the name and location of the dynamically loadable module.
In addition, the configuration may also include module parameters in the form of a single string parameter or a series of name/value pairs.
Unlike a Python module, which will be discussed later in this chapter, a C/C++ module is loaded directly by gmond itself rather than by a module that acts as a language proxy.
There are two required configuration directives that every module configuration file must provide: name and path.
Previously, we discussed how to construct a C/C++ metric module and how the primary link between a metric module and gmond is the mmodule structure.
During the implementation of the module, one of the first things that is done is to define and name the mmodule structure.
The name of this structure is very important because the name is how gmond will identify the module throughout its lifetime.
Because the mmodule structure’s name is the module’s unique identifier, the name directive in the module configuration file must match the mmodule structure name exactly.
Once gmond has imported this structure, it has all of the information it needs to continue to load and configure the C/C++ module for proper performance.
The path to the module binary is also a required directive for every metric module.
The path directive can either contain a full file path or just the module filename alone.
If the path directive contains just the module filename, gmond will derive the module location in one of the following ways: the module_dir directive specified in the globals section of the configuration file, the --with-moduledir build parameter, or the --prefix build parameter.
If none of these parameters have been specified, gmond will assume the location of the module binary to be the lib(64)/ganglia subdirectory relative to the Ganglia installation location.
The rest of the module configuration for a C/C++ module is optional.
Of course, the module section must contain the name and path directives that have already been discussed.
An optional parameter, which is not shown in the example above, is the lan guage directive.
Specifying module parameters is also optional—unless, of course, your module requires parameters.
There are two ways in which module parameters can be passed to a module.
The first way is by defining a single string parameter through the params directive.
This directive just provides the configuration with a very simple way of passing either a raw string or a module-specific formatted string to the module’s initialization function.
Parsing the parameter string or interpreting the contents of the string is strictly up to the module itself.
The second way to pass parameters to a module is through a series of param blocks.
A module configuration can define as many param blocks as required by the module.
Each param block must include a name and a value.
The name of the param block represents the key that corresponds to the parameter value.
It is again up to the module to extract the parameter values from the APR array and interpret them appropriately.
For an example of how to deal with both types of module parameters, see the mod_example.c example in the Ganglia source code.
Now that you have implemented, built, and configured your C/C++ module, it is time to deploy the module into your Ganglia environment.
Deploying a module is as simple as copying the dynamically loadable module to the location that was specified in your module configuration as well as copying the module configuration to the Ganglia configuration file directory, /etc/conf.d.
In order to verify that the new module is actually being loaded and configured within the Ganglia environment, start an instance of gmond using the -m parameter.
With the -m parameter specified, a new instance of gmond will not be started.
However, this parameter will instruct gmond to load all of the modules that it knows about, read their corresponding.
The list of defined metrics should include all of the metrics defined by the new module (as well as all other defined metrics) and the module name in which each one was defined.
Finally, once the new module has been loaded and all of the new metrics have been defined, start gmond normally and it should begin to gather and display the new metrics in the web frontend.
Two standalone module packages have been provided to the community to demonstrate the principles of building C/C++ modules.
Traditionally, modules have been built by downloading the full Ganglia source tree and then working within the gmond/ modules directory.
This adds some slight complexity to the development process, especially when the developer wants to track changes coming from the main source code repository.
At the time when the Ganglia source code is downloaded, built, and installed on the system, the make install command will deploy copies of the C headers that are required to compile a module.
Packaged distributions (particularly on Linux and Debian) typically distribute those headers in a dev package and install them into the directory /usr/include.
There are two ways of using these projects as a model, along with some general comments on autotools that are common to both methods.
Thanks to the git source control system, you can easily clone either of these projects from the public repository in order to get started.
From that point, you can create a branch and start developing a custom module within your branch.
Finally, at the top level, modify both configure.ac, declaring your Makefile in AC_OUTPUT, and Makefile.am, declaring your new module directory in SUBDIRS.
If you prefer to have your own project be completely standalone, then it is only slightly more difficult.
First, start with the tarball distribution of one of the sample projects.
In this case, rename one of the module directories to use as your own module.
Second, delete all other module directories that are not required by your module.
Finally, work through the configure.ac file and each instance of Makefile.am that you have retained, deleting references to the old modules and creating the necessary description of the module you want to build.
No matter which approach you have chosen, once you have modified the configure.ac and Makefile.am files, it is essential that you re-run the autotools command autoreconf.
Typically you would run a command such as autore conf --install in the top level directory of your project.
After that, you can then run ./configure and then invoke any of the standard autotools make targets.
By implementing a similar module with Perl, Ruby, or PHP, gmond would instantly gain the ability to be extended by implementing gmond metric modules in those languages as well.
The main point to remember is that the gmond modular interface was written with future expansion in mind.
There are plenty of opportunities to make gmond even more useful and extensible.
Mod_Python just happens to be the forerunner to many other possibilities.
As mentioned previously, Mod_Python is a gmond metric module written in the C programming language.
Because it is a module, it needs to be configured just like any other metric module.
A base configuration file is included in the Ganglia source code and should have been installed automatically through a standard package install.
The name of the dynamically loadable module is modpython.so; its corresponding configuration file is called modpython.conf.
The configuration file for Mod_Python can usually be found in Ganglia’s etc/conf.d directory of the Ganglia installation.
The file contains a very simple and straightforward module configuration that follows the same pattern as every other gmond module.
The most significant part of the Mod_Python configuration is the params directive.
The path that is assigned to this directive will be passed down to the Mod_Python module and used to search for and locate the Python modules that should be loaded.
Mod_Python will search this location for any file with a .py extension.
For each Python module that it finds, it will attempt to also locate a matching module configuration block with a module name that corresponds with the Python module filename.
Any Python module that is found in this location and has a matching module configuration will be loaded and assumed to be a metric gathering module.
The recommended file extension of a Python module configuration file is .pyconf.
However, there is nothing about a Python module that requires any specific configuration file extension.
The first thing to understand when starting to implement a Python metric module is that there are three functions that every Python module must include and implement:
The following is a stripped-down template of what a typical Python metric module might look like.
As you can see, there really isn’t much to one of these types of modules:
In order to give you a better understanding of the required elements of every Python module, we’ll start out by describing each of the three functions that every Python module must implement.
The metric init function serves exactly the same purpose as the init function in a C/C++ metric module.
Its primary purpose is to construct and return a dictionary of metric definitions, but it can also perform any other initialization functionality required to properly gather the intended metric set.
The params parameter that is passed into the init function contains a.
Each metric definition returned by the init function as a dictionary object, must supply at least the following elements:
The call_back element designates a function that will be called whenever the data for this metric needs to be gathered.
The callback function is the second required function that must be implemented in a gmond metric module.
We will explain more about the callback function in just a moment, but for now just remember that the name of this function can be anything, but whatever name is assigned to the callback function must match the name given in the metric definition.
In addition to the required elements, the metric definition may contain additional elements.
The additional elements will be ignored by gmond itself, but the name/value pairs will be included in the metric data packet as extra data.
You can consider any additional elements as extra metadata for a metric.
Metadata can be used to further identify or describe the metric and will be available to anything that consumes the metric XML produced directly by gmond or from gmetad.
Because gmetad is the primary consumer of the gmond XML data, these EXTRA_ELEMENTs will also be stored by gmetad and available in the XML data that it produces.
There are a few special elements that can be used to enhance the web frontend or cause gmond to treat a specific metric as if it originated from a different host.
The GROUP element is used to categorize a metric in the web frontend.
If the GROUP element is specified in a metric definition, the corresponding value may contain one or more group names separated by commas.
Because the GROUP element along with its value is passed as an EXTRA_ELEMENT in the XML data, when the web frontend sees the group value, it uses that information to appropriately display the metric graph under the specified group header.
Also keep in mind that the GROUP element may contain more than one group value.
If additional group values are specified, the web frontend will.
This can be very useful if the metric can be intuitively categorized in multiple ways.
Ultimately, it allows the user of the web frontend to view the metric graph alongside other metrics within the same categories.
As mentioned earlier, the second function that must be implemented by all gmond Python modules is the metric handler function or the function that was referred to as the “callback” function.
Unlike a C/C++ metric module, a Python metric module must implement at least one handler function, but it may also choose to implement more than one if needed.
The handler definitions must be defined similar to the following:
The value of the name parameter will be the name of the metric that is gathered.
This is the name of the metric that was defined in the metric definition returned by the init function described previously.
By passing the metric name as a parameter to the callback function, a handler function gains the ability to process more than one metric and to determine which metric is currently being gathered.
The callback function should implement all of the necessary code that is required to determine the identity of the metric to be gathered, gather the metric value, and return the value to gmond.
The return value from the callback function must match the data type that was specified in the corresponding metric definition.
Finally, the third function that must be implemented in all gmond Python metric modules is the cleanup function.
This function will be called once when gmond is shutting down.
The cleanup function should include any code that is required to close files, disconnection from the network, or any other type of clean up functionality.
In addition, the cleanup function must not return a value.
As described previously, building a Python metric module requires the implementation of the init function and the cleanup function, at least one metric definition, and at least one callback function.
Outside of those requirements, the metric module is free to do whatever it needs in order to appropriately gather and return the supported metric data.
One of the most convenient aspects of implementing a gmond module in the Python language is that Ganglia is not required during development.
The entire implementation, debugging, and testing of the module can be done outside of the Ganglia environment.
Thus, there is no requirement for any special development tools outside of the standard and familiar Python development tools that you are used to using.
One of the best tools that we have found for developing Python metric modules is the Eric Python IDE.
Eric is an open source IDE and Python language editor that was written.
The IDE also gives you a command-line window that is attached to the currently running Python script.
The nice thing about the command-line windows is that when the IDE is stopped at a break point, you can use the command-line window to further inspect variables, construct new source code statements, or basically just do anything you want with Python.
If you are not already familiar with writing Python code, the following statements will allow you to run your module as a standalone Python script:
The if statement at the beginning of the block will be evaluated as true only if the Python script is being run by the Python interpreter directly.
The rest of the code block defines the parameters that will be passed to the metric init function and iterates through each metric definition, calls the specified callback function with the metric name as a parameter, and finally prints out the metric value that was gathered.
In other words, this very small block of code actually simulates the interaction that gmond would have with the module.
Once you have implemented, tested, and debugged your Python metric module outside of gmond, you can be confident that your module will perform correctly when loaded by gmond.
Of course, there is always the possibility that a problem might arise when your module is run inside the gmond-embedded Python environment.
In order to debug your module under those conditions, there is always the print statement.
The configuration of a gmond Python module is really no different than any other module.
In fact, the following configuration example should look very familiar.
Most Python modules will not require any specialized configuration at all.
One difference between the configuration of a Python module and C/C++ module is that the Python module configuration does not include a path directive.
Passing parameters into a Python module is supported in exactly the same way as well.
This allows any module specific configuration to be specified in the configuration file and passed to the module at initialization time.
At the risk of duplicating how to configure a gmond metric module, we’ll go through each of the directives shown in the previous example for configuring a Python metric module.
The module section must contain a name and a language directive.
The value of the name directive must match the file name of the Python module’s .py file.
Once it finds and confirms that a .py file is actually a Python module, it then searches for a module configuration block whose name matches the name of the Python script file.
All metric modules must specify the language in which they were written.
Therefore, the value of the language directive for a Python module must be "python"
The final part of the module configuration in specifying any parameter values that should be passed into the metric_init() function.
This part of the module configuration can take multiple param blocks.
Each param block must include a name and a value.
The name of the param block represents the key that corresponds to the parameter value in the dictionary object that is ultimately passed to metric_init()
One thing to note is that the value of a parameter will always be passed to the metric module as a string within a dictionary object.
If the module requires some data type other than a string, the string value of the parameter will have to be cast to the correct data type before it is used.
In the previous section, we described how to implement, debug, and test your Python metric module.
The next step is to actually deploy the module within a running instance of gmond.
The nice thing about deploying a module into the Ganglia environment is that it is as simple as copying a file to a specific directory.
To deploy your newly developed Python module, copy your module’s .py file to the Ganglia Python module directory.
The next step is to configure your Python module by creating a .pyconf configuration file.
Then make sure that the module’s .pyconf file has been copied to the Ganglia’s etc/conf.d directory as well.
In order to verify that the new module is properly loaded and configured within the gmond environment, start an instance of gmond using the -m parameter.
With the -m parameter, a new instance of gmond will not be started, but this parameter will instruct gmond to load all of the modules that it knows about, read their corresponding configuration files, call the metric_init() functions of each module, and then display on the console all of the defined metrics.
The list of defined metrics should include all of the metrics defined by the new module as well as all other defined metrics and the module name in which each one was defined.
Once you have confirmed that the new module loads successfully, start gmond in normal operation mode.
Spoofing with Modules Spoofing is a concept that allows an instance of gmond running on one host to report the metrics that it gathers as if they were coming from an instance of gmond running on another host.
In other words, gmond can fool the rest of Ganglia into thinking that the metrics that it is gathering are really coming from somewhere else.
Spoofing was a concept originally designed and implemented as part of the gmetric utility.
The idea of being able to report metrics as if they originated somewhere else was so popular in gmetric, that it only seemed natural to extend that idea into gmond modules as well.
Spoofing a metric within a gmond Python module is a matter of adding extra metadata elements to the metric description.
Each metric definition, as previously described, may contain extra elements, which indicate to gmond that special handling of the metric is required.
The SPOOF_HOST extra element specifies the IP address and the hostname of the host for which the metric should be reported.
When gmond sees this extra element, it will automatically replace the originating IP address and hostname with the values that are specified by this element.
The SPOOF_NAME extra element is used to indicate to gmond that the metric definition should assume the name of a different metric.
This concept makes a little more sense when you consider that each spoofed metric must also have a unique name.
If you have a metric module that gathers the boot time of not only the host on which it is currently running but also several other remote hosts, the dictionary of metric definitions that is returned by this module must include a metric definition for the local boot_time metric as well as each remote host boot_time.
Because gmond requires that every metric defined by a module have a unique metric name, there would be no way to define three different metrics all with the same boot_time metric name.
But naming a metric in this way would cause each of the remote host boot time metrics to show up in the web frontend as separate metrics that don’t actually correspond to the boot time of the host.
Just to make sure that you got all of that, let’s summarize.
Specifying SPOOF_HOST as part of the metric definition tells gmond that this metric is a spoofed metric.
The format of its value should be the remote host IP address followed by the hostname separated by a colon.
Specifying SPOOF_NAME as part of the metric definition tells gmond that the spoof name is really an alias for another metric.
Finally, the name of each spoofed metric must be unique.
In addition to that, you will need to remember that when your metric callback function is called, the name parameter that is passed in will be the unique name of the metric and not the SPOOF_NAME.
By passing in the unique name, this helps your callback function determine not only the metric it needs to gather but also the remote host that it should gather the metric from.
Extending gmond with gmetric The gmetric utility, which is distributed with the monitoring core, as well as being available in multiple native variants for various programming languages, allows us to submit metrics values without having to rely on either the scheduler present in gmond, or the constraints of the native shared library or Python modules on which it depends.
Running this utility will dump out the values for each packet as it is received and parsed.
Running gmetric from the Command Line gmetric configures its metric transmission based on the settings of the local gmond.conf file (or another similarly formatted file, if one is specified)
Beyond that, there are a number of command-line options to configure the behavior of gmetric, as listed in Table 5-2
This argument should not be used with any other options.
This argument should not be used with any other options.
By default, this will be the default installation location of gmond.conf, so if you are using the configuration specified in the active gmond.conf file, this option can be omitted.
By default, gmond will script most nonalphanumeric characters, with the notable exception of underscore, so you should restrict the character set of submitted metric names; otherwise, your names will be sanitized with underscores.
This value is not used for any calculator or interpolation of the values, and is completely arbitrary.
An example unit value would be C for degrees centigrade or kb to indicate that the current metric indicates kilobytes.
The possible values are zero for a zero slope metric, positive for an increment-only metric, negative for a decrement-only metric, and both for an arbitrarily changing metric.
Using the value positive for the slope of a new metric will cause the corresponding RRD file to be generated as a COUNTER, with delta values being displayed instead of the actual metric values.
After the lifetime of the metric has expired, it will be represented as a NaN value and appear as a gap in its representative graph.
Short Option Long Option Description numbers of metrics are present for a host or cluster, as they can be viewed grouped by this value.
The current version of gmetad does not respect this extra data, so it is necessary to use a gmond.conf configuration that submits data for the desired cluster.
This behavior will most likely change in future, but for now, this flag does not have any effect.
It is not used for processing or interpretation of values, and it is blank by default.
It defaults to being blank, which means that the metric name is used instead.
This means that the metric will be identified by this information, rather than the local host identification assigned by the gmond instance(s) to which gmetric is transmitting this data.
Heartbeat packets are usually sent by gmond instances, so that their current hosts are thought to be alive by upstream instances of gmond and gmetad.
This argument should be used in tandem with the -S/--spoof option to fake a heartbeat packet for a host that does not directly correspond to a running gmond instance.
When used, the metric-specific options, such as -n, -v, and so on, should not be specified.
It essentially allows packets to be submitted for hosts other than the gmond instance receiving the packets.
You can use the fully qualified domain name of the host if you need to match what is being presented by another gmond instance.
This issue can be overcome by simply using a separately specified gmond.conf file that points at the appropriate gmond instance (if you are using the standard monitoring core gmetric instance) or directly submitting the metrics to a gmond instance in the appropriate cluster (if you are not using the monitoring core gmetric instance)
The implementation of a C/C++ API would normally be chosen when there is a demand for minimal impact on the rest of the system.
Because the C and C++ programing languages are lower-level compiled languages, the resulting modules tend to occupy a much smaller memory footprint on the system.
The implementation of a C/C++ module would be most beneficial particularly when the metric-gathering code is intended to be run frequently and must perform complex functions in a very small period of time.
This consideration is very relevant for servers running real-time systems, such as market data, Voice over IP (VoIP), or high-frequency trading.
For example, a database vendor may have provided a shared library and C headers for gathering metrics from the database.
Implementing a C/C++ module in this instance is highly recommended.
The gmetric solution offers a very simple approach to introduce new metrics into the Ganglia system quickly.
This is one of the main reasons why it should be chosen, especially when the need for simplicity is paramount and the requirements for performance are not a high priority.
For example, when a system administrator is running a long upgrade task, he may only need to run the task once.
In order to monitor the upgrade process, the admin may choose to write a shell script that invokes gmetric to gather the metric data during the upgrade.
In such cases, the shell script is often fewer than 10 lines of code but may spawn multiple processes each time it executes.
One further consideration of gmetric: if a metric changes rarely, but the application generating the metric knows when it changes, then it can be appropriate to have the application invoke gmetric.
This solution avoids the need for gmond (either a C module or Python module) to poll repetitively for a value that changes rarely.
Implementing a Python module is probably the most common choice for two reasons.
It is simpler and quicker to develop a module using the Python language than it is using C or C++
This ease of use is particularly relevant when writing a module that must work on multiple platforms.
The second reason for using Python is that it is much more efficient than gmetric, as it runs in the same address space as the gmond process.
The implementation of a Python module is often chosen as a default when there is no compelling reason to use C/C++ or gmetric.
The XDR protocol can also be used to insert metric packets into the metric stream by a third-party utility.
In fact, gmetric is a good example of how the XDR protocol can be used in this manner.
The metric information is submitted as a series of two UDP packets: one containing metadata regarding the metric in question, and a second packet containing the metric value.
Table 5-3 explains the lower-level format of XDR integer and string values.
Null string values are represented as a zero-length value, followed by four 0 bytes.
Any nonalphanumeric characters will be translated into underscore characters by gmond in modern versions of the monitoring core.
Metric name STRING Repetition of the earlier metric name field.
Units STRING Textual name of the units being used for this metric.
Most packets contain at least a GROUP value, if not a SPOOF_NAME one.
This value can be 0 if there are no additional extra data values being passed.
Hostname STRING Should match the hostname in the preceding metadata packet.
Metric name STRING Should match the metric name in the preceding metadata packet.
Spoof INT Should match the spoof value in the preceding metadata packet.
Note: ideally, this should be the printf/scanf-style format of the value being passed, but in reality, all are presently passed as string values, and are then converted to their specified types by gmond.
Packets A metadata packet is an XDR packet that contains the definition of an individual metric.
Before a metric can be understood and viewed by the Ganglia monitoring system, its metadata must have been communicated throughout the system by a metadata packet.
When an instance of gmond is started, the first thing it does is send a metadata packet over the network for each metric for which it intends to provide a value.
A value packet contains only enough information to communicate a metric value.
In order to reduce the amount of data that an instance of gmond produces, the actual.
Implementations In addition to gmetric, there are several other metric-generating utilities that are available.
Each of these utilities have generate metric data and insert the data into the metric stream through the use of the XDR protocol.
Some of these metric generating utilities are listed in Table 5-6
Here we cover gmetric4j, as it can be used as a standalone in a wide variety of contexts (an application server or an Android application)
It should also be noted that gmetricjava does not currently support the current Ganglia wire format; it supports only the format used before v3.1
If there is no local gmond instance running on the machine where gmetric4j is deployed, then the machine will not be sending a heartbeat metric.
Add this class to your GMonitor instance just as you would add any other custom sampler of your own:
In addition, GPUs designed for the cloud will soon debut.
In HPC clusters, CPUs can offload data parallel workloads to the accelerators.
The GPU/CPU hybrid architecture is desirable for its overall performance, high performance per watt, and ease of programming.
In the cloud, virtualized GPUs will provide thin clients such as smartphones and tablets and access to a high-performance graphics experience.
Virtual GPUs will enable applications ranging from computer games to 3D computer-aided design to run in the cloud.
The official NVML module is a valuable tool for cluster administrators who manage GPUs in an HPC cluster or datacenter environment.
This module offers access to various GPU metrics that are needed to ensure high GPU availability and performance.
Installation The installation process for the NVML module requires the installation of multiple packages on each gmond instance.
The Python interpreter, along with pyNVML and the NVML plug-in, must be installed.
It is best to ensure that all nodes are set up the same way using a common shell script, executed via a parallel SSH client or configuration management tool.
The first step in installing the NVML module is to install the NVIDIA display driver.
If the machine is already set up to run CUDA, then the needed NVIDIA driver is already installed.
The following command will verify that nvidia-smi is installed and working:
Both the nvidiasmi utility and the NVML library were installed with the NVIDIA display driver package.
Check the NVML feature matrix for a description of the supported features that.
The NVML plug-in uses the same interface that nvidiasmi uses, so metrics that are unsupported in nvidia-smi are also unsupported in the NVML plug-in.
The NVML plug-in requires Python 2.5 or an earlier version with the ctypes library installed.
Run the following command to ensure that you have the appropriate version of Python installed:
Once the required version of Python is installed, you can proceed to download and install pyNVML, the Python interface to the NVML library.
The README file that is included with the download contains the installation instructions.
Now that the module is installed, gmond must be restarted so that the new configuration can take effect.
Metrics The NVML plug-in provides a variety of GPU metrics.
These metrics include the GPU count and the NVIDIA driver version.
For each GPU discovered on the system, the NVML modules expose the maximum and current clock speeds utilization information for the GPU memory and SM, temperature, fan speeds, power draw, ECC mode, used and total GPU memory, performance state, and identifiers such as the PCI bus ID, the GPU UUID, and the brand.
Reporting GPU metrics in Ganglia enables a cluster administrator to better monitor the GPUs in systems that they manage.
Utilization information can provide a coarsegrained metric for assessing a cluster scheduler’s efficiency.
Temperature and fan speed information can provide insight into how effective the system cooling is working.
However, this default configuration can easily be modified to fit your needs by editing the module’s configuration file.
For HPC nodes, there is often a desire to minimize the overhead related to the collection of metrics.
Some GPU metrics are more expensive to query than others.
For example, querying fan speed can be 100 times slower than querying the performance state.
Disabling unwanted metrics or decreasing the frequency of queries to a particular collection group can help reduce undesirable overhead.
Overview Sooner or later, you may encounter a problem with the Ganglia infrastructure.
Because it is a distributed architecture, it is not always obvious which component is at fault.
Sometimes, the fault may be completely outside the scope of the Ganglia system, such as a DNS issue, a faulty network card, or even a poorly configured web browser that results in a user mistakenly asserting that the Ganglia reports are broken.
This chapter aims to provide a systematic way of categorizing the faults, investigating them, identifying which component is responsible, rectifying the issue, and, if necessary, communicating details of the issue to the Ganglia community for discussion on the mailing list or registration in the bug database.
Known Bugs and Other Limitations There are a number of known bugs and other limitations in the Ganglia system.
For example, Ganglia is dependent on the system clock, and meaningful data will not be collected and reported if the cluster machines, data collectors, and web server machines do not have clock synchronization.
This is a limitation of the Ganglia design, but it is not considered a bug.
Another known issue, fixed only just before the publication of this book (in Ganglia 3.3.7 and beyond), is that Ganglia was not working on a Solaris zone or container environment (this issue can also be worked around by disabling the network module)
To save time troubleshooting, you may wish to peruse the list of open bug reports in the Ganglia bug database maintained by the Ganglia community.
If you encounter a similar issue in your environment, you will then avoid wasting time diagnosing it.
Useful Resources In this section, we will describe helpful online/offline resources to help you troubleshoot issues with Ganglia.
Release Notes Read the release notes, published on the download page.
It usually includes important information particular to the release (or previous releases)
A large portion of issues reported to the mailing lists are already known to the developers and noted in the release notes.
Manpages Manpages are a given, but a lot of users some times forget they exist.
The manpages are a great source of information pertaining to a particular component such as gmond.conf options and syntax.
These manuals should be consulted to double-check that you have configured everything correctly.
Wiki Our wiki has a lot of examples and information on different Ganglia implementations.
Check to see whether what you are trying to do has been documented and whether you are doing it correctly.
If it is not documented, consider adding it after you have figured everything out, be it an issue you were having or some special setup you have developed.
User contribution plays a large role in the success of the project.
If you run into issues or just feel like talking shop with other users, you are encouraged to join us.
If you do post a question, please be patient, as not everybody is watching the channel all the time (not to mention in the same time zone), and you will get a response eventually.
If not, or if the folks on IRC cannot answer your questions, you can post them to the mailing lists described next.
Mailing Lists There are two main mailing lists that users can contribute to, namely: ganglia-general and ganglia-developers.
If you encounter an issue, it is worthwhile to search the mailinglist archives to see if somebody else has encountered something similar.
The current mailing-lists are hosted at SourceForge and the search functionalities are subpar.
The Ganglia developers recommend using the Mail Archive for searches.
Simply search for “ganglia” when you arrive at the main page, which will bring up links to the two mailing lists that can then be searched.
If the issue you encountered has not been previously discussed, consider starting a discussion thread and/or filing a bug in our bug tracker, discussed next.
In order to post to a mailing list, you will need to first subscribe to it.
Before filing a bug, it is worthwhile to do a quick keyword search to determine whether the issue is already in the system.
If you have already done so but nothing came up, please file a new issue in the bug tracker.
Please be as precise as possible when describing the issue and provide information that you believe is relevant for developers to troubleshoot the issue, such as the version of Ganglia you are using, the operating system, special configuration options, and so on.
Monitoring the Monitoring System It’s always a good idea to be proactive and anticipate problems.
After all, that is why most people deploy Ganglia—to monitor the rest of their systems and be alerted before they have a crisis.
Monitoring the Ganglia infrastructure itself is also a good idea: doing so can help you study any problems that arise as the network grows.
Ganglia does not automatically monitor itself, but it is not hard to make it do so.
Here are some of the key things you can do to monitor Ganglia with Ganglia:
At a bare minimum, configure this module to report the running Ganglia version.
This information can be useful for detecting agents that have not been updated.
For any node that is acting as an aggregator of gmond packets, consider enabling the multicpu module.
The gmond process is single-threaded, and the multicpu module will help identify when a core is always at capacity.
For a gmetad server, consider installing and enabling the mod_io module to monitor disk IO levels on the disk storing the RRD files.
Consider setting threshold alerts for the metrics mentioned in this section.
General Troubleshooting Mechanisms and Tools In this section, we provide some general strategies for troubleshooting Ganglia and a discussion of various tools that are helpful.
If netcat is not available, you could use the telnet tool, but it would not be possible to pipe the output for further processing.
Here are some common netcat/grep commands against gmetad port 8651
These commands also work with port 8649 on a gmond instance:
The first 51 lines of XML output of both gmond and gmetad are always the same, as follows:
The output is truncated for brevity, but each host in this cluster has 103 metrics, and it repeats for each host.
The previous example is taken from a gmond in a multicast environment.
For unicast environment, only the collector gmond will have information on all hosts, as in this example.
Logs gmond and gmetad do not log many things to syslog; however, these logs should always be checked when there is a problem.
If there is a problem, and the logs provide no clues at all, see the next section.
If you are seeing a blank page when you point your web browser to your gweb page, you should check for errors in the error logs.
Both errors in PHP code and in RRD file generation will show up in the web server error logs.
For the daemons (gmond, gmetad), this is accomplished by specifying -d on the command line.
If a debug value greater than 0 is provided, the daemon will start in the foreground.
In debug mode, a lot more messages are printed during operation such that you can see precisely what is happening.
Because there is a lot of output, it is recommended that you pipe the output to a temporary file and then examine the log file after the fact for any special notices that are displayed while the issue you are observing is happening.
For instance, here’s a sample command for piping the output of running gmond in debug mode to the file /tmp/gmond.log:
If you are trying to troubleshoot why a particular graph is not being generated, you can do so by first getting the URL of the graph, either by right-clicking the graph placeholder or just by looking at the source.
Once you have the URL, you can then put that in the browser and add &debug=3 at the end.
This change will force the RRDtool command that is used to generate the graph to be displayed on the browser.
You can then cut and paste the command into a command prompt on the gmetad server and execute it.
It should then tell you exactly why the graph was not generated.
It could be because of permission issues or syntax errors with the RRDtool command.
When one of the Ganglia programs fails with a segmentation fault or consumes 99 percent CPU, you can use strace to determine what system calls.
The system calls will often show interesting details about which files the process accessed just before the problem occurred.
If gmond or gmetad is misbehaving and you are trying to find out what it is doing, attaching to the running processes using strace could provide hints when running in debug mode (described later in this chapter) is not sufficient.
A gmond process that receives metrics from other processes (over multicast or acting as a UDP aggregator) may grow its memory usage in a manner that is directly proportional to the number of hosts/metrics received.
Any other growth in memory usage should be seen as a sign that gmond or one of the metric modules is misbehaving.
If you observe it going up at a steady rate, try to run strace against it to see whether there are any clues as to what is causing the memory consumption.
If you are using metric modules, try disabling them one by one to see if you can isolate the culprit.
Finally, if all else fails, you can run tools such as valgrind and see what sort of information you could gather.
If you believe this particular issue has not yet been reported, it is a good idea to file a bug in our bug tracker —and don’t forget to include the valgrind output in the bug report.
Memory corruption is another possibility, particularly if a third-party metric module is behaving badly.
If such errors are detected, please share them in a bug report.
If gaps are observed in all the graphs, it is often a symptom of IO saturation, and iostat can confirm this.
It can also show statistics about the IO queue performance, such as the average time an IO request is queued and the utilization rate of the block device.
Here is an example of running iostat with a logical volume:
In that case, it is necessary to either reduce the workload or increase the IO capacity of the block device.
Restarting Daemons Occasionally, Ganglia gets into some strange state—for instance, the correct number of hosts are reported but it is not the correct total number of cores.
Sometimes these issues can be resolved by restarting the daemons.
It allows you to quickly determine things such as which hosts are down and how many cores each host has, as well as their respective load.
If you invoke the command without any options, you will be presented with a quick summary of your cluster:
This option gives you a list of all the hosts in the cluster with detail information regarding number of cores, number of processes running, load, and so on.
Because -1 was specified, it prints a host and its respective details per line.
To get the same list with the IP addresses of each host instead of their hostnames printed, use -an1:
By default, it tries to talk to the gmond running on localhost, but you can specify another running gmond by specifying -i.
Common Deployment Issues There are a few common deployment issues that you should be aware of when deploying Ganglia.
Reverse DNS Lookups The first time gmond receives a metric packet from any other node, it must do a name lookup to find the hostname corresponding to the packet’s source address.
If these lookups are satisfied by /etc/hosts, then it can be quite fast.
If the lookups must be handled by DNS, this can slow down the process.
As it is a single-threaded design, this scenario can have undesirable consequences.
When a gmond process first starts, it is likely that it will have to do such name lookups for all packets it receives in the first one or two minutes of operation.
If a large number of hosts are reporting packets simultaneously, and if DNS is slow or even completely unavailable, this issue can have a severe impact and metrics will not be reported at all until the name lookups are complete and everything starts running normally.
Therefore, for those hosts that receive metrics (e.g., mute nodes or UDP aggregators), you should have multiple name servers defined and/or populate /etc/hosts if convenient.
Time Synchronization It is essential that all hosts participating in the Ganglia monitoring system have a synchronized clock.
If you see a message similar to the following in your web server logs, it is highly likely that the hosts running gmond are not time-synced:
SELinux and Firewall If you are new to Ganglia and feel that you have followed all the installation instructions to the letter, but for some reason you are not getting any graphs on the web frontend, you might want to check whether your OS has SELinux enabled by default.
Ganglia developers generally recommend disabling SELinux on systems running Ganglia daemons/web frontend, as it hinders normal operations.
If you are attempting to run Ganglia on systems that require SELinux, it is possible to create a security profile that allows Ganglia to work.
However, how to write such a SELinux security profile is outside the scope of this book.
Ganglia daemons communicate with each other via network (TCP/UDP) sockets.
The web frontend also needs to communicate with gmetad via the interactive port.
If a firewall needs to be in place on servers running the Ganglia services, please ensure that the ports are opened on the firewall.
Typical Problems and Troubleshooting Procedures In this section, we list and categorize common issues encountered with a Ganglia installation.
We categorize based on when the user will first notice the issue.
For instance, one might have misconfigured gmond to the extent that certain nodes are not shown in the web interface.
Web Issues There are a number of well-known problems that can occur when using gweb.
We’ve listed them here to help you quickly get back up and running.
Check the Apache access log: did the browser connect to the web server? If not, it could be a problem with the browser itself or a web proxy.
Does the Apache error log contain any errors? Look for errors about file permissions, missing PHP modules, and the like.
Try adding ?debug=3 to the end of the URL and check the error log by using tail -f.
Otherwise, please see the tips for the previous problem and look for more details in the various log files.
Check both the web server logs and the system logs (grep for any errors mentioning gmetad)
Often the host appears twice, with the name in uppercase and lowercase, or some other differing variations of the hostname.
Clicking some of the hosts shows a blank page and no metrics.
Older versions of Ganglia treated hostnames in a case-sensitive manner, which is incorrect.
In some environments, the hostname is uppercase in the host’s file and lowercase in DNS.
As of Ganglia 3.3, hostnames are converted to lowercase, RRD files are created with lowercase filenames, and lowercase hostnames should be used in URLs to access the metrics.
A config option allows the legacy behavior to remain, but if you incorrectly have that option enabled in either or both gmetad.conf and config.php, you may have trouble.
This is a common misconception of how Ganglia and gmond work.
The cluster name is used to generate the XML, which in turn shows up in the web frontend.
If you would like the host to show up under a different cluster, use different ports for udp_send_channel.
Host appears multiple times in web, different variations of the hostname (or IP address)
Usually, the different hostnames/IP addresses correspond to different interfaces on the host: the agent randomly picks a source IP address on the host for sending out metric packets, which can produce unexpected results.
If gmond is restarted, sometimes it will not be sending on the same interface/source IP that it used previously.
The bind_hostname parameter can be used to lock it to the correct interface.
The hostnames are inserted into the XML stream by the gmond host receiving the multicast or UDP metric packets.
It does a reverse lookup against the source address of the packet.
Sometimes that host has shortnames in /etc/hosts and it uses them instead of the FQDNs from DNS.
Therefore, consider adapting /etc/hosts to contain both FQDN and shortname for each IP address.
One or more hosts don’t appear in the web interface.
If the hosts have only recently been added to the network, check whether the RRD files for the hosts have been created yet.
If the gmetad server has a full filesystem, it will fail to create RRD files.
If using multicast, check the interface where packets are sent (a packet sniffer such as tcpdump might help)
Let’s say you have a 10-node cluster, one node has a hardware failure and cannot be repaired.
You may have replaced the node with another one with a different IP address, or you do not want the dead host to show up any more.
For unicast mode, all you need to do is restart the collector.
For multicast node, all nodes in the cluster will need to be restarted.
This step should flush the dead node out of the system.
This will flush out the node automatically after the specified number of seconds have passed.
For unicast mode, this value needs to be set only on the collector.
For multicast mode, the value needs to be set on all gmonds’ configuration.
This issue is also relevant when monitoring dynamic environments such as cloud resources (for example, Amazon EC2)
Hosts are constantly brought up and shut down and the IP addresses of hosts are generally not from a constant pool.
You may end up with a lot of dead hosts in your cluster if this option is not properly set.
If you drill down to the host view, some hosts are missing certain metrics and graphs.
In that case, you might want to reload gmond on the hosts in question.
If that does not work, try to do a systemic restart of all the daemons.
In certain situations, the fonts of the graphs may be too large or too small, making it difficult to read the text.
In that case, try to install different TrueType fonts on your system.
Alternatively, try upgrading the version of RRDtool, as newer versions have better font management.
As of RRDtool v1.3, fontconfig is used to access system fonts.
By default, it will use the font DejaVu Sans Mono.
Use fc-list to see which fonts are installed on your system.
Sometimes it is possible to have unexpected spikes in your graphs that will throw the scale totally off.
If you are certain the spikes are not normal, you can remove them from the RRDtool database using the contributed script removespikes.pl, which is usually shipped in the Ganglia release tarball under contrib/
If it is not available, you can get it from the github repository.
Certain Broadcom Network Interface Controllers (NICs) are known to cause spikes due to hardware bugs.
If the spikes are in your network graphs and are in the range of petabytes/sec, consider rebuilding Ganglia using the following flag:
This flag is known to alleviate the issue under Linux.
So you have written your first gmond metric module in Python and have confirmed by testing that it is working as expected on the host you would like to collect data.
You have installed the module and the corresponding pyconf in the right location, but no matter how many times you restart gmond, you are still not seeing the graph on the web frontend.
Traditionally, gmond is executed by an unprivileged user, such as nobody or ganglia, which has limited access.
Your Python module will also be executed by this user: therefore, it is important to check whether that user can run the script without any issue (such as accessing certain files)
It is also a good idea to run gmond with the -m parameter.
The -m parameter will instruct gmond to load each module and display a list of all of the metrics that it knows about.
Check the output listing to make sure that your new metrics are included.
You have just injected a new metric into Ganglia, which is a long string.
By default the value of a metric is stored in a 32-byte structure.
It is possible to increase this value by recompiling Ganglia; alternatively, you could also split the value into multiple metrics.
Gaps that appear randomly in the graphs are often a sign that some component is overloaded.
The UDP packets sent by gmond are likely to be dropped by congested routers and switches.
On the gmond that receives the packets (either via multicast or acting as a UDP aggregator), verify that the UDP receive buffer is big enough.
Verify that the gmond process handling the TCP polls from gmetad is not overloaded.
If the gmond is using 100 percent of CPU, that is a sign that it is overloaded.
Split the cluster, reduce the number of metrics from each member node, reduce the transmission rate from the member nodes, or put that gmond on a more powerful CPU.
Verify that the gmetad process is not overloading the CPU.
If the gmetad is using 100 percent of CPU, that is a sign that it is overloaded.
Split the workload over multiple gmetads on different physical hosts.
Confirm that the IO device storing the RRDs is not overloaded.
If gaps are showing up only on graphs of a new metric added via gmetric or the gmond metric interface, you may have specified an incorrect slope for the metric.
For most cases, use slope=both, which will cause the underlying RRD file to be of type GAUGE and thus each data point collected will be graphed.
However, if the metric you are collecting is a rate of change, then specify slope=positive such that the RRD file will be of type COUNTER.
For more information regarding the difference between the two types, refer to the documentation for RRDtool.
Confirm that the host is up and the gmond process is running.
If the gmond process won’t start, see the sections about troubleshooting gmond in debug mode.
Use a packet sniffer (tcpdump or wireshark) to verify that the host is transmitting packets.
Try restarting the gmond: this will cause it to retransmit its metadata.
If the filesystem fills up, the files are sometimes created with size 0
The ACL can be tested by executing netcat between the gmetad hosts.
The stack trace or strace output shows that it was writing to an RRD.
This may be due to a buggy version of RRDtool.
Using RRDtool v1.4.4 or greater may fix this issue; otherwise, please try the other troubleshooting tools/techniques.
In gmetad.conf, the data_source definition can list multiple nodes for a single cluster.
However, gmetad doesn’t automatically failover/poll the second node when the first is.
In many cases, it is necessary to completely restart gmetad to force it to resume polling the node that went down.
Some people have a cron job configured to restart gmetad every few hours just to avoid this issue.
The RRA definition in gmetad.conf is used only when new RRD files are created.
Two options are available for handling data in existing RRD files:
Archive old RRD files and let gmetad create new RRD files based on updated RRA definition.
Use rrddump to dump existing RRD files to XML format, massage them to conform to the new RRA definition, and reimport back to RRD files using rrdrestore.
For more information about these tools, please refer to the RRDtool documentation.
Use the ulimit command to increase the permitted number of open files or file descriptors for the rrdcached process.
The process must be able to open all the RRDs for all the metrics concurrently.
On some machines, the /etc/hosts file contains an entry mapping the hostname to the localhost address (127.0.0.1)
If the gmond memory usage grows to 2 GB or more and then gmond crashes, it is probably a memory leak or you have an extremely large number of hosts sending metrics to the gmond.
It could also be a sign of a denial-ofservice attack (someone sending random metrics to fill up the memory)
If the gmond memory usage is high (more than 100 MB) but constant, it is quite possible that this is just the normal amount of memory needed to keep the state information for all the metrics it is receiving from other nodes in the cluster.
Verify that the init script is installed and has the executable bit set.
Verify that the symlink from /etc/rcX.d exists for the run-level.
Verify that the host has an IP address before the gmond init script is invoked.
If the system obtains an IP address dynamically, it is possible that DHCP is not completed before the attempt to start gmond, and so gmond fails to run.
If network manager is in use (typically on desktop workstations), there is often no DHCP IP address until the user has logged in.
If you notice UDP receive buffer errors/dropped packets on a machine running gmond, you may find gmond itself to be the culprit.
Check /proc/net/udp to see how many packets are being dropped by the gmond process.
If gmond is dropping packets, increase the size of the UDP receive buffer (see the buffer parameter introduced in v3.4.0)
If that doesn’t help, and if the gmond process is at full capacity (100 percent of a CPU core), consider reducing the rate of metric packets from all gmonds in the cluster, or break the cluster into multiple clusters.
It’s been said that specialization is for insects, which although poetic, isn’t exactly true.
Nature abounds with examples of specialization in just about every biological kingdom, from mitochondria to clownfish.
The most extreme examples are a special kind of specialization, which biologists refer to as symbiosis.
You’ve probably come across some examples of biological symbiosis at one time or another.
Some are quite famous, like the clownfish and the anemone.
Others, like the fig wasp, are less so, but the general idea is always the same: two organisms, finding that they can rely on each other, buddy up.
Buddies have to work less and can focus more on what they’re good at.
In this way, symbiosis begets more specialization, and the individual specializations grow to complement each other.
Effective symbiotes are complementary in the sense that there isn’t much functional overlap between them.
The beneficial abilities of one buddy stop pretty close to where those of the other begin, and vice versa.
They are also complementary in the sense that their individual specializations combine to create a solution that would be impossible otherwise.
Together the pair become something more than the sum of their parts.
It would surprise us to learn that you’d never heard of Nagios.
It is probably the most popular open source monitoring system in existence today, and is generally credited for if not inventing, then certainly perfecting the centralized polling model employed by myriad monitoring systems both commercial and free.
Nagios has been imitated, forked, reinvented, and commercialized, but in our opinion, it’s never been beaten, and it remains the yardstick by which all monitoring systems are measured.
It is not, however, a valid yardstick by which to measure Ganglia, because the two are not in fact competitors, but symbiotes, and the admin who makes the mistake of choosing one over the other is doing himself a disservice.
It is not only possible, but advisable to use them together to achieve the best of both worlds.
To that end, we’ve included this chapter to help you understand the best options available for Nagios interoperability.
Sending Nagios Data to Ganglia Under the hood, Nagios is really just a special-purpose scheduling and notification engine.
All it can do is schedule the execution of little programs referred to as plug-ins and take action based on their output.
In addition to the codes, the plug-ins can also return a line of text, which will be captured by the daemon, written to a log, and displayed in the UI.
If the daemon finds a pipe character in the text returned by a plug-in, the first part is treated normally, and the second part is treated as performance data.
Performance data doesn’t really mean anything to Nagios; it won’t, for example, enforce any rules on it or interpret it in any way.
The text after the pipe might be a chili recipe, for all Nagios knows.
The important point is that Nagios can be configured to handle the post-pipe text differently than pre-pipe text, thereby providing a hook from which to obtain metrics from the monitored hosts and pass those metrics to external systems (like Ganglia) without affecting the human-readable summary provided by the pre-pipe text.
There are quite a few Nagios add-ons that use it to export metrics from Nagios for the purpose of importing them into local RRDs.
Imagine a check_ping plug-in that, when executed by the Nagios scheduler, pings a host and then return the following output:
We want to capture this plug-in’s performance data, along with details we’ll need to pass to gexec, including the name of the target host.
Then we’ll define pushToGanglia in the Nagios object configuration like so:
Careful with those delimiters! With so many Nagios plug-ins, written by so many different authors, it’s important to carefully choose your delimiter and avoid using the same one returned by a plug-in.
In our example command, we chose double pipes for a delimiter, which can be difficult to parse in some languages.
The capitalized words surrounded by dollar signs in the command definition are Nagios macros.
Using macros, we can request all sorts of interesting details about the check result from the Nagios daemon, including the nonperformance data section of the output returned from the plug-in.
The Nagios daemon will substitute these macros for their respective values at runtime, so when Nagios runs our pushToGanglia command, our input will wind up looking something like this:
Our pushToGanglia.sh script will take this input and compare it against a series of regular expressions to detect what sort of data it is.
When it matches the PING regex, the script will parse out the relevant metrics and push them to Ganglia using gexec.
This is a popular solution because it’s self-documenting, keeps all of the metrics collection logic in a single file, detects new hosts without any additional configuration, and works with any kind of Nagios check result, including passive checks.
It does, however, add a nontrivial amount of load to the Nagios server.
Consider that any time you add a new check, the result of that check for every host must be parsed against the pushToGanglia script.
The same is true when you add a new host or even a new regex to the pushToGanglia script.
It probably makes sense to process performance data globally if you rely heavily on Nagios for metrics collection.
However, for the reasons we outlined in Chapter 1, we don’t think that’s a good idea.
If you’re using Ganglia along with Nagios, gmond is the better-evolved symbiote for collecting the normal litany of performance metrics.
It’s more likely that you’ll want to use gmond to collect the majority of your performance metrics, and less likely that you’ll want Nagios churning through the result of every single check in case there might be some metrics you’re interested in sending over to Ganglia.
Here, for example, is what a wrapper for the check_ping plug-in might look like:
The wrapper approach takes a huge burden off the Nagios daemon but is more difficult to track.
If you don’t carefully document your changes to the plug-ins, you’ll mystify other administrators, and upgrades to the Nagios plug-ins will break your data collection efforts.
The imposter script then reports back to Nagios with the output and exit code of the original plug-in, and Nagios has no idea that anything extra has transpired.
This approach has several advantages, the biggest of which is that you can pick and choose which plug-ins will process performance data.
Monitoring Ganglia Metrics with Nagios Because Nagios has no built-in means of polling data from remote hosts, Nagios users have historically employed various remote execution schemes to collect a litany of metrics with the goal of comparing them against static thresholds.
These metrics, such as the available disk space or CPU utilization of a host, are usually collected by services like NSCA or NRPE, which execute scripts on the monitored systems at the Nagios server’s behest, returning their results in the standard Nagios way.
The metrics themselves, once returned, are usually discarded or in some cases fed into RRDs by the Nagios daemon in the manner described previously.
This arrangement is expensive, especially considering that most of the metrics administrators tend to collect with NRPE and NSCA are collected by gmond out of the box.
If you’re using Ganglia, it’s much cheaper to point Nagios at Ganglia to collect these metrics.
To that end, the Ganglia project began including a series of official Nagios plug-ins in gweb versions as of 2.2.0
These plug-ins enable Nagios users to create services that compare metrics stored in Ganglia against alert thresholds defined in Nagios.
This is, in our opinion, a huge win for administrators, in many cases enabling them to scrap entirely their Nagios NSCA infrastructure, speed up the execution time of their service checks, and greatly reduce the monitoring burden on both Nagios and the monitored systems themselves.
Verify that one or more values is the same across a set of hosts.
Principle of Operation The plug-ins interact with a series of gweb PHP scripts that were created expressly for the purpose.
You must functionally enable the server-side PHP scripts before they can be used and also define the location and refresh interval of the XML grid state cache by setting the following parameters in the gweb conf.php file:
Consider storing the cache file on a RAMDisk or tmpfs to increase performance.
Beware: Numerous parallel checks If you define a service check in Nagios to use hostgroups instead of individual hosts, Nagios will schedule the service check for all hosts in that hostgroup at the same time, which may cause a race condition if gweb’s grid state cache changes before the service checks finish executing.
Check Heartbeat Internally, Ganglia uses a heartbeat counter to determine whether a machine is up.
This counter is reset every time a new metric packet is received for the host, so you can safely use this plug-in in lieu of the Nagios check_ping plug-in.
Make sure that the GANGLIA_URL inside the script is correct.
Make sure that the GANGLIA_URL inside the script is correct.
Next, add the check command to the service checks for any hosts you want monitored.
Operators denote criticality The operators specified in the Nagios definitions for the Ganglia plugins always indicate the “critical” state.
If you use a notequal operator, it means that state is critical if the value is not equal.
Make sure that the variable GANGLIA_URL in the script is correct.
Then add a list of checks that are delimited with a colon.
For example, the following service would monitor the disk utilization for root (/) and /tmp:
Beware: Aggregated services Anytime you define a single service to monitor multiple entities in Nagios, you run the risk of losing visibility into “compound” problems.
For example, a service configured to monitor both /tmp and /var might only notify you of a problem with /tmp, when in fact both partitions have reached critical capacity.
Check Multiple Metrics on a Range of Hosts Use the check_host_regex plug-in to check one or more metrics on a regex-defined range of hosts.
This plug-in is useful when you want to get a single alert if a particular metric is critical across a number of hosts.
Make sure that the GANGLIA_URL inside the script is correct.
Then add a list of checks that are delimited with a colon.
Beware: Multiple hosts in a single service Combining multiple hosts into a single service check will prevent Nagios from correctly respecting host-based external commands.
For example, Nagios will send notifications if a host listed in this type of service check goes critical, even if the user has placed the host in scheduled downtime.
Nagios has no way of knowing that the host has anything to do with this service.
For example, let’s say you wanted to make sure the SVN revision of the deployed program listing was the same across all servers.
You could send the SVN revision as a string metric and then list it as a metric that needs to be the same everywhere.
Make sure that the GANGLIA_URL variable inside the script is correct.
When specified, the action_url attribute creates a small icon in the Nagios UI next to the host or service name to which it corresponds.
If a user clicks this icon, the UI will direct them to the URL specified by the action_url attribute for that particular object.
If your host and service names are consistent in both Nagios and Ganglia, it’s pretty simple to point any service’s action_url back to Ganglia’s graph.php using built-in Nagios macros so that when a user clicks on the action_url icon for that service in the Nagios UI, he or she is presented with a graph of that service’s metric data.
The hiccup, if you didn’t notice, is that Ganglia’s graph.php requires a c= attribute, which must be set to the name of the cluster to which the given host belongs.
Nagios has no concept of Ganglia clusters, but it does provide you with the ability to create custom variables in any object definition.
Custom variables must begin with an underscore, and are available as macros in any context a built-in macro would be available.
Here’s an example of a custom variable in a host object definition defining the Ganglia cluster name to which the host belongs:
The Nagios UI also supports custom CGI headers and footers, which make it possible to accomplish rollover popups of the action_url icon containing graphs from the Ganglia graph.php.
This approach requires some custom development on your part and is outside the scope of this book, but we wanted you to know it’s there.
If that sounds like a useful feature to you, we suggest checking out this information.
Monitoring Ganglia with Nagios When Ganglia is running, it’s a great way to aggregate metrics, but when it breaks, it can cause a bit of frustration with regard to locating the cause of that breakage.
Thankfully, there are a number of points to monitor, which can help stave off an inconvenient breakage.
It is most useful to monitor gmetad and rrdcached on the aggregation hosts and gmond on all hosts.
The pertinent snippets for local monitoring of a gmond process are:
Monitoring Connectivity A more “functional” type of monitoring is monitoring for connectivity on the outbound TCP ports for the varying services.
Checking these ports, with a reasonable timeout, can give a reasonably good idea as to whether they are functioning as expected.
Monitoring cron Collection Jobs cron collection jobs, which are run by your cron periodic scheduling daemon, are another way of collecting metrics without using gmond modules.
Monitoring failures in these scripts, by virtue of their extremely heterogeneous nature and lack of similar structures, has the potential for being a place for fairly serious collection failures.
These can, for the most part, be avoided by following a few basic suggestions Log, but not too much.
Using the logger utility for bash scripts or any of the variety of syslog submission capabilities available will allow you to be able to see what your scripts are doing, instead of being bombarded by logwatch emails or just seeing collection for certain metrics stop.
Touch a stamp file to allow other monitoring tools to detect the last run of your script.
That way, you can monitor the stamp file for becoming stale in a standard way.
Be wary of permissions issues, as test-running a script as a user other than the one who will be running it in production can cause silent failures.
If you’re using netcat, telnet, or other network-facing methods to gather metrics data, there is a possibility that they will fail to return data before the next polling period, potentially causing a pile-up or resulting in other nasty behavior.
Use common sense to figure out how long you should be waiting for results, then exit gracefully if you haven’t gotten them.
Collecting rrdcached Metrics It can be useful to collect metrics on the backlog and processing metrics for your rrdcached services (if you are using them to speed up your gmetad host)
This can be done by querying the rrdcached stats socket and pushing those metrics into Ganglia using gmetric.
Excessive backlogs can be caused by high IO or CPU load on your rrdcached server, so this can be a useful tool to track down rogue cron jobs or other root causes:
A longer answer requires a basic understanding of how sFlow integrates with Ganglia to extend coverage and improve efficiency.
There are strong parallels between Ganglia’s approach to monitoring large numbers of servers and the sFlow standard used to monitor the switches connecting them.
The scalability challenge of monitoring the network links mirrors the challenge of monitoring servers because each server has at least one link to the network.
However, the constraints are different, leading to divergence in the functional split between generating and consuming metrics.
Network switches perform most of their functionality in hardware and have limited processing and memory resources.
While computational resources are scarce, switches are richly connected to the network and excel at sending packets.
With sFlow, raw metrics from the switches are sent over the network to a central server, exploiting the relatively abundant network resources to shift processing and state from the switches to software running on the server.
Removing state from the switches minimizes the memory footprint and eliminates the need to dynamically allocate memory—both useful properties when embedding the agent in switch firmware.
Unlike server metrics, switch metrics are largely implemented in hardware.
For example, byte and packet counts for each switch port are implemented as hardware counters.
Standards are critical: traffic passes through many devices often from different vendors and they need to agree on how to quantify the traffic.
A core part of sFlow is the specification of standard sets of metrics, allowing each switch vendor to embed the measurements in hardware and produce interoperable results.
Ganglia’s binary protocol uses XDR to efficiently encode metrics and send them as UDP packets.
However, each packet contains only a single metric and additional packets are.
In contrast, sFlow is a unicast protocol, allowing the network to isolate measurement traffic to individual links.
Ganglia gmond agents can also be deployed in a unicast configuration.
For large clusters, switching to unicast improves scalability by reducing the amount of memory, CPU, and network resources consumed on each host in the cluster.
For most applications, the difference in scalability isn’t significant, but the improved efficiency of using sFlow as the measurement transport helps Ganglia monitor the extremely large numbers of physical and virtual servers found in cloud data centers.
Standardizing the metrics helps reduce operational complexity by eliminating configuration options that would be needed for a more flexible solution.
Again, this is very important in multivendor networking environments where each configuration option needs to be added to custom device firmware.
Generally, sFlow agents export all the metrics they are capable of generating and leave it up to the analyzer to decide which metrics to keep or discard.
This approach may seem wasteful, but often the measurements are sent to multiple applications, each of which is interested in different metrics.
Maintaining complex, per-application state in the agents consumes significant resources and becomes a challenging configuration task as matching application and agent configuration settings need to be maintained.
Shifting the task of measurement selection to the collector frees up agent resources and reduces configuration complexity.
Security of network devices is also of paramount concern and sFlow agents are intrinsically secure against remote intrusion attacks because they send but never receive or process packets.
At this point, you may be wondering how sFlow agents relate to server monitoring, as most of the discussion has been about the challenges of embedding monitoring in network switches.
Although it is easy to deploy Ganglia agents and script custom metrics.
In many ways, hypervisors have more in common with switches than they do with a general-purpose server.
The hypervisor acts as a virtual switch, connecting virtual machines to each other and to the physical network.
Just like the management processor on a switch, the hypervisor is a tightly controlled, highly secure environment with limited CPU and memory resources.
The sFlow agent is designed for embedded environments and is a natural fit for hypervisors.
For example, mod_sflow embeds sFlow instrumentation in the Apache web server.
The mod_sflow agent has a minimal footprint and a negligible impact on the performance of the web server.
The alternative of tailing the web server log files in order to derive metrics has a much greater overhead that can become prohibitive for high traffic servers.
Similar to the network, there is a value in defining standard metrics in the application space.
For example, the Apache, NGINX, and Tomcat sFlow agents generate the same set of HTTP metrics, allowing web servers to be monitored interchangeably using a variety of performance analysis tools.
Metrics charts are an extremely useful way of summarizing large amounts of information, making them a staple of operations dashboards.
For example, each data point in a chart trending HTTP activity may summarize information from hundreds of thousands of HTTP requests.
However, metrics can only take you so far; how do you follow up if you see an unusual spike in HTTP requests? With sFlow monitoring, metrics are only one part of the measurement stream; an sFlow agent also exports records describing randomly sampled transactions, providing detailed visibility into transaction attributes, data volumes, response times, and status codes (for more information on sFlow’s random sampling mechanism, see Packet Sampling Basics)
The remainder of this chapter describes in detail the architecture of a Ganglia and sFlow deployment, standard sFlow metrics, configuration, troubleshooting, and integration with the broader set of sFlow analysis tools.
Monitoring the performance of its host and sharing the metrics with other hosts in the cluster by sending multicast messages.
In an sFlow deployment, sFlow agents replace gmond agents on all the hosts within the cluster; see Figure 8-1
The sFlow agents in each cluster send metrics as unicast messages to a single gmond instance that tracks cluster state and responds to requests from gmetad.
A single gmond instance monitoring each cluster does represent a single point of failure.
If this is a concern, there are a number of strategies for making the deployment fault tolerant.
A second gmond instance can be added to each cluster and the sFlow agents can be configured to send metrics to both the primary and secondary gmond instances.
Alternatively, a virtual IP address can be assigned to gmond and used as the destination for sFlow messages and gmetad requests.
The virtual IP address can be handed to a secondary system in the event that the primary system fails.
An entire cluster should be homogeneously monitored using sFlow agents or gmond agents—mixing sFlow and gmond agents within a single cluster is not recommended.
However, you can adopt different measurement technologies within a grid, selecting the best strategy for monitoring each cluster.
Using sFlow as the agent technology works best for commodity web, memcache, virtual server, and Java clusters where sFlow’s standard metrics provide good coverage.
For specialized environments, gmond’s extensibility and extensive library of modules are likely to be a better option.
Standard sFlow Metrics Current approaches to server performance monitoring are highly fragmented.
Each operating system, server vendor, and application developer creates specific agents and software for performance monitoring, none of which interoperate.
Standardizing the metrics simplifies monitoring by decoupling agents from performance monitoring applications, allowing measurements to be made once and shared among different monitoring applications.
Base Ganglia metrics are indicated by an asterisk next to the metric name, as seen in Table 8-1
The overlap between Ganglia and sFlow server metrics is no coincidence.
One of the major contributions of the Ganglia project was identifying a common set of metrics that summarize server performance and are portable across operating systems; the Ganglia base metrics were used as a starting point for defining the standard sFlow server metrics.
Hypervisor Metrics The standard set of sFlow hypervisor and virtual machine metrics (Table 8-2) are based on metrics defined by the open source libvirt project.
The libvirt project has created a common set of tools for managing virtualization resources on different virtualization platforms, currently including: Xen, QEMU, KVM, LXC, OpenVZ, User Mode Linux, VirtualBox, and VMware ESX and GSX.
The sFlow metrics provide consistency between different virtualization platforms and between sFlow- and libvirt-based performance monitoring systems.
Per virtual machine statistics are distinguished in Ganglia by prefixing the statistic by the virtual machine name.
Java Virtual Machine Metrics The sFlow Java Virtual Machine (JVM) metrics (Table 8-3) are based on the metrics exposed through the Java Management Extensions (JMX) interface, ensuring consistency with existing JMX-based monitoring systems.
By default, Ganglia assumes that there is a single JVM instance per host.
By default, Ganglia assumes that there is a single HTTP sFlow instance per host.
In addition, an HTTP sFlow agent exports HTTP operation records for randomly sampled HTTP requests.
The HTTP operation records contain a superset of the attributes in the widely supported Combined Logfile Format (CLF) commonly used in web server logging.
Metrics that are present in the gmond module are indicated with an asterisk next to the metric name.
By default, Ganglia assumes that there is a single memcache sFlow instance per host.
In addition to the memcache metrics, a memcache sFlow agent also exports memcache operation records for randomly sampled operations.
Configuring gmond to Receive sFlow The bulk of a typical gmond configuration file, gmond.conf, is devoted to the metrics that gmond exports for the local host.
When gmond is configured as a pure sFlow collector, most configuration settings can be eliminated, resulting in a simple configuration file:
The deaf and mute global settings instruct gmond to listen for metrics but not send them.
All the settings related to local generation of metrics have been removed.
For consistency, an sFlow agent should be installed on the host running gmond if local metrics are required.
In the Ganglia architecture, each cluster is monitored by a separate gmond process.
If more than one cluster is to be monitored, then it is possible to run multiple gmond processes on a single server, each with its own configuration file.
The following settings, in a separate configuration file, configure the second gmond instance to listen on the nonstandard port:
The nonstandard port setting is only required if both gmond processes are running on a single server.
If each cluster is monitored by a separate server, then the sFlow agents on each cluster need to be configured to send metrics to the collector for their cluster.
Another alternative is to assign multiple IP addresses to a single server, one per cluster that is to be monitored.
In this case, multiple gmond instances are created, each associated with a distinct IP address, and the sFlow agents in each cluster are configured to send metrics to the associated IP address.
For example, the following settings configures the gmond instance to listen for sFlow on a specific IP address:
For more information on configuring Ganglia clusters, see Chapter 2
Host sFlow Agent The Host sFlow agent is an open source implementation of the sFlow standard for server monitoring.
The Host sFlow agent provides “scalable, multi-vendor, multi-OS performance monitoring with minimal impact on the systems being monitored.
The following example shows how to install and configure the Host sFlow daemon (hsflowd) on a Linux server in order to illustrate how to send sFlow metrics to Ganglia gmond.
The Host sFlow website should be consulted for detailed instructions on installing and configuring Host sFlow on other platforms.
By default, hsflowd is configured to use DNS Service Discovery (DNSSD) to automatically retrieve settings (i.e., DNSSD = on)
Manual configuration (i.e., DNSSD = off) is recommended when using hsflowd to send metrics to gmond because it allows each host to be configured to send sFlow to the gmond instance responsible for its cluster.
If only one cluster is being monitored, consider using DNS-SD.
Within a few minutes, metrics for the server should start appearing in Ganglia.
Host sFlow Subagents The Host sFlow website maintains a list of related projects implementing subagents that extend sFlow monitoring to HTTP, memcache, and Java applications running on the server.
These subagents require minimal additional configuration because they share configuration settings with hsflowd (Figure 8-2)
The sFlow protocol allows each subagent to operate autonomously and send an independent stream of metrics to the sFlow collector.
Distributing monitoring among the agents eliminates dependencies and synchronization challenges that would increase the complexity of the agents.
Each of the sFlow subagents is responsible for exporting a different set of metrics.
At the time of writing, the following subagents are available: hsflowd.
In addition, network traffic monitoring using iptables/ULOG is supported on Linux platforms.
It has also been integrated into many virtual management systems including OpenStack, openQRM, and OpenNebula.
Enabling the built-in sFlow monitoring on the virtual switch offers the same visibility as sFlow on physical switches, providing a unified, endto-end view of network performance across the physical and virtual infrastructure.
The sflowovsd daemon ships with Host sFlow and automatically configures sFlow monitoring on the Open vSwitch using the ovs-vsctl command.
Similar integrated sFlow support is available for the Microsoft extensible virtual switch that is part of the upcoming Windows Server 2012 version of Hyper-V.
Custom Metrics Using gmetric One of the strengths of Ganglia is the ability to easily add new metrics.
For example, the following command exports the number of users currently logged into a system:
Using the -S or --spoof option ensures that the receiving gmond instance correctly associates metrics sent using gmetric with metrics sent by hsflowd.
The spoof argument is a colon-separated string containing an IP address and a hostname.
The Host sFlow daemon writes information about its configuration and sFlow settings into the /etc/ hsflowd.auto file, including the IP address and hostname used when it sends sFlow data; the gmetric spoof string must match these values.
The gmetric command-line tool is distributed with gmond and picks up settings from the gmond.conf file.
Because there are no gmond instances running on the hosts in an sFlow deployment, eliminating the dependency on gmond.conf, gmond, and gmetric is worthwhile.
The gmetric.py utility is a simple Python script that can be used a replacement for gmetric.
The following bash script demonstrates how configuration settings can be extracted from the hsflowd.auto file and used as arguments for the gmetric.py command:
This script assumes that gmetric.py has been installed as an executable in /usr/local/bin.
Additional custom metrics can be added to the end of the script; ganglia/gmetric is a library of user-contributed gmetric scripts maintained by the Ganglia project on github.
Scheduling the script to run every minute using cron allows Ganglia to automatically track the custom metrics.
Troubleshooting Most problems with sFlow deployments occur because the sFlow datagrams are dropped somewhere between the sFlow agent and gmond.
The following steps will help identify where measurements are being dropped.
Are the Measurements Arriving at gmond? Use a packet capture tool, such as tcpdump, to verify that the sFlow packets are arriving at the server running gmond.
The following command uses tcpdump to check for packets arriving on UDP port 6343:
If the missing metrics are associated with a single host, use tcpdump to filter on the specific host.
The following command verifies that sFlow data is arriving from host 10.0.0.237:
Packet capture using tcpdump occurs before server firewall rules are applied.
Don’t assume that the fact that packets are being displayed by tcpdump means that the packets are being received by gmond—packets can still be dropped by the firewall.
Make sure that incoming sFlow packets are permitted by the local firewall iptables on a Linux system.
A quick way to check whether the firewall is the problem is to temporarily disable the firewall and see if metrics start to appear in Ganglia.
However, this procedure should be performed only if the server is in a secure environment where the security risk of turning off the firewall is acceptable.
Next, use telnet to connect to the gmond tcp_accept_channel and verify that the metrics appear in the XML document.
If metrics are missing from the XML output, the next step is to verify that the metrics are arriving in the sFlow datagrams.
The sflowtool command is similar to tcpdump, decoding and printing the contents of network packets.
However, whereas tcpdump is a general purpose packet analyzer, sflowtool is specifically concerned with sFlow data, performing a full decode of all the sFlow metrics and attributes.
The following command demonstrates how sflowtool is used in combination with tcpdump in order to print out the contents of the sFlow datagrams and verify that specific metrics are being received:
You can use sflowtool on its own to receive and decode sFlow.
However, when gmond is running, it will have opened the port to listen for sFlow, blocking sflowtool from being able to open the port.
Using tcpdump allows the packets to be captured from the open port and fed to sflowtool.
The alternative is to stop gmond before running this test.
If you are receiving sFlow data, then sflowtool will print out the detailed contents, and you should see output similar to the following:
The output of sflowtool consists of simple key/value pairs that are easily processed using scripting languages such as Perl.
At this point, if the sFlow is being received and metrics are missing, it is likely that the sFlow agent has been incorrectly configured and the metrics aren’t being sent.
Are the Measurements Being Sent? If possible, use sflowtool and tcpdump on the sending machine to verify that measurements are being transmitted.
Again, it is possible that local firewall rules on the sending machine are preventing transmission of the sFlow datagrams.
Make sure to verify that the destination IP and port correspond to the gmond.conf file settings at the receiving end.
Ganglia gmond is able to process only counter data, so it is possible that sFlow sample records are being transmitted to gmond, but without counter records, Ganglia will not show any data.
If the measurements are being sent but not received, work with your network administrator to identify any firewall or ACL setting in intermediate switches, routers, or firewall that may be dropping the sFlow datagrams.
The metrics that Ganglia displays are only part of the information contained in an sFlow stream.
Accessing the sampled HTTP operations allows you to dig deeper into a trend and identify the source of the increased traffic.
The following sflowtool output shows the HTTP counters and operation samples reported by mod_sflow:
The following example demonstrates how the sflowtool output can be used to generate additional metrics.
The Perl script uses sflowtool to decode the HTTP request data and calculates average response time over a minute, printing out the results:
The first column is the time (in seconds since the epoch) and the second column is the average HTTP response time (in microseconds):
This example demonstrated how sampled transactions can be used to generate new metrics.
There are many metrics that can be calculated based on the detailed information in transaction samples.
Using sflowtool to pretty-print sFlow records is a good way to see the information that is available and experiment with calculating different metrics.
The sFlow data can be converted into different forms for compatibility with existing tools.
For example, the following command uses sflowtool to convert the binary sFlow HTTP operation data into ASCII CLF so that the operations can be visually inspected or exported to a web log analyzer such as Webalizer:
You can also use protocol reporting tools such as Wireshark with sFlow, using sflowtool to convert sFlow into the standard PCAP format:
In addition, there are a broad range of native sFlow analysis options that provide complementary functionality to Ganglia’s.
There are two main approaches to replicating sFlow: Source replication.
Configure each sFlow agent to send sFlow packets to multiple destinations.
Because sFlow is a unicast protocol, this involves resending packets to each of the destinations and this increases measurement traffic on the network.
The additional traffic is generally not an issue, as each sFlow agent generates a small number of packets.
Destination replication Some sFlow analyzers have built-in replication and packet forwarding capabilities, and there are commercial and open source UDP replication tools available, including sflowtool.
The Ganglia project started out in 1999 with the aim of monitoring grid computing infrastructure: largely homogeneous clusters of similar compute nodes, typically in the academic and research community.
The project founders (including Matt Massie) designed the system to be lightweight and efficient.
In this chapter, we present a range of case studies that demonstrate just how widely respected Ganglia has become—not just within the original audience, but in the wider world of industry.
The SARA case study is just one example of Ganglia at home in the environment it was designed for, albeit on the other side of the Atlantic.
Some of these include Etsy and Quantcast, both of which have shared an insight into just how Ganglia keeps their business running.
The social networking trend is one of the most widely talked about revolutions in communications today, and it is no surprise to find Ganglia has had its finger in that pie, too, as it is the tool of choice at Tagged.
Stepping back from extremes of multicore CPU deployments, Ganglia has also proven itself to be truly adaptable and versatile in the face of dramatic change.
It is estimated that more people will be accessing the Web from smartphones than from desktop PCs by the time you are reading this book.
In this new world, CPUs spend more time sleeping than the average housecat; network connectivity stops and starts and nodes rarely hold the same IP address for more than an hour.
Ganglia’s lightweight protocol, which functions reliably as unidirectional UDP traffic, has proven itself to be ready for business at this level, too, as demonstrated by its presence as an embedded agent in the Lumicall app for Android.
It is this versatility that may well be the most significant acknowledgment of how perceptive Ganglia’s founders were when designing an efficient monitoring protocol.
Some of the most interesting case studies may be those that we can’t publish at all.
It is known that due to Ganglia’s bare-bones efficiency, in that it was written in low-level C with the source code available for all to see, it was chosen for a number of algorithmic trading systems, where every process on a server is closely scrutinized to maintain a competitive edge.
This case study describes how Ganglia and sFlow (Chapter 8) are used to monitor the performance of the Apache, memcached, and Java services that make up the site (thanks to Dave Mangot and Tagged for providing the information for this case study)
Site Architecture Tagged’s scale-out, tiered site architecture, shown in Figure 9-1, is typical of social networking websites.
Incoming web requests are handled by load balancers that distribute the requests to clusters of web servers.
Application logic is implemented through a combination of scripted pages in the web tier interacting with clusters that make up the application tier.
The application tier interacts with the database tier to maintain persistent user data.
The scale-out design of each tier ensures that the site can handle user growth by adding additional servers to the clusters.
The memcache tier is a critical component in most social networking sites.
Caching information retrieved from the database tier in order to reduce database load.
Providing the scatter/gather functionality needed to rapidly query each user’s friends and their current status.
Ganglia is well suited to monitoring Tagged’s infrastructure, presenting a comprehensive, near real-time view of the performance of the clusters making up the site.
Tagged had been using Ganglia for quite some time but we’re starting to see network performance issues with gmond as they scaled up the site.
Solving these performance issues was an important motivation in deciding to migrate from gmond to sFlow agents.
One of the great things about replacing the gmond processes is the increased efficiency of the monitoring infrastructure.
With gmond, every metric sends a packet across the wire.
With sFlow agents, you can sample every 15 seconds, but the agent will batch those metrics into a single packet to send across the wire.
Tagged is now able to collect more metrics, more often, with fewer packets.
On a big network like Tagged, anything that can lower packets per second is a big win.
The switch to sFlow was used as an opportunity to templatize all the Puppet CMDB configurations for this purpose.
Monitoring Configuration The diagram in Figure 9-2 shows the elements making up Tagged’s monitoring system.
The diagram has been simplified—in reality there are multiple clusters within each of the service tiers.
Puppet is used extensively by Tagged for managing server configurations.
In this case, gmond.conf, hsflowd.conf, and gmetad.conf files are all generated using Puppet ERB templates.
Coordination between the hsflowd.conf and gmond.conf settings is needed to ensure that sFlow and gmetric messages are sent to the correct gmond instance.
The gmetad.conf files should also be coordinated with the gmond.conf files on gmond01 and.
Apache stats would normally be calculated based on the output of mod_status, typically using a script to make HTTP requests to the Apache /server-status/ page and parse the results.
There are a limited number of statistics you can get using the STATS interface to memcached, and those statistics are about the memcached instance as a whole, not about individual keys.
Some of the STATS commands (like SIZES) will actually lock up the entire cache while it scans the items, leaving your memcached instance unusable for several seconds.
Before moving to sFlow, Tagged used to get Java virtual machine metrics using Zenoss, which uses a dedicated JMX poller to retrieve information from a designated list of hosts.
Consider Netflix in Amazon EC2, which has potentially thousands of machines disappearing and appearing on the network in minutes but can refresh the host list on the poller only so often.
With sFlow instrumentation of the JVM, data is pushed from the JVMs to gmond, with no polling necessary.
The jmx-flow-agent runs as a -javaagent argument to the Java command line.
Examples The following examples highlight some of the areas where Ganglia and sFlow provide visibility into critical site services to the operations and developer teams at Tagged.
Using Ganglia to monitor the cold start of a memcached cluster provides a compelling demonstration of the leverage that a memcached cluster provides.
The two charts shown in Figure 9-3 show overall bytes in and out of a cluster as it starts up.
In the figure, the chart on the left shows initial traffic to the cold cache and the chart on the right shows the same cluster minutes later once it has warmed up.
Comparing the two charts gives a clear illustration of the importance of the memcached clusters in scaling the site.
When the cache is cold, performance is limited by the database tier, data is being written into the cache and the read performance delivers around 10 MB/s.
The jump in throughput is so dramatic that the rescaling of the vertical axis on the right-hand chart makes the values shown on the left-hand chart appear as a thin line with a near zero value.
The performance of the entire site is critically dependent on how effectively the memcache clusters protect the database tier.
Because the primary bottleneck limiting performance is the database tier, any improvement in the cache hit rates reduces load on the databases and improves scalability.
The Ganglia chart shown in Figure 9-4 trends the overall efficiency of the memcached cluster and is one of the critical performance metrics tracked for the site.
Although Ganglia does an excellent job of trending the performance counters exported by sFlow, one of the benefits of using sFlow as the measurement technology is that counters aren’t the only information being exported.
In addition to periodically exporting the standard memcache counters to Ganglia, the sFlow agents embedded in the memcached servers also randomly sample memcache operations.
For example, the table in Figure 9-5 is updated every minute to show the keys associated with the most cache misses.
Measurements from the web tier provide an overview of the entire site, tracking response times and request rates to each of the applications running on the site.
The charts in Figure 9-6 show combined CPU, network, HTTP request type, and HTTP status code metrics from a static web cluster.
The CPU load is low, mostly waiting for the disk, as you would expect when serving static content.
Also notice that all the requests are HTTP GET operations.
The Ganglia chart in Figure 9-7 trends overall request rates and types to a web server cluster serving nonstatic content for the Tagged.com site.
An interesting point to note is the large number of HTTP POST operations, which is typical of a Web 2.0 site where a large proportion of the traffic is AJAX requests from JavaScript applications running in client browsers.
Again, while Ganglia is used to trend sFlow performance counters from the web cluster, sFlow agents also sample HTTP operations, providing details such as URL, user agent, client address, and response time.
This additional detail is used to identify performance problems with specific services.
The table in Figure 9-8 clearly shows that uploading photos is the slowest operation, which is unsurprising, as uploading a photo involves a significant data transfer and many users have ADSL connections with poor upstream bandwidth.
This data also allows Tagged to track response time for popular content, such as the Pets game.
This information helps the operations and development teams work together to deliver faster response times, keeping users happy and increasing revenue.
The Ganglia chart in Figure 9-9 trends the amount of heap memory allocated by an application over the course of a week.
It turns out that this application periodically needed to be restarted, but no one knew why.
With sFlow-instrumented Java, finegrained detail of heap utilization can be tracked.
After a restart, the garbage collection is able to keep up again.
Amongst other things, SARA provides computational, storage, and networking resources for the Dutch research community and institutions.
Their research fields are very diverse and vary from meteorology, chemistry, and astronomy to economics and psychology.
Researchers, students, and teachers may receive access to one or more of the facilities provided by SARA through national subsidiaries or can approach SARA directly.
For more detailed and complete information on SARA, please visit their website.
Now, 10 years later, we monitor just about every system with Ganglia.
Advantages Ganglia provides a number of advantages to both system administrators as well as our users.
We have a large TV setup in our office that rotates graphs at an interval, displaying several different systems (Figure 9-11)
Ganglia allows us to keep a watch on the general status of the systems we manage.
This usually means that a machine is getting overloaded, is crashing, or has some other issues that are out of the ordinary and needs attention.
It also enables us to see how well (or poorly) the system is being used.
Low usage or load might indicate a problem with batch job scheduling or inefficient usage of the system.
This might result in one of our consultants contacting a user about optimizing his batch jobs.
Obviously, it also allows us to see past values for certain statistics.
Our customers or users of the systems can use our Ganglia website (which is public) to see how “busy” the system is and how their job is performing.
For example, a user might like to know how much memory his job is using or why the job is not consuming CPU time.
In this case, the user could access our Ganglia website to see these statistics.
On our computational systems, we use batch job resource management and scheduling software.
Thus, users provide a sort of shell script that (amongst other things) contains.
The batch system in conjunction with the scheduler decides when the job is actually run.
This decision is based on various things such as resource availability, network fabric, fair share and backfilling mechanisms, and many more.
Thus, users can’t always be sure when their calculation is run.
We have developed an add-on to Ganglia called Job Monarch that interfaces with the resource manager and reports in which batch jobs are running in the cluster, on which nodes, and which resources they have reserved in the batch system (Figure 9-12)
Alternatively, this add-on can also store an archive of jobs for historical reference.
For more information on Job Monarch and SARA’s other open source projects, please visit here.
These allow us to see immediately when any machine is out of sync with the latest firmware.
Additionally, we report things such as support service tags or serial numbers so that we can easily report any issue.
We use the IPMI and Dell Open Manage software to poll this information from a shell script and then submit the metrics through the use of gmetric.
Additionally, some system-specific metrics are relevant for a particular system:
Thanks to the gmetric utility, the possibilities are pretty much endless.
In the latest Ganglia versions, some of these metrics now have modules that report their values including NFS.
Although custom metrics are nice to have, some metric values make sense only in a proper context in relation to other metrics.
In this case, custom graphs or reports can be useful.
With older versions of Ganglia, this type of reporting could be achieved using graph.d PHP scripts; as of the latest Ganglia releases with JSON definitions, it is fairly easy.
One of the key features of Ganglia is its usage of multicast/UDP to submit its statistics.
This approach has some advantages but can also be challenging in a complex network layout.
Some clusters might have complex network layouts with various hops and switches.
Most of the time, UDP multicast is thus not an option.
In addition, Ganglia’s monitoring daemon by default receives and sends all information to all machines in the same system.
That is no problem if you have only a couple of machines, but once you have hundreds of machines, this is no longer very useful.
The situation then arises that a simple compute node has all monitoring metrics of hundreds of machines in memory.
This setup not only starts to consume large amounts of memory but is also not very useful or necessary for the way Ganglia works.
By removing the UDP receive channel, or in later versions setting gmond to deaf mode, the compute nodes no longer receive all metrics from all machines, and their memory usage decreased.
One of our other approaches to the network issue is to set up a central collector daemon and let all Ganglia daemons send in unicast mode.
Unicast does travel through complex network layouts whereas multicast does not.
We use this setup in combination with the aforementioned deaf compute nodes.
However, the issue of memory usage remains in this setup.
For a cluster containing a few hundred machines, like ours, the memory usage can exceed 1 GB.
So make sure that you have enough memory for your central collector.
When we started out using Ganglia with only a few machines, the server side of Ganglia was under a tiny load and could handle it easily.
However, the more machines we added, the more we realized that the server and its disk IO were starting to become a bottleneck.
While visiting a conference, I learned that one performance bottleneck was the Linux kernel readahead ability.
This kernel feature tries to predict IO actions and accelerate performance by reading ahead.
This feature does not work for RRD as the IO access is fairly random.
Installing a new RRDtool version that contains a call to disable readahead for RRDs helped a lot for this particular issue.
See this article by David Plonka, Archit Gupta, and Dale Carder.) Even with readahead disabled, all the RRD activity can still be cumbersome on the Ganglia server side.
Because our Ganglia server has enough memory, we now use a RAM disk for the RRD files.
Obviously, RAM disks can be dangerous in terms of the loss of files, but as long as you are aware of this risk, you can watch out for it.
We now use a 2 GB RAM disk with Linux tmpfs.
The advantage of tmpfs is that it resizes the actual memory usage for the RAM disk, based on utilization.
We created an init.d script that copies out and writes back the RRDs upon system boot and shutdown.
Additionally, we write all RRDs to disk at least once per hour.
This way, we can be sure that the loss of metrics is only for the last hour, should the server or the RAM disk somehow fail and the RAM disk contents be lost.
Conclusion All in all, we are happy with our Ganglia setup, which translates to most of our systems using it.
Although there can be some scaling issues, using some of the techniques described here can help quite a bit.
We are confident that our current setup can handle much more, using techniques such as central collector daemons and RAM disks for the server side.
Reuters Financial Software Thomson Reuters is a well-known provider of real-time financial data, news reporting, business data, and related software products to help customers make best use of Reuters data.
Reuters Financial Software (RFS) and the Risk Management division (which was part of a private equity deal during 2012) produce software to fully automate the operations of the treasury, capturing trade events, monitoring portfolios in real time, detecting risks, and processing payments in the back office.
Ganglia in the QA Environment Before new versions of software products are released to the clients, it is essential that they pass through a testing regime.
For example, entering a foreign exchange (FX) spot trade and verifying that it flows to the back office for payment.
Nonfunctional testing For example, verifying that the price server process does not use up more memory than the previous release.
It is the nonfunctional testing requirement that has been satisfied by the use of Ganglia, RRDtool, and Nagios.
Because the technical staff have a strong background in development and system administration, the use of these tools provides a convenient way to obtain flexibility.
During the recent financial crisis, there have been times when trading activity has been significantly in excess of normal levels.
This typically happens on the days when banks like Bear Stearns and Lehman Brothers collapsed in the United States, or when England saw a bank run on the Northern Rock.
These surges in trading subsequently lead to a surge in the volume of market data (prices) transmitted from the exchanges.
Each time a deal is concluded on an exchange, the price is transmitted to all interested parties through the various networks (Reuters, Bloomberg, and some other minor players)
This surge in data comes back to all banks, not just those that are trading more actively than usual.
The Kondor+ application, the flagship front-office product from RFS, receives the market data through a single server process (known as KVS) and distributes the prices to the trader workstations in the bank.
One client noticed that their KVS had crashed during the surge in data and the trader workstations subsequently stopped showing the current prices.
The client raised a support request, demanding a full explanation and a solution.
Network cards were operating way below capacity, and CPU was not overloaded.
Therefore, it became necessary for the engineering team at RFS to conduct a more detailed study and simulation of the problem.
To create such a simulation, it is necessary to emulate the bank’s environment: a database must be created containing a similar number of deals and portfolios.
Fortunately, creating the replica environment was straightforward, and reproducing the problem was also done very quickly, proving that the client would likely face the same problem again if trading volumes went through the roof.
It is at this stage that monitoring became a factor.
Not only was it necessary to monitor the CPU and NIC, but it became necessary to focus on the individual process, the CPU core it uses, and its memory footprint.
Furthermore, it was possible to modify the application to provide some metrics about its functional load (number of price updates per second, and the time that each price update is queued)
All of the data can be combined and superimposed onto a single graph using the monitoring tools, Ganglia for the system, and some custom scripts around rrdtool to rrdupdate values from the application’s log.
As the underlying format of all data is rrdtool, it is easy to rrdgraph all the data together as required.
It was very quickly established that KVS response times become slower and slower as the market data volume increases.
Memory consumption was not unusual, but a CPU core was overloaded (due to a single thread)
The graphs show a linear relationship between the rate of incoming price ticks and the processing delay—up to a point.
When the market data level exceeds a certain threshold, the relationship between the tick rate and processing time ceases to become linear and demonstrates an exponential increase.
Using various profiling tools, some inefficient code paths were identified and optimized.
Once again, the simulations were performed and it was observed that the exponential behavior no longer occurred within the trading volumes expected by the client.
The custom rrdtool graphs provide a convenient way to show the client evidence of the testing and also allow them to forecast the market data volumes that they can safely handle in the future.
Ganglia in a Major Client Project Upgrading a Kondor+ client from an old version of the product to the current version is a major project.
Such projects often involve anywhere from six to twelve months of planning and testing before a real upgrade is attempted.
This process validates that the bank will be able to complete the upgrade safely.
In most banks, upgrades are performed on Saturdays, and postupgrade tests are performed on Sundays.
There is always a fall-back plan in case a problem is found on a Sunday so that the bank will always be open for business as usual on Monday morning.
Such a long upgrade would not be permitted, as it would not allow enough time for bank staff to test the system on Sunday.
For example, a specialist from the database vendor was called to verify the configuration of the database server.
Application support staff pored over the log files looking for update queries that ran too slowly.
System staff look for trends in metrics, such as CPU, IOPS, memory usage, or network IO, that might be correlated with the problem.
As the bank had purchased new servers for the project, there was no monitoring tool available—staff from the bank headquarters were not planning to install their commercial monitoring tool until the project want live.
Given the nature of the project (an upgrade), it should not be a big surprise that many of the processes ran on a single thread (using a single CPU core), which is often a bottleneck.
Another common feature of upgrade projects is that all the data in the database is scanned, reconstructing tables one by one where the schema has changed and recalculating values that have new meaning.
Behind the scenes, the database server has to perform an enormous amount of work updating indexes during such operations.
All of the database activity is typically IO-bound, so the SAN performance is the major bottleneck during these phases of the upgrade.
It was not expected that Ganglia would solve all the problems on this project.
In fact, it was only expected to help answer a couple questions:
To help answer these questions, Ganglia binaries are used from the OpenCSW project for Solaris.
The per-CPU metrics are very valuable for visualizing the periods when the upgrade process is bound by a single CPU.
However, the per-LUN metrics (reporting IOPS from the SAN) are slightly more challenging because the database actually stripes across multiple LUNs.
Fortunately, with rrdtool as the backend for Ganglia, it is possible to manually define a graph that aggregates metrics for all the LUNs backing each database.
Notice that the average service time for each LUN is graphed separately (colored lines), while the throughput (MB/s) for all LUNs is summed to create the shaded area graph.
Spikes in the shaded area graph show periods when there is a need for significant IO throughput.
It is interesting to note that the LUNs don’t all demonstrate the same service times during those peaks in throughput; this is a very interesting revelation that encouraged further analysis of the SAN architecture.
The results showed that the project was more often blocked by IO than by CPU, making it clear that remediation should be focused on the IO system itself, or on improving (or.
Furthermore, the visualization with RRDtool made it clear that a particularly long phase of the upgrade was IO bound all the way through.
Lumicall (Mobile VoIP on Android) Lumicall is an advanced open source mobile VoIP app for Android published as part of the OpenTelecoms.org initiative.
Mobile VoIP started as a niche area, but recent trends suggest that it has the potential to become the dominant paradigm for voice communications.
Demand for mobile VoIP is driven by the desire of consumers and businesses to break free of the straightjacket created by the telephone companies.
The paradigm of existing mobile communications is largely insecure (as demonstrated by the wholesale level of phone hacking recently exposed in the UK), overpriced (as demonstrated by the need for EU regulators to intervene and set limits on roaming charges), and largely inflexible when compared to modern unified communications solutions.
Nobody expects telephone companies to reform; rather, the increasing power of smartphones (such as those running Android) is enabling consumers and businesses to deploy VoIP apps that work around the cumbersome GSM model.
For example, when a Belgian goes to France, he no longer pays roaming charges: he simply locks his phone onto Wi-Fi and uses mobile VoIP.
Although consumers have been quick to experiment with apps like Lumicall and Viber, businesses are still one step behind.
One of their key concerns is the need to maintain the quality and consistency of phone calls.
Monitoring Mobile VoIP for the Enterprise For VoIP communications, a change in network quality (or a change in application performance due to some other app on the phone hogging the CPU) can lead to an almost immediately perceptible change in audio quality.
It is often easy for the user to be disturbed by such issues in audio quality, yet it can be troublesome for an IT department to take responsibility and diagnose the problem, particularly if the user is in a remote location.
In such cases, a comprehensive monitoring solution at the level of the handset is necessary.
The solution can gather key metrics about the audio quality (for example, the percentage of packets that are not received) and secondary metrics (such as wireless strength) that may help understand how a quality problem has been induced.
Ganglia Monitoring Within Lumicall Because the Android platform is based on Java, it is relatively easy to deploy the gmetric4j code into an Android app, which is exactly what has been done with Lumicall.
One particular issue with mobile users is that they often move between different wireless networks and mobile data networks, almost always behind network address translation (NAT)
This makes it impossible to probe them continuously using a protocol such as SNMP.
However, Ganglia’s UDP unicast mode requires communication in only one direction (from handset back to base), so it is an instant success.
The same is likely to be true for almost any mobile app, not just a VoIP app like Lumicall.
A more subtle issue is the tendency of Android phones to sleep.
Many Java applications, including the original gmetric4j code, rely on regular threading code and use the Thread.sleep() method to perform background tasks.
This code compiles and executes on Android without any conspicuous error.
However, it is obvious that when the phone sleeps, the metrics stop updating.
Consequently, the most recent version of gmetric4j allows the timing code to be replaced by a user-supplied implementation, and Lumicall leverages this mechanism to drive all background activity using the Android AlarmManager, a special class that can wake the CPU when a sleep() interval elapses.
A cutdown version of the WifiSampler class, just extracting the Wi-Fi metric, is presented here:
The output of this metric is demonstrated by the RSSI graph in Figure 9-14
Some mobile devices spend little time in Wi-Fi coverage areas, so it is also important to understand the quality of the mobile/cell network.
Lumicall: Conclusion The data collected by gmetric4j in Lumicall and the advanced reporting provided by the new Ganglia web interface provide a unique opportunity for those deploying mobile VoIP to bring all of their quality metrics and useful support metrics within a single framework.
This powerful visualization method is likely to see increases in mobile VoIP service quality, particularly in campus environments where IT managers can tailor the Wi-Fi topology based on real-time data from Lumicall.
Count what is countable, measure what is measurable, and what is not measurable, make measurable.
Galileo Galilei Quantcast offers free direct audience measurement for hundreds of millions of web destinations and powers the ability to target any online audience across tens of billions of events per day.
Operating at this scale requires Quantcast to have an expansive and reliable monitoring platform.
In order to stay on top of its operations, Quantcast collects billions of monitoring metrics per day.
The Ganglia infrastructure we’ve developed lets us collect these metrics from a wide variety of sources and deliver them to several different kinds of consumers; analyze, report, and alert on them in real time; and provide our product teams with a platform for performing their own analysis and reporting.
We have a similarly broad spectrum of consumers of monitoring data:
Ganglia gives us a system that can listen to all of these sources, serve all of these consumers, and be performant, reliable, and quick to update.
Reporting, Analysis, and Alerting Like many companies, we use Nagios to monitor our systems and alert us when something is not working correctly.
Instead of running a Nagios agent on each of our hosts, we use Ganglia to report system and application metrics back to our central monitoring servers.
Not only does this reduce system overhead, but it also reduces the proliferation of protocols and applications running over our WAN network.
Nagios runs a Perl script called check_qcganglia, which has access to every metric in our RRD files stored on ramdisk.
This allows us to have individual host checks as well as health checks for entire clusters and grids using the Ganglia summary data.
Another great benefit of this approach is that we can configure checks and alerts on the application-level data that our developers put into Ganglia.
It also allows us to alert on aggregated business metrics such as spend and revenue.
We have also implemented a custom Nagios alerting script that, through configuration of the alert type, determines which Ganglia graphs would be useful for an operator to see right away in the alert email and attaches those graphs as images to the outbound alert.
Typically, these include CPU and memory utilization graphs for the individual host as well as application graphs for the host, cluster, and grid.
These help the on-call operator immediately assess the impact of any given alert on the overall performance of the system.
Because Ganglia is built on RRDtool, we’re able to leverage some of its most powerful (if intimidating) features as well.
One of these is its ability to do Holt-Winters forecasting and aberrance detection.
The Holt-Winters algorithm works on data that is periodic and generally consistent over time.
It derives an upper and lower bound from historical data and uses that information to make predictions about current and future data.
We have several metrics that are well suited for Holt-Winters aberrance detection, such as HTTP requests per second.
This metric varies significantly over the course of a single day and each day of the week, but from one Monday to the next, our traffic is pretty consistent.
If we see a large swing of traffic from one minute to the next, it typically indicates a problem.
We use Nagios and RRDtool to monitor the Holt-Winters forecast for our traffic and alert if the traffic varies outside of the expected range.
We took a datacenter offline for maintenance, and this triggered aberrance detection.
Figure 9-20 shows an example of an unexpected aberrance alert.
Ganglia as an Application Platform Because a gmond daemon is running on every machine in the company, we encourage our application teams to use Ganglia as the pathway for reporting performance data about their applications, which has several benefits: Easy integration for performance monitoring.
The Ganglia infrastructure is already configured, so the application developer doesn’t have to do any special work to make use of it.
Especially in concert with the json2gmetrics tool described shortly, it’s easy for an application to generate and report any metrics the developers think would be useful to see.
Also, the developer doesn’t have to worry about opening up firewall ports, parsing or rotating log files, or running other services devoted to monitoring, such as JMX.
Powerful analysis tools By submitting metrics to Ganglia in this way, application developers get easy access to very powerful analysis tools (namely, the Ganglia web interface)
This set of tools is particularly useful when troubleshooting problems with an application that correlate with operating system metrics, such as attributing a drop in requests per second to a machine swapping the application out of memory.
Simple and flexible alerting Our Ganglia infrastructure is tied in with our alerting system.
Once an alert is configured, an application can trigger an alert state by generating a specific metric with a sentinel value, which lets us centralize alert generation and event correlation; for instance, we can prevent an application from generating alerts when the system it runs on will be in a known maintenance window.
This approach is much better than the alternative of each application having to reinvent the wheel of monitoring for an invalid state and sending its own emails.
Best Practices Hard-won experience has given us some best practices for using Ganglia.
Following these practices has helped us scale Ganglia up to a truly impressive level while retaining high reliability and real-time performance.
Collecting the sheer number of metrics we do and writing them to a spinning disk would be impossible.
To solve the IOPS problem, we write all our metrics to a ramdisk (Linux tmpfs) and then back them up to a spinning disk with a cron job.
Because we have multiple gmetad hosts in separate locations, we’re able to protect against an outage at either location.
Also, our cron job runs frequently enough that if the tmpfs is cleared (for instance, if the server reboots), our window of lost data is small.
Another way we deal with our large number of metrics is by splitting our gmetad collectors up into several instances.
We logically divided our infrastructure into several different groups such as webservers, internal systems, map-reduce cluster, and realtime bidding.
Each system has its own gmetad instance, and the only place all the graphs come together are on web dashboards that embed the graphs from each instance.
This design gives us the best of both worlds: a seamless interface to a large volume of data, with the added reliability and performance of multiple instances.
Tools We’ve developed several tools to get even more use out of Ganglia.
It takes a configuration file that maps a given OID (and whatever adjustments it might need) into a Ganglia metric, with a name, value, type, and units label.
The configuration file also associates each OID with one or more hosts, such as network routers or UPS devices.
On a configurable schedule, snmp2ganglia polls each host for the OIDs associated with it and delivers those values to Ganglia as metrics.
We make use of the capability to “spoof” a metric’s source IP and DNS name to create a pseudohost for each device.
We make extensive use of gmond-style plug-ins, especially for operating system metrics.
For instance, some of the plug-ins that we’ve written collect:
These metrics are a powerful tool for troubleshooting and analyzing system performance, especially when correlating systems that might seem unrelated through the Ganglia web UI.
Drawbacks Although Ganglia is both powerful and flexible, it does have its weak points.
Each of the following subsections describes a challenge we had to overcome to run Ganglia at our scale.
As our infrastructure grew, we discovered that a single gmetad process just couldn’t keep up with the number of metrics we were trying to collect.
Updates would be skipped because the gmetad process couldn’t write them to the RRD files in time, and the CPU and memory pressure on a single machine was overwhelming.
We solved this problem by scaling horizontally: setting up a series of monitoring servers, each of which runs a gmetad process that collects some specific part of the whole set of metrics.
This way, we can run as many monitoring servers as we need to reach the scale of our operations.
The downside of this solution is significantly increased management cost; we have to manage and coordinate multiple monitoring servers and track which metrics are being collected where.
We’ve mitigated this problem somewhat by setting up the Ganglia web UI on each server to redirect requests to the appropriate server for each category of metrics, so end users see a unified system that hides the sharding.
However, instead of keeping all of those data points, this RRD file might instead be configured to keep just one averaged data point per hour for the whole day.
As new updates are processed, a whole hour’s worth of 15-second data points would be averaged to get a single data point that represents the entire hour.
The file would contain high-resolution data for the last hour and a consolidated representation of that data for the rest of the day.
For Quantcast, we often need to analyze events in a very precise time window.
If we saw a CPU load spike for just one minute, for instance, RRDtool’s data consolidation would average that spike with the rest of the data for that hour into a single value that did not reflect the spike; the precise data about the event would be lost.
Quantcast is a global operation, so we needed to make some improvements to Ganglia to get it to a global scale.
One of these improvements involved the process of collecting data from a gmond daemon.
When a gmetad process collects metrics from a gmond, the data is emitted as an XML tree with a node for each metric.
This winds up being a whole lot of highly compressible plain-text data.
In order to save bandwidth over our WAN links, we patched gmond to emit a compressed version of that tree, and we also patched gmetad to work with compressed streams.
Doing so substantially reduced the amount of monitoring data we were sending over our WAN links in each site, which left us with more room for business-critical data.
Whenever a gmetad process writes a metric to an RRD file, the update process requires several accesses to the same file: one read in the header area, one write in the header area, and at least one update in the data area.
RRDtool attempts to improve the speed of this process by issuing fadvise/madvise system calls to prevent the filesystem from doing normally clever things that interfere with this specific access pattern.
Because we write our RRD files into a tmpfs, the fadvise/madvise calls were just slowing things down.
This approach allowed us to collect a much larger number of metrics with each gmetad shard and also significantly reduced CPU usage by the gmetad process.
Conclusions Ganglia is an incredibly powerful and flexible tool, but at its heart, it’s designed to collect performance monitoring data about the machines in a single high-performance computing cluster and display them as a series of graphs in a web interface.
Quantcast has taken that basic tool and stretched it to its limit by growing it to an exceptionally large scale, extending it across a global infrastructure, and scouring the hidden corners of Ganglia’s potential to find new tools and functionality.
Most important, though, we’ve made Ganglia into a platform upon which other services rely.
Monitoring Is Mandatory Having metrics on your infrastructure should be considered mandatory, not optional.
We do wonder about how the underlying foundations of our applications are performing when we’re deploying changes, not to mention the continual process of capacity planning.
We use Ganglia in the same vein that we use configuration management at Etsy: as the basic building blocks of our confidence.
A common approach in fast-growing web operations is to gather clusters of nodes that do the same work and distribute workload amongst them in one way or another.
Very rarely (if at all) do we have fewer than two nodes in a single cluster.
Because of this, Ganglia’s notion of the node/cluster/grid hierarchy makes it a good fit for tiered web applications.
Getting a node into production in its specific role within Ganglia has one requirement: the placement of a single gmond.conf file.
Because we need no a priori knowledge about the host’s existence or even its hostname, it will come up in the cluster in Ganglia for virtually no cost, which makes it ideal in an environment cooked by automation.
If you can produce a value or statistic from something, Ganglia will gladly and easily collect and display it for you in multiple contexts.
As we are a “dashboard-driven” engineering culture at Etsy (to put it mildly), we need tools that make it as free-as-possible to collect metrics.
One thing is clear to me: sending ad hoc metrics via gmetric is at least 50 percent of the reason we use Ganglia.
The benefit here is that node-based context is critical in a number of areas, which can’t be overstated.
Troubleshooting a large install of nodes is largely an exercise in reducing the problem space as quickly as possible.
Troubleshooting under time pressure means that I’m going to want to first establish that a single or group of nodes in a cluster is somehow behaving differently.
In almost every case, we put nodes into a single cluster because we expect them to behave exactly the same.
With stacked graphs and a single metric view on every node at a cluster level, outliers will stick out easily.
I can confirm (or not) the fact that all nodes are behaving identically.
If they do, I can drill into them with one action.
If they don’t, I can eliminate node-level issues and move my process of elimination elsewhere.
Above that sits Graphite, which rounds out the rest of what we need.
They are actions that depend on multiple nodes in multiple clusters acting together to produce what is, in essence, a feature of our site.
Embrace Diversity Trying to shoehorn these types of metrics into the node/cluster/grid context that Ganglia is so good at is akin to mowing a lawn with a snowplow.
In the same vein, I don’t believe that Graphite is appropriate at all for node and clusterlevel metrics; the UI and collection methods simply don’t encourage this usage and forcing it would be a tough sell for me on a cost/benefit basis.
We use Graphite for application-level metrics such as those mentioned previously: statistics on highresolution values such as page performance and error log line entries.
We have Logster (based on the original Ganglia logtailer), which can derive metrics from log files.
We can send these metrics to either Ganglia (for things that make sense in a node context) or Graphite (for things that make sense in the application context), and we use both heavily.
We aim to layer tools together, in complementary fashion, as part of a palette of tools to gain situational awareness.
To support this approach, we have a lightweight dashboard UI framework that can juxtapose graphs from Cacti, FITB, Ganglia, Graphite, and others.
Conclusion Even in this world of disposable and fully automated infrastructures, node- and clusterlevel metrics matter a great deal.
I have yet to see an implementation of any metrics collection system that is the best at every one of these layers—only individual tools that are the best at one of the layers.
To those who aspire to create such an omniscient tool that can elegantly handle metrics collection for every device possible, along with flexible anomaly detection, alerting, and escalation—all at the same time remaining human-centered enough to be used efficiently: I salute you, and godspeed.
Module Metric Definitions The following tables describe the metric modules that are part of the distribution of the Ganglia monitoring system.
In addition to these metric modules, there are also a number of other modules that are available through the Ganglia module git repository.
As new modules are developed, many developers share them with the Ganglia community through the Ganglia module repository.
The Ganglia module git repository is open to the public, and the modules are free to download and use.
Some of the additional modules that are available from this repository include modules for monitoring an Apache Web Server, MySQL database, and Xen virtual machine, as well as Tomcat and Jetty servlet monitoring through JMX.
If the hardware architecture supported multiple CPUs, gmond reported only the overall usage rather than the usage for each individual CPU.
Through the configuration of Mod_MultiCPU, all CPU-related metrics can be reported for each CPU detected on the system.
The purpose of this module was to detect and report all of the metric gathering, data packet sends, and receives as gmond was running.
If gmond is incapable of sending or receiving a packet of any kind, Mod_GStatus will report these failures as well.
In previous versions of gmond, the metrics that reported disk space added up the totals for all disks and reported this value for total disk space and used space.
Multidisk (Python module): report disk available and disk used space for each individual disk device.
The standard memory metrics only report overall memory usage and totals for the system and do not provide any further details about memory management.
This module dives a little deeper into how the memory is being used and can help to point out memory inefficiencies.
Total number of bytes sent by this server to network.
Number of bytes this server is allowed to use for storage.
Number of keys that have been stored and found present.
Number of items that have been stored and not found.
Number of keys that have been deleted and found present.
Number of items that have been deleted and not found.
TcpConn The TcpConn metric module (Table A-5) provides a way to look at TCP network connections in an effort to detect problems or misconfiguration.
By monitoring the TCP connection activity on the system, this module can help point out issues that may affect network latency or the inability to send or receive data in an efficient manner.
This module also introduced a new pattern for how to write Python metric modules that include threading and caching.
Because the TcpConn module relies heavily on the netstat Linux utility to acquire TCP metric data and the fact that gmond is not a multithreaded daemon, the module doesn’t want to cause any delays in the gmond gathering process due to latency in calling an external process utility.
In order to avoid any kind of latency issues, the TcpConn module starts up its own gathering thread, which is then free to invoke the netstat utility as required.
By invoking the netstat utility within a thread, the module is able to gather the TCP connection related values without having to worry about delaying the gmond gathering process.
As the metrics are being gathered from within the thread, these values are stored in a shared cache that can be accessed quickly whenever gmond asks the module for its metric values.
Introducing threads through Python is actually a very convenient way to make a single-threaded gmond daemon act as if it were multithreaded.
Advanced Metrics Aggregation and You There are certain types of metrics aggregation that can’t easily be accomplished by using only gmond and gmetric submission.
It is worth pointing out, at this point in this text, that Ganglia does have a way of dealing with some derivative values.
Metrics submitted using a “positive” slope generate RRDs that are created as COUNTERs; however, this mechanism is not ideal for situations involving incrementing values that submit on each iteration (i.e., Apache httpd page serving counts without log-scraping)
One of the solutions for dealing with counter values is statsd.
It is written in Node.js, though there are quite a few ports and clones available.
Table A-6 lists some of these that are available at the time of writing.
It should be noted that the original statsd implementation does not have Ganglia/gmetric submission support without an additional module, which is available here.
This implementation is a fork of the gographite port of statsd, which did not have Ganglia/gmetric submission support at the time of writing.
The protocol for statsd is relatively simple, and most of the statsd servers come with an example client for submitting statsd.
In addition, there is another piece of software called VDED, which can be used to track ever-increasing values.
Configuring statsd Most statsd instances are fairly similar to configure for submitting metrics to Ganglia.
The important considerations should be how that data is represented in your Ganglia instance.
For example, statsd and its clones don’t have any particular notion of “host,” so each statsd instance is tied to submitting metrics that will be associated with a specific Ganglia host.
Configuring VDED VDED has some of the same constraints as statsd, except that it is not constrained to submit all values as if they belonged to a single host.
The optional “spoof” parameter for submitting metrics to VDED allows different spoof arguments to be associated with.
It is a good idea to remember that VDED aggregation is limited to the cluster of which the receiving gmond instance is a member.
It can be controlled via a command socket and is distributed with the standard RRDtool packages for most Linux distributions.
Note that rrdcached may be unnecessary if you’re using a ramdisk to store your RRD files.
Installing The rrdcached package can be installed on Debian-based distributions (Debian, Ubuntu, Linux, Mint, etc.) by using apt:
Configuring gmetad for rrdcached gmetad can be configured to use rrdcached by setting the RRDCACHED_ADDRESS variable in the configuration file included by gmetad’s init script.
Along with the gmetad configuration change (which will require a restart of any running gmetad processes), it is also recommended that a change be made to the Ganglia web frontend, which will force the frontend to also use rrdcached for forming graphs.
Controlling rrdcached There are a number of useful operations that can be performed by using telnet, netcat, or socat (depending on whether you have a network or Unix socket set up as the control socket)
For example, a FLUSHALL command forces the rrdcached daemon to flush all RRD data to disk as soon as it can:
Troubleshooting There are several things that can go awry with an rrdcached Ganglia installation, primarily because an extra layer of complexity has been added.
Make sure that the permissions on the rrdcached socket file are permissive enough to allow both the gmetad service user and the web server user to be able to read and write.
Failures to communicate via the socket will be visible in gmetad’s log.
Heavy load on the server hosting the rrdcached instance may result in a backlog of metrics that have not been written properly to disk.
Note that this does not indicate that the metrics have been dropped, but rather that the rrdcached file still has not written them to their final location.) Individual metrics can be flushed to disk by using the rrdcached socket and issuing a FLUSH command followed by the full pathname to the target RRD file.
This will bring the specified RRD file to the top of the rrdcached job queue.
Alternatively, a full flush to disk of all queued RRD updates can be initiated by sending a FLUSHALL instead.
Debugging with gmond-debug gmond-debug is a useful tool for debugging inbound gmetric formatted traffic.
It can be used to debug any of the third-party gmetric libraries or to track down most unusual gmetric behaviors.
To test using gmond-debug, point your gmetric submission software at this machine, port 1234
As soon as UDP packets on port 1234 are received, gmond-debug will attempt to decode it and print a serialized version of the information contained therein.
You’ve got data—lots and lots of data that’s just too valuable to delete or take offline for even a minute.
Your data is likely made up of a number of different formats, and you know that your data will only grow larger and more complex over time.
The growing pains you’re facing have been faced by other people and there are systems to handle it: Hadoop and HBase.
If you want to use Ganglia to monitor a Hadoop or HBase cluster, I have good news— Ganglia support is built in.
Hadoop was created by Doug Cutting, who now works as an architect at Cloudera and serves as chair of the Apache Software Foundation.
He named Hadoop after his son’s yellow stuffed toy elephant.
With Hadoop, you can grow the size of your filesystem by adding more machines to your cluster.
This feature allows you to grow storage incrementally, regardless of whether you need terabytes or petabytes of space.
Hadoop also ensures your data is safe by automatically replicating your data to multiple machines.
You could remove a machine from your cluster and take it out to a grassy field with a baseball bat to reenact the printer scene from Office Space—and not lose a single byte of data.
The Hadoop MapReduce engine breaks data processing up into smaller units of work and intelligently distributes them across your cluster.
The MapReduce APIs allow developers to focus on the question they’re trying to answer instead of worrying about how to handle machine failures—you’re regretting what you did with that bat now, aren’t you? Because data in the Hadoop filesystem is replicated, Hadoop can automatically handle failures by rerunning the computation on a replica, often without the user even noticing.
HBase is an Apache-licensed open source system modeled after Google’s Bigtable.
HBase sits on top of the Hadoop filesystem and provides users random, real-time read/ write access to their data.
You can think of HBase, at a high level, as a flexible and extremely scalable database.
Hadoop and HBase are dynamic systems that are easier to manage when you have the metric visibility that Ganglia can provide.
If you’re interested in learning more about Hadoop and HBase, I highly recommend the following books:
Configuring Hadoop and HBase to Publish Metrics to Ganglia Ganglia’s monitoring daemon (gmond) publishes metrics in a well-defined format.
You can configure the Hadoop metric subsystem to publish metrics directly to Ganglia in the format it understands.
Old agents can’t communicate with new agents, and vice versa.
This file is organized into different contexts: jvm, rpc, hdfs, mapred, and hbase.
You can turn on Ganglia monitoring for one or all contexts.
I’m sure you noticed some obvious patterns from these snippets.
The prefix of the configuration keys is the name of the context (e.g., mapred), and each context has class, period, and servers properties.
The class specifies what format that metric should be published in.
If you don’t explicitly provide a port number, Hadoop will assume you want the default gmond port: 8649
If you bind the multicast address using the bind option in gmond.conf, you cannot also send a metric message to the unicast address of the host running gmond.
If you are not receiving Hadoop metrics after setting the servers property, doublecheck your gmond udp_recv_channel setting in gmond.conf.
For your metric configuration changes to take effect, you must restart your Hadoop and HBase services.
Once you have Hadoop/HBase properly configured to publish Ganglia metrics (Table B-1), you will see the metrics in your gmond XML and graphs will appear in your Ganglia web console for each metric:
At this point, you know how to turn on monitoring for any Hadoop context.
The Hadoop JVM metrics cover garbage collection, memory use, thread states, and the number of logging events for each service: DataNode, NameNode, SecondaryNameNode, JobTracker, and TaskTracker.
The Hadoop RPC metrics will give you information about the number of open connections, processing times, number of operations, and authentication successes and failures.
The Hadoop DFS metrics provide information about data block operations (read, removed, replicated, verify, written), verification failures, bytes read and written, volume failures, and local/remote client reads and writes.
The Hadoop MapReduce metrics provide information about the number of map and reduce slots used, shuffle failures, and tasks completed.
This is the number of blocks of StoreFiles (HFiles) in the cache.
Number of blocks that had to be evicted from the block cache due to heap size constraints.
The cache-hit ratio for reads configured to look in the cache (i.e., cacheBlocks=true)
Number of blocks of StoreFiles (HFiles) read from the cache.
Number of blocks of StoreFiles (HFiles) requested but not read from the cache.
Block cache size in memory (bytes), that is, memory in use by the BlockCache.
This is the number of Stores in the RegionServer that have been targeted for compaction.
Sum of all the memstore sizes in this RegionServer (MB)
Metric Name Explanation of value will result in 1 request for each "next" call (i.e., not each row)
Sum of all the StoreFile index sizes in this RegionServer (MB)
For example, if a table (which contains the column family) has three regions on a RegionServer, there will be three stores open for that column family.
We’d like to hear your suggestions for improving our indexes.
Android platform based on, 191 heap memory utilization in Tagged study,
RRAs (Round Robin Archive values), 37 definition changed in gmetad.conf, but RRD.
SELinux and firewall, 120 send channel, UDP, gmond configuration file,
User Mode Linux, 149 users (SARA), benefits provided by Ganglia,
He designed Ganglia to monitor a shared computational grid of clusters distributed across the United States for scientific research.
Matt is currently a software engineer at Cloudera and is focused on Apache Hadoop enterprise management and monitoring.
He is currently one of the maintainers of the Ganglia project.
He has been involved with HPC since 2003 and has worked on Open Source projects such as OSCAR, SystemImager, and Warewulf.
In addition to being a committer on the Apache HTTPD and APR projects, Brad is also a developer as well as one of the administrators of the Ganglia project.
He also developed and contributed several of the initial metric modules that currently ship with Ganglia.
Brad attended school at the University of Utah and Brigham Young University and holds a degree in computer science.
Vladimir Vuksan (Broadcom) has worked in technical operations, systems engineering, and software development for over 15 years.
Prior to Broadcom he has worked at Mocospace, Rave Mobile Safety, Demandware, and the University of New Mexico implementing high-availability solutions and building tools to make managing and running infrastructure easier.
Colophon The animal on the cover of Monitoring with Ganglia is a Porpita pacifica, which is found in the tropical Pacific.
Its delicate tentacles are sticky and extend from chambers in the gas-filled disc; the tentacles are usually damaged in the surf and reportedly deliver a sting that is not powerful but may cause irritation to human skin.
The blue button lives on the surface of the sea and consists of two main parts: the float and the hydroid colony.
The hard golden-brown float is round, almost flat, and about 1 inch wide.
The hydroid colony, which can range from bright blue turquoise to yellow, resembles tentacles like those of the jellyfish.
Each strand has numerous branchlets, each of which ends in knobs of stinging cells called nematocysts.
In the food web, its size makes it easy prey for several organisms.
The blue button itself is a passive drifter, meaning that it feeds on both living and dead organisms that come in contact with it.
It competes with other drifters for food and mainly feeds on small fish, eggs, and zooplankton.
The blue button has a single mouth located beneath the float, which is used for both the intake of nutrients and the expulsion of wastes.
This species reproduces by releasing tiny medusa, which go on to develop new colonies.
Mod_Python Configuring gmond to support Python metric modules Writing a Python metric module Debugging and testing a Python metric module Configuring a Python metric module Deploying a Python metric module.
Extending gmond with gmetric Running gmetric from the Command Line Spoofing with gmetric.
Typical Problems and Troubleshooting Procedures Web Issues Blank page appears in the browser Browser displays white page with error message Cluster view shows uppercase hostname, link doesn’t work Host appears in the wrong cluster Host appears multiple times in web, different variations of the hostname (or IP address) Some hosts appear with shortname instead of FQDN One or more hosts don’t appear in the web interface Hosts don’t appear/data stale after UDP aggregator restarted Dead/retired hosts still appearing in the Web Wrong CPU count or other metrics are missing Fonts in graphs are too big or too small Spikes in graphs Custom metrics don’t appear Custom metric’s value is truncated Gaps appear randomly in the graphs Some host is completely missing from the cluster gmetad hierarchy and federation; some grids don’t appear on the Web.
