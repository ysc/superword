Over 100 recipes to make Apache Solr faster, more reliable, and return better results.
No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the information presented.
However, the information contained in this book is sold without warranty, either express or implied.
Neither the author, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals.
However, Packt Publishing cannot guarantee the accuracy of this information.
Rafał Kuć is a born team leader and software developer.
Currently working as a Consultant and a Software Engineer at Sematext Inc, where he concentrates on open source technologies such as Apache Lucene and Solr, ElasticSearch, and Hadoop stack.
He has more than 10 years of experience in various software branches, from banking software to e-commerce products.
He is mainly focused on Java, but open to every tool and programming language that will make the achievement of his goal easier and faster.
Rafał is also one of the founders of the solr.pl site, where he tries to share his knowledge and help people with their problems with Solr and Lucene.
He is also a speaker for various conferences around the world such as Lucene Eurocon, Berlin Buzzwords, and ApacheCon.
Rafał began his journey with Lucene in 2002 and it wasn't love at first sight.
When he came back to Lucene later in 2003, he revised his thoughts about the framework and saw the potential in search technologies.
From then on, Rafał has concentrated on search technologies and data analysis.
Right now Lucene, Solr, and ElasticSearch are his main points of interest.
This book is an update to the first cookbook for Solr that was released almost two year ago now.
What was at the beginning an update turned out to be a rewrite of almost all the recipes in the book, because we wanted to not only bring you an update to the already existing recipes, but also give you whole new recipes that will help you with common situations when using Apache Solr 4.0
I hope that the book you are holding in your hands (or reading on a computer or reader screen) will be useful to you.
Although I would go the same way if I could get back in time, the time of writing this book was not easy for my family.
Among the ones who suffered the most were my wife Agnes and our two great kids, our son Philip and daughter Susanna.
Without their patience and understanding, the writing of this book wouldn't have been possible.
I would also like to thank my parents and Agnes' parents for their support and help.
I would like to thank all the people involved in creating, developing, and maintaining Lucene and Solr projects for their work and passion.
Ravindra Bharathi has worked in the software industry for over a decade in various domains such as education, digital media marketing/advertising, enterprise search, and energy management systems.
He has a keen interest in search-based applications that involve data visualization, mashups, and dashboards.
He divides his time between University jobs and external projects related to Oracle, and big data technologies.
He has worked in several Oracle related projects such as translation of Oracle manuals and multimedia CBTs.
His background is in database, network, web, and Java technologies.
In the XML world, he is known as the developer of the DB Generator for the Apache Cocoon project, the open source projects DBPrism and DBPrism CMS, the Lucene-Oracle integration by using Oracle JVM Directory implementation, and the Restlet.org project – the Oracle XDB Restlet Adapter, an alternative to writing native REST web services inside the database resident JVM.
Since 2006, he has been a part of the Oracle ACE program.
Oracle ACEs are known for their strong credentials as Oracle community enthusiasts and advocates, with candidates nominated by ACEs in the Oracle Technology and Applications communities.
Support files, eBooks, discount offers and more You might want to visit www.PacktPub.com for support files and downloads related to your book.
Did you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at www.PacktPub.com and as a print book customer, you are entitled to a discount on the eBook copy.
Get in touch with us at service@packtpub.com for more details.
At www.PacktPub.com, you can also read a collection of free technical articles, sign up for a range of free newsletters and receive exclusive discounts and offers on Packt books and eBooks.
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book library.
Here, you can access, read and search across Packt's entire library of books.
Why Subscribe? f Fully searchable across every book published by Packt.
Free Access for Packt account holders If you have an account with Packt at www.PacktPub.com, you can use this to access PacktLib today and view nine entirely free books.
You will be taken on a tour through the most common problems when dealing with Apache Solr.
You will learn how to deal with the problems in Solr configuration and setup, how to handle common querying problems, how to fine-tune Solr instances, how to set up and use SolrCloud, how to use faceting and grouping, fight common problems, and many more things.
Every recipe is based on real-life problems, and each recipe includes solutions along with detailed descriptions of the configuration and code that was used.
Chapter 2, Indexing Your Data, explains data indexing such as binary file indexing, using Data Import Handler, language detection, updating a single field of document, and much more.
Chapter 3, Analyzing Your Text Data, concentrates on common problems when analyzing your data such as stemming, geographical location indexing, or using synonyms.
Chapter 4, Querying Solr, describes querying Apache Solr such as nesting queries, affecting scoring of documents, phrase search, or using the parent-child relationship.
Chapter 5, Using the Faceting Mechanism, is dedicated to the faceting mechanism in which you can find the information needed to overcome some of the situations that you can encounter during your work with Solr and faceting.
Chapter 6, Improving Solr Performance, is dedicated to improving your Apache Solr cluster performance with information such as cache configuration, indexing speed up, and much more.
Chapter 8, Using Additional Solr Functionalities, explains documents highlighting, sorting results on the basis of function value, checking user spelling mistakes, and using the grouping functionality.
Chapter 9, Dealing with Problems, is a small chapter dedicated to the most common situations such as memory problems, reducing your index size, and similar issues.
Appendix, Real Life Situations, describes how to handle real-life situations such as implementing different autocomplete functionalities, using near real-time search, or improving query relevance.
Who this book is for This book is for users working with Apache Solr or developers that use Apache Solr to build their own software that would like to know how to combat common problems.
Knowledge of Apache Lucene would be a bonus, but is not required.
Conventions In this book, you will find a number of styles of text that distinguish between different kinds of information.
Here are some examples of these styles, and an explanation of their meaning.
Code words in text are shown as follows: "The lib entry in the solrconfig.xml file tells Solr to look for all the JAR files from the ../../langid directory"
When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:
Words that you see on the screen, in menus or dialog boxes for example, appear in the text like this: "clicking the Next button moves you to the next screen"
Warnings or important notes appear in a box like this.
Let us know what you think about this book—what you liked or may have disliked.
Reader feedback is important for us to develop titles that you really get the most out of.
To send us general feedback, simply send an e-mail to feedback@packtpub.com, and mention the book title through the subject of your message.
If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, see our author guide on www.packtpub.com/authors.
Customer support Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.
Downloading the example code You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.packtpub.com/support and register to have the files e-mailed directly to you.
Errata Although we have taken every care to ensure the accuracy of our content, mistakes do happen.
By doing so, you can save other readers from frustration and help us improve subsequent versions of this book.
If you find any errata, please report them by visiting http://www.packtpub.com/support, selecting your book, clicking on the errata submission form link, and entering the details of your errata.
Once your errata are verified, your submission will be accepted and the errata will be uploaded to our website, or added to any list of existing errata, under the Errata section of that title.
Piracy Piracy of copyright material on the Internet is an ongoing problem across all media.
At Packt, we take the protection of our copyright and licenses very seriously.
If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.
Please contact us at copyright@packtpub.com with a link to the suspected pirated material.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Questions You can contact us at questions@packtpub.com if you are having a problem with any aspect of the book, and we will do our best to address it.
Introduction Setting up an example Solr instance is not a hard task, at least when setting up the simplest configuration.
The simplest way is to run the example provided with the Solr distribution, that shows how to use the embedded Jetty servlet container.
If you don't have any experience with Apache Solr, please refer to the Apache Solr tutorial which can be found at: http://lucene.apache.org/solr/tutorial.html before reading this book.
If another version of Solr is mandatory for a feature to run, then it will be mentioned.
We have a simple configuration, simple index structure described by the schema.xml file, and we can run indexing.
In this chapter you'll see how to configure and use the more advanced Solr modules; you'll see how to run Solr in different containers and how to prepare your configuration to different requirements.
You will also learn how to set up a new SolrCloud cluster and migrate your current configuration to the one supporting all the features of SolrCloud.
Finally, you will learn how to configure Solr cache to meet your needs and how to pre-sort your Solr indexes to be able to use early query termination techniques efficiently.
Running Solr on Jetty The simplest way to run Apache Solr on a Jetty servlet container is to run the provided example configuration based on embedded Jetty.
In this recipe, I would like to show you how to configure and run Solr on a standalone Jetty container.
Getting ready First of all you need to download the Jetty servlet container for your platform.
You can get your download package from an automatic installer (such as, apt-get), or you can download it yourself from http://jetty.codehaus.org/jetty/
The first thing is to install the Jetty servlet container, which is beyond the scope of this book, so we will assume that you have Jetty installed in the /usr/share/jetty directory or you copied the Jetty files to that directory.
Let's start by copying the solr.war file to the webapps directory of the Jetty installation (so the whole path would be /usr/share/jetty/webapps)
In addition to that we need to create a temporary directory in Jetty installation, so let's create the temp directory in the Jetty installation directory.
Next we need to copy and adjust the solr.xml file from the context directory of the Solr example distribution to the context directory of the Jetty installation.
The final file contents should look like the following code:
You can download the example code files for all Packt books you have purchased from your account at http://www.packtpub.com.
If you purchased this book elsewhere, you can visit http://www.packtpub.com/support and register to have the files e-mailed directly to you.
Now we need to copy the jetty.xml, webdefault.xml, and logging.properties files from the etc directory of the Solr distribution to the configuration directory of Jetty, so in our case to the /usr/share/jetty/etc directory.
The next step is to copy the Solr configuration files to the appropriate directory.
I'm talking about files such as schema.xml, solrconfig.xml, solr.xml, and so on.
Those files should be in the directory specified by the solr.solr.home system variable (in my case this was the /usr/share/solr directory)
Please remember to preserve the directory structure you'll see in the example deployment, so for example, the /usr/share/solr directory should contain the solr.xml (and in addition zoo.cfg in case you want to use SolrCloud) file with the contents like so:
All the other configuration files should go to the /usr/share/solr/collection1/conf directory (place the schema.xml and solrconfig.xml files there along with any additional configuration files your deployment needs)
Your cores may have other names than the default collection1, so please be aware of that.
The last thing about the configuration is to update the /etc/default/jetty file and file.
The whole line with that variable could look like the following:
If you didn't install Jetty with apt-get or a similar software, you may not have the /etc/ parameter to the Jetty startup.
We can now run Jetty to see if everything is ok.
To start Jetty, that was installed, for example, using the apt-get command, use the following command:
If there were no exceptions during the startup, we have a running Jetty with Solr deployed and configured.
To check if Solr is running, try going to the following address with your web browser: http://localhost:8983/solr/
You should see the Solr front page with cores, or a single core, mentioned.
Congratulations! You just successfully installed, configured, and ran the Jetty servlet container with Solr deployed.
For the purpose of this recipe, I assumed that we needed a single core installation with only schema.xml and solrconfig.xml configuration files.
Multicore installation is very similar – it differs only in terms of the Solr configuration files.
The first thing we did was copy the solr.war file and create the temp directory.
The temp directory will be used by Jetty to unpack the WAR file.
The solr.xml file we placed in the context directory enables Jetty to define the context for the Solr web application.
As you can see in its contents, we set the context to be /solr, so our Solr application will be available under http://localhost:8983/solr/
We also specified where Jetty should look for the WAR file (the war property), where the web application descriptor file (the defaultsDescriptor property) is, and finally where the temporary directory will be located (the tempDirectory property)
The next step is to provide configuration files for the Solr web application.
Those files should be in the directory specified by the system solr.solr.home variable.
I decided to use the /usr/share/solr directory to ensure that I'll be able to update Jetty without the need of overriding or deleting the Solr configuration files.
When copying the Solr configuration files, you should remember to include all the files and the exact directory structure that Solr needs.
So in the directory specified by the solr.solr.home variable, the solr.xml file should be available – the one that describes the cores of your system.
The solr.xml file is pretty simple – there should be the root element called solr.
Inside it there should be a cores tag (with the adminPath variable set to the address where Solr's cores administration API is available and the defaultCoreName attribute that says which is the default core)
The cores tag is a parent for cores definition – each core should have its own cores tag with name attribute specifying the core name and the instanceDir attribute specifying the directory where the core specific files will be available (such as the conf directory)
If you installed Jetty with the apt-get command or similar, you will need to update the /etc/default/jetty file to include the solr.solr.home variable for Solr to be able to see its configuration directory.
After all those steps we are ready to launch Jetty.
If you installed Jetty with apt-get or a similar software, you can run Jetty with the first command shown in the example.
Otherwise you can run Jetty with a java command from the Jetty installation directory.
After running the example query in your web browser you should see the Solr front page as a single core.
Congratulations! You just successfully configured and ran the Jetty servlet container with Solr deployed.
There are a few tasks you can do to counter some problems when running Solr within the Jetty servlet container.
Here are the most common ones that I encountered during my work.
I want Jetty to run on a different port Sometimes it's necessary to run Jetty on a different port other than the default one.
Buffer size is too small Buffer overflow is a common problem when our queries are getting too long and too complex, – for example, when we use many logical operators or long phrases.
When the standard head buffer is not enough you can resize it to meet your needs.
To do that, you add the following line to the Jetty connector in thejetty.xml file.
Of course the value shown in the example can be changed to the one that you need:
After adding the value, the connector definition should look more or less like the following snippet:
Running Solr on Apache Tomcat Sometimes you need to choose a servlet container other than Jetty.
Maybe because your client has other applications running on another servlet container, maybe because you just don't like Jetty.
Whatever your requirements are that put Jetty out of the scope of your interest, the first thing that comes to mind is a popular and powerful servlet container – Apache Tomcat.
This recipe will give you an idea of how to properly set up and run Solr in the Apache Tomcat environment.
Getting ready First of all we need an Apache Tomcat servlet container.
It can be found at the Apache Tomcat website – http://tomcat.apache.org.
I concentrated on the Tomcat Version 7.x because at the time of writing of this book it was mature and stable.
The version that I used during the writing of this recipe was Apache Tomcat 7.0.29, which was the newest one at the time.
To run Solr on Apache Tomcat we need to follow these simple steps:
The Tomcat installation is beyond the scope of this book so we will assume that you have already installed this servlet container in the directory specified by the $TOMCAT_HOME system variable.
The second step is preparing the Apache Tomcat configuration files.
To do that we need to add the following inscription to the connector definition in the server.xml configuration file:
The portion of the modified server.xml file should look like the following code snippet:
The third step is to create a proper context file.
To do that, create a solr.xml file in the $TOMCAT_HOME/conf/Catalina/localhost directory.
The contents of the file should look like the following code:
To do that we need the apache-solr4.0.0.war file that contains the necessary files and libraries to run Solr that is to be copied to the Tomcat webapps directory and renamed solr.war.
The one last thing we need to do is add the Solr configuration files.
The files that you need to copy are files such as schema.xml, solrconfig.xml, and so on.
Those files should be placed in the directory specified by the solr/home variable (in our case /usr/share/solr/)
Please don't forget that you need to ensure the proper directory structure.
If you are not familiar with the Solr directory structure please take a look at the example deployment that is provided with the standard Solr package.
Please remember to preserve the directory structure you'll see in the example deployment, so for example, the /usr/share/solr directory should contain the solr.xml (and in addition zoo.cfg in case you want to use SolrCloud) file with the contents like so:
All the other configuration files should go to the /usr/share/solr/collection1/ conf directory (place the schema.xml and solrconfig.xml files there along with any additional configuration files your deployment needs)
Your cores may have other names than the default collection1, so please be aware of that.
Now we can start the servlet container, by running the following command: bin/catalina.sh start.
In the log file you should see a message like this: Info: Server startup in 3097 ms.
To ensure that Solr is running properly, you can run a browser and point it to an address where Solr should be visible, like the following: http://localhost:8080/solr/
If you see the page with links to administration pages of each of the cores defined, that means that your Solr is up and running.
Let's start from the second step as the installation part is beyond the scope of this book.
That means that we need to ensure that Apache Tomcat will be informed that all requests and responses made should use that encoding.
To do that, we modified the server.xml file in the way shown in the example.
The Catalina context file (called solr.xml in our example) says that our Solr application will be available under the /solr context (the path attribute)
We also specified the WAR file location (the docBase attribute)
We also said that we are not using debug (the debug attribute), and we allowed Solr to access other context manipulation methods.
The last thing is to specify the directory where Solr should look for the configuration files.
We do that by adding the solr/home environment variable with the value attribute set to the path to the directory where we have put the configuration files.
The solr.xml file is pretty simple – there should be the root element called solr.
Inside it there should be the cores tag (with the adminPath variable set to the address where the Solr cores administration API is available and the defaultCoreName attribute describing which is the default core)
The cores tag is a parent for cores definition – each core should have its own core tag with a name attribute specifying the core name and the instanceDir attribute specifying the directory where the core-specific files will be available (such as the conf directory)
There are some other options of the catalina.sh (or catalina.bat) script; the descriptions of these options are as follows:
After running the example address in the web browser, you should see a Solr front page with a core (or cores if you have a multicore deployment)
Congratulations! You just successfully configured and ran the Apache Tomcat servlet container with Solr deployed.
There are some other tasks that are common problems when running Solr on Apache Tomcat.
Changing the port on which we see Solr running on Tomcat Sometimes it is necessary to run Apache Tomcat on a different port other than 8080, which is the default one.
To do that, you need to modify the port variable of the connector definition in the server.xml file located in the $TOMCAT_HOME/conf directory.
If you would like your Tomcat to run on port 9999, this definition should look like the following code snippet:
Zookeeper is a centralized service for maintaining configurations, naming, and provisioning service synchronization.
SolrCloud uses ZooKeeper to synchronize configuration and cluster states (such as elected shard leaders), and that's why it is crucial to have a highly available and fault tolerant ZooKeeper installation.
If you have a single ZooKeeper instance and it fails then your SolrCloud cluster will crash too.
So, this recipe will show you how to install ZooKeeper so that it's not a single point of failure in your cluster configuration.
Getting ready The installation instruction in this recipe contains information about installing ZooKeeper Version 3.4.3, but it should be useable for any minor release changes of Apache ZooKeeper.
This recipe will show you how to install ZooKeeper in a Linux-based environment.
After downloading the ZooKeeper installation, we create the necessary directory: sudo mkdir /usr/share/zookeeper.
Then we unpack the downloaded archive to the newly created directory.
Next we need to change our ZooKeeper configuration file and specify the servers that will form the ZooKeeper quorum, so we edit the /usr/share/zookeeper/conf/ zoo.cfg file and we add the following entries:
And now, we can start the ZooKeeper servers with the following command: /usr/share/zookeeper/bin/zkServer.sh start.
If everything went well you should see something like the following: JMX enabled by default.
Of course you can also add the ZooKeeper service to start automatically during your operating system startup, but that's beyond the scope of the recipe and the book itself.
Let's skip the first part, because creating the directory and unpacking the ZooKeeper server there is quite simple.
What I would like to concentrate on are the configuration values of the ZooKeeper server.
The clientPort property specifies the port on which our SolrCloud servers should connect to ZooKeeper.
The dataDir property specifies the directory where ZooKeeper will hold its data.
So far, so good right ? So now, the more advanced properties; the tickTime property specified in milliseconds is the basic time unit for ZooKeeper.
The initLimit property specifies how many ticks the initial synchronization phase can take.
Finally, the syncLimit property specifies how many ticks can pass between sending the request and receiving an acknowledgement.
These three properties define the addresses of the ZooKeeper instances that will form the quorum.
However, there are three values separated by a colon character.
The first part is the IP address of the ZooKeeper server, and the second and third parts are the ports used by ZooKeeper instances to communicate with each other.
Clustering your data After the release of Apache Solr 4.0, many users will want to leverage SolrCloud distributed indexing and querying capabilities.
It's not hard to upgrade your current cluster to SolrCloud, but there are some things you need to take care of.
With the help of the following recipe you will be able to easily upgrade your cluster.
Getting ready Before continuing further it is advised to read the Installing a standalone ZooKeeper recipe in this chapter.
It shows how to set up a ZooKeeper cluster in order to be ready for production use.
In order to use your old index structure with SolrCloud, you will need to add the following field to your fields definition (add the following fragment to the schema.xml file, to its fields section):
Now let's switch to the solrconfig.xml file – starting with the replication handlers.
First, you need to ensure that you have the replication handler set up.
Remember that you shouldn't add master or slave specific configurations to it.
So the replication handlers' configuration should look like the following code:
In addition to that, you will need to have the administration panel handlers present, so the following configuration entry should be present in your solrconfig.xml file:
The last request handler that should be present is the real-time get handler, which should be defined as follows (the following should also be added to the solrconfig.xml file):
The next thing SolrCloud needs in order to properly operate is the transaction log configuration.
The following fragment should be added to the solrconfig.xml file:
It should be pointing to the default cores administration address – the cores tag should have the adminPath property set to the /admin/cores value.
The example solr.xml file could look like the following code:
And that's all, your Solr instances configuration files are now ready to be used with SolrCloud.
So now let's see why all those changes are needed in order to use our old configuration files with SolrCloud.
The _version_ field is used by Solr to enable documents versioning and optimistic locking, which ensures that you won't have the newest version of your document overwritten by mistake.
Because of that, SolrCloud requires the _version_ field to be present in your index structure.
Adding that field is simple – you just need to place another field definition that is stored and indexed, and based on the long type.
As for the replication handler, you should remember not to add slave or master specific configuration, only the simple request handler definition, as shown in the previous example.
The same applies to the administration panel handlers: they need to be available under the default URL address.
The real-time get handler is responsible for getting the updated documents right away, even if no commit or the softCommit command is executed.
This handler allows Solr (and also you) to retrieve the latest version of the document without the need for re-opening the searcher, and thus even if the document is not yet visible during usual search operations.
The configuration is very similar to the usual request handler configuration – you need to add a new handler with the name property set to /get and the class property set to solr.
In addition to that, we want the handler to be omitting response headers (the omitHeader property set to true)
One of the last things that is needed by SolrCloud is the transaction log, which enables realtime get operations to be functional.
The transaction log keeps track of all the uncommitted changes and enables a real-time get handler to retrieve those.
In order to turn on transaction log usage, one should add the updateLog tag to the solrconfig.xml file and specify the directory where the transaction log directory should be created (by adding the dir property as shown in the example)
In the configuration previously shown, we tell Solr that we want to use the Solr data directory as the place to store the transaction log directory.
Finally, Solr needs you to keep the default address for the core administrative interface, so you should remember to have the adminPath property set to the value shown in the example (in the solr.xml file)
This is needed in order for Solr to be able to manipulate cores.
Choosing the right directory implementation One of the most crucial properties of Apache Lucene, and thus Solr, is the Lucene directory implementation.
The directory interface provides an abstraction layer for Lucene on all the I/O operations.
Although choosing the right directory implementation seems simple, it can affect the performance of your Solr setup in a drastic way.
This recipe will show you how to choose the right directory implementation.
In order to use the desired directory, all you need to do is choose the right directory factory implementation and inform Solr about it.
Let's assume that you would like to use NRTCachingDirectory as your directory implementation.
In order to do that, you need to place (or replace if it is already present) the following fragment in your solrconfig.xml file:
The setup is quite simple, but what directory factories are available to use? When this book was written, the following directory factories were available:
So now let's see what each of those factories provide.
Before we get into the details of each of the presented directory factories, I would like to comment on the directory factory configuration parameter.
All you need to remember is that the name attribute of the directoryFactory tag should be set to DirectoryFactory and the class attribute should be set to the directory factory implementation of your choice.
If you want Solr to make the decision for you, you should use solr.
This is a filesystem-based directory factory that tries to choose the best implementation based on your current operating system and Java virtual machine used.
If you are implementing a small application, which won't use many threads, you can use solr.SimpleFSDirectoryFactory which stores the index file on your local filesystem, but it doesn't scale well with a high number of threads.
NIOFSDirectoryFactory scales well with many threads, but it doesn't work well on Microsoft Windows platforms (it's much slower), because of the JVM bug, so you should remember that.
This directory implementation uses virtual memory and a kernel feature called mmap to access index files stored on disk.
This allows Lucene (and thus Solr) to directly access the I/O cache.
This is desirable and you should stick to that directory if near real-time searching is not needed.
If you need near real-time indexing and searching, you should use solr.
It is designed to store some parts of the index in memory (small chunks) and thus speed up some near real-time operations greatly.
The last directory factory, solr.RAMDirectoryFactory, is the only one that is not persistent.
The whole index is stored in the RAM memory and thus you'll lose your index after restart or server crash.
Also you should remember that replication won't work when using solr.RAMDirectoryFactory.
One would ask, why should I use that factory? Imagine a volatile index for an autocomplete functionality or for unit tests of your queries' relevancy.
Just anything you can think of, when you don't need to have persistent and replicated data.
However, please remember that this directory is not designed to hold large amounts of data.
If you are used to the way spellchecker worked in the previous Solr versions, you may remember that it required its own index to give you spelling corrections.
That approach had some disadvantages, such as the need for rebuilding the index, and replication between master and slave servers.
It allowed you to use your main index to provide spelling suggestions and didn't need to be rebuilt after every commit.
So now, let's see how to use that new spellchecker implementation in Solr.
First of all, let's assume we have a field in the index called title, in which we hold titles of our documents.
What's more, we don't want the spellchecker to have its own index and we would like to use that title field to provide spelling suggestions.
In addition to that, we would like to decide when we want a spelling suggestion.
In order to do that, we need to do two things:
First, we need to edit our solrconfig.xml file and add the spellchecking component, whose definition may look like the following code:
Now we need to add a proper request handler configuration that will use the previously mentioned search component.
To do that, we need to add the following section to the solrconfig.xml file:
In order to get spelling suggestions, we need to run the following query:
In response we will get something like the following code:
If you check your data folder you will see that there is not a single directory responsible for holding the spellchecker index.
Now let's get into some specifics about how the previous configuration works, starting from the search component configuration.
The queryAnalyzerFieldType property tells Solr which field configuration should be used to analyze the query passed to the spellchecker.
The name property sets the name of the spellchecker which will be used in the handler configuration later.
The field property specifies which field should be used as the source for the data used to build spelling suggestions.
As you probably figured out, the classname property specifies the implementation class, which in our case is solr.
DirectSolrSpellChecker, enabling us to omit having a separate spellchecker index.
The next parameters visible in the configuration specify how the Solr spellchecker should behave and that is beyond the scope of this recipe (however, if you would like to read more about them, please go to the following URL address: http://wiki.apache.org/solr/ SpellCheckComponent)
Let's concentrate on all the properties that start with the spellcheck prefix.
First we have spellcheck.dictionary, which in our case specifies the name of the spellchecking component we want to use (please note that the value of the property matches the value of the name property in the search component configuration)
We tell Solr that we want the spellchecking results to be present (the spellcheck property with the value set to on), and we also tell Solr that we want to see the extended results format (spellcheck.extendedResults set to true)
In addition to the mentioned configuration properties, we also said that we want to have a maximum of five suggestions (the spellcheck.count property), and we want to see the collation and its extended results (spellcheck.collate and spellcheck.collateExtendedResults both set to true)
Let's see one more thing – the ability to have more than one spellchecker defined in a request handler.
More than one spellchecker If you would like to have more than one spellchecker handling your spelling suggestions you can configure your handler to use multiple search components.
For example, if you would like to use search components (spellchecking ones) named word and better (you have to have them configured), you could add multiple spellcheck.dictionary parameters to your request handler.
Solr cache configuration As you may already know, caches play a major role in a Solr deployment.
And I'm not talking about some exterior cache – I'm talking about the three Solr caches:
It is managed by Lucene and created when it is first used by the Searcher object.
With the help of these caches we can tune the behavior of the Solr searcher instance.
In this task we will focus on how to configure your Solr caches to suit most needs.
There is one thing to remember – Solr cache sizes should be tuned to the number of documents in the index, the queries, and the number of results you usually get from Solr.
Getting ready Before you start tuning Solr caches you should get some information about your Solr instance.
For the purpose of this task I assumed the following numbers:
Of course the above configuration is based on the example values.
So we change the appropriate value in the solrconfig.xml file to something like this:
First of all we use the solr.FastLRUCache implementation instead of solr.LRUCache.
So the called FastLRUCache tends to be faster when Solr puts less into caches and gets more.
This is the opposite to LRUCache which tends to be more efficient when there are more puts than gets operations.
This colud be the first time you see cache configuration, so I'll explain what cache configuration parameters mean:
As you can see, I tend to use the same number of entries for size and initialSize, and half of those values for autowarmCount.
The size and initialSize properties can be set to the same size in order to avoid the underlying Java object resizing, which consumes additional processing time.
Some of the Solr caches (documentCache actually) operate on internal identifiers called docid.
That's because docid is changing after every commit operation and thus copying docid is useless.
Please keep in mind that the settings for the size of the caches is usually good for the moment you set them.
But during the life cycle of your application your data may change, your queries may change, and your user's behavior may, and probably will change.
That's why you should keep track of the cache usage with the use of Solr administration pages, JMX, or a specialized software such as Scalable Performance Monitoring from Sematext (see more at http://sematext.com/spm/index.html), and see how the utilization of each of the caches changes in time and makes proper changes to the configuration.
There are a few additional things that you should know when configuring your caches.
Using a filter cache with faceting Solr will use the filter cache to check each term.
Remember that if you use this method, your filter cache size should have at least the size of the number of unique facet values in all your faceted fields.
This is crucial and you may experience performance loss if this cache is not configured the right way.
When we have no cache hits When your Solr instance has a low cache hit ratio you should consider not using caches at all (to see the hit ratio you can use the administration pages of Solr)
Cache insertion is not free – it costs CPU time and resources.
So if you see that you have a very low cache hit ratio, you should consider turning your caches off – it may speed up your Solr instance.
Before you turn off the caches please ensure that you have the right cache setup – a small hit ratio can be a result of bad cache configuration.
When we have more "puts" than "gets" When your Solr instance uses put operations more than get operations you should consider using the solr.LRUCache implementation.
It's confirmed that this implementation behaves better when there are more insertions into the cache than lookups.
Filter cache This cache is responsible for holding information about the filters and the documents that match the filter.
Actually this cache holds an unordered set of document IDs that match the filter.
If you don't use the faceting mechanism with a filter cache, you should at least set its size to the number of unique filters that are present in your queries.
This way it will be possible for Solr to store all the unique filters with their matching document IDs and this will speed up the queries that use filters.
Query result cache The query result cache holds the ordered set of internal IDs of documents that match the given query and the sort specified.
That's why if you use caches you should add as many filters as you can and keep your query (the q parameter) as clean as possible.
For example, pass only the search box content of your search application to the query parameter.
If the same query will be run more than once and the cache has enough capacity to hold the entry, it will be used to give the IDs of the documents that match the query, thus a no Lucene (Solr uses Lucene to index and query data that is indexed) query will be made saving the precious I/O operation for the queries that are not in the cache – this will boost up your Solr instance performance.
The maximum size of this cache that I tend to set is the number of unique queries and their sorts that are handled by my Solr in the time between the Searcher object's invalidation.
Document cache The document cache holds the Lucene documents that were fetched from the index.
Basically, this cache holds the stored fields of all the documents that are gathered from the Solr index.
The size of this cache should always be greater than the number of concurrent queries multiplied by the maximum results you get from Solr.
This cache can't be automatically warmed – that is because every commit is changing the internal IDs of the documents.
Remember that the cache can be memory consuming in case you have many stored fields, so there will be times when you just have to live with evictions.
Query result window The last thing is the query result window.
This parameter tells Solr how many documents to fetch from the index in a single Lucene query.
This is a kind of super set of documents fetched.
In our example, we tell Solr that we want the maximum of one hundred documents as a result of a single query.
Our query result window tells Solr to always gather two hundred documents.
Then when we need some more documents that follow the first hundred they will be fetched from the cache, and therefore we will be saving our resources.
The size of the query result window is mostly dependent on the application and how it is using Solr.
If you tend to do a lot of paging, you should consider using a higher query result window value.
You should remember that the size of caches shown in this task is not final, and you should adapt them to your application needs.
The values and the method of their calculation should only be taken as a starting point to further observation and optimization of the process.
Also, please remember to monitor your Solr instance memory usage as using caches will affect the memory that is used by the JVM.
See also There is another way to warm your caches if you know the most common queries that are sent to your Solr instance – auto-warming queries.
Please refer to the Improving Solr performance right after a startup or commit operation recipe in Chapter 6, Improving Solr Performance.
For information on how to cache whole pages of results please refer to the Caching whole result pages recipe in Chapter 6, Improving Solr Performance.
How to fetch and index web pages There are many ways to index web pages.
We could download them, parse them, and index them with the use of Lucene and Solr.
The indexing part is not a problem, at least in most cases.
That's why this recipe will cover how to fetch and index web pages using Apache Nutch.
Getting ready For the purpose of this task we will be using Version 1.5.1 of Apache Nutch.
To download the binary package of Apache Nutch, please go to the download section of http://nutch.
Let's assume that the website we want to fetch and index is http://lucene.apache.org.
To do that we just need to extract the downloaded archive to the directory of our choice; for example, I installed it in the directory /usr/share/nutch.
Of course this is a single server installation and it doesn't include the Hadoop filesystem, but for the purpose of the recipe it will be enough.
Then we'll open the file $NUTCH_HOME/conf/nutch-default.xml and set the value http.agent.name to the desired name of your crawler (we've taken SolrCookbookCrawler as a name)
Now let's create empty directories called crawl and urls in the $NUTCH_HOME directory.
After that we need to create the seed.txt file inside the created urls directory with the following contents: http://lucene.apache.org.
So the appropriate entry should look like the following code: +^http://([a-z0-9]*\.)*lucene.apache.org/
One last thing before fetching the data is Solr configuration.
We start with copying the index structure definition file (called schema-solr4
We also create an empty stopwords_en.txt file or we use the one provided with Solr if you want stop words removal.
Now we need to make two corrections to the schema.xml file we've copied:
Now we can start crawling and indexing by running the following command from the $NUTCH_ HOME directory:
Depending on your Internet connection and your machine configuration you should finally see a message similar to the following one:
This means that the crawl is completed and the data was indexed to Solr.
After installing Nutch and Solr, the first thing we did was set our crawler name.
Nutch does not allow empty names so we must choose one.
The file nutch-default.xml defines more properties than the mentioned ones, but at this time we only need to know about that one.
In the next step, we created two directories; one (crawl) which will hold the crawl data and the second one (urls) to store the addresses we want to crawl.
The crawl-urlfilter.txt file contains information about the filters that will be used to check the URLs that Nutch will crawl.
In the example, we told Nutch to accept every URL that begins with http://lucene.apache.org.
The schema.xml file we copied from the Nutch configuration directory is prepared to be used when Solr is used for indexing.
We finally came to the point where we ran the Nutch command.
We specified that we wanted to store the crawled data in the crawl directory (first parameter), and the addresses to crawl data from are in the urls directory (second parameter)
The –solr switch lets you specify the address of the Solr server that will be responsible for the indexing crawled data and is mandatory if you want to get the data indexed with Solr.
We decided to index the data to Solr installed at the same server.
The –depth parameter specifies how deep to go after the links defined.
In our example, we defined that we want a maximum of three links from the main page.
There is one more thing worth knowing when you start a journey in the land of Apache Nutch.
Multiple thread crawling The crawl command of the Nutch command-line utility has another option – it can be configured to run crawling with multiple threads.
So if you would like to crawl with 20 threads you should run the crawl command like sot:
See also If you seek more information about Apache Nutch please refer to the http://nutch.
Sometimes indexing prepared text files (such as XML, CSV, JSON, and so on) is not enough.
There are numerous situations where you need to extract data from binary files.
For example, one of my clients wanted to index PDF files – actually their contents.
To do that, we either need to parse the data in some external application or set up Solr to use Apache Tika.
This task will guide you through the process of setting up Apache Tika with Solr.
In order to set up the extracting request handler, we need to follow these simple steps:
First let's edit our Solr instance solrconfig.xml and add the following configuration:
Next create the extract folder anywhere on your system (I created that folder in the directory where Solr is installed), and place the apache-solr-cell-4.0.0.jar from the dist directory (you can find it in the Solr distribution archive)
After that you have to copy all the libraries from the contrib/extraction/lib/ directory to the extract directory you created before.
In addition to that, we need the following entries added to the solrconfig.xml file:
And that's actually all that you need to do in terms of configuration.
To simplify the example, I decided to choose the following index structure (place it in the fields section in your schema.xml file):
To test the indexing process, I've created a PDF file book.pdf using PDFCreator which contained the following text only: This is a Solr cookbook.
Binary file parsing is implemented using the Apache Tika framework.
Tika is a toolkit for detecting and extracting metadata and structured text from various types of documents, not only binary files but also HTML and XML files.
To add a handler that uses Apache Tika, we need to add a handler based on the solr.extraction.ExtractingRequestHandler class to our solrconfig.xml file as shown in the example.
In addition to the handler definition, we need to specify where Solr should look for the additional libraries we placed in the extract directory that we created.
The dir attribute of the lib tag should be pointing to the path of the created directory.
The regex attribute is the regular expression telling Solr which files to load.
The fmap.content parameter tells Solr what field content of the parsed document should be extracted.
In our case, the parsed content will go to the field named text.
The next parameter lowernames is set to true; this tells Solr to lower all names that come from Tika and have them lowercased.
It tells Solr how to handle fields that are not defined in the schema.xml file.
The name of the field returned from Tika will be added to the value of the parameter and sent to Solr.
For example, if Tika returned a field named creator, and we don't have such a field in our index, then Solr would try to index it under a field named attr _ creator which is a dynamic field.
The last parameter tells Solr to index Tika XHTML elements into separate fields named after those elements.
Next we have a command that sends a PDF file to Solr.
We are sending a file to the /update/ extract handler with two parameters.
It's useful to be able to do that during document sending because most of the binary document won't have an identifier in its contents.
The second parameter we send to Solr is the information to perform the commit right after document processing.
See also To see how to index binary files please refer to the Indexing PDF files and Extracting metadata from binary files recipes in Chapter 2, Indexing Your Data.
Most of the time, the default way of calculating the score of your documents is what you need.
But sometimes you need more from Solr; that's just the standard behavior.
Let's assume that you would like to change the default behavior and use a different score calculation algorithm for the description field of your index.
The current version of Solr allows you to do that and this recipe will show you how to leverage this functionality.
Getting ready Before choosing one of the score calculation algorithms available in Solr, it's good to read a bit about them.
The description of all the algorithms is beyond the scope of the recipe and the book, but I would suggest going to the Solr Wiki pages (or look at Javadocs) and read the basic information about available implementations.
For the purpose of the recipe let's assume we have the following index structure (just add the following entries to your schema.xml file to the fields section):
The string and text_general types are available in the default schema.xml file provided with the example Solr distribution.
But we want DFRSimilarity to be used to calculate the score for the description field.
In order to do that, we introduce a new type, which is defined as follows (just add the following entries to your schema.xml file to the types section):
Also, to use per-field similarity we have to add the following entry to your schema.xml file:
Now let's have a look and see how that works.
The index structure presented in this recipe is pretty simple as there are only three fields.
The one thing we are interested in is that the description field uses our own custom field type called text_general_dfr.
The thing we are mostly interested in is the new field type definition called text_general_ dfr.
As you can see, apart from the index and query analyzer there is an additional section – similarity.
It is responsible for specifying which similarity implementation to use to calculate the score for a given field.
You are probably used to defining field types, filters, and other things in Solr, so you probably know that the class attribute is responsible for specifying the class implementing the desired similarity implementation which in our case is solr.DFRSimilarityFactory.
Also, if there is a need, you can specify additional parameters that configure the behavior of your chosen similarity class.
In the previous example, we've specified four additional parameters: basicModel, afterEffect, normalization, and c, which all define the DFRSimilarity behavior.
In addition to per-field similarity definition, you can also configure the global similarity:
Changing the global similarity Apart from specifying the similarity class on a per-field basis, you can choose any other similarity than the default one in a global way.
For example, if you would like to use BM25Similarity as the default one, you should add the following entry to your schema.xml file:
As well as with the per-field similarity, you need to provide the name of the factory class that is responsible for creating the appropriate similarity class.
Introduction Indexing data is one of the most crucial things in every Lucene and Solr deployment.
When your data is not indexed properly your search results will be poor.
When the search results are poor, it's almost certain the users will not be satisfied with the application that uses Solr.
That's why we need our data to be prepared and indexed as well as possible.
On the other hand, preparing data is not an easy task.
We need to index multiple formats of data from multiple sources.
This chapter will concentrate on the indexing process and data preparation beginning from how to index data that is a binary PDF file, teaching how to use the Data Import Handler to fetch data from database and index it with Apache Solr, and finally describing how we can detect the document's language during indexing.
Indexing PDF files Imagine that the library on the corner that we used to go to wants to expand its collection and make it available for the wider public though the World Wide Web.
It asked its book suppliers to provide sample chapters of all the books in PDF format so they can share it with the online users.
Solr can do it with the use of Apache Tika.
This recipe will show you how to handle such a task.
Getting ready Before you start getting deeper into the task, please refer to the How to set up the extracting request handler recipe in Chapter 1, Apache Solr Configuration, which will guide you through the process of configuring Solr to use Apache Tika.
We will use the same index structure and Solr configuration presented in that recipe, and I assume you already have Solr properly configured (according to the mentioned recipe) and ready to work.
To test the indexing process I've created a PDF file book.pdf using PDFCreator (http://sourceforge.net/projects/pdfcreator/) which contained the following text only: This is a Solr cookbook..
To see what was indexed I've run the following within a web browser:
The curl command we used sends a PDF file to Solr.
We are sending a file to the /update/ extract handler along with two parameters.
It's useful to be able to do that during document sending because most of the binary documents won't have an identifier in its contents.
The second parameter we send asks Solr to perform the commit operation right after document processing.
The test file I've created, for the purpose of the recipe, contained a simple sentence: "This is a Solr cookbook"
Remember the contents of the PDF file I created? It contained the word "Solr"
That's why I asked Solr to give me documents which contain the word "Solr" in a field named text.
In response, I got one document which matched the given query.
To simplify the example, I removed the response header part.
Those fields contained information about the file such as the size, the application that created it, and so on.
As we can see, we have our identifier indexed as we wished, and some other fields that were present in the schema.xmlfile that Apache Tika could parse and return to Solr.
Generating unique fields automatically Imagine you have an application that crawls the web and index documents found during that crawl.
The problem is that for some particular reason you can't set the document identifier during indexing, and you would like Solr to generate one for you.
This recipe will help you, if you faced a similar problem.
The following steps will help you to generate unique fields automatically:
First let's create our index structure by adding the following entries to the schema.
In addition to that, we need to define the uuid field type by adding the following entry to the types section of our schema.xml file:
In addition to that, we must remove the unique field definition, because Solr doesn't needs to be removed:
And now, let's try to index a document without an id field, for example one like this:
In order to see if Solr generated an identifier for the document, let's run the following query:
As you can see in the response, our document had one additional field we didn't add manually – the id field, which is what we wanted to have.
The idea is quite simple – we let Solr generate the id field for us.
To do that, we defined the id field to be based on the uuid field type, and to have a default value of new look at the uuid field type, you can see that it is a simple type definition based on solr.
Having your document's identifiers generated automatically is handy in some cases, but it also comes with some restrictions from Solr and its components.
One of the issues is that you can't have the unique field defined, and because of that, the elevation component won't work.
But if your application doesn't know the identifiers of the documents and can't generate them, then using solr.UUIDField is one of the ways of having document identifiers for your indexed documents.
Extracting metadata from binary files Suppose that our current client has a video and music store.
Not the e-commerce one, just the regular one – just around the corner.
And now he wants to expand his business to e-commerce.
But his IT department said that this will be tricky – because they need to hire someone to fill up the database with the product names and their metadata.
And that is the place where you come in and tell them that you can extract titles and authors from the MP3 files that are available as samples.Now let's see how that can be achieved.
Getting ready Before you start getting deeper into the task, please refer to the How to set up the extracting request handler recipe in Chapter 1, Apache Solr Configuration, which will guide you through the process of configuring Solr to use Apache Tika.
To do that, let's run the following command: curl "http://localhost:8983/solr/update/extract?literal.
To do that type a query like the following to your web browser:
First we define an index structure that will suit our needs.
I decided that besides the unique ID, I need to store the title and author name.
We also defined a dynamic field called ignored to handle the data we don't want to index (not indexed and not stored)
The next step is to define a new request handler to handle our updates, as you already know.
We also added a few default parameters to define our handler behavior.
In our case the parameter uprefix tells Solr to index all unknown fields to the dynamic field whose name begins with ignored_, thus the additional data will not be visible in the index.
The last parameter tells Solr to index Tika XHTML elements into separate fields named after those elements.
Next we have a command that sends an MP3 file to Solr.
We are sending a file to the / update/extract handler with two parameters.
First we define a unique identifier and pass that identifier to Solr using the literal.id parameter.
The second parameter we send to Solr is information to perform a commit right after document processing.
The query is a simple one, so I'll skip commenting on this part.
As you can see, there are only fields that are explicitly defined in schema.xml – no dynamic fields.
Solr and Tika managed to extract the name and author of the file.
See also f If you want to index other types of binary files please refer to the Indexing PDF files.
His database of users grew to such size that even the simple SQL select statement is taking too much time, and he seeks how to improve the search time.
Of course he heard about Solr but he doesn't want to generate XML or any other data format and push it to Solr; he would like the data to be fetched.
This task will show you how to configure the basic setup of Data Import Handler and how to use it.
In addition to that, we need the following entry to be added to the solrconfig.xml file:
You should add an entry like the following code: dataimport.DataImportHandler">
If you want to use other database engines, please change the driver, url, and user and password attributes.
To do that we need to modify the fields section of the schema.xml file to something like the following snippet:
One more thing before the indexing – you should copy an appropriate JDBC driver to the lib directory of your Solr installation or the dih directory we created before.
You can get the library for PostgreSQL here http://jdbc.postgresql.org/ download.html.
As you may know, the HTTP protocol is asynchronous, and thus you won't be updated on how the process of indexing is going.
To check the status of the indexing process, you can run the command once again.
First we have a solrconfig.xml part which actually defines a new request handler, Data of the Data Import Handler configuration file.
The second listing is the actual configuration of Data Import Handler.
I used the JDBC source connection sample to illustrate how to configure Data Import Handler.
The contents of this configuration file start with the root tag named dataConfig which is followed by a second tag defining a data source and named dataSource.
In the example, I used the PostgreSQL database and thus the JDBC driver is org.postgresql.Driver.
We also define the database connection URL (attribute named url), and the database credentials (attributes user and password)
Next we have a document definition – a tag named document.
This is the section containing information about the document that will be sent to Solr.
The document definition is made of database queries – the entities.
The entity is defined by a name (the name attribute) and a SQL query (the query attribute)
As you may already have noticed, entities can be nested to handle sub-queries.
The SQL query is there to fetch the data from the database and use it to fill the entity variables which will be indexed.
There is a single field tag for every column returned by a query, but that is not a must – Data Import Handler can guess what the mapping is (for example, where the entity field name matches the column name), but I tend to use mappings because I find it easier to maintain.
The field tag is defined by two attributes: column which is the column name returned by a query, and name which is the field to which the data will be written.
Next we have a Solr query to start the indexing process.
Remember that the default behavior is to delete the index contents at the beginning.
If you don't want to delete the index contents at the start of the full indexing using Data look like this:
Let's assume that we want to index the Wikipedia data, and we don't want to parse the whole Wikipedia data and make another XML file.
Instead we asked our DB expert to import the data dump information from the PostgreSQL database, so we could fetch that data.
This task will guide you through how to do it.
Getting ready Please refer to the How to properly configure Data Import Handler recipe in this chapter to get to know the basics about how Data Import Handler is configured.
I'll assume that you already have Solr set up according to the instructions available in the mentioned recipe.
The Wikipedia data I used in this example is available under the Wikipedia downloads page at http://download.wikimedia.org/
To do that we need to modify the fields section of the schema.xml file so it looks like the following code:
The next step is to add the request handler definition to the solrconfig.xml file, like so: dataimport.DataImportHandler">
Now we have to add a db-data-config.xml file to the conf directory of your Solr instance (or core): page">
Solr will show us a response like the following reponse:
Running the same query after the importing process is done should result in a response like the following: documents.
I decided to store four fields: page identifier, page name, page revision number, and its contents.
The field definition part is fairly simple so I decided to skip commenting on this.
The request handler definition, the Data Import Handler configuration, and command queries were discussed in the How to properly configure Data Import Handler with JDBC recipe in this chapter.
The portions of interest in this task are in the db-data-config.xml file.
The first entity gathers data from the page table and maps two of the columns to the index fields.
The next entity is nested inside the first one and gathers the revision identifier from the table revision with the appropriate condition.
The revision identifier is then mapped to the index field.
The last entity is nested inside the second and gathers data from the pagecontent table again with the appropriate condition.
And again, the returned column is mapped to the index field.
We have the response which shows us that the import is still running (the listing with <str see there is information about how many data rows were fetched, how many requests to the database were made, how many Solr documents were processed, and how many were deleted.
There is also information about the start of the indexing process.
One thing you should be aware of: this response can change in the next versions of Solr and Data Import Handler.
The last listing shows us the summary of the indexing process.
How to import data using Data Import Handler and delta query.
His database is now not used for searching but only updating.
What we can do is an incremental import which will modify only the data that has changed since the last.
Getting ready Please refer to the How to properly configure Data Import Handler recipe in this chapter to get to know the basics of the Data Import Handler configuration.
I assume that Solr is set up according to the description given in the mentioned recipe.
After that we run a new kind of query to start delta import:
First we modified our database table to include a column named last_modified.
We need to ensure that the column will be modified at the same time as the table is.
Solr will not modify the database, so you have to ensure that your application will do that.
When running a delta import, Data Import Handler will create a file named dataimport.
In that file, the last index time will be stored as a timestamp.
This timestamp will be later used to distinguish whether the data was changed or not.
It can be used in a query by using a special variable: ${dataimporter.
The first one is responsible for getting the information about which users were modified since the last index.
It uses the last_modified field to determine which users were modified since the last import.
This query gets users with the appropriate unique identifier, to get all the data which we want to index.
One thing worth noticing is the way that I used the user identifier in deltaImportQuery.
You may have noticed that I left the query attribute in the entity definition.
It's left on purpose; you may need to index the entire data once again, so that configuration will be useful for full imports as well as for the partial ones.
Next we have a query that shows how to run the delta import.
You may have noticed that compared to the full import, we didn't use the full-import command – we've sent the delta-import command.
The statuses that are returned by Solr are the same as with the full import, so please refer to the appropriate chapters to see what information they carry.
One more thing – the delta queries are only supported for the default SqlEntityProcessor class.
This means that you can only use those queries with JDBC data sources.
Do you remember the first example with the Wikipedia data? We asked our fellow DB expert to import the data dump into PostgreSQL and we fetched the data from there.
Getting ready Please refer to the How to properly configure Data Import Handler recipe in this chapter to get to know the basics of the Data Import Handler configuration.
I assume that Solr is set up according to the description given in the mentioned recipe.
To be consistent, I chose to index the Wikipedia data, which you should already be familiar with.
Our field definition part of schema.xml should look like the following code:
The next step is to define a Data Import Handler request handler (put that definition in the solrconfig.xml file): dataimport.DataImportHandler">
Now let's start indexing by sending the following query to Solr:
After the import is done, we will have the data indexed.
The Wikipedia data I used in this example is available under the Wikipedia downloads page at http://download.wikimedia.org/enwiki/
We only want to index some of the data from the file: page identifier, name, revision, and page contents.
I also wanted to skip articles that are only linking to other articles in Wikipedia.
The field definition part of the schema.xml file is fairly simple and contains only four fields and there is nothing unusual within it, so I'll skip commenting on it.
The solrconfig.xml file contains the handler definition with the information about the Data Import Handler configuration filename.
Next we have the data-config.xml file where the actual configuration is written.
We have a new data source type here named FileDataSource.
This data source will read the data from a local directory.
You can use HttpDataSource if you want to read data from an outer location.
The XML tag defining the data source also specifies the file encoding (the encoding attribute) and in our example it's UTF-8
Next we have an entity definition, which has a name under which it will be visible, a processor which will process our data.
The processor attribute is only mandatory when not using a database source.
This value must be set to XPathEntityProcessor in our case.
The stream attribute, which is set to true, informs Data Import Handler to stream the data from the file which is a must in our case when the data is large.
Following that we have a forEach attribute which specifies an XPath expression – this path will be iterated over.
There is a location of the data file defined in the url attribute and a transformer defined in the transformer attribute.
A transformer is a mechanism that will transform every row of data and process it before sending it to Solr.
We have columns which are the same as the index field names thus I skipped the name field.
There is one additional attribute named xpath in the mapping definitions.
It specifies the XPath expression that defines where the data is located in the XML file.
If you are not familiar with XPath please refer to the http://www.w3schools.com/xpath/default.asp tutorial.
It tells Solr which documents to skip (if the value of the column is true then Solr will skip the document)
The column is defined by a regular expression (attribute regex), a column to which the regular expression applies (attribute sourceColName), and the value that will replace all the occurrences of the given regular expression (replaceWith attribute)
The actual indexing time was more than four hours on my machine, so if you try to index the sample Wikipedia data please take that into consideration.
How to modify data while importing with Data Import Handler.
After we indexed the users and made the indexing incremental (the How to properly configure Data Import Handler and How to import data using Data Import Handler and delta query recipes), we were asked if we could modify the data a bit.
Actually it would be perfect if we could split name and surname into two fields in the index while those two reside in a single column in the database.
And of course, updating the database is not an option (trust me – it almost never is)
Can we do that? Of course we can, we just need to add some more configuration details in Data Import Handler and use a transformer.
Getting ready Please refer to the How to properly configure Data Import Handler recipe in this chapter to get to know the basics about the Data Import Handler configuration.
Also, to be able to run examples in this chapter, you need to run Solr in the servlet container run on Java 6 or later.
I assume that Solr is set up according to the description given in the mentioned recipe.
To select users from our table we use the following SQL query:
Our task is to split the name from the surname and place it in two fields: name and surname.
First of all change the index structure, so our field definition part of schema.xml should look like the following code:
How to properly configure Data Import Handler recipe in this chapter.
The first two listings are the sample SQL query and the result given by a database.
Next we have a field definition part of the schema.xml file which defines four fields.
See the difference? We have four fields in our index while our database rows have only three columns.
We must split the contents of the user_name column into two index fields: firstname and surname.
To do that, we will use JavaScript language and the script transformer functionality of Data Import Handler.
The solrconfig.xml file is the same as the one discussed in the How to properly configure Data Import Handler recipe in this chapter, so I'll skip that as well.
Next we have the updated contents of the db-data-config.xml file which we use to define the behavior of Data Import Handler.
The first and the biggest difference is the script tag that will be holding our scripts that parse the data.
I defined a simple function called splitName that takes one parameter, database row (remember that the functions that operate on entity data should always take one parameter)
The first thing in the function is getting the contents of the user_name column, split it with the space character, and assign it into a JavaScript table.
Then we create two additional columns in the processed row – firstname and surname.
The contents of those rows come from the JavaScript table we created.
Then we remove the user_name column because we don't want it to be indexed.
The last operation is the returning of the processed row.
To enable script processing you must add one additional attribute to the entity definition – the transformer attribute with the contents such as script:functionName.
It tells Data Import Handler to use the defined function name for every row returned by the query.
The rest is the usual indexing process described in the How to properly configure Data Import Handler task in this chapter.
If you want to use a different language other than JavaScript, then you have to specify it in the language attribute of the <script> tag.
Just remember that the scripting language that you want to use must be supported by Java 6
Updating a single field of your document Imagine that you have a system where you store a document your users upload.
In addition to that, your users can add other users to have access to the files they uploaded.
As you probably know, before Solr 4.0, when you wanted to update a single field in a document you had to re-index the whole document.
Solr 4.0 allows you to update a single field if you fulfill some basic requirements.
So let's see how we can do that in Solr 4.0
For the purpose of the recipe, let's assume we have the following index structure (put the following entries to your schema.xml file's fields section):
And that's all when it comes to the schema.xml file.
In addition to that, let's assume we have the following data indexed:
So, we have a sample file and two user names specifying which users of our system can access that file.
But what if we would like to add another user called jack.
To add the value to a field which has multiple values, we should send the following command:
Let's see if it worked by sending the following query:
Imagine that now one of the users changed the name of the document, and we would also like to update the file field of that document to match that change.
In order to do so, we should send the following command:
And again, we send the same query as before to see if the command succeeded:
As you can see the index structure is pretty simple; we have a document identifier, its name, and users that can access that file.
As you can see all the fields in the index are marked as is because, under the hood, Solr takes all the values from the fields and updates the one we mentioned to be updated.
So it is just a typical document indexing, but instead of you having to provide all the information, it's Solr's responsibility to get it from the index.
Another thing that is required for the partial update functionality to work is the _version_ field.
You don't have to set it during indexing, it is used internally by Solr.
The example data we are indexing is also very simple.
As you can see, that command parameter tells Solr to perform the commit operation right after update.
The -H 'Contenttype:application/json' part is responsible for setting the correct HTTP headers for the update request.
We want to change the user field and we want to add the jack value to that field (the add command)
So as you can see, the add command is used when we want to add a new value to a field which can hold multiple values.
The second command shown as an example shows how to change the value of a single-valued field.
It is very similar to what we had before, but instead of using the add command, we use the set command.
Handling multiple currencies Imagine a situation where you run an e-commerce site and you sell your products all over the world.
One day you say that you would like to calculate the currencies by yourself and have all the goodies that Solr gives you on all the currencies you support.
You could of course add multiple fields, one for each currency.
On the other hand, you can use the new functionality introduced in Solr 3.6 and create a field that will use the provided currency exchange rates.
This recipe will show you how to configure and use multiple currencies using a single field in the index:
Let's start with creating a sample index structure, by modifying the fields section in your schema.xml file so it looks like the following code:
In addition to that, we need to provide the definition for the type the price field is based on (add the following entry to the types section in your schema.xml file):
For this recipe, I decided to index the following documents:
Let's check that by sending the following query to Solr: UR TO 9.00,EUR]
As you can see, we got the document we wanted.
The idea behind the functionality is simple – we create a field based on a certain type and we provide a file with a currency exchange rate, and that's all.
After that we can query our Solr instance with the use of all the currencies we defined exchange rates for.
But now, let's discuss all the previous configuration changes in detail.
The index structure is very simple; it contains three fields of which one is responsible for holding the price of the document and is based on the currencyField type.
Its defaultCurrency attribute specifies the default currency for all the fields using this type.
This is important, because Solr will return prices in the defined default currency, no matter what currency is used during the query.
The currencyConfig attribute specifies the name of the file with the exchange rate definition.
The file should be structured similar to the example one previously shown.
This means that each exchange rate should have the from attribute telling Solr from which currency the exchange will be done, the to attribute specifying to which currency the exchange will be done, and the rate attribute specifying the actual exchange rate.
In addition to that, it can also have the comment attribute if we want to include some short comment.
During indexing, we need to specify the currency we want the data to be indexed with.
This is done by specifying the price, a colon character, and the currency code after it.
So 10.10,USD will mean ten dollars and ten cents in USD.
As you can see, you can query Solr with different currencies from the one used during indexing.
This is possible because of the provided exchange rates file.
As you can see, when we use a range query for a price field, we specify the value, the colon character, and the currency code after it.
Please remember that if you provide a currency code unknown to Solr, it will throw an exception saying that the currency is not known.
You can also have the exchange rates being updated automatically by specifying the currency provider.
Setting up your own currency provider Specifying the currency exchange rate file is great, but we need to update that file because the exchange rates change constantly.
Luckily for us, Solr committers thought about it and gave us the option to provide an exchange rate provider instead of a plain file.
The provider is a class responsible for providing the exchange rate data.
The default exchange rate provider available in Solr uses exchange rates from http://openexchangerates.org, which are updated hourly.
In order to use it, we need to modify our currencyField field type definition and introduce three new properties (and remove the currencyConfig one):
So the final configuration should look like the following snippet:
You can download the sample exchange file from the http://openexchangerates.org site after creating an account there.
Detecting the document's language Imagine a situation where you have users from different countries and you would like to give them a choice to only see content you index that is written in their native language.
Sounds quite interesting, right? Let us see how we can identify the language of the documents during indexing and store that information along with the documents in the index for later use.
For the language identification we will use one of the Solr contrib modules, but let's start from the beginning.
For the purpose of the recipe, I assume that we will be using the following index structure (add the following to the fields section of your schema.xml file):
We will use the langId field to store the information about the identified language.
The next thing we need to do is create a langid directory somewhere on your filesystem (I'll assume that the directory is created in the same directory that Solr is installed) and copy the following libraries to that directory:
Next we need to add some information to the solrconfig.xml file.
First we need to inform Solr that we want it to load the additional libraries.
We do that by adding the following entry to the config section of that file:
In addition to that we configure a new update processor by adding the following to the config section of the solrconfig.xml file: LangDetectLanguageIdentifierUpdateProcessorFactory">
I decided to use the following test data (stored in a data.xml file): the chemical formula H2O.
A water molecule contains one oxygen and two hydrogen atoms connected by    covalent bonds.
Wasser ist die einzige chemische Verbindung auf der Erde, die in der Natur in allen drei Aggregatzuständen vorkommt.
Die Bezeichnung Wasser wird dabei besonders für den flüssigen Aggregatzustand verwendet.
Im festen (gefrorenen) Zustand spricht man von Eis, im gasförmigen Zustand von Wasserdampf.</field>
To index the above test file I used the following commands: --data-binary @data.xml -H 'Content-type:application/xml'
After sending the previous two commands, we can finally test if that worked.
A water molecule contains one oxygen and two hydrogen atoms connected by covalent bonds.
Water is a liquid at ambient conditions, but it often co-exists on Earth with its solid state, ice, and gaseous state (water vapor or steam)
Wasser ist die einzige chemische Verbindung auf der Erde, die in der Natur in allen drei Aggregatzuständen vorkommt.
Die Bezeichnung Wasser wird dabei besonders für den flüssigen Aggregatzustand verwendet.
Im festen (gefrorenen) Zustand spricht man von Eis, im gasförmigen Zustand von Wasserdampf.</str>
As you can see, the langId field was filled with the correct language.
The index structure we used is quite simple; it contains four fields and we are most interested in the langId field which won't be supplied with the data, but instead of that we want Solr to fill it.
The mentioned libraries are needed in order for the language identification to work.
The lib entry in the solrconfig.xml file tells Solr to look for all the JAR files from the ../../ langid directory.
The langid.fl property tells the defined processor which fields should be used to detect the language.
The last property, langid.fallback, tells the language detection library what language should be set if it fails to detect a language.
The solr.LogUpdateProcessorFactory and solr.RunUpdateProcessorFactory processors are there to log the updates and actually run them.
As for data indexing, in order to use the defined update request processor chain, we need to tell Solr that we want it to be used.
In order to do that, when sending data to Solr we specify the additional parameter called update.chain with the name of the update chain we want to use, which in our case is langid.
The --data-binary switch tells the curl command to send that data in a binary format and the -H switch tells curl which content type should be used.
In the end we send the commit command to write the data to the Lucene index.
If you don't want to use the previously mentioned processor to detect the document language, you can use the one that uses the Apache Tika library:
Language identification based on Apache Tika If LangDetectLanguageIdentifierUpdateProcessorFactory is not good enough for you, you can try using language identification based on the Apache Tika library.
In order to do that you need to provide all the libraries from the contrib/ extraction directory in the Apache Solr distribution package instead of the ones from contrib/langid/lib, and instead of using the org.apache.solr.update.
So the final configuration should look like the following code:
However, remember to still specify the update.chain parameter during indexing or add the defined processor to your update handler configuration.
Optimizing your primary key field indexing Most of the data stored in Solr has some kind of primary key.
Primary keys are different from most of the fields in your data as each document has a unique value stored; because they are primary in most cases they are unique.
Because of that, a search on this primary field is not always as fast as you would expect when you compare it to databases.
Let's assume we have the following field defined as a unique key for our Solr collection.
So, in your schema.xml file, you would have the following:
The following steps will help you optimize the indexing of your primary key field:
Now, we would like to use the Lucene flexible indexing and use PulsingCodec to handle our id field.
In order to do that we introduce the following field type (just place it in the types section of your schema.xml file):
In addition to that, we need to change the id field definition to use the new type.
So, we should change the type attribute from string to string_pulsing:
In addition to that we need to put the following entry in the solrconfig.xml file:
It allows us to modify the way data is written into an inverted index and thus configure it to our own needs.
In the previous example, we used PulsingCodec idea behind that codec is that the data for low frequency terms is written in a special way to save a single I/O seek operation when retrieving a document or documents for those terms from the index.
That's why in some cases, when you do a noticeable amount of search to your unique field (or any high cardinality field indexed with PulsingCodec), you can see a drastic performance increase for that fields.
The last change, the one we made to the solrconfig.xml file, is required; without it Solr wouldn't let us use specified codes and would throw an exception during startup.
It just specifies which codec factory should be used to create codec instances.
Please keep in mind that the previously mentioned method is very case dependent and you may not see a great performance increase with the change.
Introduction The process of data indexing can be divided into parts.
One of the parts, actually one of the last parts of that process, is data analysis.
It defines how your data will be written into an index.
A type's behavior can be defined in the context of the indexing process or the context of the query process, or both.
Furthermore, a type definition is composed of a tokenizer (or multiple ones–one for querying and one for indexing) and filters (both token filters and character filters)
A tokenizer specifies how your data will be pre-processed after it is sent to the appropriate field.
Analyzer operates on the whole data that is sent to the field.
The result of the tokenizer's work is a stream of objects called tokens.
They can do anything with the tokens – change them, remove them, or make them lowercase, for example.
They do not operate on tokens from the token stream.
They operate on the data that is sent to the field, and they are invoked before the data is sent to the analyzer.
This chapter will focus on data analysis and how to handle common day-to-day analysis questions and problems.
Imagine you have a powerful preprocessing tool that can extract information about all the words in the text.
Your boss would like you to use it with Solr or at least store the information it returns in Solr.
So what can you do? We can use something called payload to store that data.
I assume that we already have an application that takes care of recognizing the part of speech in our text data.
What we need to add is the data to the Solr index.
To do that we will use a payload – a metadata that can be stored with each occurrence of a term.
First of all, you need to modify the index structure.
To do this, we will add the new field type to the schema.xml file (the following entries should be added to the types section):
Now we'll add the field definition part to the schema.xml file (the following entries should be added to the fields section):
Now let's look at what the example data looks like (I named it ch3_payload.xml):
To check if the payloads were written to the index, we will use the analysis capabilities or the Solr administration panel.
We will test the test|7 term with the associated payload.
What information can the payload hold? It may hold information that is compatible with the encoder type you define for the solr.DelimitedPayloadTokenFilterFactory filter.
In our case, we don't need to write our own encoder – we will use the supplied one to store integers.
We will use it to store the boost of the term.
We defined a new type in the schema.xml file named tokenizer splits the given text on whitespace characters.
Then we have a new filter which handles our payloads.
The filter defines an encoder which in our case is an integer (attribute the payload.
In our case the separator is the pipe character (|)
The one that we are interested in is the speech field.
It contains pairs which are made of a term, a delimiter, and a boost value.
I also decided that words that cannot be identified by my application, which is used to identify parts of speech, will be given a boost of 1
Pairs are separated with a space character, which in our case will be used to split those pairs – that is the task of the tokenizer which we defined earlier.
To index the documents we use simple post tools provided with the example deployment of Solr.
To use it, invoke the command shown in the example.
The post tools will send the data to the default update handler found under the address http://localhost:8983/solr/ update.
The following parameter is the file that is going to be sent to Solr.
You can also post a list of files, not only a single one.
Eliminating XML and HTML tags from text There are many real-life situations when you have to clean your data.
Let's assume that you want to index web pages that your client sends you.
You don't know anything about the structure of that page; one thing you know is that you must provide a search mechanism that will enable searching through the content of the pages.
Of course you could index the whole page, splitting it by whitespaces, but then you would probably hear the clients complain about the HTML tags being searchable and so on.
So before we enable searching the contents of the page, we need to clean the data.
In this example we need to remove the HTML tags.
This recipe will show you how to do it with Solr.
Let's start with assuming that our data looks like this (the ch3_html.xml file):
The next step is to add the following to the field definition part of the schema.xml file: />
In the example we see one file with two fields, the identifier and some HTML data nested in the CDATA section.
You must remember to surround HTML data in CDATA tags if they are full pages and start from HTML tags like our example.
But if you only have some tags present in the data, you shouldn't worry.
It is based on solr.TextField to enable full text searching.
Following that we have a character filter which handles the HTML and the XML tag stripping.
The character filters are invoked before the data is sent to the tokenizer.
In our case the character filter strips the HTML and XML tags, attributes, and so on and then sends the data to the tokenizer which splits the data by whitespace characters.
The one and only filter defined in our type makes the tokens lowercase to simplify the search.
If you want to check how your data was indexed, remember not to be mistaken when you original one sent to Solr, so you won't be able to see the filters in action.
If you wish to check the actual data structures, take a look at the Luke utility (a utility that lets you see the index structure and field values, and operate on the index)
Instead of using Luke, I decided to use the analysis capabilities of the Solr administration pages and see how the html field behaves when we pass the example value provided in the example data file.
Copying the contents of one field to another Imagine that you have many big XML files that hold information about the books that are stored on library shelves.
There is not much data, just a unique identifier, the name of the book and the author.
One day your boss comes to you and says: "Hey, we want to facet and sort on the basis of book author"
You can change your XML and add two fields, but why do that, when you can use Solr to do that for you? Well, Solr won't modify your data, but can copy the data from one field to another.
Let's assume that our data looks like the following code:
Now let's add the following fields' definition to the fields section of your schema.xml file:
Now we can index our example data file by running the following command from the exampledocs directory (put the data.xml file there): java -jar post.jar data.xml.
As you can see in the example, we only have three fields defined in our sample data XML file.
There are two fields which we are not particularly interested in – id and name.
The field that interests us the most is the author field.
As I have previously mentioned, we want to place the contents of that field into three fields:
Those instructions are defined in the schema.xml file, right after the field definitions; that is, after the </fields> tag.
To define a copy field, we need to specify a source field (attribute source) and a destination field (attribute dest)
After the definitions, like those in the example, Solr will copy the contents of the source fields to the destination fields during the indexing process.
There is one thing that you have to be aware of – the content is copied before the analysis process takes place.
That means that the data is copied as it is stored in the source.
Solr also allows us to do more with copy fields than a simple copying from one field to another.
Copying contents of dynamic fields to one field You can also copy multiple fields' content to one field.
To do that you should define a copy field like so:
The definition, like the one previously mentioned, would copy all of the fields that end with _author to one field named authors.
Remember that if you copy multiple fields to one field, the destination field should be defined as multi-valued.
Limiting the number of characters copied There may be situations where you only need to copy a defined number of characters from one field to another.
To do that we add the maxChars attribute to the copy field definition.
This attribute can be very useful when copying the content of multiple fields to one field.
Changing words to other words Let's assume we have an e-commerce client and we are providing a search system based on Solr.
Our index has hundreds of thousands of documents which mainly consist of books.
And everything works fine! Then one day, someone from the marketing department comes into your office and says that he wants to be able to find books that contain the word "machine" when he types "electronics" into the search box.
The first thing that comes to mind is, "Hey, I'll do it in the source and index that"
But that is not an option this time, because there can be many documents in the database that have those words.
That's when synonyms come into play and this recipe will show you how to use them.
To make the example as simple as possible, I assumed that we only have two fields in our index.
Let's start by defining our index structure by adding the following field definition section to the schema.xml file (just add it to your schema.xml file in the field section):
Now let's add the text_syn type definition to the schema.xml file as shown in the following code snippet:
As you have noticed there is a file mentioned – synonyms.txt.
The synonyms.txt file should be placed in the same directory as other configuration files, which is usually the conf directory.
Finally we can look at the analysis page of the Solr administration panel to see if the synonyms are properly recognized and applied:
The second one should be of interest r to us ight now.
It's based on the new type text_syn which is shown in the second listing.
Its definition is divided; it behaves in one way while indexing and in a different way while querying.
So the first thing we see is the query time analyzer definition.
It consists of the tokenizer that splits the data on the basis of whitespace characters, and then the lowercase filter converts all the tokens to lowercase.
It starts with the same tokenizer, but then the synonyms filter comes into play.
Its definition starts like all the other filters – with a factory definition.
Next we have a synonyms attribute which defines which file contains the synonyms definition.
Following that we have the ignoreCase attribute which tells Solr to ignore the case of the tokens and the contents of the synonyms file.
This means that Solr won't be expanding the synonyms – all equivalent synonyms will be reduced to the first synonym in the line.
If the attribute is set to true, all synonyms will be expanded to all equivalent forms.
The example synonyms.txt file tells Solr that when the word "machine" appears in the field based on the text_syn type it should be replaced by "electronics"
Each synonym rule should be placed in a separate line in the synonyms.txt file.
Also remember that the file should be written in the UTF-8 file encoding.
This is crucial and you should always remember it because Solr will expect the file to be encoded in UTF-8
As you can see in the provided screenshot from the Solr administration pages, the defined synonym was properly applied during the indexing phase.
There is one more thing associated to using synonyms in Solr.
Equivalent synonyms setup Let's get back to our example for a second.
What if the person from the marketing department says that he/she wants not only to be able to find books that have the word "machine" to be found when entering the word "electronics", but also all the books that have the word "electronics", to be found when entering the word "machine"
First, we would set the expand attribute (of the filter) to true.
Then we would change our synonyms.txt file to something like this:
As I said earlier Solr would expand synonyms to equivalent forms.
Splitting text by CamelCase Let's suppose that you run an e-commerce site with an electronic assortment.
The marketing department can be a source of many great ideas.
Imagine that your colleague from this department comes to you and says that they would like your search application to be able to find documents containing the word "PowerShot" by entering the words "power" and "shot" into the search box.
So can we do that? Of course, and this recipe will show you how.
To split text in the description field, we should add the following type definition to the schema.xml file:
Finally, let's run the following query in the web browser:
First of all we have the field definition part of the schema.xml file.
We have two fields defined–one that is responsible for holding information about the identifier (the id field) and the second one that is responsible for the product description (the description field)
We name our type text_split and have it based on a text type, solr.TextField.
We also told Solr that we want our text to be tokenized by whitespaces by adding the whitespace tokenizer (the tokenizer tag)
Actually we need a filter named WordDelimiterFilter which is created by the solr.WordDelimiterFilterFactory class and a filter tag.
We also need to define the appropriate behavior of the filter, so we add two attributes – generateWordParts and splitOnCaseChange.
The values of those two parameters are set to 1 which means that they are turned on.
The first attribute tells Solr to generate word parts, which means that the filter will split the data on non-letter characters.
We also add the second attribute which tells Solr to split the tokens by case change.
What will that configuration do with our sample data? As you can see we have one document sent to Solr.
The data in the description field will be split into two words: text and test.
Please remember that we won't see the analyzed text in the Solr response, we only see the stored fields and the original content of those, not the analyzed one.
Splitting text by whitespace only One of the most common problems that you probably came across is having to split text with whitespaces in order to segregate words from each other, to be able to process it further.
To split the text in the description field, we should add the following type definition:
To test our type, I've indexed the following XML file:
Finally, let's run the following query in the web browser:
In the response to the preceding query, we got the indexed document:
On the other hand, we won't get the indexed document in the response after running the following query:
First of all we have the field definition part of the schema.xml file.
We told Solr that we want our text to be tokenized by whitespaces by adding a whitespace tokenizer (the tokenizer tag)
Because there are no filters defined, the text will only be tokenized by whitespace characters and nothing more.
That's why our sample data in the field description_text will be split into two words, test and text.
On the other hand, the text in the description_string field won't be split.
That's why the first example query will result in one document in the response, while the second example won't find the example document.
Please remember that we won't see the analyzed text in the Solr response, we only see stored fields and we see the original content of those, not the analyzed one.
Nowadays it's nice to have stemming algorithms (algorithms that will reduce words to their stems or root form) in your application, which will allow you to find the words such as cat and cats by typing cat.
But let's imagine you have a search engine that searches through the contents of books in the library.
One of the requirements is changing the plural forms of the words from plural to singular – nothing less, nothing more.
Can Solr do that? Yes, the newest version can and this recipe will show you how to do that.
Now let's define the text_light_stem type which should look like this (add this to your schema.xml file):
Now let's check the analysis tool of the Solr administration pages.
You should see that words such as ways and, keys are changed to their singular forms.
Let's check the for that words using the analysis page of the Solr administration pages:
First of all we need to define the fields in the schema.xml file.
To do that we add the contents from the first example into that file.
It tells Solr that our index will consist of two fields – the id field which will be responsible for holding information about the unique identifier of the document, and the description file which will be responsible for holding the document description.
The description field is actually where the magic is being done.
We defined a new field type for that field and we called it text_light_stem.
The field definition consists of a tokenizer and two filters.
If you want to know how this tokenizer behaves please refer to the Splitting text by whitespace only recipe in this chapter.
This is the light stemming filter that we will use to perform minimal stemming.
The class that enables Solr to use that filter is solr.EnglishMinimalStemFilterFactory.
This filter takes care of the process of light stemming.
You can see that using the analysis tools of the Solr administration panel.
The second filter defined is the lowercase filter – you can see how it works by referring to the How to lowercase the whole string recipe in this chapter.
After adding this to your schema.xml file you should be able to use the light stemming algorithm.
To use the light stemmers for your respective language, add the following filters to your type:
In the case of solr.IndonesianStemFilterFactory, you need to add the.
Lowercasing the whole string Imagine you have a system where you only want to have perfect matches for names of the documents.
No matter what the cause of such a decision is, you would want such a functionality.
However there is one thing you would like to have – you would like your search to be case independent, so it doesn't  matter if the document or query is lower cased or uppercased.
Can we do something with that in Solr? Of course Solr can do that, and this recipe will describe how to do it.
To make our strings lowercase, we should add the following type definition to the schema.xml file:
Then we will run the following query in the web browser:
You should also be able to get the indexed document in response to the following query:
First of all we have the field definition part of the schema.xml file.
First, the field named id which is responsible for holding our unique identifier.
The second one is the name field which is actually our lowercased string field.
The third field will hold the description of our documents and is based on the standard text type defined in the example Solr deployment.
The string_ lowercase type consists of an analyzer which is defined as a tokenizer and one filter.
The solr.KeywordTokenizerFactory filter tells Solr that the data in that field should not be tokenized in any way.
It just should be passed as a single token to the token stream.
Next we have our filter, which changes all the characters to their lowercased equivalents.
It doesn't matter if you type lowercase or uppercase characters, the document will be found anyway.
What matters is that you must type the whole string as it is because we used the keyword tokenizer which, as I already said, is not tokenizing but just passing the whole data through the token stream as a single token.
Storing geographical points in the index Imagine that up till now your application stores information about companies – not much information, just unique identification and the company name.
But now, your client wants to store the location of the companies.
In addition to that, your users would like to sort by distance and filter by distance from a given point.
Is this doable with Solr? Of course it is and this recipe will show you how to do it.
Next we will also add one dynamic field (add this to your schema.xml file in the field definition section):
The next step is to define the location type which should look like the following code:
In addition to that, we will need the tdouble field type, which should look like the following code:
The next step is to create the example data looking like the following code (I named the data file task9.xml):
After indexing we should be able to use the query, such as the following one, to get our data:
First of all we have three fields and one dynamic field defined in our schema.xml file.
The first field is the one responsible for holding the unique identifier.
The third one named location is responsible for holding geographical points and is based on the location type.
It uses the tdouble field which was taken from the schema.xml file distributed with Solr.
It's based on the solr.LatLonType class which is specially designed for spatial search and is defined by a single attribute – subFieldSuffix.
That attribute specifies which fields (in our case it's the dynamic *_coordinate field) will be used internally for holding the actual values of latitude and longitude.
So how does this type of field actually work? When defining a two-dimensional field, like we did, there are actually three fields created in the index.
The first field is named like the field we added in the schema.xml file, so in our case it is location.
This field will be responsible for holding the stored value of the field.
And one more thing – this field will only be created when we set the field attribute store to true.
The next two fields are based on the defined dynamic field.
First we have the field name, the _ character, then the index of the value, and finally the suffix defined by the subFieldSuffix attribute of the type.
We can now look at the way the data is indexed.
You can see that the values in each pair are separated by the comma character, and that's how you can add the data to the index:
We send a query to retrieve all the documents from the index uses the Solr local params syntax to send a distance filter.
It can look strange comparing it to a standard query, but it works.
In addition to that, we've specified the point we will calculate the distance from (the pt parameter) as 54.00,23.00
This is a pair of latitude and longitude values separated by a comma character.
The last parameter is d, which specifies the maximum distance that documents can be, from the given point, to be considered as a match.
We defined as 54.02,23.10 we found it with our query because of the distance we specified.
Stemming your data One of the most common requirements I meet is stemming – the process of reducing the word to their root form (or stems)
Let's imagine the book e-commerce store, where you store the books' names and descriptions.
We want to be able to find words such as shown or showed when you type the word show and vice versa.
This recipe will show you how to add stemming to your data analysis.
Now let's define our text_stem type which should look like the following code:
Now we can index our data – to do that we need to create an example data file, for example, the following code:
After indexing, we can test how our data was analyzed.
That's right, Solr found two documents matching the query which means that our fields and types are working as intended.
Our index consists of three fields; one holding the unique identifier of the document, the second one holding the name of the document, and the third one holding the document description.
The last field is the field that will be stemmed.
The stemmed field is based on a Solr text field and has an analyzer that is used at query and indexing time.
It is tokenized on the basis of the whitespace characters, and then the stemming filter is used.
Please note that in order to properly use stemming algorithms they should be used at query and indexing time.
As you can see, our test data consists of two documents.
One of the documents contains the word showed and the other has the word show in their description fields.
After indexing and running the sample query, Solr would return two documents in the results which means that the stemming did its job.
There are too many languages that have stemming support integrated into Solr to mention them all.
If you are using a language other than English, please refer to the http://wiki.
Many users coming from traditional RDBMS systems are used to wildcard searches.
The most common of them are the ones using * characters which means zero or more characters.
You have probably seen searches like the one as follows:
So how to do that with Solr and not kill our Solr server? This task will show you how to prepare your data and make efficient searches.
Now, let's define our string_wildcard type (add this to the schema.xml file):
The third step is to create the example data which looks like the following code:
The Solr response for the previous query is as follows:
As you can see, the document has been found, so our setup is working as intended.
First of all let's look at our index structure defined in the schema.xml file.
We have two fields – one holding the unique identifier of the document (the id field) and the second one holding the name of the document (the name field) which is actually the field we are interested in.
This type is responsible for enabling trailing wildcards, the ones that will enable the LIKE 'WORD%' SQL queries.
As you can see the field type is divided into two analyzers, one for the data analysis during indexing and the other for query processing.
The querying analyzer is straight; it just tokenizes the data on the basis of whitespace characters.
Now the indexing time analysis (of course we are talking about the name field)
Similar to the query time, during indexing the data is tokenized on the basis of whitespace characters, but there is also an additional filter defined.
The solr.EdgeNGramFilterFactory class is responsible for generating the filter called n-grams.
We also defined that the analysis should be started from the beginning of the text (the side attribute set to front)
It will create tokens by adding the next character from the string to the previous token, up to the maximum length of the n-gram filter that is given in the filter configuration.
So by typing the example query, we can be sure that the example document will be found because of the n-gram filter defined in the configuration of the field.
We also didn't define the n-gram filter in the querying stage of analysis because we didn't want our query to be analyzed in such a way that it could lead to false positive hits.
This functionality, as described, can also be used successfully to provide autocomplete features to your application (if you are not familiar with the autocomplete feature please take a look at http://en.wikipedia.org/wiki/Autocomplete)
Please remember that using n-grams will make your index a bit larger.
Because of that you should avoid having n-grams on all the fields in the index.
You should carefully decide which fields should use n-grams and which should not.
If you would like your field to be able to simulate SQL LIKE '%ABC' queries, you should change the side attribute of the solr.EdgeNGramFilterFactory class to the back value.
It would change the end from which Solr starts to analyze the data.
See also f If you want to propose another solution for that kind of search, please refer to the.
Analyzing the text data is not only about stemming, removing diacritics (if you are not familiar with the word, please take a look at http://en.wikipedia.org/wiki/Diacritic), and choosing the right format for the data.
Let's assume that our client wants to be able to search by words and numbers that construct product identifiers.
The second step is to define our text_split type which should look like the following code (add this to your schema.xml file):
To do that let's create an example data file: adding-documents</field>
After indexing we can test how our data was analyzed.
Solr found our document which means that our field is working as intended.
We have our index defined as three fields in the schema.xml file.
We have a unique identifier (an id field) indexed as a string value.
We have a document name (the name field) indexed as text (type which is provided with the example deployment of Solr), and a document description (a description field) which is based on the text_split field which we defined ourselves.
Our type is defined to make the same text analysis, both on query time and on index time.
The solr.WordDelimiterFilterFactory behavior, in our case, is defined by the following parameters:
For example, token ABC-EFG would be split into ABC and EFG.
The second filter is responsible for changing the words that lowercased the equivalents and is discussed in the recipe How to lowercase the whole string in this chapter.
Therefore, after sending our test data to Solr we can run the example query to see if we defined our filter properly.
In addition, you probably know the result; yes, the result will contain one document – the one that we send to Solr.
In case you would like to preserve the original token that is passed to solr.
See also f If you would like to know more about solr.WordDelimiterFilterFactory,
Using Hunspell as a stemmer Solr supports numerous stemmers for various languages.
You can use various stemmers for English, and there are ones available for French, German, and most of the European languages.
But sometimes they provide stemming results that are not of great quality.
Alternatively, maybe you are wondering if there is a stemmer out there that supports your language, which is not included in Solr.
No matter what the reason, if you are looking for a different stemmer you should look at the Hunspell filter if it suits your needs, and this recipe will show you how to use it in Solr.
Getting ready Before starting, please check the http://wiki.openoffice.org/wiki/Dictionaries page to see if Hunspell supports your language.
Now we should define the text_english type as follows (if you don't have it in your schema.xml file, please add it to the types section of the file):
Let's assume that we are not satisfied with the quality of solr.
PorterStemFilterFactory and we would like to have that improved by using Hunspell.
In order to do that, we need to change the solr.
So the final text_english type configuration would look like the following code:
Solr conf directory (the one where you have all your configuration files stored)
They are the dictionaries for English used in Great Britain.
The configuration of the solr.HunspellStemFilterFactory filter factory is not difficult.
Of course, there are a few attributes of the filter tag that need to be specified:
The last thing we need to do is provide Solr with the dictionary files so that the Hunspell filter can do its work.
Before using a new dictionary, you should always properly conduct A/B testing and see if things did not get worse in your case.
If you would like to use other languages with Hunspell, the only thing you will need to do is provide the new dictionary file and change the name of the dictionaries, so change the dictionary and affix attributes of the solr.
Using your own stemming dictionary Sometimes, stemmers provided with Lucene and Solr don't do what you would like them to do.
That's because most of them are based on an algorithmic approach and even the best algorithms can come to a place where you won't like the results of their work and you would like to make some modifications.
Of course, modifications to the algorithm code can be challenging and we don't usually do that.
The good thing is that Solr supports a method of overriding the stemmer work and this recipe will show you how to use it.
Getting ready Before we continue please remember that the method described in this recipe may not work with custom stemmers that are not provided with Solr.
Let's say that we want some of the words to be stemmed in a way we want.
For example, we want the word dogs to be stemmed as doggie (of course that's only an example)
What we have to do first is write the words dogs and doggie in a file (let's call it override.txt)
Words should be separated from each other by a tab character and each line of the file should contain a single stemming overwrite.
For example, our override.txt file could look like this: dogs doggie.
Now we should put the override.txt file in the same directory as the schema.
Please remember to have that file written in UTF-8 encoding.
If you have characters from the classic ASCII character set, they won't be recognized properly if you don't use UTF-8
Next we need to add the solr.StemmerOverrideFilterFactory filter to our text types.
I assume we only use text_english with the following definition (put the following definition to your types section of the schema.xml file):
In order for our list of protected words to work, we need to put solr.
The final type definition for text_ english would look like the following code:
This is what the analysis page of the Solr administration pages shows:
Now, the fields that are based on the text_english type will not be stemmed.
The work of the solr.StemmerOverrideFilterFactory class is simple – it changes the words we want it to change and then marks them as protected so that the stemmer won't do any further processing of those words.
In order for this functionality to work properly, you should remember to put solr.StemmerOverrideFilterFactory before any stemmers in your analysis chain.
The actual configuration of solr.StemmerOverrideFilterFactory is pretty simple and similar to other filters.
It requires two attributes; the usual class attribute, which informs Solr which filter factory should be used in order to create the filter, and the dictionary attribute, which specifies the name of the file containing the dictionary that we want to use for our custom stemming.
Looking at the analysis page of the Solr administration pages, we can see that our dogs word was protected from being stemmed with the default stemmer and changed to what we wanted, that is, doggie.
Protecting words from being stemmed Sometimes, the stemming filters available in Solr do more than you would like them to do.
For example, they can stem brand names or the second name of a person.
Sometimes, you would like to protect some of the words that have a special meaning in your system or you know that some words would cause trouble to a stemmer or stemmers.
Getting started Before we continue, please remember that the method described in this recipe may not work with custom stemmers that are not provided with Solr.
In order to have the defined words protected we need a list of them.
Let's say that we don't want the words cats and dogs to be stemmed.
To achieve that, we should start by writing the words we want to be protected from stemming into a file.
Let's create the file called dontstem.txt with the following contents: cats dogs.
Now let's put the created file in the same directory as the schema.xml file (usually it's the conf directory)
Please remember to have that file written in UTF-8 encoding.
If you have characters from the classic ASCII character set they won't be recognized properly if you don't use UTF-8
Now, we need to add the solr.KeywordMarkerFilterFactory filter to our text types.
I assume we only use the text_english type with the following definition:
In order for our list of protected words to work, we need to put solr.
So the final type definition for the text_ english type would look like the following code:
This is what the analysis page of the Solr administration pages shows:
Now, the fields that are based on the text_english type won't be stemmed.
With the use of solr.KeywordMarkerFilterFactory, we mark the protected words and that information is used by the stemmers available in Solr and Lucene.
In order for this functionality to work properly, you should remember to put the solr.KeywordMarkerFilterFactory filter before any stemmers in your analysis chain.
The actual configuration of solr.KeywordMarkerFilterFactory is pretty simple and similar to other filters.
It requires two attributes; the usual class attribute, which informs Solr which filter factory should be used in order to create the filter, and the attribute protected which specifies the name of the file containing words that we want to protect from stemming.
Looking at the analysis page of the Solr administration pages, we can see that our dogs word was protected from being stemmed, compared to the birds word which was changed to bird.
Introduction Making a simple query is not a hard task, but making a complex one, with faceting, local params, parameters dereferencing, and phrase queries can be a challenging task.
On the top of all that, you must remember to write your query with performance factors in mind.
That's why something that is simple at first sight can turn into something more challenging such as writing a good, complex query.
This chapter will try to guide you through some of the tasks you may encounter during your everyday work with Solr.
Asking for a particular field value There are many cases where you will want to ask for a particular field value.
For example, when searching for the author of a book in the Internet library or an e-commerce shop.
Of course Solr can do that, and this recipe will show you how to do it.
To ask for a value in the author field, send the following query to Solr:
The documents you'll get from Solr will be the ones with the requested value in the author field.
Remember that the query shown in the example uses the standard query parser, not DisMax.
We defined three fields in the index, but this was only for the purpose of the example.
As you can see in the previous query, to ask for a particular field value, you need to send a q parameter syntax such as FIELD_NAME:VALUE, and that's all there is to it.
Of course you can add the logical operator to the query to make it more complex.
Remember that if you omit the field name from the query your values will be checked again in the default search field that is defined in the schema.xml file.
When asking for a particular field value, there are a few things that are useful to know:
Querying for a particular value using the DisMax query parser Sometimes you may need to ask for a particular field value when using the DisMax query parser.
Unfortunately the DisMax query parser doesn't support full Lucene query syntax and thus you can't send a query like that, but there is a solution to it.
You can use the extended DisMax query parser which is an evolved DisMax query parser.
It has the same list of functionalities as DisMax and it also supports full Lucene query syntax.
The following is the query shown in this task, but by using edismax, it would look like the following:
Querying for multiple values in the same field You may sometimes need to ask for multiple values in a single field.
For example, let's suppose that you want to find the solr and cookbook values in the title field.
To do that you should run the following query (notice the brackets surrounding the values):
Sorting results by a field value Imagine an e-commerce site where you can't choose the sorting order of the results, you can only browse the search results page-by-page and nothing more.
That's terrible, right? That's why with Solr you can specify the sort fields and order in which your search results should be sorted.
Let's assume that you want to sort your data by an additional field, for example, the field that contains the name of the author of the book.
First we add the following to your schema.xml file's field section:
Now, let's create a simple data file which will look like the following code:
As I wrote earlier, we want to sort the result list by author name in ascending order.
Additionally, we want the books that have the same author to be sorted by relevance in the descending order.
To do that we must send the following query to Solr: score+desc.
As you can see, our data is sorted exactly how we wanted it to be.
As you can see I defined three fields in our index.
The most interesting to us is the author field, based on which we will perform the sorting operation.
Notice one thing – the type on which the field is based is the string type.
In order to sort the values of the field from the index, you need to prepare your data well, that is, use the appropriate number types (the ones based on the Trie types), and to sort the text field using the string field type (or text type using the KeywordTokenizer type and a lowercase filter)
The following what you see is the data which is very simple – it only adds three documents to the index.
I've added one additional parameter to the query that was sent to Solr – the sort parameter.
Each field must be followed by the order in which the data should be sorted; asc which tells Solr to sort the data in the ascending order, and desc which tells Solr to sort in the descending order.
Pairs of field and order should be delimited with the comma character as shown in the example.
The result list that Solr returned tells us that we did a perfect job on defining the sort order.
How to search for a phrase, not a single word.
Imagine that you have an application that searches within millions of documents that are generated by a law company.
One of the requirements is to search the titles of the documents as a phrase, but with stemming and lowercasing.
In that case, is it possible to achieve this using Solr? Yes, and this recipe will show you how to do that.
First let's define the following type (add this part to your schema.xml file):
Now let's add the following fields to our schema.xml file:
The third step is to create an example data which looks like the following code:
Now let's try to find the documents that have the phrase 2012 report in them.
As you can see we only got one document which is perfectly good.
As I said in the Introduction section, our requirement was to search for phrases over fields that are stemmed and lowercased.
If you want to know more about stemming please refer to the Stemming your data recipe in Chapter 3, Analyzing Your Text Data.
Lowercasing is described in the Lowercasing the whole string recipe in Chapter 3
We only need two fields because we will only search the title, and return the title and unique identifier of the field; thus the configuration is as shown in the example.
The example data is quite simple so I'll skip commenting on it.
The query is something that we should be more interested in.
The query is made to the standard Solr query parser, thus we can specify the field name and the value we are looking for.
The query differs from the standard word searching query by the use of the " character at the start and end of the query.
It tells Solr to use the phrase query instead of the term query.
Using the phrase query means that Solr will search for the whole phrase not a single word.
That's why only the document with identifier 1 was found, because the third document did not match the phrase.
The debug query only ensured that the phrase query was made instead of the usual term query, and Solr showed us that we created the right query.
When using queries there is one thing that is very useful to know.
Defining the distance between words in a phrase You may sometimes need to find documents that match a phrase, but are separated by some other words.
Let's assume that you would like to find the first and third document in our example.
This means that you want documents that could have an additional word between the word 2010 and report.
To do that, we add a so-called phrase slop to the phrase.
In our case the distance (slop) between words can be the maximum of one word, so we add the ~1 part after the phrase definition:
Boosting phrases over words Imagine you are a search expert at a leading e-commerce shop in your region.
One day disaster strikes and your marketing department says that the search results are not good enough.
They would like you to favor documents that have the exact phrase typed by the user over the documents that have matches for separate words.
Can you do it? Of course you can, and this recipe will show you how to achieve it.
Getting ready Before you start reading this task I suggest you read the How to search for a phrase not a single word recipe in this chapter.
I assume that we will be using the DisMax query parser, not the standard one.
We will also use the same schema.xml file that was used in the How to search for a phrase not a single word recipe in this chapter.
Let's start with our sample data file which looks like the following code:
As I already mentioned, we would like to boost those documents that have phrase matches over others matching the query.
To do that, run the following query to your Solr instance:
To visualize the results better, I decided to include the results returned by Solr for the same query but without adding the pf parameter, and received the following results:
Some of the parameters that are present in the example query may be new to you.
The first parameter is defType that tells Solr which query parser we will be using.
In this example we will be using the DisMax query parser (if you are not familiar with the DisMax query parser please have a look at the following address http://wiki.apache.org/solr/DisMax)
One of the features of this query parser is the ability to tell what field should be used to search for phrases, and we do that by adding the pf parameter.
The pf parameter takes a list of that the phrase found in the title field will be boosted with a value of 100
The q parameter is the standard query parameter that you are familiar with.
This time we passed the words we are searching for and the logical operator AND.
This means that we are looking for documents which contain the words 2012 and report.
You should remember that you can't pass queries such as fieldname:value to the q parameter and use the DisMax query parser.
The fields you are searching against should be specified using the qf parameter.
In our case we told Solr because we want AND to be our logical operator for the query.
The one that matches the exact query is returned first and that is what we intended to achieve.
You can of course boost phrases with standard query parsers, but that's not as elegant as the DisMax query parser method.
To achieve similar results, you should run the following query to your Solr instance:
Imagine a situation when your client tells you that he/she wants to promote some of his/her products by placing them at the top of the search result list.
Additionally, the client would like the product list to be flexible, that is, he/she would like to be able to define the list for some queries and not for others.
Many thoughts come into your mind such as boosting, index time boosting, or maybe some special field to achieve that.
But don't bother, Solr can help you with a component that is known as solr.QueryElevationComponent.
The following recipe will help you to place document over others based on your priorities:
To do that add the following section to your solrconfig.xml file:
Now let's add the proper request handler that will include the elevation component.
You may notice that the query elevation component contained information about a mysterious elevate.xml file.
For now you need to create that file in the configuration directory of your Solr instance and paste the following content:
Our field definition part of the file should contain the following code:
The query without using the elevation component returned the following result:
The first part of the configuration defines a new search component with a name under which the component will be visible to other components and the search handler (the name attribute)
In our case the component name is elevator and it's based on the solr.QueryElevationComponent class (the class attribute)
The following that we have are two additional attributes that define the elevation component's behavior:
The next part of the solrconfig.xml configuration procedure is the search handler definition.
It simply tells Solr to create a new search handler with the name of /promotion (to be the value of the name attribute) and using the solr.SearchHandler class (the class attribute)
In addition to that, the handler definition also tells Solr to include the component named elevator in this search handler.
This means that this search handler will use our defined component.
For your information, you can use more than one search component in a single search handler.
We've also included some standard parameters to the handler, such as df, which specifies the default search field.
What we see next is the actual configuration of the elevator component.
You can see that the behavior of the component when a user passes solr to the q parameter.
Under this tag you can see a list of the documents' unique identifiers that will be placed at the top of the results list for the defined query.
Each document is defined by a doc tag and an id attribute (which have to be defined on the basis of solr.StrField) which holds the unique identifier.
You can have multiple query tags in a single configuration file which means that the elevation component can be used for a variety of queries.
The index contains two fields that are responsible for holding information about the document.
In the example datafile, we can see three documents present.
As the explanation is not crucial, I'll skip discussing it further.
The query you see in the example returns all the documents.
The query is made to our new handler with just a simple one word q parameter (the default search field is set to name in the schema.xml file)
Recall the elevate.xml file and the documents we defined for the query you can see, the documents were positioned exactly as we wanted them so it seems that the component did its job.
There is one more thing I would like to say about the query elevation functionality in Solr.
Excluding documents with QueryElevationComponent The elevate component can not only place documents on top of the results list, but it can also attribute to the document definition in your elevate.xml file.
See also If you would like to know how to mark the documents that were positioned by the solr.
QueryElevationComponent class, please read the Modifying returned documents recipe in this chapter.
Imagine an e-commerce book shop where the users have only one way to find books, that is, by searching.
Most of the users requested that the OR operator should be the default logical operator, so that we can have many results for most of the popular queries.
Once every few days an angry user calls the call center and says that by typing "solr cookbook" the first few pages are not relevant to the query he/she typed in, so in other words this is not what he/ she searched for.
So that's the problem, now what can be done? The answer is to boost documents with query words closer to each other.
For the purpose of this task I will be using the DisMax query parser.
Let's start with the following index structure (just add the following to your schema.xml file to the field definition section):
In addition to that we need to define a new request handler in the solrconfig.xml file, which looks like the following code:
As I wrote earlier, we want to get the documents with the words typed by the user close to each other first in the result list.
Let's assume our user typed in the dreaded solr cookbook query.
To handle the query we use the new /closer request handler we defined earlier and we send the query using the mainQuery parameter, not the q one (I'll describe why this is so later)
For the purpose of the example, I assumed that our book description will consist of three fields and we will be searching with the use of the title field which is based on the standard text field defined in the standard schema.xml file provided with the Solr distribution.
As you can see in the provided data file example there are three books.
The first book has two other words between the words solr and cookbook.
The second book has one word between the given words, and the third book has the words next to each other.
In a perfect situation, we would like to have the third book from the example file as the first one in the result list, the second book from the file in the second place, and the first book from the example data file as the last in the results list.
Now let's take a look at our new request handler.
We defined it to be available under a name /closer (the name attribute of the requestHandler tag)
We also said that it should be based on the solr.StandardRequestHandler class (in the class attribute)
Next, in the defaults list we have a number of parameters defining the query behavior we want to achieve.
This contains the query constructed with the use of local params.
The _query_:"…" part of the q parameter is a way of specifying the new query.
We tell Solr that we want to use DisMax query parser (the !dismax part) and we want to pass the value of the qfQuery parameter as the qf DisMax parser parameter.
We we want the boost query to be used – the one that is defined in the boostQuery parameter.
Next we have boostQuery – another query constructed using local parameters.
As you can see we use the DisMax query parser (the !dismax query part), and we specify the qf and mm DisMax query parser parameters.
The value of the qf parameter will be taken from the boostQueryQf parameter and the value of the mm parameter is set to 100%, so we want the boost query to return only the documents that have all the words specified by the user.
The boostQueryQf parameter is used by the boost query to specify which fields should be used for search, in the query used for boosting.
The first query tells Solr to return all the documents with at least one of the words that the user entered (this is defined by setting the mm parameter to 1)
But we also say that we want to boost phrases; that's why we use a phrase query on the title field.
In addition to that we specified that we want to use our boost query, which will increase the boost of all the documents that have all the words entered by the user occupied by those documents that have all the words entered by the user present in the title field and where all those words are close to each other.
We specify that we want to get a calculated score in the results for each document, we want the id field, and the title field.
We also pass the mainQuery parameter because we have the v attribute of both the q and boostQuery parameters set to the $mainQuery parameter.
This means that Solr will take the mainQuery parameter value and pass it to the v parameter of those queries.
Because we prepared our request handler configuration and pasted it into solrconfig.xml, now at query time we only need to pass a single parameter that passes the words specified by our users.
As you can see the documents are sorted in the way we wanted them to be.
You should take a look at one thing – the score field.
This field shows how relevant the document is to the query we sent to Solr.
Sorting results by a distance from a point Suppose we have a search application that is storing information about the companies.
Every company is described by a name and two floating point numbers that represent the geographical location of the company.
One day your boss comes to your room and says that he/she wants the search results to be sorted by distance from the user's location.
Getting ready Before continuing please read the Storing geographical points in the index recipe from Chapter 3, Analyzing Your Text Data.
We also have the following type defined in the schema.xml file:
I assumed that the user location will be provided from the application that is making a query.
Now let's index our example data file, which looks like the following code:
So our user is standing at the North Pole and is using our search application.
Now let's assume that we want to get the companies sorted in such a way that the ones that are nearer the user are at the top of the results list.
The query to find such companies could look like the following query:
If you would like to calculate the distance by hand, you would see that the results are sorted as they should be.
As you can see in the index structure and in the data, every company is described by the following three fields:
I'll skip commenting on how the actual location of the company is stored.
If you want to read more about it, please refer to the Storing geographical points in the index recipe from Chapter 3, Analyzing Your Text Data.
We wanted to get the companies that match the given query and are sorted in the ascending order from the North Pole.
To do that we run a standard query with a non-standard sort.
The sort parameter consists of a function name, geodist, which calculates the distance between points.
After the function there is the order of the sort which in our case is asc (ascending order)
See also If you would like to learn how to return the calculated distance that we used for sorting please refer to the Returning the value of a function in results recipe in this chapter.
Getting documents with only a partial match Imagine a situation where you have an e-commerce library and you want to make a search algorithm that tries to bring the best search results to your customers.
But you noticed that many of your customers tend to make queries with too many words, which result in an empty results list.
So you decided to make a query that will require the maximum of two of the words that the user entered to be matched.
Getting ready This method can only be used with the DisMax query parser.
As you can see our books are described by two fields.
The third step is to made a query that will satisfy the requirements.
As you can see, even though the query was made up of too many words, the result list contains all the documents from the example file.
Every book is described by two fields: a unique identifier and a title.
The query is the thing that we are interested in.
We have passed about eight words to Solr (the q parameter), we defined that we want to use the DisMax query parser (the defType parameter), and we sent the mysterious mm parameter set to the value of 2
Yes, you are right, the mm parameter, also called minimum should match, tells the DisMax query parser how many of the words passed into the query must be matched with the document, to ascertain that the document is a match.
In our case we told the DisMax query parser that there should be two or more words matched to identify the document as a match.
You should also note one thing – the document that has three words matched is at the top of the list.
The relevance algorithm is still there, which means that the documents with more words that matched the query will be higher in the result list than those that have fewer words that matched the query.
The documentation about the mm parameter can be found at http://wiki.apache.org/solr/DisMaxQParserPlugin.
Affecting scoring with functions There are many situations where you would want to have an influence on how the score of the documents is calculated.
For example, you would perhaps like to boost the documents on the basis of the purchases of it.
Like in an e-commerce boost store, you would like to show relevant results, but you would like to influence them by adding yet another factor to their score.
Is it possible? Yes, and this recipe will show you how to do it.
So we want to boost our documents on the basis of a sold field while retaining the relevance sorting.
Our user typed revised into the search box, so the query would look like the following:
Now let's add the sold factor by adding the following to the query:
And the results for the preceding query are as follows:
As you can see, adding the parameter changed the whole results list.
Each of the books has the same number of words in the title.
That's why when typing the first query all documents got the same score.
As you can see, the first book is the one with the fewest pieces sold and that's not what we want to achieve.
It tells Solr what function to use to affect the scoring computation (in this case the result of the function will be added to the score of the document)
In our case it is the product function that returns the product of the values we provide as its arguments; in our case the one and only argument of the function will be the value of the book's sold field.
The result list of the modified query clearly shows how the scoring was affected by the function.
In the first place of the results list we have the book that was most popular during the last week.
The next book is the one which was less popular than the first book, but more popular than the last book.
The last book in the results is the least popular book.
See also If you would like to know more about the functions available in Solr, please go to the Solr wiki page at the following address: http://wiki.apache.org/solr/FunctionQuery.
Nesting queries Imagine a situation where you need a query nested inside another query.
Let's imagine that you want to run a query using the standard request handler but you need to embed a query that is parsed by the DisMax query parser inside it.
This is possible with Solr 4.0 and this recipe will show you how to do it.
Imagine you are using the standard query parser to support the Lucene query syntax, but you would like to boost phrases using the DisMax query parser.
At first it seems that it is impossible, but let's assume that we want to find books that have the words book and revised in their title field, and we want to boost the book revised phrase by 10
The results of the preceding query should look like the following:
As you can see, the results list was sorted exactly the way we wanted.
It consists of two fields – one holding the unique identifier (the id field) and another one holding the title of the book (the title field)
The first one, book+revised, is just a usual query composed from two terms.
The second part of the query starts with a strange looking expression, that is, _query_
This expression tells Solr that another query should be made that will affect the results list.
Then we will see the expression tells Solr to use the DisMax query parser (the !dismax part) and the parameters that will be passed to the parser (qf and pf)
The v parameter is used to pass the value of the q parameter.
The value passed to the DisMax query parser in our case will be book+revised.
By using the $qq expression, we tell Solr to use the value of the qq parameter.
Of course, we could pass the value to the v parameter, but I wanted to show you how to use the dereferencing mechanism.
The qq parameter is set to book+revised and it is used by Solr as a parameter for the query that was passed to the DisMax query parser.
The results show that we achieved exactly what we wanted.
Modifying returned documents Let's say we are using the elevate component that Solr provides to promote some books when necessary.
But as you may already know, the standard Solr response doesn't include the information about document being elevated or not.
What we would like to achieve is to get that information somehow from Solr.
Actually we would like it to be as simple as running a Solr query and getting the results back.
This recipe will show you how to use document transformers with the elevation component.
We also need to have the elevation component defined along with the search component (place the following entries in your solrconfig.xml file):
The contents of the elevate.xml file located in the conf directory look like the following code:
Our example data that we indexed looks like the following code:
And the response we get from Solr is as follows:
As you can see each document does not only have its name and identifier, but also the information about whether it was elevated or not.
Our index structure consists of two fields, the id field which is our unique key and the name field used for holding the name of the document.
Please remember that in order to use the elevation component you have to have a unique key defined in your schema.xml file, and this field has to be based on the string type.
The /select request handler configuration is quite standard, although we've added the last-components sections that define what component should be used during a query.
We defined that we want to use the component named elevator.
The next thing we did is the elevator search component definition.
QueryElevationComponent class (the class attribute) and we set its name to elevator (the attribute name)
In addition to that, we specified two attributes needed by the query elevation component:
We specified the string type because we want only exact matches to include elevated documents.
The elevate.xml file we use for storing the query elevation component is simple.
The root tag is named elevate and can have multiple query tags inside it.
Each query tag is responsible for elevating documents for a query defined with the text attribute.
Inside the query tag we can have multiple doc tags with an id attribute, which should have a value of the identifier of the document to which we want add to results or modify positions.
In our case, we want the document with an identifier value of 3 to be placed in the first position when users enter the book query.
The query we sent was simple; we asked for documents that have book in the default field (the df parameter) which is name in our case.
Using parent-child relationships When using Solr you are probably used to having a flat structure of documents without any relationships.
However, there are situations where decomposing relationships is a cost we can't take.
Because of that Solr 4.0 comes with a join functionality that allows us to use some basic relationships.
For example, imagine that our index consists of books and workbooks and we would like to use that relationship.
Now let's look at our test data that we are going to index:
Now, let's assume we want to get all the books from Solr that have workbooks for them.
Also we want to narrow the books we got to only those that have the character 2 in their names.
In order to do that, we run the following query:
The Solr response for the preceding query is as follows:
As you can see, the returned document was exactly the one we expected.
Although the example index structure is simple I would like to comment on it.
The id field is responsible for holding the unique identifier of the document, the name field is the document name, and the type field holds the document's types.
The book field is optional and specifies the identifier of the parent document.
Let's pause for a bit now before looking at the query, and look at our example data.
So looking at the query result it works as intended, but how does the query actually work?
We specified that child documents should use the book field (the from parameter) and join it with the id field (the to parameter)
The type:workbook part specifies the query we run, that is, we want only those documents that have the workbook value in the type field.
The fq parameter, which narrows the result set to only those documents that have the value 2 in the name field, is applied after the join is executed, so we only apply it to the parent documents.
Ignoring typos in terms of performance Sometimes there are situations where you would like to have some kind of functionality that would allow you to give your user the search results even though he/she made a typo or even multiple typos.
In Solr, there are multiple ways to undo that: using a spellchecker component to try and correct the user's mistake, using the fuzzy query, or for example, using the ngram approach.
This recipe will concentrate on the third approach and show you how to use ngrams to handle user typos.
For the purpose of the recipe, let's assume that our index is built up of four fields: identifier, name, description, and the description_ngram field which will be processed with the ngram filter.
So let's start with the fields definition of our index which should look like the following code (place this in your schema.xml file in the fields section): />
As we want to use the ngram approach, we will include the following filter in our text_ngram field type definition:
The filter will be responsible for dividing the indexed data and queries into two bi-grams.
To better illustrate what I mean, take a look at the following screenshot, which shows how the filter worked for the word "multiple":
So the whole text_ngram type definition will look like the following code:
We also need to add the copy field definition to our schema.xml file, to automatically copy the value of the description field to the description_ngram field.
For the purpose of the recipe I used the following data sample: recipes helping you with your every day work with Solr :)</field>
After indexing it, I decided to test if my query can handle a single typo in each of the words provided to the query, so I've sent the following query to Solr, where the words I was really interested in were "contains" and "multiple":
The result of the query was as follows: ngram:(kontains multyple)</str>
As you can see the document we were interested in was found.
As you can see from the index structure, we have two fields, namely name and description, which we defined to use the text_ngram field because we want these fields to support the returning of the search results even when the user enters a typo of some sort.
To allow this we use the solr.NGramFilterFactory filter with two attributes defined, namely, the minGramSize which sets the minimum size of the produced ngram, and the maxGramSize which sets the maximum size of the produced ngram.
The third attribute of the filter tag is the class attribute that specifies the filter factory class we want to use.
Let's concentrate on the provided screenshot (refer to step 2 in the How to do it...
As I wrote earlier, we want the ngram filter to produce grams built up of two characters.
From the word multiple it created the following bi-grams (n-grams built from 2 characters):
So, the idea of the algorithm is quite simple – divide the word, so that we take the first character and the character after it, and we make a bi-gram from it.
Then we take the next character and the character after it and create the second bi-gram and so on until we can't make any more bi-grams.
Now if you look at the query there are two words we are looking for and both of them contain a typo.
The kontains word should be contain without a typo and the multyple should be multiple without a typo.
Our query also specifies that the logical query operator we want to use is the OR operator.
We use it because we want to match all documents with even a single match to any bi-gram.
If we turn the kontains and multyple tokens into bi-grams, we would get the following (I'll use the pipe (|) character to separate the words from each other):
If we turn the contains multiple tokens into bi-grams we would get the following:
If you compare those bi-grams you would see that only three of those differ between the proper words and the ones with typos.
Because of that our query finds the document we indexed.
You may wonder why we queried both the description and description_ngram fields.
We did that because we don't know if the client's query is the one with typos or without.
If it is without, we want the documents with better matches to be higher up on the results lists, than the ones that are not perfectly matched.
Of course all of that doesn't come without any downsides.
One of the major downsides of this approach is the growth of the index size because of the number of tokens produced by the ngram filter.
The second downside is the number of results produced with such an approach; there will be many more results than you are used to and that's why we did a query to both the description and description_ngram fields.
We wanted to increase the score value of the perfectly matched documents (you can also boost the description field higher during a query)
You can also try having the same approach work with the edismax query parser and the "minimum should match" (mm) parameter, but this is beyond the scope of this recipe.
Detecting and omitting duplicate documents Imagine your data consists of duplicates because they come from different sources.
For example, you have books that come from different suppliers, but you are only interested in a single book with the same name.
Of course you could use the field collapsing feature during the query, but that affects query performance and we would like to avoid that.
This recipe will show you how to use the Solr deduplication functionality.
For the purpose of the recipe, we assume that we have the following data stored in the data.xml file: it will be a dupe because it's almost the same as the second document we are going to index</field> it will be a dupe because it's almost the same as the second document we are going to index</field>
As you can see, the file contains two documents and they only differ by a single word; the first document contains is the is a book phrase, while the second contains the is the book phrase.
In my opinion the second document is a dupe of the first one.
In order to have those two documents detected and overwritten, we need to create a new update request processor chain called dedupe and configure org.apache.
Now let's index our data by running the following command: curl 'http://localhost:8983/solr/update?update.
If everything went well, we should only see the second document as the first one should be overwritten.
So we should check that by running the following query:
The response to it was the following: will be a dupe because it's almost the same as the second document we are going to index</str>
As you can see we got only a single document, and if you look again at the example data, you would notice that it is the second document we sent, so the first one was overwritten.
Our index structure is simple and consists of three fields – the id field which holds the unique identifier, the name field which is a name of the book, and the type field which holds the type of the book.
The example data you see doesn't contain the id field, which isn't a mistake, it was prepared this way on purpose.
We want our deduping to use the id field to generate a unique identifier for us and use it to overwrite duplicate documents.
Also, you can see that the two sample documents are almost the same, so they should be marked as dupes and we should only see one of them in the index, probably the second one.
Next we define a new update request processor chain in the solrconfig.xml file with the name dedupe (the name property)
The first processor we need to add in order to have the deduping functionality is org.apache.solr.update.processor.
We do so by setting the class attribute of the processor tag to the mentioned class.
By setting the enabled property to true, we turn on the deduping mechanism.
The signatureField field configures the name of the field where the generated signature will be stored, which in our case is the id field.
This is crucial, because Solr will use that information to identify duplicate documents.
The fields field contains information of which fields (a list separated by the comma character) should be used to identify the duplication.
Finally, the signatureClass class is the class implementing the signature calculation.
We've chosen org.apache.solr.update.processor.TextProfileSignature because it works best on longer text and we expect that.
The last two processors, solr.LogUpdateProcessorFactory and solr.RunUpdateProcessorFactory, write information about the update to the log file and run the update.
As you can see in the response for our "match all documents" query, only the second document is present.
This is because when the index was empty the first document was indexed.
Then, the second document came and it was identified as a dupe and thus it overwrote the first one.
Using field aliases Imagine your products have multiple prices, and depending on your client's location you search one of the defined fields.
So you have a field for price in US dollars, in Euros, and so on.
But what you would like to do is return the field you are using for displaying the price of the document as a "price" no matter what field you use.
And the results of the preceding query would be as follows:
As you can see, we have our sample document returned but we've got the price_ usd value returned as well.
As you can see in the result document we got a field called price instead of price_usd field.
The index structure is pretty simple, it only contains the field responsible for holding the identifier, name of the document, and three prices in different currencies.
All the fields are marked as stored because we want to return them (not all at the same time though) at query time.
The sample data is also simple so I decided to skip commenting on that.
We also want to return (the fl parameter) the following fields as a document: id, name, and price_usd.
As you can see there is a different fl parameter that you may be used to.
The first part of the fl parameter is pretty obvious; we want to return the id and name fields.
The second part is new though; we specified the following value: price:price_usd.
This means that we want the price_usd field to be returned as price.
Returning a value of a function in the results Imagine you have a service where your users can search for different companies.
Your users can enter a simple keyword(s) and then return all the companies matching that keyword(s)
But a day comes when you give your users the ability to choose their location, and you would like to show how far they are from each company returned in the results.
Getting ready Before reading further I advise you to read the Using field aliases recipe in the current chapter and the Storing geographical points in the index recipe from Chapter 3, Analyzing Your Text Data.
It should look like the following code (put the following definition in to your schema.xml file in the types section):
Let's also assume that we have the following data indexed:
Now, in order to get all the documents with the word company in the name field we would run the following query:
In order to do that we add the following part to the fl parameter: dist:geodist(loc,50.0,28.0)
As you can see, in addition to all the stored fields, Solr returned the additional field called dist.
The index structure is simple, it contains the identifier (the id field), name of the company (the name field), and the geographical location of the company (the loc field)
Description of how geographical points should be stored were described in Chapter 3, Analyzing Your Text Data, in the Storing geographical points in the index recipe, so please refer to that for the explanation.
The initial query returning all the companies that have the word company in their name field dist:geodist(loc,50.0,28.0) part of the fl parameter.
As you remember from the Using field aliases recipe, we told Solr that we want to have a new field called dist returned and we want it to be a value of the dist function query which takes three parameters: the field in the index (in our case it is loc), the latitude, and the longitude, and returns the distance between the point stored in the loc field, and the point described by the latitude and longitude.
The value is then returned as the dist field of each of the returned documents.
Introduction One of the advantages of Solr is the ability to group results on the basis of some fields' contents.
The Solr classification mechanism, called faceting, provides the functionalities which can help us in several tasks that we need to do in everyday work, from getting the number of documents with the same values in a field (for example, the companies from the same city) using the ability of date and range faceting, to the autocomplete features based on the faceting mechanism.
This chapter will show you how to handle some of the common tasks when using the faceting mechanism.
Getting the number of documents with the same field value.
Imagine a situation where besides the search results, you have to return the number of documents with the same field value.
For example, imagine that you have an application that allows the user to search for companies in Europe, and your client wants the number of companies in the cities where the companies that were found by the query are located.
To do this, you could of course run several queries but Solr provides a mechanism called faceting that can do that for you.
For getting the number of documents with the same field value, follow these steps:
To start, let's assume that we have the following index structure (just add this to your schema.xml file in the field definition section; we will use the city field to do the faceting):
The next step is to index the following example data:
Let's suppose that our hypothetical user searches for the word company.
The query that will get us what we want should look like this:
As you can see, besides the normal results list, we got faceting results with the numbers that we wanted.
The index structure and the data are pretty simple and they make the example easier to understand.
This is the field that we want to use to get the number of companies that have the same value in this field—which basically means that they are in the same city.
To do that, we run a query to Solr and inform the query parser that we want the documents that have the word company in the title field.
Additionally we say that we want to enable tells Solr which field to use to calculate faceting numbers.
You can specify the facet.field parameter multiple times to get faceting numbers for different fields in the same query.
As you can see in the results list, the results of all types of faceting are grouped in the list facet.field parameter has its own list which has the attribute name, the same as the value of the parameter in the query—in our case it is city.
Then finally you can see the results that we are interested in: the pairs of values (name attribute) and how many documents have the value in the specified field.
There are two more things I would like to share about field faceting:
If you want to show only the parameter to the query (you can set this parameter to another value if you are interested in any arbitrary value)
Getting the number of documents with the same value range.
Imagine that you have an application where users can search the index to find a car for rent.
One of the requirements of the application is to show a navigation panel, where the user can choose the price range for the cars that they are interested in.
To do it in an efficient way, we will use range faceting and this recipe will show you how to do it.
For getting the number of documents with the same value range, follow these steps:
Let's begin with the following index structure (just add this to your schema.xml file in the field definition section; we will use the price field to do the faceting):
The example data that we will use is like this:
Now, as you recall, our requirement was to show the navigation panel with price ranges.
To do that, we need to get that data from Solr.
To get the price ranges from Solr, we send the following query:
There are three fields, one responsible for the unique identifier, one responsible for the car name, and the last one responsible for the price of rent.
As we are not interested in the search results, we mechanism, that is, the field based faceting.
Instead we will use range faceting which is optimized to work with ranges.
So, we tell Solr which field will be used for range faceting by adding the parameter facet.range with the price value.
That means that the price field will be used for the range faceting calculation.
Then we specify the lower boundary from which the range faceting calculation will begin.
We do this by adding the facet.range.start parameter; in our example we set it to 0
Next we have the facet.range.end parameter which tells Solr when to stop the calculation of the range faceting.
The last parameter (facet.range.gap) informs Solr about the length of the periods that will be calculated.
Remember that when using the range faceting mechanism you must specify the three parameters:
Getting the number of documents matching the query and subquery.
Imagine a situation where you have an application that has a search feature for cars.
One of the requirements is not only to show search results, but also to show the number of cars with the price period chosen by the user.
There is also another thing—those queries must be fast because of the number of queries that will be run.
For getting the number of documents matching the query and subquery, follow these steps:
Let's start with creating an index with the following structure (just add this to your schema.xml file in the field definition section; we will use the price field to do the faceting):
Now, recall our requirement cars that match the query (let's suppose that our user typed car), and show the counts in the chosen price periods.
For the purpose of the recipe let's assume that the user has chosen two periods of prices:
The query to achieve such a requirement should look like this:
The result list of the query should look like this:
There are three fields, one responsible for the unique identifier, one responsible for the car name, and the last one responsible for the price.
First you can see a standard query where we tell Solr that we want to parameter to the query.
This means that we can pass the query to the faceting mechanism and as a result we will get the number of documents that match the given query.
In our example case, we wanted two periods like this:
This is achieved by adding the facet.query parameter with the appropriate value.
The second query is very similar, just with different values.
The value passed to the facet.query parameter should be a Lucene query written using the default query syntax.
As you can see in the results, the query faceting results are grouped under the <lst You can see that Solr correctly calculated the number of cars in each of the periods, which means that this is a perfect solution for us when we can't use the range faceting mechanism.
Removing filters from faceting results Let's assume for the purpose of this recipe that you have an application that can search for companies within a city and state.
But the requirements say that you should show not only the search results but also the number of companies in each city and the number of companies in each state (in the Solr way we say that you want to exclude the filter query from the faceting results)
Can Solr do that in an efficient way? Sure it can, and this recipe will show you how to do it.
Getting ready Before you start reading this recipe, please take a look at the Getting the number of documents with the same field value recipe in this chapter.
The second step would be to index the following example data:
Let's suppose that our hypothetical user searched for the word company, and told our application that he needs the companies matching the word in the state of New York.
In that case, the query that will fulfill our requirement should look like this:
The index structure is pretty simple—it contains four fields that describe the company.
The search will be performed against the name field, while the filtering and the faceting is done with the use of the state and city fields.
As you can see, we have some typical elements there.
It tells Solr to only show those results that have New York in the a name (stateTag), which we will use further.
Our requirement was to show the number of companies in all states and in all cities.
The only thing that was preventing us from getting those numbers was the filter query we added to the query.
So let's exclude it from the faceting exclude the filter with the passed name.
As you can see in the results list, we got the correct numbers which means that the exclude works as intended.
Imagine a situation where you have a website, where you present some kind of advertisements, for example, house rental advertisements.
One of the requirements is to show a list of cities in which the offer, that matched the query typed by the user, are located.
So the first thing you think is to use the faceting mechanism – and that's a good idea.
But then, your boss tells you that he is not interested in the counts and you have to sort the results in the alphabetical order.
So, is Solr able to do it? Of course it is and this recipe will show you how to do it.
Getting ready Before you start reading this recipe, please take a look at the Getting the number of documents with the same field value recipe in this chapter.
This index structure is responsible for holding information about companies and their location.
Now, let's index the example data matching the presented index structure:
Let's assume that our hypothetical user typed house in the search box.
The query to return the search results with the faceting results sorted alphabetically should be like this:
The results returned by Solr for the query should look like this:
As you can see the faceting results returned by Solr are not sorted by counts but in alphabetical order.
The index structure and the example data are only here to help us make a query so I'll skip discussing them.
The query shown in the recipe differs from the standard faceting query by only one parameter—facet.sort.
For the purpose of the recipe we chose the second option and as you can see in the returned results, we got what we wanted.
There are plenty of web-based applications that help users choose what they want to search for.
One of the features that helps users is the autocomplete (or autosuggest) feature, like the one that most of the most used search engines have.
Let's assume that we have an e-commerce library and we want to help the user to choose a book title—we want to enable autosuggest on the basis of the title.
Getting ready Before you start reading this recipe, please take a look at the Getting the number of documents with the same field value recipe in this chapter.
We also want to add some field copying to do some operations automatically.
To do that we need to add the following line after the fields section in your schema.xml file:
The lowercase field type should look like this (just add this to your schema.xml file to the type definitions):
Now, let's index a sample data file which could look like this:
Let's assume that our hypothetical user typed the letters so in the search box and we want to give him the first 10 suggestions with the highest counts.
We also want to suggest the whole titles, not just single words.
To do that, we should send the following query to Solr:
As a result for the query, Solr returned the following output:
As you can see, we got what we wanted in the faceting results.
You can see that our index structure defined in the schema.xml file is pretty simple.
Every book is described by two fields, id and title.
The additional field will be used to provide the autosuggest feature.
The copy field section is there to automatically copy the contents of the title field to the title_autocomplete field.
The lowercase field type is a type we will use to provide the autocomplete feature; this is the same for lowercase words typed by the users as well as uppercase words.
If we want to show different results for uppercased and lowercased letters then the string type will be sufficient.
As you can see we are searching the whole index (the it will be field-based faceting on the basis of the title_autocomplete field (the facet.
Basically it tells Solr to return only those faceting results that begin with the prefix specified as the value of this parameter, which in our case is the value of so.
The use of this parameter enables us to show the suggestions that the user is interested in, and we can see in the results that we achieved what we wanted.
There is one more thing I would like to say about autosuggestion functionality.
Suggesting words not whole phrases If you want to suggest words instead of a whole phrase you don't have to change much of the previous configuration.
You should remember, though, not to use heavily analyzed text (like stemmed text) to be sure that your word won't be modified too much.
Getting the number of documents that don't have a value in the field.
Let's imagine we have an e-commerce library where we put some of our books on a special promotion, for example, we give them away for free.
But how do you make a query to Solr to retrieve the data that we want? This recipe will show you how.
Getting ready Before you start reading this recipe, please take a look at the Getting the number of documents matching the query and the subquery recipe in this chapter.
As you can see, the first three documents have a value in the price field, while the last one doesn't.
So now, for the purpose of the example, let's assume that our hypothetical user is trying to find books that have solr in their title field.
Besides the search results, we want to show the number of documents that don't have a value in the price field.
To do that, we send the following query to Solr:
The query should result in the following output from Solr:
You can see that our index structure defined in the schema.xml file is pretty simple.
Every book is described by three fields, id, title, and price.
Their names speak for the type of information they will hold.
The query is in most parts something you should be familiar with.
Then we say that we want to have the faceting mechanism enabled by adding the number of documents that don't have a value in the price field.
We do that by adding the with how the facet.query parameter works, so I'll skip that part.
By adding the ! character before the fieldname, we tell Solr to negate the condition and in fact we get the number of documents that don't have any value in the specified field.
Having two different facet limits for two different fields in the same query.
Imagine a situation where you have a database of cars in your application.
Besides the standard search results, you want to show two faceting by field results.
The first of those two faceting results, the number of cars in each category, should be shown without any limits, while the second faceting, the one showing the cars by their manufacturer, should be limited to a maximum of 10 results.
Can we achieve it in one query? Yes, we can, and this recipe will show you how to do it.
Getting ready Before you start reading this recipe please take a look at the Getting the number of documents with the same field value recipe in this chapter.
For example we can use a file that has the following content:
For the purpose of the example, let's assume that our hypothetical user is trying to search the index for the word car.
To do that we should send Solr the following query:
As you can see in the field definition section of the schema.xml file and the example data, every document is described by four fields—id, name, category, and manufacturer.
I think that their names speak for themselves and I don't need to discuss them.
We ask for documents which have the word the parameter limits in a way shown in the example (f.FIELD_NAME.facet.limit) we tell Solr to set the limits for the faceting calculation for the particular field.
In our example want any limits on the number of faceting results for the category field.
Following the pattern you can specify per-field values for faceting properties such as sorting and minimum count.
Using decision tree faceting Imagine that in our store we have products divided into categories.
In addition to that, we store information about the stock of the items.
Now, we want to show our crew how many of the products in the categories are in stock and how many we are missing.
The first thing that comes to mind is using the faceting mechanism and some additional calculation.
But why bother, when Solr 4.0 can do that calculation for us with the use of so called pivot faceting.
Let's start with the following index structure (just add this to your schema.xml file in the field definition section; we will use the category and stock fields to do the faceting): />
Let's assume we are running a query from the administration panel of our shop and we are not interested in the documents at all; we only want to know how many documents are in stock or out of stock for each of the categories.
You will notice that we received what we wanted, now let's see how it works.
As you can see in the field definition section of the schema.xml file and the example data, every document is described by four fields—id, name, category, and stock.
I think that their names speak for themselves and I don't need to discuss them.
We specified that we want the query to match all parameter) and we want to use the decision tree faceting, also known as pivot faceting.
We do that by specifying which fields should be included in the tree faceting.
In our case we want the top level of the pivot facet to be calculated on the basis of the category field, and the second level (the one nested in the category field calculation) should be based on the values available in the stock field.
Of course, if you would like to have another value of another field nested under the stock field you can do that by adding another field to the facet.pivot query parameter.
Assuming you would like to see faceting on the price field nested under the stock field, your facet.pivot parameter would look like this: facet.
As you can see in the response, each nested faceting calculation result is written inside level of your facet pivot tree is based on the category field.
But in the end, the calculation is correct and that's what we wanted!
If you have ever used the field collapsing functionality of Solr you may be wondering if there is a possibility of using that functionality and faceting.
Of course there is, but the default behavior still works so that you get the faceting calculation on the basis of documents not document groups.
In this recipe, we will learn how to query Solr so that it returns facets calculated for the most relevant document in each group in order for your user facet counts to be more or less grouped.
Getting ready Before reading this recipe please look at the Using field to group results, Using query to group results, and Using function query to group results recipes in Chapter 8, Using Additional Solr Functionalities.
Also, if you are not familiar with faceting functionality, please read the first three recipes in this chapter.
We will use some example data which looks like this:
So, let's assume we want our results to be grouped on the values of the category field, and we want the faceting to be calculated on the stock field.
And remember that we are only interested in the most relevant document from each result group when it comes to faceting.
So, the query that would tell Solr to do what we want should look like this:
As you can see in the field definition section of the schema.xml file and the example data, every document is described by four fields—id, name, category, and stock.
I think that their names speak for themselves and I don't need to discuss them.
Next, we say that we want to use faceting and we want it to be calculated on the stock field.
We want a grouping mechanism to be active and we want to group documents on the basis of the category field (all the query parameters responsible for defining the faceting and grouping behavior are described in the appropriate recipes in this book, so please look at those if you are not familiar with those parameters)
And finally something new—the group.truncate parameter is set to true.
If set to true, like in our case, facet counts will be calculated using only the most relevant document in each of the calculated groups.
So in our case, for the group with the category field equal to books, we have the true value in the stock field and for the second group we have false in the stock field.
Of course we are looking at the most relevant documents, so the first ones in our case.
So, as you can easily see, we've got two facet counts for the stock field, both with a count of 1, which is what we would expect.
There is one thing more—at the time of writing this book, the group.truncate parameter was not supported when using distributed search, so please be aware of that.
Introduction Performance of the application is one of the most important factors.
Even if our application is perfect in terms of usability, the users won't be able to use it if they will have to wait for minutes for the search results.
The standard Solr deployment is fast enough, but sooner or later a time will come when you will have to optimize your deployment.
This chapter and its recipes will try to help you with the optimization of Solr deployment.
If your business depends on Solr, you should keep monitoring it even after optimization.
There are numerous solutions available in the market, from the generic and open-sourced ones such as Gangila (http://ganglia.sourceforge.net/) to search-specific ones such as Scalable Performance Monitoring (http://www.sematext.com/spm/index.
Paging your results quickly Imagine a situation where you have a user constantly paging through the search results.
For example, one of the clients I was working for was struggling with the performance of his website.
His users tend to search for a word and then page through the result pages – the statistical information gathered from the application logs showed that typical users changed the page about four to seven times.
Apart from improving the query relevance (which isn't what we will talk about in this recipe), we decided to optimize the paging.
How do we do that? This recipe will show you.
As I mentioned, typical users typed a word into the search box and then used the paging mechanism to go through a maximum of seven pages.
My client's application was showing 20 documents on a single page.
First of all, we modified the queryResultWindowSize property in the solrconfig.xml file and changed it to the following value:
We then changed the maximum number of documents that can be cached for a single query to 160, by adding the following property to the solrconfig.xml file:
We also modified queryResultCache, but that's a discussion for another recipe.
To learn how to change that cache, please refer to the How to configure the query result cache recipe in this chapter.
Therefore, after doing the initial query, we gather more documents than we actually need.
Because of this we are sure that when a user clicks on the next page button, which is present in our application, the results will be taken from the cache.
So there won't be a need for intensive I/O operations.
You must remember that the 160 documents IDs will be stored in the cache and won't be visible in the results list, as the result size is controlled by the rows parameter.
The queryResultMaxDocsCached property tells Solr about the maximum number of document IDs that can be cached for a single query (please remember than in this case, the cache stores the document identifiers and not whole documents)
We told Solr that we want a maximum of 160 document IDs for a single query, because the statistics showed us that we don't need more, at least for a typical user.
Of course, there is another thing that should be done – setting the query result cache size, but that is discussed in another recipe.
Configuring the document cache Cache can play a major role in your deployment's performance.
One of the caches that you can configure when setting up Solr is the document cache.
It is responsible for storing the Lucene internal documents that have been fetched from the disk.
The proper configuration of this cache can save precious I/O calls and therefore boost the whole deployment performance.
This recipe will show you how to properly configure the document cache.
With the preceding parameters, our document cache should look similar to the following code snippet (add this code to the solrconfig.xml configuration file):
Notice that we didn't specify the autowarmCount parameter—this is because the document cache uses Lucene's internal ID to identify documents.
These identifiers can't be copied between index changes and thus we can't automatically warm this cache.
We define it in the documentCache XML tag and specify a few parameters that define the document cache's behavior.
First of all, the class parameter tells Solr which Java class should be used for implementation.
In our example, we use solr.LRUCache because we will be adding more information into the cache than we will be fetching from it.
When you see that you are getting more information than you add, consider using solr.FastLRUCache.
The next parameter tells Solr the maximum size of the cache (the size parameter)
As the Solr wiki says, we should always set this value to more than the maximum number of results returned by the query multiplied by the maximum concurrent queries than we think will be sent to the Solr instance.
This will ensure that we always have enough place in the cache, so that Solr will not have to fetch the data from the index multiple times during a single query.
The last parameter tells Solr the initial size of the cache (the initialSize parameter)
I tend to set it to the same value as the size parameter to ensure that Solr won't be wasting its resources on cache resizing.
The more fields marked as stored in the index structure, the higher the memory usage of this cache will be.
Please remember that when using the values shown in this example, you must always observe your Solr instance and act when you see that your cache is acting in the wrong way.
Remember that having a very large cache with very low hit rate can be worse than having no cache at all.
Along with everything else, you should pay attention to your cache usage as your Solr instances work.
If you see evictions, then this may be a signal that your caches are too small.
If you have a very poor hit rate, then it's sometimes better to turn the cache off.
Cache setup is one of those things in Apache Solr that is very dependent on your data, queries, and users; so I'll repeat once again—keep an eye on your caches and don't be afraid to react and change them.
Configuring the query result cache The major Solr role in a typical e-commerce website is handling user queries.
Of course, users of the site can type multiple queries in the Search box and we can't easily predict how many unique queries there may be.
But, using the logs that Solr gives us, we can check how many different queries there were in the last day, week, month, or year.
Using this information, we can configure the query result cache to suit our needs in the most optimal way, and this recipe will show you how to do it.
Each query can be sorted by four different fields (the user can choose by which field)
By analyzing the logs for the past three months, we know that there are about 2000 unique queries that users tend to type in the search box of our application.
We also noticed that our users don't usually use the paging mechanism.
On the basis of this information, we configure our query results cache as follows (add this code to the solrconfig.xml configuration file):
Adding the query result cache to the solrconfig.xml file is a simple task.
We define it in the queryResultCache XML tag and specify a few parameters that define the query result's cache behavior.
First of all, the class parameter tells Solr which Java class should be used for implementation.
In our example, we use solr.LRUCache because we will be adding more information into the cache than we will fetching from it.
When you see that you are get more information than you add, consider using solr.FastLRUCache.
The next parameter tells Solr about the maximum size of the cache (the size parameter)
This cache should be able to store the ordered identifiers of the objects that were returned by the query with its sort parameter and the range of documents requested.
This means that we should take the number of unique queries, multiply it by the number of sort parameters and the number of possible orders of sort.
So in our example, the size should be at least the result of the following equation:
I tend to set the initial size of this cache to the maximum size; so in our case, I set the initialSize parameter to a value of 16000
This is done to avoid the resizing of the cache.
The last parameter (autowarmCount) says how many entries should be copied when Solr invalidates caches (for example, after a commit operation)
I tend to set this parameter to a quarter of the maximum size of the cache.
This is done because I don't want the caches to be warming for too long.
However, please remember that the auto-warming time depends on your deployment and the autowarmCount parameter should be adjusted if needed.
Please remember that when using the values shown in this example, you must always observe your Solr instance and act when you see that your cache is acting in the wrong way.
Along with everything else, you should pay attention to your cache usage as your Solr instances work.
If you see evictions, then this may be a signal that your caches are too small.
If you have a very poor hit rate, then it's sometimes better to turn the cache off.
Cache setup is one of those things in Apache Solr that is very dependent on your data, queries, and users; so I'll repeat once again—keep an eye on your caches and don't be afraid to react and change them.
Configuring the filter cache Almost every client of mine who uses Solr, tends to forget or simply doesn't know how to use filter queries or simply filters.
People tend to add another clause with a logical operator to the main query—they forget how efficient filters can be, at least when used wisely.
And that's why whenever I can, I tell people using Solr to use filter queries.
But when using filter queries, it is nice to know how to set up a cache that is responsible for holding the filters results – the filter cache.
This recipe will show you how to properly set up the filter cache.
For the purpose of this recipe, let's assume that we have a single Solr slave instance to handle all the queries coming from the application.
We took the logs from the last three months and analyzed them.
From this we know, that our queries are making about 2000 different filter queries.
By getting this information, we can set up the filter cache for our instance.
This configuration should look similar to the following code snippet (add this code to the solrconfig.xml configuration file):
As you may have noticed, adding the filter cache to the solrconfig.xml file is a simple task; you just need to know how many unique filters your Solr instance is receiving.
We define this in the filterCache XML tag and specify a few parameters that define the query result cache behavior.
First of all, the class parameter tells Solr which Java class should be used for implementation.
In our example, we use solr.LRUCache because we will be adding more information into the cache than we will fetching from it.
When you see that you are getting more information than you add, consider using solr.FastLRUCache.
The next parameter tells Solr the maximum size of the cache (the size parameter)
In our case, we said that we have about 2000 unique filters and we set the maximum size to that value.
This is done because each entry of the filter cache stores the unordered sets of Solr document identifiers that match the given filter.
In this way, after the first use of the filter, Solr can use the filter cache to apply filtering and thus save the I/O operations.
The next parameter – initialSize tells Solr about the initial size of the filter cache.
I tend to set it's value to the same as that of the size parameter to avoid cache resizing.
So in our example, we set it to the value of 2000
The last parameter (autowarmCount) says how many entries should be copied when Solr invalidates caches (for example, after a commit operation)
I tend to set this parameter to a quarter of the maximum size of the cache.
This is done because I don't want the caches to be warming for too long.
However, please remember that the auto-warming time depends on your deployment and the autowarmCount parameter should be adjusted if needed.
Please remember that when using the values shown in this example, you must always observe your Solr instance and act when you see that your cache is acting in the wrong way.
Along with everything, you should pay attention to your cache usage as your Solr instances work.
If you see evictions, then this may be a signal that your caches are too small.
If you have a very poor hit rate, then it's sometimes better to turn the cache off.
Cache setup is one of those things in Apache Solr that is very dependent on your data, queries, and users; so I'll repeat once again—keep an eye on your caches and don't be afraid to react and change them.
For example, take a look at the following screenshot that shows that the filter cache is probably too small, because the evictions are happening (this is a screenshot of the Solr administration panel):
Improving Solr performance right after the startup or commit operation.
Anyone with some experience with Solr would have noticed that – right after the startup, Solr doesn't have as much of an improved query performance as after running a while.
This happens because Solr doesn't have any information stored in caches, the I/O is not optimized, and so on.
Can we do something about it? Of course we can, and this recipe will show you how to do it.
The following steps will explain how we can enhance Solr performance right after the startup or commit operation:
First of all, we need to identify the most common and the heaviest queries that we send to Solr.
I have two ways of doing this—first of all, I analyze the logs that Solr produces and see how queries behave.
I tend to choose those queries that are run often and those that run slowly in my opinion.
The second way of choosing the right queries is by analyzing the application that use Solr and seeing what queries they produce, which queries will be the most crucial, and so on.
Based on my experience, the log-based approach is usually much faster and can be done using self-written scripts.
But let's assume that we have identified the following queries as good candidates:
What we will do next is just add the so called warming queries to the solrconfig.
So the listener XML tag definition in the solrconfig.xml file should look similar to the following code snippet:
Basically we added the so-called warming queries to the startup of Solr.
By adding the preceding fragment of configuration to the solrconfig.xml file, we told Solr that we want it to run those queries whenever a firstSearcher event occurs.
The firstSearcher event is fired whenever a new searcher object is prepared and there is no searcher object available in the memory.
So basically, the firstSearcher event occurs right after Solr startup.
So what happens after Solr startup? After adding the preceding fragment, Solr runs each of the defined queries.
By doing this, the caches get populated with the entries that are significant for the queries that we identified.
This means that if we did the job right, we have Solr configured and ready to handle the most common and heaviest queries right after its startup.
Let's just go over what all the configuration options mean.
The warm up queries are always defined under the listener XML tag.
The event parameter tells Solr what event should trigger the queries; in our case, it is firstSearcher.
The class parameter is the Java class that implements the listener mechanism.
Next, we have an array of queries that the warming queries is defined as a list of parameters that are grouped by the lst tag.
There is one more thing that I would like to mention (in the following section)
Improving Solr performance after commit operations If you are interested in improving the performance of your Solr instance, you should also look at the newSearcher event.
This event occurs whenever a commit operation is performed by Solr (for example, after replication)
Assuming that we identified the same queries as before as good candidates to warm the caches, we should add the following entries to the solrconfig.xml file:
Please remember that the warming queries are especially important for the caches that can't be automatically warmed.
Caching whole result pages Imagine a situation where you have an e-commerce library and your data changes rarely.
What can you do to take away the stress on your search servers? The first thing that comes to mind is caching; for example, HTTP caching.
But do we have to set up external caches prior to Solr, or can we tell Solr to use its own caching mechanism? We can use Solr to cache whole result pages and this recipe will show you how to do it.
Getting ready Before you continue to read this recipe, it would be nice for you to know some basics about the HTTP cache headers.
To do this, we need to configure the Solr request dispatcher.
Let's start by replacing the request dispatcher definition in the solrconfig.xml file with the following content:
Now, let's try sending a query similar to the following to see the HTTP headers:
The cache definition is defined inside the requestDispatcher XML tag.
Then, we see the httpCaching tag (notice the lack of the <httpCaching will be relative to when the current searcher object was opened (for example, relative to the last replication execution date)
You can also set this parameter value to dirLastMod to be relative to when the physical index was modified.
Next, we have the eTagSeed attribute, which is responsible for generating the ETag HTTP cache header.
The next configuration tag is the cacheControl tag, which can be used to specify the parameter tells Solr that it should generate an additional HTTP cache header, which will confirm that the cache is valid for a maximum of one hour.
The public directive means that the response can be cached by any cache type.
As you can see from the response, the headers that we got as a part of the results returned by Solr tell us that we got what we wanted.
Let's assume that our data which we use to calculate faceting can be considered to have low distinct values.
For example, we have an e-commerce shop with millions of products – clothes.
Each document in our index, apart from name and price, is also described by additional information – target size.
So, we have values such as XS, S, M, L, XL, and XXL (that is, six distinct values), and each document can only be described with a single value.
In addition to this, we run field faceting on that information and it doesn't work fast by default.
The following steps will explain how we can improve faceting performance for low cardinality fields:
Let's begin with the following index structure (add the following entries to your schema.xml fields section):
The size field is the one in which we store our XS, S, M, L, XL, and XXL values (remember: one value per document)
Assuming that our user typed black skirt into the Search box, our query would look similar to the following code snippet:
Assuming that the query is matching one-fourth of our documents, we can expect the query to be executing longer than usual.
This is because the default faceting calculation is optimized for fields that have many unique values in the index and we have the opposite—we have many documents but few unique terms.
If you measure the performance before and after the change you will notice the difference; let's discuss why.
Let's take a look at the query—we search for the given words in the name field using the AND logical operator (q.op parameter)
As our requirements state, we also run faceting on the.
We know that our fields have only six distinct values, and we also assumed that our queries can return vast amount of documents.
To handle such faceting calculation faster than the default method, we decided to use the enum method of facet calculation.
The default faceting sums the terms that appear in the field that we are calculating faceting on.
The enum method does the other thing – it enumerates all the terms in the field that we want to calculate faceting on, and intersects the documents that match the query with the documents that match the enumerated terms.
In this way, less time and processing is needed to calculate field faceting for low cardinality fields, such as size in our case, and thus we see faster query execution.
It is good to know that for field faceting on Boolean fields, Solr uses the enum faceting method by default.
You can also use the faceting method for each field you perform faceting upon.
Specifying faceting method per field If you have multiple fields on which you run faceting, then you may only want to change the method for one of them (or more than one, but not all)
To do that, instead of adding the parameter for each field whose faceting calculation method you would want to change.
For example, if you would like to change the faceting method for the size field, you can add the following parameter:
One of the most common problems when indexing a vast amount of data is the indexing time.
Some of the problems with indexing time are not easily resolvable, but others are.
Imagine that you need to index about 300,000 documents that are in a single XML file.
Is there something we can do to speed it up? Sure, and this recipe will tell you how to.
The solution to the situation is very simple – just add the commit operation every now and then.
But as you may have noticed, I mentioned that our data is written in a single XML file.
If you start the indexing described in the indexing process, you will notice that a commit command will be sent once a minute while the indexing process is takes place.
Solr tends to slow down the indexing process when indexing a vast amount of data without the commit commands being sent once in a while.
This behavior is completely understandable and is bound to the memory and how much of it Solr can use.
We can avoid the slowing down behavior by adding the commit command after the set amount of time or set amount of data.
We assumed that it would be good to send the commit command once every minute.
We've also specified that we want the search to be reopened after the commit and thus the data available for search (the <openSearcher>true</openSearcher> option)
If you would only like to write the data to the index and not have it available for search, just change the <openSearcher>true</openSearcher> option to false.
After this change, Solr will send a commit command after every minute passes during the indexing operation, and we don't have to worry that Solr indexing speed will decrease over time.
There are two more things about automatic commits that should be mentioned.
Commit after a set amount of documents Sometimes, there is a need to rely not on the time between commit operations, but on the amount of documents that were indexed.
If this is the case, we can choose to automatically send the commit command after a set amount of documents are processed.
To do this, we add the <maxDocs> XML tag with the appropriate amount.
For example, if we want to send the commit command after every 50000 documents, the update handler configuration should look similar to the following code snippet:
Commit within a set amount of time There may be situations when you want some of the document to be committed faster than the auto commit settings.
In order to do that, you can add the commitWithin attribute to the <add> tag of your data XML time.
This attribute will tell Solr to commit the documents within the specified time (specified in milliseconds)
For example, if we want the portion of documents to be indexed within 100 milliseconds, our data file would look similar to the following code snippet:
Analyzing query performance Somewhere along the experience with Apache Solr (and not only Solr), you'll end up at a point where some of your queries are not running as you would like them to run – some of them are just slow.
Of course, such a situation is not desirable and we have to do something to make those queries run faster.
But how do we know which part of the query is the one we should look at ? This recipe will tell you what information you can get from Solr.
Let's start with the assumption that we have a query that has parts that are not as fast as we would like it to be.
The response from Solr is as follows (I've cut some parts of the response, because it is quite large and we are only interested in the last section):
As you can see in the preceding response, there is some information about query time.
Let's not concentrate on the query, because it is only an example that allows us to discuss This parameter turns on the debug mode in Solr, as you can see in the response.
All these categories are nested inside the parses your query and how it is passed to Lucene, but it's beyond the scope of this chapter we will talk about it in Chapter 9, Dealing with Problems.
The first information you see under this tag is the total time of your query, which the following two lists:
You can see that nested inside those lists are components and their time.
The prepare list tells us how much time each component spends during the query preparation phase.
The process list tells us how much time was spent during the query processing phase, which is the phase that is usually the longest one, because of all the computation and I/O operations needed to execute the query.
You can see that in our case, there were three components that were working for longer than 0 milliseconds.
The second component, which was running for 43 milliseconds, was org.apache.solr.
It still takes about 10 percent time of the whole query, which is not what we are looking for.
As you can see, with the use of the debugQuery parameter, we identified which part of the query is problematic and we can start optimizing it; But it's beyond the scope of this recipe.
Avoiding filter caching Imagine that some of the filters you use in your queries are not good candidates for caching.
You may wonder why, for example, those filters have a date and time with seconds or are spatial filters scattered all over the world.
Such filters are quite unique and when added to the cache, their entries can't be reused much.
Caching such filters is a waste of memory and CPU cycles.
Is there something you can do to avoid filter queries caching? Yes, there is a way and this recipe will show you how to do it.
Let's assume we have the following query being used to get the information we need:
The filter query we don't want to cache is the one filtering our documents on the basis of the date field.
Of course, we still want the filtering to be done.
In order to turn off caching, we similar to the following code snippet:
The first query is very simple; we just search for the words solr cookbook and we want the result set to be narrowed in the books category.
We also want to narrow the results further to only those that have 2012-06-12T13:22:12Z in the date field.
As you can imagine, if we have many filters with such dates as the one in the query, the filter cache can be filled very fast.
In addition to this, if you don't reuse the same value for that field, the entry in the field cache becomes pretty useless.
That's why, by adding the results to be put into the filter cache.
With such an approach we won't pollute the filter cache and thus save some CPU cycles and memory.
There is one more thing – the filters that are not cached will be executed in parallel with the query, so this may be an improvement to your query execution time.
If you use filter queries extensively, which isn't a bad thing at all, you may be wondering if there is something you can do to improve the execution time of some of your filter queries.
For example, if you have some filter queries that use heavy function queries, you may want to have them executed only on the documents that passed all the other filters.
Getting ready Before continuing reading please read the Avoiding filter caching recipe in this chapter.
The following steps will explain how we can control the order of execution of filter queries:
Let's assume we have the following query being used to get the information we need: 0,price_a),sum(0,price))
We would also like the second filter to execute only on the documents that were narrowed by other filters.
In order to do this, we need to modify our query so that it looks similar to the following code snippet:
As you can see, we search for the words solr cookbook in the first query and we want the result set to be narrowed by book category.
Our requirements are such that the second filter query (the one with log function query) be executed in the end, only on the documents narrowed by other filters.
To do this, we set those two filters to not cache and we introduced the cost parameter.
The cost parameter in filter queries specifies the order in which non-cached filter queries are executed; the higher the cost value, the later the filter query will be executed.
So our second filter (the one with second non-cached filter query is higher or equal to 100, this filter will be executed only on the documents that matched the main query and all the other filters.
Forgive me, but I have to say it once again—please remember that the cost attribute only works when the filter query is not cached.
Let's assume we have the Apache Solr 4.0 deployment where we use range queries.
Some of those are run against string fields, while others are run against numerical fields.
Using different techniques, we identified that our numerical range queries execute slower than we would like.
The following steps will explain how we can control the order of execution of numerical range queries:
Let's begin with the definition of a field that we use to run our numerical range queries:
The second step is to define the float field type:
Now the usual query that is run against the preceding field is as follows:
So, our field type definition would look similar to the following code snippet:
After the preceding change, you will have to re-index your data and your numerical queries should be run faster.
As you can see, in the preceding examples, we used a simple float-based field to run numerical range queries.
Before the changes, we specified precisionStep on our field type as 8
This attribute (specified in bits) tells Lucene (which Solr is built on top of) how many tokens should be indexed for a single value in such a field.
However, please remember that decreasing the precisionStep value will lead to slightly larger indices.
Also, setting the precisionStep value to 0 turns off indexing of multiple tokens per value, so don't use that value if you want your range queries to perform faster.
Introduction As you know, Apache Solr 4.0 introduced the new SolrCloud feature that allows us to use distributed indexing and searching on a full scale.
We can have automatic index distribution across multiple machines, without having to think about doing it in our application.
In this chapter, we'll learn how to manage our SolrCloud instances, how to increase the number of replicas, and have multiple collections inside the same cluster.
Creating a new SolrCloud cluster Imagine a situation where one day you have to set up a distributed cluster with the use of Solr.
The amount of data is just too much for a single server to handle.
Of course, only you can set up a second server or go for another master database with another set of data.
But before Solr 4.0, you would have to take care of the data distribution yourself.
In addition to this, you would also have to take care of setting up replication, thinking about data duplication, and so on.
You don't have to do this now because Solr 4.0 can do it for you.
This recipe shows how to set up a ZooKeeper cluster ready for production use.
However, if you already have ZooKeeper running, you can skip that recipe.
Let's assume we want to create a cluster that will have four Solr servers.
We would also like to have our data divided between four Solr servers in such a way that we would have the original data sharded to two machines.
In addition to this we would also have a copy of each shard available, in case something happens with one of the Solr instances.
Let's start with populating our cluster configuration into the ZooKeeper cluster.
In order to do this, you need to run the following command:
Now that we have our configuration populated, let's start the second node with the following command:
We now have our two shards created and want to create replicas.
This is very simple since we have already created the configuration.
We just need to start two additional servers with the following command run on each of them:
If you look at the cloud configuration of the Solr administration panel, you will see that you have a cluster that has four nodes, where the first two nodes act as leaders for the shards and the other two nodes act as their replicas.
You can start indexing your data to one of the servers now, and Solr will take care of data distribution and will also automatically copy the data to the replicas.
What we need to do first is send all the configuration files to ZooKeeper in order for the Solr servers to be able to fetch it from there.
That's why, when running the first server (only during the first start of it), we add the -Dboostrap_confdir and -Dcollection.configName parameters.
The first parameter specifies the location of the directory with the configuration files that we would like to put into ZooKeeper.
During the first start, we also need to specify the number of shards that should be available in our cluster, and in this example we set it to 2 (the -DnumShards parameter)
The -DzkHost parameter is used to tell Solr about the location and the port used by the Zookeeper cluster.
As you can see, all the other commands are similar to the ones you used while running the Solr instances.
The only difference is that we specify one additional parameter, -DzkHost, which tells Solr where to look for the ZooKeeper server on the cluster.
When setting up the SolrCloud cluster, please remember to choose the number of shards wisely, because you can't change that for your existing cluster, at least not right now.
You can add replicas to an already created cluster, but the number of shards will remain constant.
Starting the embedded ZooKeeper server You can also start an embedded ZooKeeper server shipped with Solr for your test environment.
In order to do this, you should pass the -DzkRun parameter instead configuration to the ZooKeeper cluster.
So the final command should look similar to the following code snippet:
Imagine that you would like to have more than a single collection inside the same Apache Solr 4.0 cluster.
For example, you would like to store books in one collection and users in the second one.
SolrCloud allows that, and this recipe will show you how to do it.
Getting ready Before continuing, I advise you to read the Installing standalone ZooKeeper recipe in Chapter 1, Apache Solr Configuration, because this recipe assumes that we already have ZooKeeper up and running.
We assume that ZooKeeper is running on localhost and is listening on port 2181
On both instances of Solr, the solr.xml file should look similar to the following code snippet:
Now, we need to add the configuration files, which we want to create collections with, to ZooKeeper.
Let's assume that we have all the configuration files stored in /usr/ share/config/books/conf for the books collection, and the configuration files for the users collection stored in /usr/share/config/users/conf.
We have pushed our configurations into the ZooKeeper, so we can now create the collections we want.
In order to do this, we use the following commands:
Now, just to test if everything went well, we will query the newly created collections as follows:
The response to the preceding command will be as follows:
But as we don't have any data indexed, we got 0 documents.
As you can see, our solr.xml file on both the instances is the same and it doesn't contain any information about the cores.
This is done on purpose, since we want to have a clean cluster – one without any collections present.
The mentioned configuration directories should store all the files (solrconfig.xml, schema.xml, and stopwords.txt) that are needed for your Solr instance to work, if you use one.
Please remember this before sending them to ZooKeeper or else Solr will fetch those files from ZooKeeper and create collections using them.
Now, let's look at the most interesting aspect – the scripts used to upload the configuration files to ZooKeeper.
We used the zkcli.sh script provided with the standard Solr 4.0 distribution and placedit  in the cloud-scripts directory by default.
The first thing is the cmd parameter, which specifies what we want to do – in this case upconfig means that we want to upload the configuration files.
The zkhost parameter allows us to specify the host and port of the ZooKeeper instance we want to put the configuration to.
Finally the last parameter, confname, specifies the name of the collection we will use the configuration with.
The command in the fourth step lets us create the actual collection in the cluster.
In order to do this, we send a request to the /admin/collections handler, which uses the newly introduced the name specified in the name parameter is the same as the confname parameter value used during configuration files upload.
The last two parameters specify the number of shards and replicas that the collection should be created with.
The number of shards is the initial number of cores that will be used to hold the data in the collection (numShards)
The number of replicas (replicationFactor) is the exact number of copies of the shards that can be distributed among many servers, and may increase query throughput and reliability.
Managing your SolrCloud cluster In addition to creating a new collection with the API exposed by SolrCloud, we are also allowed to use two additional operations.
The first is to delete our collection and the second one is to reload the whole collection.
Along with the ability to create new collections, we are able to dynamically manage our cluster.
This recipe will show you how to use the delete and reload operations and where they can be useful.
Getting ready The content of this recipe is based on the Setting up two collections inside a single cluster recipe in this chapter.
So our cluster view looks similar to the following screenshot:
In order to test it, let's update the spellings.txt file located at /usr/share/config/books/conf directory.
The original file looks similar to the following code snippet:
After the update, it will look similar to the following code snippet:
Now, we need to update the collection configuration in ZooKeeper.
To do this we use the following command, which is run from our Solr instance's home directory: cloud-scripts/zkcli.sh -cmdupconfig -zkhost localhost:2181 -confdir /usr/share/config/books/conf -confnamebookscollection.
Now that we have the updated version of our configuration files to bookscollection in ZooKeeper, we can send the reload command to Solr:
First, let's check if the Solr administration panel sees the changes in ZooKeeper.
To do this, we'll use the tree view of the cloud section and navigate to /configs/ bookscollection/spellings.txt.
We should be able to see something similar to the following screenshot:
In the final check, let's see if Solr itself sees the update.
In order to do this we run the following command:
The response of the preceding command would be as follows:
So it seems like everything is working as it should.
But we want to delete one of them and update the second one.
In order to do this we use the collections API provided by Solr 4.0
In addition to this, we need to provide the name of the collection we want to delete – to do this, we use the name parameter with the name of the collection that we want to delete.
After sending the command and refreshing the Solr administration panel, we see that the second collection was deleted just as we wanted.
Now, let's discuss the process of updating the second collection.
First of all, we've changed the contents of the spellings.txt file in order to see how it works.
However, be careful when updating collections, because some changes may force you to re-index your data; but let's get back to our update.
So after we update the file, we use one of the scripts provided with Solr 4.0 in order to upload all the configuration files that belong to this collection into the ZooKeeper ensemble (if you are not familiar with that command, please see the Setting up two collections inside a single cluster recipe, later in this chapter)
Now, we needed to tell Solr URL as the delete command.
Of course, just like with the delete command, we needed to provide the name of the collection we want to reload using the name parameter.
As you can see, on the previous screenshot, the collection was updated at least in the ZooKeeper ensemble.
However, we want to be sure that Solr sees those changes, so we use the /admin/file handler to get the contents of the spellings.txt file.
In order to do this, the changed contents, so the collection was updated and reloaded successfully.
With the release of Solr 4.0, we've got the ability to use a fully-distributed Solr cluster with fully-distributed indexing and searching.
Along with this comes the reworked Solr administration panel with parts concentrated on Cloud functionalities.
This recipe will show you how to use this part of the administration panel; for example, how to see your cluster distribution and detailed information about shards and replicas.
Getting ready This recipe assumes that the SolrCloud cluster is up and running.
If you are not familiar with setting up the SolrCloud cluster, please refer to the Creating a new SolrCloud cluster recipe in this chapter.
We should be able to see something similar to the following screenshot:
There is also a second view of the same information that can be accessed by viewing the Graph (Radial) section, and it should look similar to the following screenshot:
Looks nice, doesn't it? However, there is some additional information that can be retrieved.
So now, let's look at the Tree section of the Cloud administration panel:
As you can see, there is some very detailed information available.
First of all, remember that the best way to get used to the new administration panel is to just run a simple SolrCloud cluster by yourself and play with it.
However, let's look at the provided examples to see what information we have there.
As you can see, in the first two screenshots provided, our cluster consists of a single collection named collection1
Both diagrams shown in the screenshots provide the same amount of information and they only differ in the way they present the data.
Now, let's look and discuss the last screenshot – the Tree view of the Cloud section of the Solr administration panel.
As you can see, there is much more information available there.
The tree presented in the administration panel is what your ZooKeeper ensemble sees.
The first thing is clusterstate.json, which holds detailed information about the current state of the cluster.
Next, you can see the collections section, which holds information about each collection deployed in the cluster – you can see the information about each shard and its replicas, such as leaders, and some detailed information needed by the Solr and ZooKeeper.
In addition to the preceding information, you can also see the configuration files (the /configs section) that were sent to the ZooKeeper and are used as the configuration files for your collection or collections.
Not visible in the screenshot is the additional information connected to ZooKeeper, which is not needed during the usual work with Solr, so I decided to omit discussing it.
Distributed indexing and searching Having a distributed SolrCloud cluster is very useful; you can have multiple shards and replicas, which are automatically handled by Solr itself.
This means that your data will be automatically distributed among shards and replicated between replicas.
However, if you have your data spread among multiple shards, you probably want them to be queried while you send the query.
With earlier versions of Solr before 4.0, you had to manually specify the list of shards that should be queried.
Now you don't need to do that, and this recipe will show you how to make your queries distributed.
Getting ready If you are not familiar with setting up the SolrCloud cluster, please refer to the Creating a new SolrCloud cluster recipe in this chapter.
If you are not familiar with how to modify the returned documents using the fl parameter, please read the Modifying returned documents recipe in Chapter 4, Querying Solr.
For the purpose of this recipe, I'm using the example configuration files provided with Solr and the example documents stored in the XML files in the exampledocs directory of the Solr distribution package.
If we look at the Solr administration panel, this is what the Cloud graph will show:
Now, let's use the non-distributed queries to check if the documents were sent to all the shards.
The first query is run to the Solr instance holding the first shard:
The second query is run to the Solr instance holding the second shard:
The third query is run to the Solr instance holding the last shard:
Everything seems to be in the perfect order now,  at least by judging the number of documents.
So now, let's run the default distributed query to see if all the shards were queried.
In order to do this we run the following query:
Since the response was quite big, I decided to cut it a bit and show only a single document from each shard:
As you can see, we got documents from each shard that builds our cluster, so it works as intended.
As you can see, as shown in the previous screenshot, our test cluster created for the purpose of this recipe contains thee Solr instances, where each of them contains a single shard of the collection deployed on the cluster.
This means that the data indexed to any of the shards will be automatically divided and distributed among the shards.
In order to choose which shard the document should go to, Solr uses a hash value of the identifier of the document.
During indexing, we sent the documents to the Solr instance that is working on port 8983
However, as our example queries show, when querying only a particular shard (the is expected.
If we had many more documents, the amount of documents on each shard would be probably almost the same if not equal.
As you must have guessed by now, the to in a non-distributed manner, and we want such behavior to see how many documents are hosted on each of the shards.
Let's now focus on the query that was used to fetch all the documents in the cluster.
It's a way that the returned document contains the id field and the information about the shard coming from all the shards that build the collection in the response.
This is because when using the SolrCloud deployment, Solr automatically queries all the relevant shards that are needed to be queried in order to query the whole collection.
The information about shards (and replicas, if they exist) is fetched from ZooKeeper, so we don't need to specify it.
Increasing the number of replicas on an already live cluster.
If you used Solr before the release of the 4.0 version, you are probably familiar with replication.
The way deployments usually worked is that there was a single master server and multiple slave servers that were pulling the index from the master server.
We can also set up our instances in a way to achieve a similar setup as that of multiple replicas of a single shard where data is stored.
Getting ready If you are not familiar with setting up a SolrCloud cluster, please refer to the Creating a new SolrCloud cluster recipe in this chapter.
For the purpose of this recipe, I'll assume that we want to have a cluster with a single shard running just like the usual Solr deployment, and we want to add two additional replicas to that shard.
The first step is starting a new Solr 4.0 server.
We will use the configuration provided with the example Solr server, but you can use your own if you want.
We will also use the ZooKeeper server embedded into Solr, but again, you can use the standalone one.
So finally, the command that we use for starting the first instance of Solr is as follows:
Now, let's take a look at the Solr administration panel to see how our cluster state looks:
As you can see, we have a single shard in our collection that has a single replica.
This can be a bit misleading, because the single replica is actually the initial shard we've created.
So we actually have a single shard and zero copies of it.
As we said earlier, we want to change that in order to have two additional replicas of our shard.
In order to do this, we need to run two additional Solr instances.
But in a real life situation, you'd probably want to have them on different servers.
In order to run these two additional Solr servers, we use the following commands:
Now, let's see how our cluster state changes, by looking at the cluster state in the Solr administration panel again.
The cluster state information looks similar to the following screenshot after we start the two additional instances of Solr:
But right now, we also have two additional replicas present that will be automatically updated and will hold the same data as the primary shard that we created in the beginning.
We start our single shard instance with the command that allows us to run the embedded ZooKeeper server along with Solr.
Of course, this was only used as an example, and in a production environment you should set up a standalone ZooKeeper server.
As shown in the previous screenshot, you can see that our collection consists of a single shard and the only replica we have is this shard.
So, we have a single primary shard with no data replication at all.
In order to create two replicas that will be automatically populated with exactly the same data as the primary shard, we just need to start the two additional Solr servers.
For the purpose of the recipe, we started these new instances on the same machine, but usually in a production environment you would set them up on separate machines.
As you can see in the second screenshot, after adding these two new Solr instances, our cluster is composed of a primary shard and two replicas, which will have their contents updated automatically.
In most cases, the standard distribution of documents between your SolrCloud instances will be enough, and what's more, it will be the right way to go.
However, there are situations where controlling the documents distribution outside of Solr (that is, in your application) may be better.
For example, imagine that you'll only allow your users to search in the data they indexed.
In such situations, it would be good to have documents for a single client stored in a single shard (if that's possible)
In such cases, automatic documents distribution based on the documents identifier may not be the best way.
Solr allows us to turn off automatic document distribution and this recipe will show you how to do that.
Getting ready If you are not familiar with setting up the SolrCloud cluster, please refer to the Creating a new SolrCloud cluster recipe in this chapter.
If you are not familiar with how to modify the returned documents using the fl parameter, please read the Modifying the returned documents recipe in Chapter 4, Querying Solr.
Let's assume that we have the following index structure (schema.xml) defined,
In order to be able to stop the automatic document distribution between shards, we need the following update request processor chain to be defined in the solrconfig.xml file:
I assume that we already have a cluster containing at least two nodes up and running, these nodes use the preceding configuration files, and that our collection name is collection1
As we discussed earlier, we want to manually distribute the data to Solr instances.
So, we use the following commands to index the data: post.jar data1.xml.
In order to do this, we will use the Solr functionality that enables us to see which shard the document is stored at.
It seems that we have achieved what we wanted, so let's see how it works.
It contains three fields that are used by our data files at the _version_ field used internally by Solr.
The actual data is nothing new as well, so I'll skip discussing it.
The thing we want to look at is the update request processor chain definition.
As you can see, apart from the standard solr.LogUpdateProcessorFactory and solr.RunUpdateProcessorFactory processors, it contains a solr.
You can think of this additional processor as the one that forces the update command to be indexed on the node it was sent to.
We used the standard post.jar library distributed with Solr in order to index the data.
In order to specify which server the data should be sent to, we use the –Durl parameter.
In order to check this, we use a query that returns all the documents contains not only all the stored fields, but also the shard the document was fetched from.
One more thing: please remember that when turning off automatic documents distribution, you may end up with shards being uneven.
This is because of the different number of documents being stored in each of them.
Introduction There are many features of Solr that we don't use every day.
You may not encounter highlighting words, ignoring words, or statistics computation in everyday use, but they can come in handy in many situations.
In this chapter, I'll try to show how to overcome some typical problems that can be fixed by using some of the Solr functionalities.
In addition to that we will see how to use the Solr grouping mechanism in order to get documents that have some fields in common.
Getting more documents similar to those returned in the results list.
Imagine a situation where you want to show similar documents to those returned by Solr.
Let's imagine a situation where you have an e-commerce library shop, and you want to show users the books similar to the ones they found while using your application.
Let's start with the following index structure (just add this to your schema.xml file,
Let's assume that our hypothetical user wants to find books that have cookbook and second in their names.
The results returned by Solr for the preceding query are as follows:
As you can see the index structure and the data are really simple.
One thing to note is the termVectors attribute set to true in the name field definition.
It is a good thing to have when using the more like this component, and should be used whenever possible in the fields on which we plan to use this component.
As you can see, we added some additional parameters besides the standard q one (and the ones such as mm and defType which specify how our like this component to the result processing.
The mlt.fl parameter specifies which fields we want to use with the more like this component.
The mlt.mintf parameter asks Solr to ignore terms from the source document (the ones from the original result list) with the term frequency below the given value.
In our case we don't want to include the terms that will have a frequency lower than 1
The last parameter, mlt.mindf, tells Solr that words appearing less than the value of the parameter documents should be ignored.
In our case we want to consider words that appear in at least one document.
As you can see, there is an additional section (<lst component results.
For each document in the results there is one more like this section added to the response.
In our case, Solr added a section for the document with the unique similar documents found.
The value of the id attribute is assigned the value of the unique identifier of the document for which the similar documents are calculated for.
Highlighting matched words Imagine a situation where you want to show your users which words were matched in the document shown in the results list.
For example, you want to show which words in the book name were matched and display that to the user.
Do you have to store the documents and do the matching on the application side? The answer is no.
We can force Solr to do that for us and this recipe will show you how to do that.
Let's assume that our user is searching for the word book.
To tell Solr that we want to highlight the matches, we send the following query:
As you can see the index structure and the data are really simple, so I'll skip discussing this part of the recipe.
Please note that in order to use the highlighting mechanism, your fields should be stored and not analysed by aggressive filters (such as stemming)
Otherwise the highlighting results can be misleading to the users.
Let's think of a simple example of such behavior – imagine the user types the word bought in the search but Solr highlighted the word buy because of the stemming algorithm.
We can see the standard q parameter that passes the query to Solr.
But there is also one additional parameter, the hl parameter set to true.
This parameter tells Solr to include the highlighting component results to the results list.
As you can see in the results list, in addition to the standard results, there is a new section result is presented for the document with the unique identifier value of 4), there is a list of fields that contain the sample data with the matched words (or words) highlighted.
By highlighted I mean surrounded by the HTML tag, in this case the <em> tag.
You should also remember one other thing: if you are using the standard LuceneQParser query parser then the default field used for highlighting will be the one set in the schema.
If you are using DismaxQParser then the default fields used for highlighting are the ones specified by the qf parameter.
There are a few things that can be useful when using the highlighting mechanism.
Specifying the fields for highlighting In many real life situations we want to decide what fields we would want to show the highlighting for.
To do that, you must add an additional parameter – hl.fl with the list of fields separated by the comma character.
For example, if we would like to show the highlighting for the fields name and description, our query should look as follows:
To do that you should add the hl.simple.pre and hl.simple.
The first one specifies the prefix that will be added in front of the matched word and the second one specifies the postfix that will be added after the matched word.
How to highlight long text fields and get good performance.
In certain situations, the standard highlighting mechanism may not be performing as well as you would like it to be.
For example, you may have long text fields and you want the highlighting mechanism to work with them.
We will use the test data which looks like the following code:
Let's assume that our user is searching for the word book.
To tell Solr that we want to highlight the matches, we send the following query:
As you can see the index structure and the data are really simple, but there is a difference between using the standard highlighter and the new FastVectorHighlighting feature.
To be able to use the new highlighting mechanism, you need to store the information about term vectors, position, and offsets.
This is done by adding the following attributes to the.
Please note that in order to use the highlighting mechanism, your fields should be stored and not analysed by aggressive filters (such as stemming)
Otherwise the highlighting results can be misleading to the users.
An example of such a behavior is simple – imagine that the user types the word bought in the search box but Solr highlighted the word buy because of the stemming algorithm.
We can see the standard q parameter that passes the query to Solr.
But there is also one additional parameter, the hl parameter set to true.
This parameter tells Solr to include the highlighting component results to the results list.
In addition we add the parameter to tell Solr to use the FastVectorHighlighting.
As you can see in the results list, in addition to the standard results, there is a new section result is presented for the document with the unique identifier value of 4), there is a list of fields that contain the sample data with the matched words (or words) highlighted.
By highlighted I mean surrounded by the HTML tag, in this case the <em> tag.
Sorting results by a function value Let's imagine that you have an application that allows the user to search through the companies that are stored in the index.
You would like to add an additional feature to your application to sort the results on the basis of the distance of a certain geographical point.
Is this possible with Solr? Yes, and this recipe will show you how to do that.
If you are not familiar with geographical search in Solr please read the Storing geographical points in the index recipe in Chapter 3, Analyzing Your Text Data.
Let's start with the following index structure (just add this to your schema.xml file,
Our test data that we want to index looks like the following code:
In addition to that we also need to define the following field type in the schema.xml file in the types section:
So, in order to show the results of the query and sort them by the distance from the given point, we send the following query to Solr:
The results list returned by the query is as follows:
As you can see, everything is working as it should be.
We have four fields – one for holding the unique identifier (the id field), one for holding the name of the company (the name field), and one field responsible for the geographical location of the company (the geo field)
The last field, the dynamic one, is needed for the location type to work.
The data is pretty simple so let's just skip discussing that.
Besides the standard q parameter responsible for the user query, you can see the sort parameter.
But the sort is a bit different from the ones you are probably used to.
It uses the geodist function to calculate the distance from the given point, and the value returned by the function is then used to sort the documents in the results list.
The first argument of the geodist function (the geo value) tells Solr which field to use to calculate the distance.
The next two arguments specify the point from which the distance should be calculated.
Of course as with every sort we specify the order in which we want the sort to take place.
In our case we want to sort from the nearest to the furthest company (the asc value)
As you can see in the results, the documents were sorted as they should be.
Searching words by how they sound One day your boss comes to your office and says "Hey, I want our search engine to be able to find the same documents when I enter phone or fone into the search box"
You tried to say something, but your boss is already at the other side of the door to your office.
So, you wonder if this kind of functionality is available in Solr.
I think you already know the answer – yes it is, and this recipe will show you how to configure it and use with Solr.
We start with the following index structure (just add this to your schema.xml file,
Next we define the phonetic type, which looks like the following code (paste it into the schema.xml file):
Now we need to index our test data, which looks like the following code:
Now let's assume that our user wants to find documents that have the word that sounds like fon.
The result list returned by the query is as follows:
So, the filter worked! We got two documents in the results list.
As you can see we have two fields, the id field responsible for holding the unique identifier of the product and the name field responsible for holding the name of the product.
The name field is the one that will be used for phonetic search.
For that we defined a new field type named phonetic.
Besides the standard parts (such as class among many others) we defined a new filter: DoubleMetaphoneFilterFactory.
It is responsible for analysis and checking how the words sound.
This filter uses an algorithm named double tells Solr to replace the existing tokens instead of inserting additional ones, which mean that the original tokens will be replaced by the ones that the filter produces.
As you can see from the query and the data, the fon word was matched to the word phone and also to the word fone, which means that the algorithm (and thus the filter) works quite well.
But take into consideration that this is only an algorithm, so some words that you think should be matched will not match.
See also If you would like to know other phonetic algorithms, please take a look at the Solr Wiki page that can be found at the following URL address: http://wiki.apache.org/solr/ AnalyzersTokenizersTokenFilters.
Ignoring defined words Imagine a situation where you would like to filter the words that are considered vulgar from the data we are indexing.
Of course, by accident, such words can be found in your data and you don't want them to be searchable thus you want to ignore them.
Can we do that with Solr? Of course we can, and this recipe will show you how to do that.
Let's start with the following index structure (just add this to your schema.xml file,
The second step is to define the text_ignored type, which looks like the following code:
The next step is to index our test data, which looks as follows:
In the standard situation there shouldn't be any results because we don't have a document that matches the two given words.
But let's look at what Solr returned to us as the preceding query's result:
To be perfectly sure, let's look at the analysis page found at the administration interface, as shown in the following screenshot:
As you can see the word vulgar was cut and thus ignored.
As you can see we have two fields, the id field responsible for holding the unique identifier of the product and the name field responsible for holding the name of the product.
The name field is the one we will use to mention the ignoring functionalities of Solr – StopFilterFactory.
As you can see the text_ignored type definition is analysed the same way both in the query and index time.
The words attribute of the filter definition specifies the name of the file, encoded in UTF-8, which consists of words (a new word at every file line) that should be ignored.
The defined file should be placed in the same directory in which we placed the schema.xml file.
The ignoreCase attribute set to true tells the filter to ignore the case of the tokens and the increment the position of the tokens in the token stream.
The enablePositionIncrements parameter should be set to true if you want to preserve the next token after the discarded one to increment its position in the token stream.
As you can see in the query, our hypothetical user queried Solr for two words with the logical operator AND, which means that both words must be present in the document.
But, the filter we added cut the word vulgar and thus the results list consists of the document that has only one of the words.
The same situation occurs when you are indexing your data.
The words defined in the ignored.txt file will not be indexed.
If you look at the provided screenshot from the analysis page of the Solr administration interface (refer to step 6 of the How to do it...
Computing statistics for the search results Imagine a situation where you want to compute some basic statistics about the documents in the results list.
For example, you have an e-commerce shop where you want to show the minimum and the maximum price of the documents that were found for a given query.
Of course you could fetch all the documents and count them by yourself, but imagine Solr doing it for you.
Yes, it can! And this recipe will show you how to use that functionality.
The example data that we index looks like the following code:
Let's assume that we want our statistics to be computed for the price field.
To do that, we send the following query to Solr:
As you can see, in addition to the standard results list, there was an additional section available.
It contains three fields – one for holding the unique identifier (the id field), one for holding the name (the name field), and one for holding the price (the price field)
The file that contains the example data is simple, so I'll skip discussing it.
In addition to the q parameter we have two new parameters.
In our case, we told Solr to use the price field.
As you can see, StatsComponent, added an additional section to the results.
The section contains the statistics generated for the field that we told Solr we wanted the statistics for.
You should also remember that you can specify a number of the stats.field parameters to calculate the statistics for the different fields in a single query.
Please be careful when using this component on the multivalued fields as it can be a performance bottleneck.
Checking the user's spelling mistakes Most modern search sites have some kind of user spelling mistakes correction mechanism.
Some of those sites have a sophisticated mechanism, while others just have a basic one.
If all search engines have it then there is a high probability that your client or boss will want one too.
Is there a way to integrate such a functionality into Solr? Yes there is, and this recipe will show you how to do it.
Getting ready In this recipe we'll learn how to use the Solr spellchecker component.
The detailed information about setting up the spellchecker component can be found in the Configuring spellchecker to not use its own index recipe in Chapter 1, Apache Solr Configuration.
The data that we are going to index looks like the following code:
Our spell checking mechanism will work on the basis of the name field.
Now, let's add the appropriate search component to the solrconfig.xml file:
In addition to that we would like to have it integrated into our search handler, so we make the default search handler definition the same as in the following code (add this to your solrconfig.xml file):
To do that we will send a query that contains a spelling mistake.
We will send the words other boak instead of other book.
The Solr response for that query looks like the following response:
As you can see for the preceding response, Solr corrected the spelling mistake we made.
It contains two fields, one for holding the unique identifier (the id field), one for holding the name (the name field)
The file that contains the example data is simple, so I'll skip discussing it.
The spellchecker component configuration is something we discussed already in the Configuring spellchecker to not use its own index recipe in the first chapter.
So again, I'll look at only the most important fragments.
As you can see in the configuration, we've defined a spellchecker component that will use Solr DirectSolrSpellChecker in order to not store its index on the hard disk drive.
In addition to that, we configured it to use the name field for spellchecking and also to use that field analyzer to process queries.
Our /spell handler is configured to automatically discussed in the previously mentioned recipe.
We send the boak and othar words in the query parameter (q)
The spellchecker component will be activated automatically because of the configuration of our /spell handler, and that's actually all there is to it when it comes to the query.
As you can see there were no documents found for the word boak and the word other, that's what we actually were expecting.
But as you can see there is a spellchecker component section added to the results list see, the spellchecker component informed us about the number of suggestions found returned was the book word (<str>book</str> under the suggestion array)
There is an additional section in the spellchecker component results generated by the book)</str>
We can either show the query to the user or send it automatically to Solr and show to the user the corrected results list and this one is up to you.
Using field values to group results Imagine a situation where your data set is divided into different categories, subcategories, price ranges, and things like that.
What if you would like to not only get information about counts in such a group (with the use of faceting), but would also like to show the most relevant documents in each of the groups? Is there a grouping mechanism of some kind in Solr? Yes there is, and this recipe will show you how to use this functionality in order to divide documents into groups on the basis of field values.
The example data, which we are going to index, looks like the following code:
Let's assume that we would like to get our data divided into groups on the basis of their category.
In order to do that we send the following query to Solr:
The results returned by the preceding query are as follows:
As you can see the grouped results are different from the ones returned during a usual search.
But as you can see we got a single document per group which means it worked.
It consist four fields – one responsible for the document identifier (the id field), one used for holding the name of the book (the name field), its category (the category field), and the last one used to hold the price of the book (the price field)
Our example data is also very simple, but please know that the first and second book belongs to the same it category and the second book belongs to another category.
We said that we want to have our documents divided on the basis of contents of the category field.
In order to do that, we've added a new parameter called group, which is set to true.
This tells Solr that we want to enable the grouping functionality.
And similar to faceting, we've added a second parameter we are not familiar with.
If we look at the results returned by Solr, they are a bit different than the usual results.
You can see the usual response header, however, the resulting groups are returned in the <lst field parameter passed in the query; this time it tells us that the following results will be documents were found for our query.
This is the same as the numFound value during our usual query.
Next we have the groups array, which holds the information about the groups that were created by Solr in the results.
Each group is described by the it value, that is, the <str in that group have the it value in the field used for grouping.
In the result tag we can see the documents returned for the group.
By default Solr will return the most relevant document for each group.
I'll skip commenting on the result tag as it is almost identical to the results Solr returns for a non-grouped query and we are familiar with those, right?
One last thing – you can specify multiple group.field parameters with different fields in a single query in order to get multiple grouping.
There is one more thing about grouping on the basis of field values and I would like to share a few thoughts about that.
More than a single document in a group Sometimes you may need to return more than a single document in a group.
In order to do that you will need to use the group.limit parameter and set it to the maximum number of documents you want to have.
For example, if we would like to have 10 documents per group of results, we would send the following query:
Using queries to group results Sometimes grouping results on the basis of field values is not enough.
Solr allows us to group results on the basis of query results.
Getting ready In this chapter we will use the same index structure and test data as we used in the Using field values to group results recipe in this chapter.
How to do it… As we are reusing the data and index structure from the Using field values to group results recipe, we can start with the query.
In order to group our documents on the basis of query results, we can send the following query:
As you can see in the query we told Solr that we want to use the grouping functionality two groups calculated on the basis of the queries.
If you look at the results, they are very similar to the ones for grouping on the basis of field values.
The only difference is in the name of the groups.
When using the field values for grouping, groups were named after the used field names.
However, when using queries to group documents, groups are named as our grouping queries.
Using function queries to group results Imagine that you would like to group results not by using queries or field contents, but instead you would like to use a value returned by a function query.
Imagine you could group documents on the basis of their distance from a point.
Sounds good, Solr allows that and in the following recipe we will see how we can use a simple function query to group results.
Getting ready In this chapter we will use the same index structure and test data we used in the Sorting results by a function value recipe in this chapter.
We will also use some knowledge that we gained in the Using field values to group results recipe in this chapter.
I assume that we would like to have our documents grouped on the basis of the distance from a given point (in real life we would probably like to have some kind of bracket calculated, but let's skip that for now)
As we are using the same index structure and test data as we used in the Sorting results by a function value recipe in this chapter, we'll start with the query.
In order to achieve what we want we send the following query:
The following results were returned by Solr after running the preceding query:
Everything worked as it should have, so now let's see how it worked.
As you can see, the query is very similar to the one we used when grouping our documents on but this time in addition to that we pass the group.func parameter with the value, that is, our function query based on whose results Solr should group our documents.
If you look at the results, they are again very similar to the ones seen in grouping on the basis of field values.
The only difference is in the names of the groups.
When using the field values for grouping, groups were named after the used field names.
However, when using function queries to group documents, groups are named by the result of the function query.
So in our case, we have three groups because our function query returned three different results, as illustrated in the following list:
Introduction Every Solr deployment will, sooner or later, have some kind of problem.
It doesn't matter if the deployment is small and simple or if it's a big and complicated deployment containing multiple servers and shards.
In this chapter I'll try to help you with some of the problems you can run into when running Solr.
I hope this will help you and make your task easier.
How to deal with too many opened files Sometimes you might encounter a strange error, something that lies on the edge between Lucene and the operating system—the "too many files opened" exception.
Is there something we can do about it? Yes, we can, and this recipe will show you how.
The following steps show how to deal with too many opened files:
So, for the purpose of the recipe let's assume that the header of the exception thrown by Solr looks like this: java.io.FileNotFoundException: /use/share/solr/data/index/_1.fdx (Too many open files)
What can you do instead of pulling your hair out? First of all, this probably occurred on a Unix-/Linux-based operating system.
So, let's start with setting the opened files' limit higher.
The probable cause of the "too many files opened" exception is the number of files the index is built of.
The more segments the index is built of, the more files will be used.
The next thing sometimes worth considering is lowering the mergeFactor parameter.
To make things simple, the lower the mergeFactor setting, the fewer files will be used to construct the index (please read the How it works...
We modify the following line in the solrconfig.xml file and set it with the appropriate value (2 in our case):
After we set that configuration value, we need to run the optimization of the index.
We don't discuss the operating system's internal working in this book, but in this section we will make an exception.
The mentioned limits.conf file in the /etc/security directory lets you specify the opened files limit for the users of your system.
In the example shown earlier, we set the two necessary limits to 32000 for the user solr, so if you had problems with the number of opened files in the default setup you should see the difference after restarting Solr.
However, remember that if you are working as the user and you change the limits then you may need to log out and log in again to see those changes.
This configuration parameter lets you determine how often Lucene segments will be merged.
The lower the value of mergeFactor, the smaller the number of index files will be.
However, you have to remember that having a small mergeFactor value will lead to more background merges being done by Lucene, and thus the indexing speed will be lower compared to the ones with a higher mergeFactor value and your node's I/O system will be used more extensively.
On the other hand, lower values of mergeFactor will speed up searching.
How to deal with out-of-memory problems As with every application written in Java, sometimes memory problems happen.
When talking about Solr, those problems are usually related to heap size.
They usually happen when the heap size is too low.
This recipe will show you how to deal with those problems and what to do to avoid them.
Let's consider what to do when we see an exception like this:
Firstly, you can do something to make your task easier.
You can add more memory that the Java virtual machine can use if you have some free physical memory available in your system.
To do that, you need to add the Xmx and, preferably, the Xms parameter to the start-up script of your servlet container (Apache Tomcat or Jetty)
To do that, I used the default Solr deployment and modified the parameters.
This is how Solr was run with more than the default heap size:
So what do the Xmx and Xms Java virtual machine parameters do? The Xms parameter specifies how much heap memory should be assigned by the virtual machine at the start and thus this is the minimal size of the heap memory that will be assigned by the virtual machine.
The Xmx parameter specifies the maximum size of the heap.
The Java virtual machine will not be able to assign more memory for the heap than the Xmx parameter.
You should remember one thing—sometimes it's good to set the Xmx and Xms parameters to the same values.
It will ensure that the virtual machine won't be resizing the heap size during application execution and thus won't lose precious time in heap resizing.
One additional thing—be careful when setting the heap size to be too big.
It is usually not advised to give the heap size more than 60 percent of your total memory available in the system, because your operating system's I/O cache will suffer.
There are a few more things I would like to discuss when it comes to memory issues.
Monitoring heap when an out-of-memory error occurs If the out-of-memory errors occurs even after the actions you've done, you should start monitoring your heap.
One of the easiest ways to do that is to add the appropriate Java virtual machine parameters.
Those two parameters tell the virtual machine to dump the heap on the out-of-memory error and write it to a file created in the specified directory.
So the default Solr deployment's start command would look like this:
Reducing the amount of memory needed by Solr However there are times (even if your system has a large amount of memory available), when you may be forced to think about Solr memory consumption reduction.
In such cases there is no general advice, but these are a few things that you can keep in mind:
How to sort non-English languages properly As you probably already know, Solr supports UTF-8 encoding and thus can handle data in many languages.
But, if you ever needed to sort some languages that have characters specific to them you probably know that it doesn't work well on a standard Solr string type.
This recipe will show you how to deal with sorting in Solr.
These steps tell us how to sort non-English languages properly:
For the purpose of this recipe, I have assumed that we will have to sort text that contains Polish characters.
To show the good and bad sorting behaviour we need to create the following index structure (add this to your schema.xml file):
Here is how they are defined (add this after the fields section in the schema.xml file):
The last thing about the schema.xml file is the new type.
First, let's take a look at how the incorrect sorting order looks.
To do this, we send the following query to Solr:
And now the response that was returned for the query is as follows:
Now let's send the query that should return the documents sorted in the correct order.
As you can see the order is different and believe me it's correct.
Every document in the index is built on four fields.
The id field is responsible for holding the unique identifier of the document.
The name field is responsible for holding the name of the document.
The name_sort_bad field is nothing new; it's just a field based on string, which is used to perform sorting.
The field is based on the solr.TextField type and on solr.
KeywordTokenizerFactory, which basically means that our text won't be tokenized.
We used this trick because we want to sort on that field and thus we don't want the data in it to be tokenized, but we want to use a special filter on that field.
The filter that allows Solr to sort correctly is the solr.CollationKeyFilterFactory filter.
First, the language attribute, which tells Solr about the language of the field.
The second attribute is country which tells Solr about the country variant (this can be skipped if necessary)
The strength attribute informs Solr about the collation strength used.
More information about those parameters can be found in the JDK documentation.
One thing that is crucial is that you need to create an appropriate field and set the appropriate attribute's value for every non-English language you want to sort on.
The two queries you can see in the examples differ in only one thing, the field used for sorting.
When sorting on this field, the document order will be incorrect when there are non-English characters present.
However, when sorting on the name_sort_good field everything will be in the correct order as shown in the example.
How to make your index smaller There may be situations where you would like to make your index smaller.
The reasons may be different—you may want to have a smaller index so that it would fit into the operating system's I/O cache or you want to store your index in RAMDirectory.
This recipe will try to help you with the process of index slimming.
The following steps tell us how to make your index smaller:
For the purpose of this recipe, I assumed that we will have four fields that describe the document.
I created the following index structure (add this to your schema.xml file): />
Now, the last thing to do is add the term options.
I've indexed 1,000,000 sample documents with the use of the original schema.xml file.
After changing the schema.xml file and indexing the same data the index size was 84,301,603 bytes.
So as you can see, the index size was reduced.
Now let's see why we see this reduction in the index size.
The first schema.xml file you see is the standard index structure provided with Solr example deployment, at least when talking about the types.
We have four fields, all of them indexed and stored, which means all of them are searchable and are shown in the result list.
First of all we only need to search on the name and description fields, which mean that the rest of the fields can be set up as not indexed searchable, as we need that to avoid duplicates.
When the indexed attribute is set to false, the information in that field is not indexed which basically means that it isn't written into the Lucene-inverted index and thus it is not available; this saves index space.
Of course you can't set this attribute to false if you need this field to be searchable.
The second requirement tells us what fields we are obligated to show in the search results.
Those field are the ones that need the stored attribute set to true, and the rest can have this attribute set to false.
Setting this attribute to true on many fields will increase the index size substantially.
The last requirement is actually information; we don't need to worry about highlighting functionality so we can reduce the index size in a greater way.
To do that we add the attributes to the name and description fields.
By doing that we tell Solr not to store any information about terms in the index.
This basically means that we can't use the highlighting functionalities of Solr, but we have reduced our index size substantially and we don't need highlighting.
If we don't need index time boosting and we do not care about length normalization, we could based on the text type (for primitive types such as string, integer, and so on it's turned on by default in Solr 4.0)
This would shrink the index a bit more and in addition to that save us some memory during queries.
Every time you think about reducing the index size, first do the optimization, then look at your schema.xml file and see if you need all those fields.
Then check which fields shouldn't be stored and which you can omit when indexing.
The last thing should be removing the information about terms, because there may come a time when you will need this information and the only thing you will be able to do is a full indexation of millions of documents.
There is one additional thing I would like to mention.
Estimating your index size and memory usage Sometimes it's necessary to have a rough estimate of the index size and the memory usage of your Solr instance.
Currently there is a draft of the Microsoft Excel spreadsheet that lets you do that kind of estimation.
If you are interested in it, download the following file: http://svn.apache.org/repos/asf/lucene/dev/trunk/dev-tools/sizeestimator-lucene-solr.xls.
Diagnosing Solr problems There are many tools out there that can help you diagnose problems with Solr.
You can monitor your operating system by yourself by using different operating system commands such as vmstat, dstat, and iostat.
You can use different Java tools such as jconsole and jvisualvm to look at the JMX mbeans, you can monitor your garbage collector work, and so on.
However in order to properly diagnose what's happening with your Apache Solr cluster you'll need to see the whole view as well as the specifics.
There are different tools out there that you can use, however this recipe will show you what you can find in one of them—Scalable Performance Monitoring.
Getting ready This recipe assumes that you have Scalable Performance Monitoring installed and running.
If you don't, please go to http://sematext.com/spm/index.html, create a free account, and download the client that's suitable for you.
The installation is very simple and you'll be guided by the Scalable Performance Monitoring installer from the beginning to the end.
This is an overview of the system, however we would like to see some details.
By now we know what our index and Solr caches' usage looks like and we know if we need to tune them or not, so now let's look at the query rate and its latency:
Here we can see the warm-up queries' time and execution:
We've got all the information that is connected to queries, so now we can go and see the other crucial information such as memory and CPU usage, Java heap usage, and how Java garbage collector works.
And finally we can see how the garbage collector works:
That's all we need during the usual work when we want to see how different parts of Solr work.
If we would like to go in depth and see how the I/O subsystem works or the swap usage we can use other aggregated reports available in any of the monitoring systems, or you could just use the appropriate system commands like the ones mentioned in the introduction to the recipe.
Let's discuss the provided statistics in a bit more dtail.
On the first screenshot provided you can see the overview of the system.
This part of Scalable Performance Monitoring will be shown to you as soon as you log in to the system.
You'll usually use it to get the whole idea about the system, but you'll want to look at the detailed reports in order to see a higher granularity of your data.
On the second screenshot you can see the index statistics (or indices depending on the options you've chosen)
You can see the information about the number of documents in the index, the maximum size of the index, the number of segments, and the delta, which is calculated as the maximum number of documents minus the current number of documents.
Not shown on the screenshot are the filesystem statistics which tell you about the size of the index on the disk.
With the use of this data you can see the complete information about your core's or collection's disk drive usage.
The third screenshot is one of the most important ones—the information about Apache Solr caches.
Although I haven't shown all the information here, you can see a single cache on the screenshot—the query result cache (we didn't show the document cache and the filter cache)
You can see information about the size of the cache, the maximum size of the cache, the number of evictions, and so on.
Remember, if your cache is too low, its size will be equal to the maximum size and you'll start seeing evictions, which is not good and you'll probably want to change the cache configuration.
The query rate and latency report shown in the fourth screenshot provides information about the number of queries and their average latency.
You can see how your queries were executed and if you need to start thinking about the optimization of your system.
In order to check how your warm-up queries were executed you can look at the fifth of the provided screenshots.
You can see the amount of time for which your warm-up queries were executed and how long it took to auto-warm your query result cache and your filter cache.
This information can be valuable when dealing with problems such as Solr hanging during the opening of new or first searches.
The last three screenshots provide the information that is not directly connected to Apache Solr, but very valuable from our point of view, when we have to see what is happening with our Solr instance.
The sixth screenshot shows information about the CPU and memory usage.
For the CPU information you can see how it works; the percent of time spent idling, working on user-run software, working with operating system software, handling interruptions, and so on.
If you look at the memory graph you will find the total, used, free, cached, and buffered statistics.
That's basically all you need in order to see if your CPU is 100 percent utilized and how your system memory is utilized.
This is crucial when your system is not working in the way that you would like it to.
The seventh screenshot provides information about the Java virtual machine.
You can see the heap memory statistics and the threading information (which is not shown in the screenshot)
The heap usage graph allows us to see if the amount of memory we specified for our Solr instance is enough to handle all the operations that need to be done.
The final screenshot provides information about how your JVM garbage collector works.
In most situations you will want it to run more frequently, but for a shorter period of time stop the world events which may cause your Solr instances to stop handling queries or indexing for a short period of time.
To sum up, all the information can be gathered manually by using different system and Java tools.
The crucial part of every monitoring system is the ability to show you graphs that will let you point to a certain event in time.
We looked at a single monitoring solution, but there are many more available and if you don't like Scalable Performance Monitoring you can use any available.
One more thing; please remember that we only scraped the surface in this recipe and the book (or e-book) you are holding in your hands doesn't describe all the information regarding monitoring and dealing with problems.
However I hope that this recipe will help you at least start with this topic.
If you don't want to use Scalable Performance Monitoring, you can choose some other technology that is available like Ganglia (http://ganglia.sourceforge.net/), Mumin (http://munin-monitoring.org/), Zabix (http://www.zabbix.com/), Cacti (http:// www.cacti.net/), or any commercial ones like New Relic (http://newrelic.com/)
How to avoid swapping One of the crucial things when running your Solr instance in production is performance.
What you want is to give your clients relevant results in the blink of an eye.
If your clients have to wait for results for too long, some of them may choose other vendors or sites that provide similar services.
One of the things to remember when running a Java application such as Apache Solr is to ensure that the operating system won't write the heap to disk.
This ensures that the part of the memory used by Solr won't be swapped at all.
This recipe will show you how to achieve that on a Linux operating system.
Getting ready Please note that the following recipe is only valid when running Apache Solr on a Linux operating system.
In addition to that, please be advised that turning off swapping should only be done when you have enough memory to handle all the necessary application in your system and you want to be sure that there won't be any swapping.
In order to do that let's look at the main page of the Solr administration panel:
As you can see some swap memory is being used.
In order to demonstrate how to turn off swap usage I've freed some memory on the virtual machine I was using for tests and after that I've run the following commands:
After the second command is done running, refresh the main page of the Solr admin instance and this is what it will show:
It seems like it is working, but in order to be sure I've run the following command:
And again we can see that there is no swap usage.
On the first provided screenshot you can see that there is a bit more than 183 MB of swap memory being used.
This is not good; in a production environment you want to avoid swapping, of course, if you have the necessary amount of memory.
Swapping will make the contents of the memory to be written onto the hard disk drive, thus making your operating system and applications execute slower.
So, in order to turn off swapping in a Linux operating system, we've run two commands.
The first one sets the vm.swappiness operating system property to 0, which means that we want to avoid swapping.
We needed to use sudo, because in order to set that property with the use of the sysctl command we need administration privileges.
The second command (the / sbin/swapoff -a one) disables swapping on all known devices.
As you can see on the second screenshot, the Solr administration panel didn't even include the swapping information so we may suspect that it was turned off.
However in order to be sure, we've used another Linux command, the free command with the -m switch, in order to see the memory usage on our system.
As you can see, the Swap section shows 0, so we can now be sure that swapping was turned off.
Introduction In the previous nine chapters, we discussed about the different Apache Solr functionalities and how to overcome some common problems and situations.
However, I decided that we will describe a few of the most common problems that arise on the Apache Solr mailing list and during our work with our clients.
This chapter is dedicated to describing how to handle such situations, and I hope that you'll find it useful.
You can find it in most e-commerce sites, on Google, Bing, and so on.
It enables your users or clients to find what they want and do it fast.
In most cases, the autocomplete functionality also increases the relevance of your search by pointing to the right author, title, and so on, right away without looking at the search results.
What's more, sites that use autocomplete report higher revenue after deploying it in comparison to the situation before implementing it.
Seems like a win-win situation, both for you and your clients.
So, let's look at how we can implement a product's autocomplete functionality in Solr.
Let's assume that we want to show the full product name whenever our users enter a part of the word that the product name is made up of.
In addition to this, we want to show the number of documents with the same names.
Let's start with an example data that is going to be indexed:
We will need two main fields in the index – one for the document identifier and one for the name.
We will need two additional fields – one for autocomplete and one for faceting that we will use.
So, our index structure will look similar to the following code snippet (we should add it to the schema.xml fields section):
So, we should add the following copy fields section to the schema.xml file:
Now, if we would like to show all the products that start with the word sol to our users, we would send the following query:
As you can see, the faceting results returned by Solr are exactly what we were looking for.
Our example documents are pretty simple – they are only built of an identifier and a name that we will use to make autocomplete.
The first two fields are the ones that you would have expected – they are used to hold the identifier of the document and its name.
The name_show field is based on a string type, because we want to have a single token per name when using faceting.
With the use of the copy field sections, we can let Solr automatically copy the values of the fields defined by the source attribute to the field defined by the dest field.
During query time, we divide the entered query on the basis of white space characters using solr.WhitespaceTokenizerFactory, and we lowercase the tokens with the use of solr.LowerCaseFilterFactory.
For query time, this is what we want because we don't want any more processing.
For index time, we not only use the same tokenizer and filter, but also solr.NGramFilterFactory.
This is because we want to allow our users to efficiently search for prefixes, so that when someone enters the word sol, we would like to show all the products that have a word starting with that prefix, and solr.NGramFilterFactory allows us to do that.
For the word solr, it will produce the tokens s, so, sol, and solr.
We've also said that we are interested in grams starting from a single character (the minGramsSize property) and the maximum size of grams allowed is 25 (the maxGramSize property)
As you can see, we've sent the prefix of the word that the users have this, we've also said that we want words in our query to be connected with the logical AND operator (the q.op parameter), and that we are not interested in the search results (the because we need the information about the number of documents with the same titles, so also only interested in faceting a calculation for the values that have at least one document.
As you can see, we've got two distinct values in the faceting results; both with a single document with the same title, which matches our sample data.
Sometimes we are not just interested in our product's name for autocomplete.
Imagine that we want to show the category of our products in the autocomplete box along with the number of products in each category.
Let's see how we can use faceting to do that.
This recipe will show how we can implement a category's autocomplete functionality.
Let's start with the example data, which is going to be indexed and which looks similar to the following code snippet:
The fields section of the schema.xml configuration file that can handle the preceding data should look similar to the following code snippet:
One final thing is the text_lowercase type definition, which should be placed in the types section of the schema.xml file.
So now, if we would like to get all the categories that start with boo, along with the number of products in those categories, we would send the following query:
As you can see, we have two categories, each containing a single product.
We have three fields for each of our documents – one for the identifier fields, one for holding the name of the document, and one for its category.
We will use the category field to do the autocomplete functionality, and we will use faceting for it.
What it does is that it stores the category as a single token in the index because of solr.KeywordTokenizerFactory.
This is because we want to send the lowercased queries while using faceting.
How to use different query parsers in a single query.
Sometimes, it is good to be able to choose different query parsers in the same query.
For example, imagine that you would like to use the Extended DisMax query parser for the main query, but in addition to this, we would like to use the field query parser for filter queries.
This recipe will show how we can use different query parsers in a single query.
Let's start with the following index structure (this should go to the field section in the schema.xml file):
So, if we search for all the documents using the Extended DisMax query parser and want to narrow our results to the Books And Tutorials category, then we can send the following query:
Our index structure and example data are not that relevant for this recipe, so I'll skip discussing them.
What we want to achieve is be sure that the data we filter will be properly processed, and we want to avoid thinking about any kind of query parsing and Lucene special characters escaping.
In order to do this, we use the term query parser.
To inform Solr that we want to use this query parser in the filter query (the fq parameter), we use local parameter syntax part of the filter query says which query parser we want to use, and the f property specifies the field to which we want to send the provided Books And Tutorials value.
That's all; as you can see in the provided results, everything works as intended.
How to get documents right after they were sent for indexation.
Let's say that we would like to get our documents as soon as they were sent for indexing, but without any commit (both hard and soft) operation occurring.
Solr 4.0 comes with a special functionality called real-time get, which uses the information of uncommitted documents and can return them as documents.
This recipe will show how we can get documents right after they were sent for indexation.
Let's begin with defining the following index structure (add it to the field section in your schema.xml file):
In addition to this, we need the _version_ field to be present, so let's also add the following field to our schema.xml file in its field section:
The third step is to turn on the transaction log functionality in Solr.
In order to do this, we should add the following section to the updateHandler configuration section (in the solrconfig.xml file):
The last thing we need to do is add a proper request handler configuration to our solrconfig.xml file:
In order to do this, let's index the following document (which we've stored in the data.xml file):
In order to do this, we run the following query:
As you can imagine, we didn't get any documents returned, because we didn't send any commit command – not even the soft commit one.
As you can see, our document is returned by our get handler.
The real-time get functionality needs that field to be present in our documents, because the transaction log relies on it.
However, as you can see in the provided example data, we don't need to worry about this field, because its filled and updated automatically by Solr.
But let's backtrack a bit and discuss the changes made to the solrconfig.xml file.
The first one is the update log (the updateLog section), which Solr uses to store the so-called transaction log.
Solr stores recent index changes there (until hard commit), in order to provide write durability, consistency, and the ability to provide the real-time get functionality.
The second thing is the handler we defined under the name of /get with the use of the solr.RealTimeGetHandler class.
It uses the information in the transaction log to get the documents we want by using their identifier.
It can even retrieve the documents that weren't committed and are only stored in the transaction log.
So, if we want to get the newest version of the document, we can use it.
The other configuration parameters are the same as with the usual request handler, so I'll skip commenting them.
The next thing we do is send the update command without adding the commit command, so that we shouldn't be able to see the document during a standard search.
If you look at the results returned by the first query, you'll notice that we didn't get that document.
However, when using the /get handler that we previously defined, we get the document we requested.
This is because Solr uses the transaction log in order to even the uncommitted document.
How to search your data in a near real-time manner.
Sometimes, we need our data to be available as soon as possible.
Imagine that we have a SolrCloud cluster up and running, and we want to have our documents available for searching with only a slight delay.
For example, our application can be a content management system where it would be very weird if a user adds a new document, and it would take some time for it to be searchable.
In order to achieve this, Solr exposes the soft commit functionality, and this recipe will show you how to set it up.
This recipe will show how we can search for data in a near real-time manner.
For the purpose of this recipe, let's assume that we have the following index structure (add it to the field section in your schema.xml file):
In addition to this, we need to set up the hard and soft automatic commits, for which we will need to add the following section to the updateHandler section in the solrconfig.xml file:
In order to do this, let's index the following document (which we've stored in the data.xml file):
But, let's check that out by running the following simple search command:
As you may know, the standard commit operation is quite resource-intensive – it flushes the changes since the last commit to the disk to the new segment.
If you would like to do that every second, we could run into a problem of a very high amount of I/O writes and thus our searches would suffer (of course, this depends on the situation)
As we are usually lazy and don't want to remember when it's time to send the commit and when to use soft commit, we'll let Solr manage that so we properly need to configure the update handler.
First, we add the standard auto commit by adding the autoCommit section and saying that we want to commit after every 60 seconds (the maxTime property is specified in milliseconds), and that we don't want to reopen the searcher after the standard commit (the openSearcher property is set to false)
The next thing is to configure the soft auto commit functionality by adding the softAutoCommit section to the update handler configuration.
We've specified that we want the soft commit to be fired every second (the maxTime property is specified in milliseconds), and thus our searcher will be reopened every second if there are changes.
As you can see, even though we didn't specify the commit command after our update command, we are still able to find the document we've sent for indexation.
How to get the documents with all the query words to the top of the results set.
One of the most common problems that users struggle with when using Apache Solr is how to improve the relevancy of their results.
Of course, relevancy tuning is, in most cases, connected to your business needs, but one of the common problems is to have documents that have all the query words in their fields at the top of the results list.
You can imagine a situation where you search for all the documents that match at least a single query word, but you would like to show the ones with all the query words first.
This recipe will show how we can get the documents with all the query words to the top of the results set.
Let's start with the following index structure (add it to the field section in your schema.xml file):
The second step is to index the following sample data: This is a book about Solr and Lucene.
Let's assume that our usual queries look similar to the following code snippet:
Nothing complicated; however, the results of such query don't satisfy us, because they look similar to the following code snippet:
In order to change this, let's introduce a new handler in our solrconfig.xml file:
For the purpose of the recipe, we've used a simple index structure that consists of a document identifier, its name, and description.
Our data is very simple as well; it just contains two documents.
During the first query, the document with the identifier 1 is placed at the top of the query results.
However, what we would like to achieve is be able to boost the name.
In addition to this, we would like to have the documents with words from the query close to each other at the top of the results.
In order to do this, we've defined a new request handler named /better, which will leverage the local params.
The first thing is the defined q parameter, which is the standard query.
It uses the Extended DisMax parser (the {!edismax part of the query), and defines several additional parameters:
We tell Solr that we will provide the fields by specifying the qfQuery parameter by using the $qfQuery value.
We tell Solr that we will provide the fields by specifying the mmQuery parameter, by using the $mmQuery value.
Similar to the previous parameters that we've specified, we will provide the fields by specifying the pfQuery parameter, by using the $pfQuery value.
Again, we use the parameter dereferencing functionality and tell Solr that we will provide the value in the bqQuery parameter, by using the $bqQuery value.
Basically, the preceding queries say that we will use the edismax query parser, phrase, and boost queries.
The first thing is the qfQuery parameter, which is exactly the same as the qf parameter in the first query we sent to Solr.
Using it, we just specify the fields that we want to be searched and their boosts.
Next, we have the mmQuery parameter set to 1 that will be used as mm in edismax, which means that a document will be considered a match when a single word from the query will be found in it.
As you will remember, the pfQuery parameter value will be passed to the pf parameter, and thus the phrase query will be automatically made on the fields defined in those fields.
Now, the last and probably the most important part of the query, the boostQuery parameter, specifies the value that will be passed to the bq parameter.
Our boost query is very similar to our main query, however, we say that the query should only match the documents that have that match that query should be boosted by adding the ^100000 part at the end of it.
To sum up all the parameters of our query, they will promote the documents with all the words from the query present in the fields we want to search on.
In addition to this, we will promote the documents that have phrases matched.
So finally, let's look at how the newly created handler work.
As you can see, when providing our query to it with the mainQuery parameter, the previous document is now placed as the first one.
Imagine that you would like to place documents that are newer above the ones that are older.
For example, you have a book store and want to promote the books that have been published recently, and place them above the books that have been present in our store for a long time.
Solr lets us do this, and this recipe will show you how.
This recipe will show how we can boost documents based on their publishing date.
Let's begin with the following index structure (add it to the field section in your schema.xml file):
For the preceding query, Solr will return the following results:
As you can see, the newest document is the second one, which we want to avoid.
So, we need to change our query to the following one:
Our index structure consists of three fields; one responsible for holding the identifier of the document, one for the name of the document, and the last one; the one which we will be most interested in, in which we hold the publishing date.
The published field has one nice feature – if we don't define it in the document and send it for indexation, then it will get the value of the date and time when it is processed (the.
As you can see, the first query that we sent to Solr returned results not in a way we would like them to be sorted.
Of course, we could have sorted them by date, but we don't want to do that, because we would like to have the most recent and the most relevant documents at the top, not only the newest ones.
In order to achieve this, we use the bf (boost function) parameter.
At first, it can look very complicated, but it's not.
In order to boost our documents, we use the recip(ms(NOW/HOUR,published),3.16e-11,1,1) function query.
We've also used NOW/HOUR to reduce the precision of the published field, in order for our function query to consume less memory and because we don't need that granularity; our results will be just fine.
As you can see, our query with the bf parameter and the time-based function query work as intended.
If you want to read more about function queries, please refer to the http://wiki.apache.
Apache Tomcat, running on different port  13 issues, Jetty servlet container.
About Packt Publishing Packt, pronounced 'packed', published its first book "Mastering phpMyAdmin for Effective MySQL Management" in April 2004 and subsequently continued to specialize in publishing highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting and customizing today's systems, applications, and frameworks.
Our solution based books give you the knowledge and power to customize the software and technologies you're using to get the job done.
Packt books are more specific and less general than the IT books you have seen in the past.
Our unique business model allows us to bring you more focused information, giving you more of what you need to know, and less of what you don't.
Packt is a modern, yet unique publishing company, which focuses on producing quality, cuttingedge books for communities of developers, administrators, and newbies alike.
This book is part of the Packt Open Source brand, home to books published on software built around Open Source licences, and offering information to anybody from advanced developers to budding web designers.
The Open Source brand also runs Packt's Open Source Royalty Scheme, by which Packt gives a royalty to each Open Source project about whose software a book is sold.
Writing for Packt We welcome all inquiries from people who are interested in authoring.
If your book idea is still at an early stage and you would like to discuss it first before writing a formal book proposal, contact us; one of our commissioning editors will get in touch with you.
We're not just looking for published authors; if you have strong technical skills but no writing experience, our experienced editors can help you develop a writing career, or simply get some additional reward for your expertise.
Enhance your search with faceted navigation, result highlighting relevancy ranked sorting, and more.
Advice on data modeling, deployment considerations to include security, logging, and monitoring, and advice on scaling Solr and measuring performance.
Move large amounts of data into HBase and learn how to manage it efficiently.
Set up HBase on the cloud, get it ready for production, and run it smoothly with high performance.
Realistic, simple code examples to solve problems at scale with Hadoop and related technologies.
Solutions to common problems when working in the Hadoop environment.
In depth code examples demonstrating various analytic models, analytic solutions, and common best practices.
Over 150 recipes to design and optimize large-scale Apache Cassandra deployments.
Get the best out of Cassandra using this efficient recipe bank.
Well illustrated, step-by-step recipes to make all tasks look easy!
Chapter 2: Indexing Your Data Introduction Indexing PDF files Generating unique fields automatically Extracting metadata from binary files How to properly configure Data Import Handler with JDBC Indexing data from a database using Data Import Handler How to import data using Data Import Handler and delta query How to use Data Import Handler with the URL data source How to modify data while importing with Data Import Handler Updating a single field of your document Handling multiple currencies Detecting the document's language Optimizing your primary key field indexing.
Chapter 3: Analyzing Your Text Data Introduction Storing additional information using payloads Eliminating XML and HTML tags from text Copying the contents of one field to another Changing words to other words Splitting text by CamelCase Splitting text by whitespace only Making plural words singular without stemming Lowercasing the whole string Storing geographical points in the index Stemming your data Preparing text to perform an efficient trailing wildcard search Splitting text by numbers and non-whitespace characters Using Hunspell as a stemmer Using your own stemming dictionary Protecting words from being stemmed.
Chapter 4: Querying Solr Introduction Asking for a particular field value Sorting results by a field value How to search for a phrase, not a single word Boosting phrases over words Positioning some documents over others on a query Positioning documents with words closer to each other first Sorting results by a distance from a point Getting documents with only a partial match Affecting scoring with functions Nesting queries Modifying returned documents Using parent-child relationships Ignoring typos in terms of performance Detecting and omitting duplicate documents Using field aliases Returning a value of a function in the results.
Chapter 5: Using the Faceting Mechanism Introduction Getting the number of documents with the same field value Getting the number of documents with the same value range Getting the number of documents matching the query and subquery Removing filters from faceting results Sorting faceting results in alphabetical order Implementing the autosuggest feature using faceting Getting the number of documents that don't have a value in the field Having two different facet limits for two different fields in the same query Using decision tree faceting Calculating faceting for relevant documents in groups.
Chapter 6: Improving Solr Performance Introduction Paging your results quickly Configuring the document cache Configuring the query result cache Configuring the filter cache Improving Solr performance right after the startup or commit operation Caching whole result pages Improving faceting performance for low cardinality fields What to do when Solr slows down during indexing Analyzing query performance Avoiding filter caching Controlling the order of execution of filter queries Improving the performance of numerical range queries.
Chapter 7: In the Cloud Introduction Creating a new SolrCloud cluster Setting up two collections inside a single cluster Managing your SolrCloud cluster Understanding the SolrCloud cluster administration GUI Distributed indexing and searching Increasing the number of replicas on an already live cluster Stopping automatic document distribution among shards.
Chapter 8: Using Additional Solr Functionalities Introduction Getting more documents similar to those returned in the results list Highlighting matched words How to highlight long text fields and get good performance Sorting results by a function value Searching words by how they sound Ignoring defined words Computing statistics for the search results Checking the user's spelling mistakes Using field values to group results Using queries to group results Using function queries to group results.
Chapter 9: Dealing with Problems Introduction How to deal with too many opened files How to deal with out-of-memory problems How to sort non-English languages properly How to make your index smaller Diagnosing Solr problems How to avoid swapping.
Appendix: Real-life Situations Introduction How to implement a product's autocomplete functionality How to implement a category's autocomplete functionality How to use different query parsers in a single query How to get documents right after they were sent for indexation How to search your data in a near real-time manner How to get the documents with all the query words to the top of the results set How to boost documents based on their publishing date.
