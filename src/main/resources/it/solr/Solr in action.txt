Please post comments or corrections to the Author Online forum:
Copyright 2013 Manning Publications  For more information on this and other Manning titles go to  www.manning.com.
Please post comments or corrections to the Author Online forum:
Please post comments or corrections to the Author Online forum:
Characteristics of the types of data handled by search engines.
With fast-growing technologies like social media, cloud computing, mobile applications, and big data, these are exciting times to be in computing.
One of the main challenges facing software architects is the need to handle massive volumes of data consumed and produced by a huge global user base.
In addition, users expect online applications to always be available and responsive.
To address the scalability and availability needs of modern web applications, we’ve seen a growing interest in specialized, non-relational data storage and processing technologies, collectively known as NoSQL (Not only SQL)
These systems share a common design pattern of matching the storage and processing engine to specific types of data rather than forcing all data into the once de facto standard relational model.
In other words, NoSQL technologies are optimized to solve a specific class of problems for specific types of data.
The need to scale has led to hybrid architectures composed of a variety of NoSQL and relational databases; gone are the days of the one-size-fits-all data processing solution.
This book is about a specific NoSQL technology, Apache Solr, which, like its non-relational brethren, is optimized for a unique class of problems.
Specifically, Solr is a scalable, readyto-deploy enterprise search engine that’s optimized to search large volumes of text-centric data and return results sorted by relevance.
That was a bit of a mouthful, so let’s break the previous statement down into its basic parts:
Please post comments or corrections to the Author Online forum:
Scalable—Solr scales by distributing work (indexing and query processing) to multiple servers in a cluster.
Ready to deploy—Solr is open-source, is easy to install and configure, and provides a preconfigured example to help you get started.
Large volumes of documents—Solr is designed to deal with indexes containing millions of documents.
Text-centric—Solr is optimized for searching natural language text, like emails, web pages, resumes, PDF documents, and social messages like tweets or blogs.
Results sorted by relevance—Solr returns documents in ranked order based on how relevant each document is to the user’s query.
In this book, you’ll learn how to use Solr to design and implement scalable search solutions.
We’ll begin our journey by learning about the types of data and uses cases Solr supports.
This will help you understand where Solr fits into the big picture of modern application architectures and which problems Solr is designed to solve.
Therefore, rather than speculate on why you’re considering Solr, we’ll get right down to the hard questions you need to answer about your data and use cases in order to decide if a search engine is right for you.
In the end, it comes down to understanding your data and users and then picking a technology that works for both.
Let’s start by looking at the properties of data that a search engine is optimized to handle.
A hallmark of modern application architectures is matching the storage and processing engine to your data.
If you’re a programmer, then you know to select the best data structure based on how you use the data in an algorithm, that is, you don’t use a linked list when you need fast random lookups.
There are four main characteristics of data search engines like Solr are optimized to handle.
It goes without saying that Solr can deal with large volumes of data.
Please post comments or corrections to the Author Online forum:
Although these are the four main characteristics of data that search engines like Solr handle efficiently, you should think of them as rough guidelines and not strict rules.
Let’s dig into each of these characteristics to see why they’re important for search.
For now, we’ll focus on the high-level concepts and get into the “how” in later chapters.
We think “unstructured” is a little ambiguous because any text document based on human language has implicit structure.
You can think of the term “unstructured” as being from the perspective of a computer, which sees text as a stream of characters.
The character stream must be parsed using language-specific rules to extract the structure and make it searchable, which is exactly what search engines do.
We think the label “text-centric” is more appropriate for describing the type of data Solr handles because a search engine is specifically designed to extract the implicit structure of text into its index to improve searching.
Text-centric data implies that the text of a document contains “information” that users are interested in finding.
Of course, a search engine also supports non-text data like dates and numbers, but its primary strength is handling text data based on natural language.
Also, the “centric” part is important because if users aren’t interested in the information in the text, then a search engine may not be the best solution for your problem.
For example, consider an application where employees create travel expense reports.
Each report contains a number of structured data fields like date, expense type, currency, and amount.
In addition, each expense may include a notes field where employees can provide a brief description of the expense.
This would be an example of data that contains text but isn’t “text-centric” in that it’s unlikely that the accounting department needs to search the notes field when generating monthly expense reports.
Put simply, despite data containing text fields doesn’t mean it’s a natural fit for a search engine.
Take a moment and think about whether your data is “text-centric,” The main consideration for us is whether or not the text fields in your data contain information that users will want to query.
If yes, then a search engine is probably a good choice.
First, though, let’s be clear that Solr does allow you to update existing documents in your index.
You can think of read-dominant as meaning that documents are read much more often then they’re created or updated.
But don’t take this to mean that you can’t write a lot of data or that you have limits on how frequently you can write new data.
In fact, one of the key features in Solr 4 is near-real-time search, which allows you to index thousands of documents per second and have them be searchable almost immediately.
Please post comments or corrections to the Author Online forum:
The key point behind read-dominant data is that when you do write data to Solr, it’s intended to be read and reread many, many times over its lifetime.
You can think of a search engine as being optimized for executing queries (a read operation), for example, as opposed to storing data (a write operation)
Also, if you have to update existing data in a search engine often, then that could be an indication that a search engine might not be the best solution for your needs.
Another NoSQL technology, like Cassandra, might be a better choice when you need fast random-writes to existing data.
In a search engine, a document is a self-contained collection of fields where each field only holds data and doesn’t contain nested fields.
In other words, a document in a search engine like Solr has a flat structure and doesn’t depend on other documents.
The “flat” concept is slightly relaxed in Solr in that a field can have multiple values but fields don’t contain sub-fields.
That is, you can store multiple values in a single field but you can’t nest fields inside of other fields.
The flat, document-oriented approach in Solr works well with data that’s already in document format, such as a web page, blog, or PDF document, but what about modeling normalized data stored in a relational database? In this case, you need to denormalize data spread across multiple tables into a flat, self-contained document structure.
We’ll learn how to approach problems like this in chapter 3
You also want to consider which fields in your documents must be stored in Solr and which should be stored in another system, such as a database.
Put simply, a search engine isn’t the place to store data unless it’s useful for search or displaying results.
For example, if you have a search index for online videos, then you don’t want to store the binary video files in Solr.
Rather, large binary fields should be stored in another system, such as a content distribution network (CDN)
In general, you should store the minimal set of information for each document needed to satisfy search requirements.
This is a clear example of not treating Solr as a general data storage technology; Solr’s job is to find videos of interest and not to manage large binary files.
This means that documents in a search index don’t need to have a uniform structure.
In a relational database, every row in a table has the same structure.
Of course, there should be some overlap between the fields in documents in the same index but they don’t have to be identical.
For example, imagine a search application for finding homes for rent or sale.
Listings will obviously share fields like location, number of bedrooms, and number of bathrooms, but they’ll also have different fields based on the listing type.
A home for sale would have fields for listing price and annual property taxes, whereas a home for rent would have a field for monthly rent and pet policy.
Please post comments or corrections to the Author Online forum:
To summarize, search engines in general and Solr in particular are optimized to handle.
Overall, this implies that Solr isn’t a general-purpose data storage and processing technology, which is one of the main differentiating factors of NoSQL (not only SQL) technologies.
The whole point of having a wide variety of options for storing and processing data is that you don’t have to find a one-size-fits-all technology.
Search engines are good at specific things and quite horrible at others.
This means in most cases, you’re going to find Solr complements relational databases and other NoSQL technologies more than it replaces them.
Now that we’ve talked about the type of data Solr is optimized to handle, let’s think about the primary use cases a search engine like Solr is designed to handle.
These use cases are intended to help you understand how a search engine is different than other data processing technologies.
In this section, we look at some of the things you can do with a search engine like Solr.
As with our discussion of the types of data in section 1.1.1, use these as guidelines and not strict rules.
Before we get into specifics though, you should keep in mind that the bar for excellence in search is high.
Modern users are accustomed to web search engines like Google and Bing being fast and effective at serving modern web information needs.
Moreover, most popular websites have powerful search solutions to help people find information quickly.
When you’re evaluating a search engine like Solr and designing your search solution, make sure you put user experience as a high priority.
But it’s worth mentioning because keyword search is the most typical way users will begin working with your search solution.
It’d be rare for a user to want to fill out a complex search form initially.
Given that basic keyword search will be the most common way users will interact with your search engine, then it stands to reason that this feature must provide a great user experience.
In general, users want to type in a few simple keywords and get back great results.
This may sound like a simple task of matching query terms to documents but consider a few of the issues that must be addressed to provide a great user experience:
Relevant results must be returned quickly, within a second or less in most cases.
Spell correction in case the user misspells some of the query terms.
Auto-suggestions to save users some typing, particularly for mobile applications.
Please post comments or corrections to the Author Online forum:
Phrase handling, that is, does the user want documents matching all words or any of the words in a phrase.
Giving the user a way to see more results if the top results aren’t satisfactory.
As you can see, a number of issues exist that make a seemingly simple feature hard to implement without a specialized approach.
But with a search engine like Solr, these features come out of the box and are easy to implement.
Once you give users a powerful tool to execute keyword searches, you need to consider how to display the results.
This brings us to our next use case of ranking results based on their relevance to the user’s query.
In a SQL query to a relational database, a row either matches a query or not and results are sorted based on one of the columns.
On the other hand, a search engine returns documents sorted in descending order by a score that indicates the strength of the match of the document to the query.
How strength of match is calculated depends on a number of factors but in general a higher score means the document is more relevant to the query.
Ranking documents by relevancy is important for a couple of reasons.
First, modern search engines typically store a large volume of documents, often millions or billions of documents.
Without ranking documents by relevance to the query, users can become overloaded with results without any clear way to navigate them.
Second, users are more comfortable and accustomed to getting results from other search engines using only a few keywords.
To influence ranking, you can assign more weight or “boost” certain documents, fields, or specific terms.
For example, you can boost results by their age to help push newer documents towards the top of search results.
For many users, though, this is only the first step in a more interactive session where the search results give them the ability to keep exploring.
One of the primary use cases of a search engine is to drive an information discovery session.
Frequently, your users won’t know exactly what they’re looking for and typically don’t have any idea what information is contained in your system.
A good search engine helps users narrow in on their information needs.
The central idea here is to return some documents from an initial query, as well as some tools to help users refine their search.
In other words, in addition to returning matching documents, you also return tools that give your users an idea of what to do next.
Please post comments or corrections to the Author Online forum:
This is known as faceted-search and is one of the main strengths of Solr.
We’ll see an example of faceted search for a real estate search in section 1.2
DON’T USE A SEARCH ENGINE TO DO … Lastly, let’s consider a few use cases where a search engine wouldn’t be useful.
More documents for the same query can be retrieved using Solr’s built-in paging support.
Consider a query that matches a million documents—if you request all of those documents back at once, you should be prepared to wait a long time.
The query itself will likely execute quickly but reconstructing a million documents from the underlying index structure will be extremely slow, as engines like Solr store fields on disk in a format from which it’s easy to create a few documents, but takes a long time to reconstruct many documents when generating results.
Another use case where you shouldn’t use a search engine is for deep analytics tasks that require access to a large subset of the index.
Even if you avoid the previous issue by paging through results, the underlying data structure of a search index isn’t designed for retrieving large portions of the index at once.
We’ve touched on this previously, but we’ll reiterate that search engines aren’t the place for querying across relationships between documents.
Solr does support querying using a parent-child relationship, but doesn’t provide support for navigating complex relational structures.
In chapter 3, you’ll learn some techniques to adapt relational data to work with Solr’s flat document structure.
Lastly, there’s no direct support in most search engines for document-level security, at least not in Solr.
If you need fine-grained permissions on documents, then you’ll have to handle that outside of the search engine.
Now that we’ve seen the types of data and use cases where a search engine is the right solution, it’s time to dig into what Solr does and how it does it from a high level.
In the next section, you’ll learn what Solr does and how it approaches important software design principles like integration with external systems, scalability, and high-availability.
This will help you understand what specific features Solr provides and the motivation for their existence.
But, before we get into the specifics of what Solr is, let’s make sure you know what Solr isn’t:
Now, imagine we need to design a real estate search web application for potential.
The central use case for this application will be searching for homes to buy across the United States using a web browser.
Figure 1.1 depicts a screen shot from this fictitious web application.
Don’t focus too much on the layout or design of the user interface,
Please post comments or corrections to the Author Online forum:
What’s important is the type of experience that Solr can support.
Figure 1.1 Mock-up screen shot of fictitious search application to depict Solr features.
Let’s take a quick tour of the screen shot in figure 1.1 to illustrate some of Solr’s key features.
First, starting at the top-left corner, working clock-wise, Solr provides powerful features to support a keyword search box.
As we discussed in section 1.1.2, providing a great user experience with basic keyword search requires complex infrastructure that Solr provides out-of-the-box.
Solr also provides a powerful solution for implementing geo-spatial queries.
In figure 1.1, matching home listings are displayed on a map based on their distance from the latitude / longitude of the center of some fictitious neighborhood.
With Solr’s geo-spatial support, you can sort documents by geo-distance or even rank documents by geo-distance.
It’s also allows users to zoom in and out and move around on a map.
Once the user performs a query, the results can be further categorized using Solr’s faceting support to show features of the documents in the result set.
Please post comments or corrections to the Author Online forum:
In figure 1.1, search results are categorized into three facets for features, home style, and listing type.
Now that we have a basic idea of the type of functionality we need to support our real estate search application, let’s see how we’d implement these features with Solr.
To begin, we need to know how Solr matches home listings in the index to queries entered by users, as this is the basis for all search applications.
Solr is built on Apache Lucene, a popular Java-based open source information retrieval library.
We’ll save a detailed discussion of what information retrieval is for chapter 3
For now, we’ll touch on the key concepts behind information retrieval starting with the formal definition taken from one of the prominent academic texts on modern search concepts:
Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers)
In the case of our example real estate application, the user’s primary information need is finding a home to purchase based on location, home style, features, and price.
Under the covers, Lucene is a Java-based library for building and managing an inverted index, which is a specialized data structure for matching query terms to text-based documents.
Figure 1.2 provides a simplified depiction of a Lucene inverted index for our example real estate search application.
Please post comments or corrections to the Author Online forum:
Figure 1.2 The key data structure supporting information retrieval is the inverted index.
You’ll learn all about how an inverted index works in chapter 3
You might be thinking that a relational database could easily return the same results using a SQL query, which is true for this simple example.
But one key difference between a Lucene query and a database query is that in Lucene, results are ranked by their relevance to a query and database results can only be sorted by one of the table columns.
In other words, ranking documents by relevance is a key aspect of information retrieval and helps differentiate it from other types of search.
Please post comments or corrections to the Author Online forum:
It might surprise you that search engines like Google also use an inverted index for searching the web.
In fact, the need to build a web-scale inverted index led to the invention of MapReduce.
MapReduce is a programming model that distributes large-scale data processing operations across a cluster of commodity servers by formulating an algorithm into two phases: map and reduce.
With roots in functional programming, MapReduce was adapted by Google for building its massive inverted index to power web search.
Using MapReduce, the map phase produces unique term and document ID where the term occurs.
In the reduce phase, terms are sorted so that all term / docID pairs are sent to the same reducer process for each unique term.
The reducer sums up all term frequencies for each term to generate the inverted index.
Apache Hadoop provides an open source implementation of MapReduce and is used by the Apache Nutch open source project to build a Lucene inverted index for web-scale search.
A thorough discussion of Hadoop and Nutch are beyond the scope of this book, but we encourage you to investigate these projects if you need to build a large-scale search index.
Now that we know that Lucene provides the core infrastructure to support search, let’s look at what value Solr adds on top of Lucene, starting with how you define how your index is structured using Solr’s flexible schema.xml configuration document.
Although Lucene provides the infrastructure for indexing documents and executing queries, what’s missing from Lucene is an easy way to configure how you want your index to be structured.
With Lucene you need to write Java code to define fields and how to analyze those fields.
Solr adds a simple, declarative way to define the structure of your index and how you want fields to be represented and analyzed using an XML configuration document named schema.xml.
Under the covers, Solr translates the schema.xml document into a Lucene index.
This saves you programming and makes your index structure easier to understand and communicate to others.
On the other hand, a Solr built index is 100% compatible with a programmatically built Lucene index.
Solr also adds a few nice constructs on top of the core Lucene indexing functionality.
Copy fields provide a way to take the raw text contents of one or more fields and have them applied to a different field.
Dynamic fields allow you to apply the same field type to many different fields without explicitly declaring them in schema.xml.
This is useful for modeling documents that have many fields.
Please post comments or corrections to the Author Online forum:
In terms of our example real estate application, it might surprise you that we can use the Solr example server out-of-the-box without making any changes to the schema.xml.
This shows how flexible Solr’s schema support is, because the example Solr server is designed to support product search but works fine for our real estate search example.
At this point, we know that Lucene provides a powerful library for indexing documents, executing queries, and ranking results.
And, with schema.xml, you have a flexible way to define the index structure using an XML configuration document instead of having to program to the Lucene API.
Now you need a way access these services from the web.
In the next section, we learn how Solr runs as a Java web application and integrates with other technologies using proven standards such as XML, JSON, and HTTP.
Solr is a Java web application that runs in any modern Java Servlet engine like Jetty or Tomcat, or a full J2EE application server like JBoss or Oracle AS.
Figure 1.3 depicts major software components of a Solr server.
Please post comments or corrections to the Author Online forum:
Admittedly, figure 1.3 is a little overwhelming at first glance.
Take a moment to scan over the diagram to get a feel for some of the terminology.
Don’t worry if you’re not familiar with all of the terms and concepts represented in the diagram--after reading this book, you should have a strong understanding of the all concepts presented in figure 1.3
As we mentioned in the introduction to this chapter, the Solr designers recognized that Solr fits better as a complementary technology that works within existing architectures.
In fact, you’ll be hard-pressed to find an environment where Solr doesn't drop right in.
As we'll see in chapter 2, you can start the example Solr server in a couple of minutes after you finish the download.
To achieve the goal of easy integration, Solr’s core services need to be accessible from many different applications and languages.
Solr provides simple REST-like services based on proven standards of XML, JSON, and HTTP.
As a brief aside, we avoid the “RESTful” label for Solr’s HTTP-based API as it doesn’t strictly adhere to all REST (representational state transfer) principles.
For instance, in Solr you use HTTP POST to delete documents instead of HTTP DELETE.
A REST-like interface is nice as a foundation, but often times, developers like to have access to a client library in their language of choice to abstract away some of the boilerplate machinery of invoking a web service and processing the response.
The good news here is that most popular languages have a Solr client library including Python, PHP, Java, .NET, and Ruby.
One hallmark of modern application architectures is the need for flexibility in the face of rapidly changing requirements.
One of the ways Solr helps this situation is that you don’t have to do all things in Solr with one index, because Solr supports running multiple cores in a single engine.
In figure 1.3, we’ve depicted multiple cores as separate layers all running in the same Java web application environment.
Think of each core as a separate index and configuration and there can be many cores in a single Solr instance.
This allows you to manage multiple cores from one server so that you can share server resources and administration tasks like monitoring and maintenance.
Solr provides an API for creating and managing multiple cores.
One use of Solr’s multicore support is data partitioning, such as having one core for recent documents and another core for older documents, known as chronological sharding.
Another use of Solr’s multicore support is to support multitenant applications.
In our real estate application, we might use multiple cores to manage different types of listings that are different enough to justify having different indexes for each.
Consider real estate listings for rural land instead of homes.
Buying rural land is a different process than buying a home in a city, so it stands to reason that we might want to manage our land listings in a separate core.
Please post comments or corrections to the Author Online forum:
Of course, these are high-level abstractions for complex subsystems in Solr; we’ll learn about each later in the book.
Each of these systems is composed of a modular “pipeline” that allows you to plug-in new functionality.
This means that instead of overriding the entire query-processing engine in Solr, you plug-in a new search component into an existing pipeline.
This makes the core Solr functionality easy to extend and customize to meet your specific application needs.
Lucene is an extremely fast search library and Solr takes full advantage of Lucene’s speed.
But regardless of how fast Lucene is, a single server will reach its limits in terms of how many concurrent queries from different users it can handle due to CPU and I/O constraints.
As a first pass to scalability, Solr provides flexible cache management features that help your server avoid recomputing expensive operations.
Specifically, Solr comes preconfigured with a number of caches to save expensive recomputations, such as caching the results of a query filter.
We’ll learn about Solr’s cache management features in chapter 4
Caching only gets you so far so at some point, you’re going to need to scale out your capacity to handle more documents and higher query throughput by adding more servers.
For now let’s focus on the two most common dimensions of scalability in Solr.
First is query throughput, which is the number of queries your engine can support per second.
Even though Lucene can execute each query quickly, it’s limited in terms of how many concurrent requests a single server can handle.
For higher query throughput, you add replicas of your index so that more servers can handle more requests.
This means if your index is replicated across three servers, then you can handle roughly three times the number of queries per second because each server handles one-third of the query traffic.
In practice, it’s rare to achieve perfect linear scalability so adding three servers may only allow you to handle two and one half times the query volume of one server.
The other dimension of scalability is the number of documents indexed.
If you’re dealing with large volumes of documents, then you’ll likely reach a point where you have too many documents in a single instance and query performance will suffer.
To handle more documents, you split the index into smaller chunks called “shards” and then distribute the searches across the shards.
One trend in modern computing is building software architectures that can scale horizontally using virtualized commodity hardware.
Put simply, add more commodity servers to handle more traffic.
Fueling this trend towards using virtualized commodity hardware are cloud-computing providers such as Amazon EC2
Although Solr will run on virtualized hardware, you should be aware that search is I/O and memory intensive.
Please post comments or corrections to the Author Online forum:
Therefore, if search performance is a top priority for your organization, then you should consider deploying Solr on higher end hardware with high-performance disks, ideally solid-state drives (SSD)
Hardware considerations for deploying Solr are discussed in chapter 13
Scalability is important, but ability to survive failures is also important for a modern system.
In the next section, we discuss how Solr handles software and hardware failures.
Beyond scalability, you need to consider what happens if one or more of your servers fails, particularly if you’re planning to deploy Solr on virtualized hardware or commodity hardware.
The bottom line is that you must plan for failures.
Even the best architectures and high-end hardware will experience failures.
Let’s assume you have four shards for your index and the server hosting shard #2 loses power.
At this point, Solr can’t continue indexing documents and can’t service queries, so your search engine is effectively “down.” To avoid this situation, you can add replicas of each shard.
In this case, when shard #2 fails, Solr reroutes indexing and query traffic to the replica and your Solr cluster remains online.
The impact of this failure is that indexing and queries can still be processed but may not be as fast because you’ve one less server to handle requests.
At this point, you’ve seen that Solr has a modern, well-designed architecture that’s scalable and fault-tolerant.
Although these are important aspects to consider if you’ve already decided to use Solr, you still might not be convinced that Solr is the right choice for your needs.
In the next section, we describe the benefits of Solr from the perspective of different stakeholders, such as the software architect, system administrator, and CEO.
Let’s begin by addressing why Solr is attractive to software architects.
When evaluating new technology, software architects must consider a number of factors, but chief among those are stability, scalability, and fault-tolerance.
In terms of stability, Solr is a mature technology supported by a vibrant community and seasoned committers.
One thing that shocks new users to Solr and Lucene is that it isn’t unheard of to deploy from source code pulled directly from the trunk rather than waiting for an official release.
We won’t advise you either way on whether this is acceptable for your organization.
We only point this out because it’s a testament to the depth and breadth of.
Please post comments or corrections to the Author Online forum:
Put simply, if you have a nightly build off trunk where all the automated tests pass, then you can be sure the core functionality is solid.
As an architect, you’re probably most curious about the limitations of Solr’s approach to scalability and fault-tolerance.
First, you should realize that the sharding and replication features in Solr have been rewritten in Solr 4 to be robust and easier to manage.
Under the covers, SolrCloud uses Apache Zookeeper to distribute configuration across a cluster of Solr nodes and to keep track of cluster state.
Here are some highlights of the new SolrCloud features in Solr:
Queries can be sent to any node in a cluster to trigger a full distributed search across all shards with fail-over and load-balancing support built-in.
But this isn’t to say that Solr scaling doesn’t have room for improvement.
First, not all features work in distributed mode, such as joins.
Second, the number of shards for an index is a fixed value that can’t be changed without reindexing all of the documents.
We’ll get into all of the specifics of SolrCloud in chapter 16, but we want to make sure architects are aware that Solr scaling has come a long way in the past couple of years yet still has some more work to do.
As a system administrator, high among your priorities in adopting a new technology like Solr is whether it fits into your existing infrastructure.
Out of the box, Solr embeds Jetty, the open source Java servlet engine provided by Oracle.
Otherwise, Solr is a standard Java web application that deploys easily to any Java web application server like JBoss and Oracle AS.
All access to Solr can be done via HTTP and Solr is designed to work with caching HTTP reverse proxies like Squid and Varnish.
Solr also works with JMX so you can hook it up to your favorite monitoring application, such as Nagios.
Lastly, Solr provides a nice administration console for checking configuration settings, statistics, issuing test queries, and monitoring the health of SolrCloud.
We’ll learn more about the administration console in chapter 2
Please post comments or corrections to the Author Online forum:
Although it’s unlikely that a CEO will be reading this book, here are some key talking points about Solr in case your CEO stops you in the hall.
First, executive types like to know an investment in a technology today is going to payoff in the long term.
And, you have access to the source code, which means if something is broken and you need a fix, you can do it yourself.
Many commercial service providers also can help you plan, implement, and maintain your Solr installation; many of which offer training courses for Solr.
Please post comments or corrections to the Author Online forum:
This may be one for the CFO, but Solr doesn’t require much initial investment to get started.
Without knowing the size and scale of your environment, we’re confident in saying that you can start up a Solr server in a few minutes and be indexing documents quickly.
A modest server running in the cloud can handle millions of documents and many queries with sub-second response times.
Finally, let’s do a quick run-down of Solr’s main features organized around the following categories:
Providing a great user experience with your search solution will be a common theme throughout this book so let’s start by seeing how Solr helps make your users happy.
Solr provides a number of important features that help you deliver a search solution that’s easy to use, intuitive, and powerful.
But you should note that Solr only exposes a REST-like HTTP API and doesn’t provide search-related UI components in any language or framework.
You’ll have to roll up your sleeves and develop your own search UI components that take advantage of some of the following user experience features:
Geo-spatial search PAGINATION AND SORTING Rather than returning all matching documents, Solr is optimized to serve paginated requests where only the top N documents are returned on the first page.
If users don’t find what they’re looking for on the first page, then you can request subsequent pages using simple API request parameters.
Please post comments or corrections to the Author Online forum:
In our real estate example (figure 1.1) we saw how search results from a basic keyword search were organized into three facets: Features, Home Style, and Listing Type.
Solr faceting is one of the more popular and powerful features available in Solr; we cover faceting in depth in chapter 8
Auto-suggest helps users by allowing them to see a list of suggested terms and phrases based on documents in your index.
Solr’s auto-suggest features allows user to start typing a few characters and receive a list of suggested queries as they type.
This reduces the number of incorrect queries, particularly because many users may be searching from a mobile device with small keyboards.
Auto-suggest gives users examples of terms and phrases available in the index.
Again, users expect to be able to type misspelled words into the search box and the search engine should handle it gracefully.
Auto-correct—Solr can make the spell correction automatically based on whether the misspelled term exists in the index.
Most useful for longer format documents, so you can help users find relevant information in longer documents by quickly scanning highlighted sections in search results.
GEO-SPATIAL Geographical location is a first-class concept in Solr 4 in that it has built-in support for indexing latitude and longitude values as well as sorting or ranking documents by geographical distance.
Solr can find and sort documents by distance from a geo-location (latitude and longitude)
In the real estate example, matching listings are displayed on an interactive map where users can zoom in/out and move the map center point to find near-by listings using geo-spatial search.
Please post comments or corrections to the Author Online forum:
Another exciting addition to Solr 4 is that you can index geographical shapes such as polygons, which allows you to find documents that intersect geographical regions.
This might be useful for finding home listings in specific neighborhoods using a precise geographical representation of a neighborhood.
As we discussed in section 1.1, Solr is optimized to work with specific types of data.
In this section, we provide an overview of key features that help you model data for search, including:
Multilingual support FIELD COLLAPSING / GROUPING Although Solr requires a flat, denormalized document, Solr allows you to treat multiple documents as a group based on some common property shared by all documents in a group.
Field grouping, also known as field collapsing, allows you to return unique groups instead of individual documents in the results.
The classic example of field collapsing is threaded email discussions where emails matching a specific query could be grouped under the original email message that started the conversation.
Phrase queries with slop to allow for some distance between terms.
Don’t worry if you don’t know what all of these terms mean as we’ll cover all of them in depth in chapter 7
Please post comments or corrections to the Author Online forum:
But in Solr, joins are more like sub-queries in SQL in that you don’t build documents by joining data from other documents.
For example, with Solr joins, you can return child documents of parents that match your search criteria.
One example where Solr joins are useful would be grouping all retweets of a Twitter message into a single group.
This is helpful to avoid returning many documents containing the same information in search results.
For example, if your search engine is based on news articles pulled from multiple RSS feeds, then it’s likely that you’ll have many documents for the same news story.
Rather than returning multiple results for the same story, you can use clustering to pick a single representative story.
With Solr this is easy because it integrates with Apache Tika project that supports most popular document formats.
We cover Solr’s data import handler (DIH) in chapter 12
Solr has language detection built-in and provides language-specific text analysis solutions for many languages.
We’ll see Solr’s language detection in action in chapter 6
In general, version 4 is a huge milestone for the Apache Solr community as it addresses many of the major pain-points discovered by real users over the past several years.
We selected a few of the main features to highlight here but we’ll also point out new features in Solr 4 throughout the book.
Please post comments or corrections to the Author Online forum:
Easy sharding and replication using Zookeeper NEAR-REAL-TIME SEARCH Solr’s Near-Real-Time (NRT) search feature supports applications that have a high velocity of documents that need to be searchable within seconds of being added to the index.
With NRT, you can use Solr to search rapidly changing content sources such as breaking news and social networks.
For example, if the price of a home in our example real estate application from section 1.2 changes, then we can send an atomic update to Solr to change the price field specifically.
You might be wondering what happens if two different users attempt to change the same document concurrently.
In this case, Solr guards against incompatible updates using optimistic concurrency.
In a nutshell, Solr uses a special version field named _version_ to enforce safe update semantics for documents.
In the case of two different users trying to update the same document concurrently, the user that submits updates last will have a stale version field so their update will fail.
Atomic updates and optimistic concurrency are covered in chapter 12
REAL-TIME GET At the beginning of this chapter, we stated that Solr is a NoSQL technology.
Solr’s real-time get feature definitely fits within the NoSQL approach by allowing you to retrieve the latest version of a document using its unique identifier regardless of whether that document has been committed to the index.
This is similar to using a key-value store like Cassandra to retrieve data using a row key.
Prior to Solr 4, a document wasn’t retrievable until it was committed to the Lucene index.
With the real-time get feature in Solr 4, you can safely decouple the need to retrieve a document by its unique ID from the commit process.
This can be useful if you need to update an existing document after it’s sent to Solr without having to do a commit first.
As we’ll learn in chapter 5, commits can be expensive and impact query performance.
Solr’s transaction log sits between the client application and the Lucene index.
It also plays a role in servicing real-time get requests as documents are retrievable by their unique identifier regardless of whether they’re committed to Lucene.
The transaction log allows Solr to decouple update durability from update visibility.
This means that documents can be on durable storage but aren’t visible in search results yet.
This gives your application control over when to commit documents to make them visible in search results without risking data loss if a server fails before you commit.
We’ll discuss durable writes and commit strategies in chapter 5
Please post comments or corrections to the Author Online forum:
With SolrCloud, scaling is simple and automated because Solr uses Apache Zookeeper to distribute configuration and manage shard leaders and replicas.
In Solr, Zookeeper is responsible for assigning shard leaders and replicas and keeps track of which servers are available to service requests.
SolrCloud bundles Zookeeper so you don’t need to do any additional configuration or setup to get started with SolrCloud.
We’ll dig into the details of SolrCloud in chapter 16
We hope you now have a good sense for what types of data and use cases Solr supports.
As you learned in section 1.1, Solr is optimized to handle data that’s text-centric, readdominant, document-oriented, and has a flexible schema.
We also learned that search engines like Solr aren’t general-purpose data storage and processing solutions but are intended to power keyword search, ranked retrieval, and information discovery.
Using the example of a fictitious real estate search application, we saw how Solr builds upon Lucene to add declarative index configuration and web services based on HTTP, XML, and JSON.
Solr 4 can be scaled in two dimensions to support millions of documents and high-query traffic using sharding and replication.
We also touched on some key reasons why to choose Solr based on the perspective of key stakeholders.
We saw how Solr addresses the concerns of software architects, system administrators, and even the CEO.
Lastly, we covered some of Solr’s main features and gave you pointers where you can learn more about each feature in this book.
We hope you’re excited to continue learning about Solr, so now it’s time to download the software and run it on your local system, which is what we’ll do in chapter 2
Please post comments or corrections to the Author Online forum:
It’s natural to have a sense of uneasiness when you start using an unfamiliar technology.
You can put your mind at ease because Solr is designed to be easy to install and set up.
In the spirit of being agile, you can start out simple and incrementally add complexity to your Solr configuration.
For example, Solr allows you to split a large index into smaller subsets, called shards, as well as add replicas to increase your capacity to serve queries.
But you don’t need to worry about index sharding or replication until you need them.
By the end of this chapter, you’ll have Solr running on your computer, know how to start and stop Solr, know your way around the web-based administration console, and have a basic understanding of key Solr terminology such as solr home, core, and collection.
You may have heard of SolrCloud and wondered what the difference is between Solr 4 and SolrCloud.
Technically, SolrCloud is the code name for a sub-set of features in Solr 4 that makes it easier to configure and run a scalable, fault-tolerant cluster of Solr servers.
Think of SolrCloud as a way to configure a distributed installation of Solr 4
Please post comments or corrections to the Author Online forum:
Also, SolrCloud doesn’t have anything to do with running Solr in a cloud-computing environment like Amazon EC2, although you can run Solr in a cloud.
We presume that the “cloud” part of the name reflects the underlying goal of the SolrCloud feature set to enable elastic scalability, high-availability, and ease of use we’ve all come to expect from cloud-based services.
Let’s get started by downloading Solr from the Apache website and installing it on your computer.
Before we can get to know Solr, we have to get it running on your local computer.
This starts with downloading the binary distribution of Solr 4.1 from Apache and extracting the downloaded archive.
Once installed, we’ll show you how to start the example Solr server and verify that it’s running by visiting the Solr administration console from your web browser.
Throughout this process, we assume you’re comfortable executing simple commands from the command line of your chosen operating system.
There is no GUI installer for Solr, but you’ll soon see that the process is so simple you won’t need a GUI-driven installer.
Installing Solr is a bit of a misnomer in that all you really need to do is download the binary distribution (.zip or .tgz) and extract it.
To verify you have the correct version of Java, open a command-line on your computer and do:
All client interaction with Solr happens over HTTP so you can use any language that provides an HTTP client library.
In addition, a number of open source client libraries are available for Solr for popular languages like .NET, Python, Ruby, PHP, and of course Java.
Assuming you’ve Java installed, you’re now ready to install Solr.
Apache provides source and binary distributions of Solr; for now, we’ll focus on the installation steps using the binary distribution.
We cover how to build Solr from source in the appendix.
In your browser, go to the Solr home page http://lucene.apache.org/solr and click on the Download button for Apache Solr 4.1 on the right; there is also a button for downloading.
Please post comments or corrections to the Author Online forum:
This will direct you to a mirror site for Apache downloads; it’s advisable to download from a mirror site to avoid overloading the main Apache site.
After downloading, move the downloaded file to a permanent location on your computer.
For example, on Windows, you could move it to the C:\ root directory, or on Linux, choose a location like /opt/solr.
For Windows users, we highly recommend that you extract Solr to a directory that doesn’t have spaces in the name, i.e.
Your mileage may vary on this, but being a Java-based software, you're likely to run into issues with paths that contain a space.
There is no formal installer needed because Solr is self-contained in a single archive file— all you need to do is extract it.
When you extract the archive, all files will be created under the solr-4.1.0 directory.
On Windows, you can use the built-in Zip extraction support or a tool like WinZip.
This will create the directory structure shown in figure 2.1
We'll refer to the top-level directory as $SOLR_INSTALL in the rest of this chapter.
We refer to the location where you extracted the Solr archive (.zip or .tgz) as $SOLR_INSTALL in the rest of this chapter.
Please post comments or corrections to the Author Online forum:
Solr home will be a different path, so we didn't want to use $SOLR_HOME as the alias for the top-level directory where you extracted Solr.
So now that Solr is installed, you’re ready to start it up.
To start Solr, open a command line and do the following:
Remember that $SOLR_INSTALL is the alias we're using to represent the directory where.
During initialization, you'll see some log messages printed to the console.
If all goes well, you should see the following log message at the bottom:
To be clear, you now have a running version of Solr 4.1 on your computer.
You can verify that Solr started correctly by directing your web browser to the Solr administration page at: http://localhost:8983/solr.
Figure 2.2 provides a screen shot of the Solr administration console; please take a minute to get acquainted with the layout and navigational tools in the console.
Please post comments or corrections to the Author Online forum:
Click on the link labeled "collection1" to access more tools such as the query form.
Behind the scenes, start.jar launched a Java web server named Jetty, listening on port 8983
Figure 2.3 illustrates what is now running on your computer.
Please post comments or corrections to the Author Online forum:
Figure 2.3 Solr from a systems perspective showing the Solr web application (solr.war) running in Jetty on top of Java.
There is one Solr home directory set per Jetty server using Java system property solr.solr.home.
Solr can host multiple cores per server and each core has a separate directory containing core-specific configuration and index (data) under Solr home, e.g.
The most common issue if the server doesn't start correctly is the default port 8983 is already in use by another process.
If this is the case, you’ll see an error that looks like: java.net.BindException: Address already in use.
This is easy to resolve by changing the port Solr binds to by instead of 8983
We recommend just staying with Jetty when first learning Solr.
If your organization already uses Tomcat or some other Java web application server, such as Resin, then you can deploy the Solr WAR file.
Since we're just getting to know Solr in this chapter, we'll refer you to the Appendix B to learn how to deploy the Solr WAR.
Solr uses Jetty to make the initial setup and configuration process a no-brainer.
However, this doesn’t mean that Jetty is a bad choice for production deployment.
Please post comments or corrections to the Author Online forum:
However, if you have some choice, then we recommend you try out Jetty.
It's fast, stable, mature, and easy to administer and customize.
Jetty does provide a safer mechanism for stopping the server, which will be discussed in chapter 12
Now we have a running server, let's take a minute to understand where Solr gets its configuration information and where it manages its Lucene index.
Understanding how the example server you just started is configured will help you when you're ready to start configuring a Solr server for your application.
In Solr, a “core” is composed of a set of configuration files, Lucene index files, and Solr's transaction log.
One Solr server running in Jetty can host multiple cores.
Recall in chapter 1, we designed a real estate search application that had multiple cores, one for houses and a separate core for land listings.
We used two separate cores because the indexed data was different enough to justify having two different index structures.
As a brief aside, Solr also uses the term "collection", which really only has meaning in the context of a Solr cluster where a single index is distributed across multiple servers.
Consequently, we feel it's easier to focus on understanding what a Solr core is for now.
We'll return to the distinction between core and collection in chapter 13 when we discuss SolrCloud.
Solr home is a directory structure that encapsulates one or more cores, which are configured by solr.xml.
Solr also provides a core admin API that allows you to create, update, and delete cores programmatically from your application.
Behind the scenes the core admin API makes changes to the solr.xml configuration file.
Listing 2.1 shows the default solr.xml for the example server.
Out of the box, you don’t need to make any changes to this file.
Please post comments or corrections to the Author Online forum:
Each Solr process has one and only one solr home directory, which is set by a global Java system property: solr.solr.home.
Figure 2.4 shows a directory listing of the default Solr home "solr" for the example server.
Figure 2.4 Directory listing of the default Solr home directory for the Solr examples.
It contains a single core named "collection1," which is configured in solr.xml.
Please post comments or corrections to the Author Online forum:
We'll learn more about the main Solr configuration file for a core, named solrconfig.xml, in chapter 4
Also, schema.xml is the main configuration file that governs index structure and text analysis for documents and queries; you'll learn all about schema.xml in chapter 5
For now, just take a moment to scan figure 2.3 so that you have a sense for the basic structure.
The example directory contains two other solr home directories for exploring advanced functionality.
Specifically, the example/example-DIH directory provides a Solr core for learning about the data import handler (DIH) feature in Solr.
Also, the example/multicore directory provides an example of a multi-core configuration.
We'll learn more about these features later in the book.
For now, let's continue with the simple example by adding some documents to the index, which you'll need to work through the examples in section 2.2 below.
When you first start Solr, there are no documents in the index.
It's just an empty server waiting to be filled with data to search.
For now, we'll gloss over the details in order to get some example data in Solr index so that we can try out some queries.
Please post comments or corrections to the Author Online forum:
Figure 2.5 shows what you should see after executing the find all documents query.
Figure 2.5 Screenshot of the Query form on the Solr administration console.
You can verify that the example documents were indexed correctly by executing the find all documents query *:*
At this point, we have a running Solr instance with some example documents loaded.
Without a doubt, Solr's main strength is powerful query processing.
Think about it this way, who cares how scalable or fast a search engine is if the results it returns aren't useful or accurate? In this section, you'll see Solr query processing in action, which we think will help you see why Solr is such a powerful search technology.
Throughout this section, pay close attention to the link between each query we execute and the documents that Solr returns, especially the order of the documents in the results.
This will help you start to think like a search engine, which will come in handy in chapter 3 when we cover core search concepts.
Please post comments or corrections to the Author Online forum:
You've already used Solr’s query form to execute the "find all documents" query (*:*)
Let's take a quick tour of the other features on this form so that you get a sense for the types of queries Solr supports.
Figure 2.6 provides some annotations of key sections of this form.
Take a minute to read through each annotation in the diagram.
Figure 2.6 Annotated screen shot of Solr's query form to illustrate the main features of Solr query processing, such as filters, results format, sorting, paging, and search components.
Take a moment to fill-out the form and execute the query in your own environment.
Please post comments or corrections to the Author Online forum:
In this example, we filter results that have manufacturer field "manu" equal to "Belkin"
Start should be incremented by the page size to advance to the next page.
List of fields to return for each document in the result set.
The "score" field is a built-in field that holds each document's relevancy score for the query.
You have to request the score field explicitly as is done in this example.
When you fill out the query form, an HTTP GET request is created and sent to Solr.
The form field names shown in table 2.1 correspond to parameters passed to Solr in the HTTP GET request.
Please post comments or corrections to the Author Online forum:
But, if you don't want to wait that long and want to see more queries in action, then we recommend looking at the Solr tutorial provided with Solr.
Lastly, we probably don’t have to tell you that this form isn't designed for end users; the Solr team built the query form so that developers and administrators have a way to send queries without having to formulate HTTP requests manually or develop a client application just to send a query to Solr.
But, let's be clear that with Solr, you’re responsible for developing the user interface to Solr.
As we'll see in section 2.2.5 below, Solr provides an example search UI, called Solritas, to help you get started building your own awesome search application.
We've seen what gets sent to Solr, so now let's learn about what comes back from Solr in the results.
The key point in this section is that Solr returns documents that match the query as well as additional information that can be processed by your Solr client to deliver a quality search experience.
The operative phrase being "by your Solr client"! Solr returns the raw materials that you need to translate into a quality search experience for your users.
As you can see, the results are in XML format and are sorted by lowest to highest price.
Paging doesn't really come into play with this result set because there are only two results total.
Please post comments or corrections to the Author Online forum:
So far, we've only seen results returned as XML, but Solr also supports other formats such as CSV, JSON, and language specific formats for popular languages.
For instance, Solr can return a Python specific format that allows the response to be safely parsed into a Python object tree using the eval function.
Let's see ranked retrieval at work with some of the example documents you indexed in section 2.1.4
To begin, enter "iPod" in the q text box, "name,features,score" in the fl text field, and push the Execute button.
This should return three documents sorted in descending order by score.
Take a moment to scan the results and decide if you agree with the ranking for this simple query.
Intuitively, the ordering makes sense because the query term "iPod" occurs three times in the first document listed, twice in the name and once in the features; it only occurs once in.
Please post comments or corrections to the Author Online forum:
The actual numeric value of the score field isn’t as important as it's used internally by Lucene to do the ranking.
The key take-away is that every document that matches a query is assigned a relevance score for that specific query and results are returned in descending order by score; the higher the score, the more relevant the document is to the query.
Next, change your query to be "iPod power" and you'll see that the same three documents are returned and are in the same order.
This is because all three documents contain both query terms in either their name or features field.
This makes sense because "power" occurs twice in the second document so its relevance to the "iPod power" query is much higher than its relevance to the "iPod" query.
In a nutshell, this means that the "power" term is twice as important to this query as the "iPod" term, which has an implicit boost of 1
Again, the same three documents are returned but in a different order.
Now the top document in the results is "Belkin Mobile Power Cord for iPod w/ Dock" because it contains the term "power" in the name and features field and we told Solr that "power" is twice as important as "iPod" for this query.
Now you have a taste of what ranked retrieval looks like.
Let's move on and see some other features of query processing, starting with how to work with queries that return more than three documents using paging and sorting.
Our example Solr index only contains 32 documents, but a production Solr instance typically has millions of documents.
You can imagine that in a Solr instance for an electronics superstore, a query for "iPod" would probably match thousands of products and accessories.
To ensure results are returned quickly, especially on bandwidth constrained mobile devices, you don't want to return thousands of results at once, even if the most relevant are listed first.
Paging is a first-class concept in Solr query processing in that every query includes parameters that control the page size (rows) and starting position (start)
By default, Solr uses a page size of 10, but you can control that using the "rows" parameter in the query request.
To request the "next" page in the results, you increment the start parameter by the page size.
It's important to use as small a page size as possible to satisfy your requirements because the underlying Lucene index isn’t optimized for returning many documents at once.
Rather, Lucene is optimized for query processing so the underlying data structures are.
Please post comments or corrections to the Author Online forum:
Once the search results are identified, Solr must re-construct each document, in most cases by reading data off disk.
It uses intelligent caching to be as efficient as possible, but in comparison to query processing, results construction is a slow process, especially for large page sizes.
Consequently, you’ll get much better performance from Solr using small page sizes.
However, you can request Solr to sort results by other fields in your documents.
You've already seen an example of this in section 2.2.1 where we sorted results by the price field in ascending order, which produces the lowest priced products at the top.
Sorting and paging go hand-in-hand because the sort order determines the page position for results.
To help get you thinking about sorting and paging, consider the question of whether Solr will return deterministic results when paging without specifying a sort order? On the surface, this seems obvious because the results are descending sorted by score if you don't specify a sort parameter.
But, what if all documents in a query have the same score? For example, if your query is "inStock:true" then all matching documents will have the same score; be sure to verify this yourself using the Query form.
It turns out that Solr will indeed return all documents when you page through the results even though the score is the same.
This works because Solr finds all documents that match a query and then applies the sorting and paging offsets to the entire set of documents.
In other words, Solr keeps track of the entire set of documents that match a query independently of the sorting and paging offsets.
The query form contains a list of check boxes that enable advanced functionality during query processing; in Solr speak these additional features are called "search components"
As shown in figure 2.6, the form contains check boxes that reveal additional form fields to activate the following search components:
If you click on any of these checkboxes, you'll see that it’s not clear what to do when you look at the form.
That’s because using these search components from the query form requires some additional knowledge that we can't cover quickly in this getting started chapter.
Rest assured that we cover each of these components in-depth later in the book.
Please post comments or corrections to the Author Online forum:
For now, though, we can see some of these search components in action using Solr's example search interface, called "Solritas", available in your local Solr instance at: http://localhost:8983/solr/collection1/browse.
Navigate to this URL in your web browser and you’ll see a screen that looks like figure 2.8
Figure 2.8 The Solritas Simple example, which illustrates how to use various search components, such as faceting, More Like This, hit highlighting, and spatial, to provide a rich search experience for your users.
As shown at the top of figure 2.8, Solr provides three examples to choose from: Simple, Spatial, and Group By.
We'll briefly cover the key aspects of the Simple example here and encourage you to explore the other two examples on your own.
Take a moment to scan over figure 2.8 to identify the various search components at work.
One of the more interesting search components in this example is the facet search component shown on the left side of the page, starting with the header Field Facets.
The facet component categorizes field values in the search results into useful sub-sets to help the user refine their query and discover new information.
For instance, when we search for "video", Solr returns three example documents and the faceting component categorizes the.
Please post comments or corrections to the Author Online forum:
Click on the music facet link and you'll see the results are filtered from three documents down to only one.
The idea here is that in addition to search results, you can help users refine their search criteria by categorizing the results into different filters.
Take a few minutes to explore the various facets shown in the example.
Next, let's take a look at another search component that isn’t immediately obvious from figure 2.8, namely the spell check component.
To see how spell checking works, type "vydeoh" in the search box instead of "video"
Of course no results are found, as shown in figure 2.9, but Solr does return a link that effectively asks the user if they meant "video" and if so, they can re-search using the link.
Figure 2.9 Example of how the spell check component allows your search UI to prompt the user to research using the correct spelling of a query term, in this case, Solr found "video" as the closest match for "vydeoh"
There's a lot of powerful functionality packed into the three Solritas examples and we encourage you to spend a little time with each.
For now, let's move on and tour the rest of the administration console.
Please post comments or corrections to the Author Online forum:
At this point, you should have a good feel for the query form, so let's take a quick tour of the rest of the administration console shown in figure 2.10
Figure 2.10 The Solr administration console; explore each page using the toolbar on the left.
Rather than spending your time reading about the administration panel, we think it's better to just start clicking through some of the pages yourself.
Thus, we leave it as an exercise for you to visit all the links in the administration console to get a sense for what is available on each page.
To get you started, here are some highlights of what the administration console provides:
Get a dump of all active threads in your JVM from Thread Dump.
In addition to the main pages described above, there are a number of core specific pages for each core in your server.
Recall that the example server we've been working with has only one core named "collection1"
Please post comments or corrections to the Author Online forum:
View core specific properties such as the number of Lucene segments from the main core page, e.g.
Send a quick request to a core to make sure it’s alive using Ping.
View the currently active solconfig.xml for the core from Config; you’ll learn more about solrconfig.xml in chapter 4
See how your index is replicated to other servers from Replication.
Analyze text from Analysis; you’ll learn all about text analysis in chapter 6, including how to use the Analysis form.
Determine how fields in your documents are analyzed from Schema Browser.
Get information about the top terms for a field using Load Term Info on the Schema Browser.
View the status and configuration for Plug-ins from PlugIns / Stats; you’ll learn all about PlugIns in chapter 4
View statistics about core Solr cache regions, such as how many documents are in the documentCache from PlugIns / Stats.
Manage the data import handler from Dataimport; this isn’t enabled in the example server.
We'll dig into the details for most of these pages in various places throughout the book, when it's more appropriate.
For instance, you'll learn all about the Analysis page in chapter 6 when we cover text analysis.
For now, take a few moments to explore these pages on your own.
To give your self-guided tour some direction, see if you can answer the following questions about your Solr server.
What’s the value of the lucene-spec version property for your Solr server?
What’s the top term for the manu field? (hint: select the "manu" field in the schema browser and click on the Load Term Info button)
What’s the current size of your documentCache? (hint: think stats)
Please post comments or corrections to the Author Online forum:
What’s the analyzed value of the name "Belkin Mobile Power Cord for iPod w/ Dock"? (hint: select the name field on the Analyzed page)
Let's now turn our attention to what needs to be done to start customizing Solr for your specific needs.
So now that you've had a chance to work with the example server, you might be.
First, you could just use the example directory as-is and start making changes to it to meet your needs.
However, we think it's better to keep a copy of example around and make your application specific changes in a clone of example.
This allows you to refer back to example in case you break something when working on your own application.
If you choose the latter approach, then you need to select a name for the directory that is more appropriate for your application than "example"
For example, if we were building the real estate search application described in chapter 1, then we might name the directory: realestate.
Once you've settled on a name, do the following steps to create a clone of the example directory in Solr:
Update solr.xml to point to the name of your new collection by replacing "collection1"
Note that you don't need to make any changes to the Solr configuration files, such.
These files are designed to provide a good experience out-of-the-box and let you adapt them to your needs iteratively without having to swallow such a big pill at once.
There may come a time when you want to start with a fresh index.
After stopping Solr, you can remove all documents by deleting the contents of the data directory for your core, such as solr/collection1/data/*
When you restart Solr, you’ll have a fresh index with 0 documents.
Restart Solr from your new directory using the same process from section 2.1.2
For example, to restart our clone for the realestate application, we’d do:
Please post comments or corrections to the Author Online forum:
To recap, we started by installing Solr 4.1 from the binary distribution Apache provided.
In reality, the installation process was only a matter of choosing an appropriate directory where to extract the downloaded archive (.zip or .tgz) and then doing the extraction.
Next, we started the example Solr server and added some example documents using the post.jar command-line application.
After adding documents, we introduced you to Solr's query form, where you learned the basic components of a Solr query.
We also introduced you to search components and provided some insights into how they work in Solr using the Solritas example user interface.
Specifically, you saw an example of how the facet component allows users to refine their search criteria using dynamically generated filters called facets.
Next, we gave you some tips on what other tools are available in the Solr administration console.
You’ll find many great tools and statistics available about Solr, so we hope you were able to answer the questions we posed as you walked through the administration console in your browser.
Lastly, we presented the steps to clone the example directory to begin customizing it for your own application.
We think this is a good way to start so that you always have a working example to refer to as you customize Solr for your specific needs.
So now that you have a running Solr instance, it’s time to learn about core Solr concepts.
In chapter 3 you'll gain a better understanding of core search concepts that will help you throughout the rest of your Solr journey.
Please post comments or corrections to the Author Online forum:
How Solr performs complex queries using terms, phrases, and fuzzy matching.
How Solr calculates scores for matching queries to most relevant documents.
How to balance returning relevant results versus ensuring they’re all returned.
How search scales across servers to handle billions of documents and queries.
Now that we have Solr up and running, it’s important to gain a basic understanding of how a search engine operates and why you’d choose to use Solr to store and retrieve your content as opposed to a traditional database.
In this chapter, we’ll provide a solid understanding of how a search engine stores documents in its internal index, how it calculates a relevancy score to ensure only the “best” results are returned for display, and how Solr is able to scale to handle billions of documents as it still maintains lightning-fast search response times.
Our main goal for this chapter is to provide you with the theoretical underpinnings necessary to understand and maximize your use of Solr.
If you have a solid background in search technology or information retrieval already, then you may wish to skip some or all of this chapter, but if not, it will help you understand more advanced topics later in this book and maximize the quality of your users’ search experience.
Although the content in this chapter is generally applicable to most search engines, we’ll be specifically focusing upon Solr’s implementation of each of the concepts.
By the end of this chapter, you should have a solid understanding of how Solr’s internal index works, how to perform complex Boolean and fuzzy queries with Solr, how Solr’s default relevancy scoring model works, and how Solr’s.
Please post comments or corrections to the Author Online forum:
Let’s begin with a discussion of the core concepts behind search in Solr, including how the search index works, how a search engine matches queries and documents, and how Solr enables powerful query capabilities to make finding content a problem of the past.
Many different kinds of systems exist today to help us solve challenging data storage and retrieval problems—relational databases, key-value stores, map-reduce engines operating upon files on disk, and graph databases, among thousands of others.
Search engines, and Solr in particular, help to solve a particular class of problem very well – those requiring the ability to search across large amounts of unstructured text and pull back the most relevant results.
In this section, we’ll describe the core features of a modern search engine, including a explanation of a search “document”, an overview of the inverted search index at the core of Solr’s fast full-text searching capabilities, and a broad overview of how this inverted search index enables arbitrarily complex term, phrase, and partial matching queries.
It is important, however, that we have a solid understanding of the kind of information which we can put into Solr to be searched upon (a document), and how that information is structured.
Every piece of data submitted to Solr for processing is a document.
A document could be a newspaper article, a resume or social profile, or in an extreme case even an entire book.
Each document contains one or more fields, each of which is modeled as a particular field type: string, tokenized text, boolean, date-time, lat/long, etc.
The number of potential field types is infinite because a field type is composed of zero or more analysis steps that change how the data in the field is processed and mapped into the Solr index.
Each field is defined in Solr’s schema (discussed in chapter 5) as a particular field type, which allows Solr to know how to handle the content as it is received.
Listing 3.1 shows an example document, defining the values for each field.
Please post comments or corrections to the Author Online forum:
When we run a search against Solr, we can search on one or more of these fields (or even fields not contained in this particular document), and Solr will return documents that contain content in those fields matching the query specified in the search.
It is worth noting that, unlike Lucene, Solr is not schema-less.
All field types must be defined, and all field names (or dynamic field naming patterns) must be specified in Solr’s schema.xml, as we’ll discuss further in chapter 5
This does not mean that every document must contain every field, only that all possible fields must be mapped to a particular field type should they appear in a document and need to be processed.
A document, then, is a collection of fields that map to particular field types defined in a schema.
Each field in a document is analyzed according to its field type and stored in a search index in order to later retrieve the document by sending in a related query.
The primary search results returned from a Solr query are documents containing one or more fields.
Before we dive into an overview of how search works in Solr, it is helpful to understand what fundamental problem search engines are solving.
Let’s say, for example, you were tasked with creating some search functionality that helps users search for books.
Figure 3.1 Example search interface, as would be seen on a typical website, demonstrating how a user would submit a query to your application.
Please post comments or corrections to the Author Online forum:
All other book titles, as listed in table 3.2, would not be considered relevant for customers interested in purchasing a new home.
Buying a New Car A naïve approach to implementing this search using a traditional SQL (Structured Query.
Language) database would be to simply query for the exact text that users enter: SELECT * FROM Books The problem with this approach, of course, is that none of the book titles in your book.
In addition, customers will only ever see results for future queries if the query matches the full book title exactly.
Perhaps a more forgiving approach would be to search for each single word within a customer’s query:
Please post comments or corrections to the Author Online forum:
Of course, you may believe that requiring documents to match all of the words your customers include in their queries is overly restrictive.
You could easily make the search experience more flexible by only requiring a single word to exist in any matching book titles, by issuing the following SQL query:
Additionally, because this query is performing only partial string matching on each keyword, any book title that merely contains the letter “a” is also returned.
The preceding example, which required all of the terms, also matched on the letter “a”, but we did not experience this problem of returning too many results because the other keywords were more restrictive.
We have just seen that the first query (requiring all words to match) resulted in many.
Please post comments or corrections to the Author Online forum:
There is no sense of relevancy ordering in the results – books which only match one of the queried words often show up higher than books matching multiple or all of the words in the customer’s query.
These queries will become slow as the size of the book catalog grows or the number of customer queries grows, as the query must scan through every book’s title to find partial matches instead of using an index to look up the words.
Search engines like Solr really shine in solving problems like the ones listed above.
Solr accomplishes all of this utilizing an index that maps content to documents instead of mapping documents to content like a traditional database model.
Solr utilizes Lucene’s inverted index to power its fast searching capabilities, as well as.
While we’ll not get into many of the internal Lucene data structures in this book (I recommend picking up a copy of Lucene in Action if you want a deeper dive), it is important to understand the high-level structure of the inverted index.
Recalling our previous book-searching example, we can get a feel for what an index mapping each term to each document would look like from table 3.5
Please post comments or corrections to the Author Online forum:
Table 3.5  Mapping of text from multiple documents into an inverted index.
The left table contains nine original documents with their content, while the right table represents an inverted search index containing each of the terms from the original content mapped back to the documents in which they can be found.
Table 3.5 demonstrates the process of mapping the content from documents into an.
While a traditional database representation of multiple documents would contain a document’s id mapped to one or more content fields containing all of the words/terms in that document, an inverted index inverts this model and maps each.
Please post comments or corrections to the Author Online forum:
You can tell from looking at table 3.5 that the original input text was split on spaces and that each term was transformed into lowercase text before being inserted into the inverted index, but everything else remained the same.
A few final important details should be noted about the inverted index:
All terms in the index map to one or more documents.
Terms in the inverted index are sorted in ascending alphanumeric order.
As you will see in the next section, the structure of Lucene’s inverted index allows many powerful query capabilities that maximize both the speed and flexibility of keyword based searching.
Now that we’ve seen what content looks like in Lucene’s inverted index, let’s jump into.
In this section, we’ll go over the basics of looking up terms and phrases in this inverted search index and utilizing Boolean logic and fuzzy queries to enhance these lookup capabilities.
Figure 3.2 Simple search to demonstrate nuances of query interpretation.
We saw in the last section that all of the text in our content field was broken up into individual terms when inserted into the Lucene index.
Now that we have an incoming query, we need to select from among several options for querying the index:
Please post comments or corrections to the Author Online forum:
All of these options are perfectly valid approaches, depending upon your use case, and thanks to Solr’s powerful querying capabilities built using Lucene, are very easy to accomplish using boolean logic.
There are two identical ways to write this query using the standard query parser in Solr:
These two are logically identical, and in fact, the second example gets parsed and ultimately reduced down to the first example.
The “+” symbol is a unary operator which means that part of the query immediately following it is required to exist in any documents matched, whereas the “AND” keyword is a binary operator which means that the part of the query immediately preceding and the part of the query immediately following it are both required.
By default, Solr is also configured to treat any part of the query without an explicit operator as an optional parameter, making the following identical:
While the default configuration in Solr assumes that a term or phrase by itself is an optional term, this is configurable on a per-query basis using the q.op url parameter with many of Solr’s query handlers.
Do note, however, that if you change the default operator from OR to AND that now parts of the query without an operator will always be required unless you explicitly place an OR between them to override the default AND operator.
Please post comments or corrections to the Author Online forum:
The Solr query syntax can represent arbitrarily complex queries through grouping terms together using parenthesis like the following examples:
The use of required terms, optional terms, negated terms, and grouped expressions provides a very powerful and flexible set of query capabilities that allow arbitrarily complex lookup operations against the search index, as we’ll see in the following section.
With a basic understanding of terms, phrases, and Boolean queries in place, we can now dive into exactly how Solr is able to utilize the internal Lucene inverted index to find matching documents.
Let us recall our index of books from earlier, re-produced in table 3.6
Table 3.6  Inverted index of terms from a collection of book titles.
Please post comments or corrections to the Author Online forum:
If a customer passes in a query now of new home, how exactly is Solr able to find.
As such both terms must be looked up separately in the Lucene Index:
Term  Documents Once the list of matching documents is found for each term, Lucene will perform set.
Assuming our default operator is an OR, this query would result in a union of the result sets for both terms, as pictured in the Venn diagram in figure 3.3
Please post comments or corrections to the Author Online forum:
In addition to union and intersection queries, negating particular terms is also common.
Figure 3.5 demonstrates a breakdown of the results expected for each of the result set permutations of this two-term search query (assuming a default OR operator)
Figure 3.5  Graphical representation of using common boolean query operators.
Please post comments or corrections to the Author Online forum:
As you can see, the ability to search for required terms, optional terms, negated terms, and grouped terms provides a very powerful mechanism for looking up single keywords.
As we’ll see in the following section, Solr also provides the ability to query for multi-term phrases.
We saw earlier that, in addition to querying for terms in our Lucene Index, that it is also.
Recalling that the index contains only individual terms, however, you may be wondering exactly how we can search for full phrases.
The short answer is that each term in a phrase query is still looked up in the Lucene Index individually, just as if the query new home had been submitted instead of “new home.” Once the overlapping document set is found, however, a feature of the index that we conveniently left out of our initial inverted index discussion is utilized.
This feature, called Term Positions, is the optional recording of the relative position of terms within a document.
Table 3.7 demonstrates how documents (on the left side of the table) map into an inverted index containing Term Positions (on the right side of the table)
Please post comments or corrections to the Author Online forum:
The term position actually goes one step further, telling us where in the document each term appears.
Table 3.8 shows a condensed version of the inverted index focused upon only the primary terms under discussion: new and home.
Searching for specific phrases is not the only benefit provided by term positions, though.
We’ll see in the next section another great example of their use to improve our search results quality.
It’s not always possible to know up-front exactly what will be found in the Solr index for any given search, so Solr provides the ability to perform several types of fuzzy matching queries.
Fuzzy matching is defined as the ability to perform inexact matches on terms in the search index.
For example, someone may want to search for any words that start with a particular prefix (known as wildcard searching), may want to find spelling variations within one or two characters (known as fuzzy or edit distance searching), or may even want to match two terms within some maximum distance of each other (known as proximity searching)
For use cases in which multiple variations of the terms or phrases queried may exist across the documents being searched, these fuzzy matching capabilities serve as a very powerful tool.
In this section, we’ll explore multiple fuzzy matching query capabilities in Solr, including wildcard searching, range searching, edit distance searching, and proximity searching.
Suppose you wanted to find any documents which started with the letters “offic"
One way to do this would be to create a query which enumerates all of the possible variations:
Please post comments or corrections to the Author Online forum:
Requiring that this list of words be turned into a query up-front can be an unreasonable expectation for customers, or even for you on behalf of your customers.
Since all of the variations you could match already exist in the Solr index, you can use the asterisk (*) wildcard character to perform this same function for you:
In addition to matching the end of a term, a wildcard character can be used inside of the.
The asterisk wildcard (*) shown above matches zero or more characters in a term.
While the wild card functionality in Solr is fairly robust, it is only possible, by default, to use wildcards inside or at the end of a term.
If you needed to match all terms ending in “ing” (like caring, liking, and smiling), for example, you would receive an exception if you tried running this search:
The reason for this is that Solr searches through the inverted index for terms which begin with the characters provided before the wildcard, and without some initial characters to begin that lookup, it is too expensive to walk the entire index to find matching terms.
If you really need to be able to search using these leading wildcards, a solution to this problem does exist, but it will require you to perform some additional configuration.
The solution is achieved by adding the ReversedWildcardFilterFactory to your field type’s analysis chain (configuring text processing will be discussed in chapter 5)
The ReversedWildcardFilterFactory works by double storing the indexed content in the Solr index (once for the text of each term, and once for the reversed text of each term):
Do note, however, that turning this feature on requires dual-storing all terms in the Solr index, increasing the size of the index and thus slowing overall searches down somewhat.
Turning this capability on is thus not recommended unless it is needed within your search application.
Please post comments or corrections to the Author Online forum:
One last important point to note about wildcard searching is that wildcards are only meant to work on individual search terms, not on phrase searches, as demonstrated by the following example:
If you need the ability to perform wildcard searches within a phrase, you will have to store the entire phrase in the index as a single term, which you should feel comfortable doing by the end of chapter 5
This can be useful when you want to search for a particular subset of documents falling within a range.
Each of the above range queries surrounds the range with square brackets, which is the.
Solr also supports exclusive range searching through use of curly braces:
It is important to note that the ordering of terms of range queries is exactly that – the order in which they are found in the Solr index, which is an alphanumeric sorted order.
Numeric types in Solr, at least the ones we’ll recommend in the coming chapters, compensate for this by indexing the incoming content in a special way, but it is important to understand that the sort order within the Solr index is dependent upon how the data within the field is processed when it is written to the Solr index.
We’ll dive much deeper into this kind of content analysis in chapter 5
FUZZY/EDIT DISTANCE SEARCHING For many search application, it is important not only to match a customer’s text exactly,
Solr provides the ability to handle character variations using edit distance.
Please post comments or corrections to the Author Online forum:
Solr achieves these fuzzy edit distance searches through the use of the tilde (~) character as follows:
The above query matches both the original term (administrator) and any other terms.
An edit distance here is defined as an insertion, a deletion, a substitution, or a transposition of characters.
It is also possible to modify the strictness of edit distance searches to allow matching of terms with an edit distance of greater than one:
This edit distance principle is applicable beyond just searching for alternate characters within a term – it can also be applied between terms for variations of phrases.
Let’s say that that you want to search across a Solr index of employee profiles for executives within your company.
One way to do this would be to enumerate each of the possible executive titles within your company:
Of course, this assumes you know all of the possible titles, which may be unrealistic if you’re searching across other companies with which you’re poorly acquainted, or if you have a more challenging use case.
Another possible strategy is to search for each term independently:
Please post comments or corrections to the Author Online forum:
Query: chief AND officer This should match all of the possible use cases, but it will also match any document which.
Thankfully, Solr provides a simple solution to this problem: proximity searching.
The edit distances above can be seen as nothing more than sloppy phrase searches.
These queries will yield the exact same results, because an edit distance of zero is the very definition of an exact phrase search.
Both mechanisms make use of the term positions stored in the Solr index (which we discussed in section 3.1.6) to calculate the edit distances.
At this point, you should have a basic grasp of how Solr stores information in its inverted index and queries that index to find matching documents.
This includes looking up terms, using Boolean logic to create arbitrarily complex queries, and getting results back as a result of the set intersections of each of the term lookups.
We also discussed how Solr stores term positions and is able to use those to find exact phrases and even fuzzy phrase matches through the use of proximity queries and edit distance calculations.
For fuzzy searching within single terms, we examined the use of wildcards and edit distance searching to find misspellings or very similar words.
While Solr’s query capabilities will be expanded upon in chapter 6, these key operations serve as the foundation for generating most Solr queries.
They also prepare us nicely with the needed background for our discussion of Solr’s keyword relevancy scoring model, which we’ll discuss in the next section.
Finding matching documents is the first critical step in creating a great search experience, but it is only the first step.
No customer is willing to wade through page after page of search results to find the document he or she is seeking.
Please post comments or corrections to the Author Online forum:
Solr does a very good job out of the box at ensuring the ordering of search results brings back the best matches at the top of the results list.
It does this by calculating a relevancy score for each document and then sorting the search results from the top score to the lowest.
This section will provide an overview of how these relevancy scores are calculated and what factors influence them.
We’ll dig into both the theory behind Solr’s default relevancy calculation and also into the specific calculations used to calculate the relevancy scores, providing intuitive examples along the way to ensure you leave this section with a solid understanding of what, to many, can be the most eluding aspect of working with Solr.
We’ll start by discussing the Similarity class, which is responsible for most aspects of a query’s relevancy score calculation.
Solr’s relevancy scores are based upon a Similarity class which can be defined on a per-field basis in Solr’s Schema.xml (discussed later in chapter 5)
The Similarity class is a Java class that defines how a relevancy score is calculated based upon the results of a query.
While you can choose from multiple similarity classes, or even write your own, it is important to understand Solr’ default Similarity implementation and the theory behind why it works so well.
First, it makes use of a Boolean model (described in the last section) to filter out any documents that do not match the customer’s query.
Then, it utilizes a Vector Space model for scoring, drawing the query as a vector, as well as an additional vector for each document.
The similarity score for each document is based upon the cosine between the query vector and that document’s vector, as depicted in figure 3.6
The smaller the angle between the query term vector and a document term vector, the more similar the query and the document are considered to be.
Please post comments or corrections to the Author Online forum:
In this Vector Space scoring model a term vector is calculated for each document and is compared with the corresponding term vector for the query.
The similarity of two vectors can be found by calculating a cosine between them: with a cosine of 1 being a perfect match and a cosine of zero representing no similarity.
More intuitively, the closer the two vectors appear together, as in the image above, the more similar they are.
Thus, the smaller the angle between vectors, or the larger the cosine, the closer the match.
Of course, the most challenging part of this whole process is actually coming up with reasonable vectors which represent the important features of the query and of each document for comparison.
Let’s take a look at the entire, complicated relevancy formula for the DefaultSimilarity class.
We’ll then go line by line to explain intuitively what each component of the relevancy formula is attempting to accomplish.
Given a query (q) and a document (d), the similarity score for the document to the query can be calculated as shown in figure 3.7
Each component in this formula will be explained in detail in the following sections.
Wow – that equation can be quite overwhelming, especially at first glance.
Fortunately, it is much more intuitive when each of the pieces are broken down.
The math is presented above for reference, but you will likely never need to really dig into the full equation unless you decide to overwrite the similarity for your search application.
Please post comments or corrections to the Author Online forum:
At a high level, the important concepts are demonstrated by the high-level formula – namely, Term Frequency (tf), Inverse Document Frequency (idf), Term Boosts (t.getBoost), the Field Normalization (norm), the Coordination Factor (coord), and the Query Normalization (queryNorm)
Furman University, one of the top liberal arts universities in the southern United States.
Furman also consistently ranks among the most beautiful campuses to visit and ranks among the top 50 liberal arts colleges nation-wide each year.
Today, international leaders met with the President of the United States to discuss options for dealing with growing instability in global financial markets.
President Obama indicated that the United States is cautiously optimistic about the potential for significant improvements in several struggling world economies pending the results of upcoming elections.
The President indicated that the United States will take whatever actions necessary to promote continued stability in the global financial markets.
In general, a document is considered to be more relevant for a particular topic (or query term) if the topic appears multiple times.
This is the basic premise behind the TF (Term Frequency) component of the default Solr relevancy formula.
The more times the search term appears within a document, the more relevant that document is considered.
Please post comments or corrections to the Author Online forum:
Imagine if someone were to search for the book The Cat in the Hat by Dr.
Common sense would indicate that words which are more rare across all documents are likely to be better matches for a query than terms which are more common.
Because the Inverse Document Frequency appears for the term in both the query and the document, it is squared in the relevancy formula.
Figure 3.8  Visual depiction of the relative significance of terms as measured by Inverse Document Frequency.
The terms which are more rare are depicted as larger, indicating a larger Inverse Document Frequency.
Instead, we would expect the important terms to resemble the largest terms in figure 3.9
Figure 3.9 Another demonstration of relative score of terms derived from Inverse Document Frequency.
Once again, a higher Inverse Document Frequency indicates a more rare and more relevant term, which is depicted here using larger text.
Please post comments or corrections to the Author Online forum:
Clearly the user is looking for someone who knows Solr who can be a team lead, so these terms stand out with considerably more weight when found in any document.
Term Frequency and Inverse Document Frequency, when multiplied together in the relevancy calculation, provide a nice counter-balance to each other.
The term frequency elevates terms which appear multiple times within a document, while the inverse document frequency penalizes those terms which appear commonly across many documents.
Query-time boosting is the most flexible and easiest to understand form of boosting, utilizing the following syntax:
In addition to query time boosting, it is also possible to boost documents or fields within.
These boosts are factored into the Field Norm, which is covered in the following section.
The default Solr relevancy formula calculates three kinds of normalization factors.
Please post comments or corrections to the Author Online forum:
This byte packs a lot of information in: the boost set on the document when indexed, the boost set on the field when indexed, and a length normalization factor which penalizes longer documents and helps shorter documents (under the assumption that finding any given keyword in a longer document is more likely and thus less relevant)
The actual field norms are calculated using the formula in figure 3.10
Field Norms factor in the matching documents boost, the matching field’s boost, and a length normalization factor which penalizes longer documents.
It is worth mentioning that Solr actually allows the same field to be added to a document multiple times (performing some magic under the covers to actually map each separate entry for the field into the same underlying Lucene field)
Because duplicate fields are ultimately mapped to the same underlying field, if multiple each of the multiple fields with the same name.
In addition to the index time boosts, a parameter called the Length Norm is also factored.
The Length Norm is computed by taking the square root of the number of terms in the field for which it is calculated.
The purpose of the Length Norm is to adjust for documents of varying lengths, such that longer documents do not maintain an unfair advantage simply by having a larger likelihood of containing any particular term a given number of times.
For example, let’s say that you perform a search for the keyword “Beijing”
Common sense would indicate that document in which Beijing is proportionally more prevalent is probably a better match, everything else being equal.
This is what the Length Norm attempts to take into account.
The overall Field Norm, calculated from the product of the document boost, the field boost, and the length norm, is encoded into a single byte which is stored in the Solr index.
Because the amount of information being encoded from this product is larger than a single byte can store, some precision loss does occur during this encoding.
Please post comments or corrections to the Author Online forum:
It does not affect the overall relevancy ordering, as the same queryNorm is applied to all documents.
It merely serves as a normalization factor to attempt to make scores between queries comparable.
It utilizes the sum of the squared weights for each of the query terms to generate this factor, which is multiplied with the rest of the relevancy score to normalize it.
The Query Norm should not affect the relative weighting of each document that matches a given query.
The Coord factor’s role is to measure how much of the query each document matches.
If all four of these terms match, the Coord factor is 4/4
The idea here behind the Coord factor is that, all things being equal, documents which contain more of the terms in the query should score higher than documents which only match a few.
We have now discussed all of the major components of the default relevancy algorithm in Solr.
We discussed term frequency, inverse document frequency, the two most key components of the relevancy score calculation.
We then went through boosting and normalization factors, which refine the scores calculated by term frequency and inverse document frequency alone.
With a solid conceptual understanding and a detailed overview of the specific components of the relevancy scoring formula, we’re now set to discuss Precision and Recall, two important aspects for measuring the overall quality of the result sets returned from any search system.
The information retrieval concepts of Precision (a measure of accuracy) and Recall (a measure of thoroughness) are very simple to explain, but are also very important to understand when building any search application or understanding why the results being returned are not meeting your business requirements.
We’ll provide a brief summary here of each of these key concepts.
Please post comments or corrections to the Author Online forum:
Let’s return to our earlier example from section 3.1 about searching for a book on the topic of buying a new home.
We’ve determined that by our internal company measurements that the books in table 3.10 would be considered good matches for such a query.
All other book titles, for purposes of this example, would not be considered relevant for.
For this example, if all of the documents which were supposed to be returned (documents Matches / 3 Total Matches), which would be perfect.
Because Precision only considers the overall accuracy of the results that come back and not the comprehensiveness of the result set, we need to counterbalance the Precision measurement with one that takes thoroughness into account – Recall.
Please post comments or corrections to the Author Online forum:
Whereas Precision measures how “correct” each of the results being returned is, Recall is.
Recall is essentially answering the question of “How many of the correct documents were returned.
In the next section, we’ll talk about strategies for striking an appropriate balance between Precision and Recall.
Though there is clearly tension between the two, Precision and Recall are not mutually.
Maximizing for full Precision and full Recall is the ultimate goal of just about every search relevancy-tuning endeavor.
With a contrived example (or a hand-tuned set of results), this seems easy, but in reality, this is a very challenging problem.
Many techniques can be undertaken within Solr to improve either Precision or Recall, though most are geared more toward increasing Recall in terms of the full document set being returned.
Aggressive textual analysis (to find multiple variations of words) is a great.
Please post comments or corrections to the Author Online forum:
One common way to approach the Precision versus Recall problem in Solr is to actually attempt to solve for both: measuring for Recall across the entire result set and measuring for Precision only within the first page (or few pages) of search results.
Following this model, “better” matches will be boosted to the top of the search results based upon how well you tune your use of Solr's relevancy scoring calculations, but you will also find that many poorer matches appear at the bottom of your search results list if you actually went to the last page of the search results.
This is only one way to approach the problem, however.
Since many websites, for example, want to appear to have as much content as possible, and since those sites know that visitors will never actually go beyond the first few pages, they can actually show very precise results on the first few pages which yielding a very high Recall value across the entire result set since they are increasing the chances of pulling back more content by being very lenient in which keywords are able to match the initial query.
The decision on how to best balance Precision and Recall is ultimately dependent upon your use case.
In scenarios like legal discovery, there is a very heavy emphasis placed on Recall, as there are legal ramifications if any documents are missed.
For other use cases, the requirement may simply be to find a few really great matches and find nothing that does not exactly match every term within the query.
Most search applications fall somewhere between these two extremes, and striking the right balance between Precision and Recall is a never-ending challenge – mostly because there is generally no one right answer.
Regardless, understanding the concepts of Precision and Recall and why changes you make swing you more towards one of these two conceptual goals (and likely away from the other) is critical to effectively improving the quality of your search results.
We have an entire chapter dedicated to relevancy tuning, chapter 16, so you can be sure you will see this tension between precision and recall surface again.
One of the most appealing aspects of Solr, beyond it’s speed, relevancy, and powerful text searching features, is how well it scales.
Solr is able to scale to handle billions of documents and an infinite number of queries by adding servers.
Chapter 12 will provide an in-depth overview of scaling Solr in production, but this section will lay the groundwork for how to think about the necessary characteristics for operating a scalable search engine.
Specifically, we’ll discuss the nature of Solr documents as denormalized documents and why this enables linear scaling across servers, how distributed searching works, the conceptual shift from thinking about servers to thinking about clusters of servers, and some of the limits of scaling Solr.
Please post comments or corrections to the Author Online forum:
Central to Solr is the concept of all documents being denormalized.
A completely denormalized document is one in which all fields are self-contained within the document, even if the values in those fields are duplicated across many documents.
This concept of denormalized data is common to many NoSQL technologies.
This is in contrast to a normalized document where relationships between parts of the document may be broken up into multiple smaller documents, the pieces of which can be joined back together at query time.
If you have any training whatsoever in building normalized tables for relational databases, please leave that training at the door when thinking about modeling content into Solr.
Figure 3.11 Solr documents do not follow the traditional normalized model of a relational database.
This figure demonstrates how NOT to think of Solr Documents.
Instead of thinking in terms of multiple entities with relationship to each other, a Solr Document is modeled as a flat, denormalized data structure, as shown in listing 3.2
While this figure shows the data nicely normalized into separate tables for the employees’ personal information, location, and company, this is not.
Please post comments or corrections to the Author Online forum:
Listing 3.2 shows the denormalized representation for each of these employees as mapped to a Solr Document.
In a traditional relational database, a query can be constructed which will join data from multiple tables when resolving a query.
Solr knows about terms that map to documents but does not natively know about any relationships between documents.
That is, if you wanted to search for all users (in the previous example) who work for companies in Decatur, GA, you would.
Please post comments or corrections to the Author Online forum:
While this denormalized document data model may sound limiting, it also provides a sizable advantage – extreme scalability.
Because we can make the assumption that each document is self-contained, this means that we can also partition documents across multiple servers without having to keep related documents on the same server (since documents are independent of one another)
This fundamental assumption of document independence allows queries to be parallelized across multiple partitions of documents and multiple servers to improve query performance, and this ultimately allows Solr to scale horizontally to handle querying billions of documents.
This ability to scale across multiple partitions and servers is called Federated or Distributed Searching, and will be covered in the following section.
The world would be a much simpler place if every important data operation could be run using a single server… it would also be a much more boring world.
In reality, sometimes your search servers become overloaded by either too many queries at a time or by too much data needing to be searched through for a single server to handle.
In the latter case, it is necessary to break your content into two or more separate Solr indexes, each of which contains separate partitions of your data.
Then, every time a search is run, it will actually be sent to both servers, and the results will be returned and aggregated before being returned from the search engine.
Solr includes this kind of distributed searching capability out of the box.
We’ll discuss how to manually segment your data into multiple partitions in chapter 12 when we talk about scaling Solr for production.
Conceptually, each solr index (called a Solr Core) is available through it’s own unique url, and each of those Solr Cores can be told to perform an aggregated search across other Solr shards using the following syntax:
The “shards” parameter is used to specify the location of one or more Solr Cores.
A shard is a partition of your index, so the shards parameter on the url tells Solr to aggregate results from multiple partitions of your data which are found in separate Solr Cores.
There is no requirement that separate Solr cores be located on separate machines.
Please post comments or corrections to the Author Online forum:
The important take-away here is the nature of scaling Solr.
The reason for this is that a federated search across multiple Solr Cores is run in parallel on each of those index partitions.
Thus, if you divide one Solr index into two Solr indexes with the same number of documents, the distributed search across the two indexes should be approximately 50% faster, minus any aggregation overhead.
This should also theoretically scale to any other number of servers (in reality, you will eventually hit a limit at some point)
The conceptual formula for determining total query speed after adding an additional index partition (assuming the same total number of documents) is thus as follows:
Query Speed on N indexes)/(N+1) This formula is useful for estimating the benefit you can expect from increasing the.
Since Solr scales nearly linearly, you should be able to reduce your query times proportional to the additional number of Solr cores (partitions) you add, assuming you’re not constrained by server resources due to heavy load.
In the last section we introduced the concept of distributed searching to enable scaling to handle large document sets.
It is also possible to add multiple essentially identical servers into your system to balance the load of high query volumes.
Both of these scaling strategies rely on a conceptual shift away from thinking about servers and toward thinking about “clusters” of servers.
A cluster of servers is essentially defined as multiple servers, working in concert, to perform the same function.
Take the following example, which should look similar to the example from section 3.3.6:
Please post comments or corrections to the Author Online forum:
Notice that the servers, for this use case, are mutually dependent.
If one becomes unavailable for searching, they all become unavailable for searching and begin failing, as indicated in the exception in listing 3.3
It is thus important to think in terms of “clusters” of servers instead of single servers when building out Solr solutions which must scale beyond a single box, as those servers are essentially combining to serve as a single computing resource.
Solr provides some built-in cluster management capabilities, through the use of Apache Zookeeper, which will be covered in chapter 13
As we wrap up our discussions of the key concepts behind searching at scale, we should be clear that Solr does have its limitations, several of which will be discusses in the next section.
Solr is an incredibly powerful document-based NoSQL datastore which supports full text searching and data analytics.
We have already discussed the powerful benefits of Solr’s inverted index and complex keyword-based Boolean query capabilities.
We have also seen how important relevancy is, and we’ve seen that Solr can scale essentially linearly across multiple servers to handle additional content or query volumes.
One limit, as we have already seen, is that Solr is NOT relational in any way across documents.
It is not well suited for joining significant amounts of data across different fields on different documents, and it cannot perform join operations at all across multiple servers.
While this is a functional limit of Solr, as compared to relational databases, this assumption of independence of documents is a tradeoff common among many NoSQL technologies, as it enables them to scale well beyond the limits of relational databases.
We have also already discussed the denormalized nature of Solr documents – data which is redundant must be repeated across each document for which that data applies.
This can be particularly problematic when the data in one field which is shared across many documents changes.
For example, let’s say that you were creating a search engine of social networking user profiles, and one of your user’s, John Doe becomes friends with another user named Coco.
Please post comments or corrections to the Author Online forum:
This harkens back to the notion of Solr not being relational in any way.
An additional limitation of Solr is that it currently serves primarily as a document storage mechanism – that is, you can insert, delete and update documents, but not single fields (very easily)
Solr does currently have some minimal capability to update a single field, but only if the field is attributed in such a way that its original value is stored in the index, which can be very wasteful.
Even then, Solr is internally still updating the entire document based upon re-indexing all of the stored fields internally.
What this means is that, whenever a new field is added to Solr or the contents of an existing field has changes, every single document in the Solr index must be reprocessed in its entirety before the data will be populated for the new field.
Many other NoSQL systems suffer from this same problem, but it is worth noting that data updates across the corpus require a non-trivial amount of document management to ensure the updates make it to Solr and in a timely fashion.
Solr is also optimized for a very specific use case, which is taking search queries with small numbers of search terms and rapidly looking up each of those terms to find matching documents, calculating relevancy scores and ordering them all, and then only returning a few actual results for display.
Solr is not optimized, however, at processing very long (1000’s of terms) queries or returning back very large result sets to users.
One final limitation of Solr worth mentioning is its elastic scalability: the ability to automatically add and remove servers and redistribute content to handle load.
While Solr scales very well across servers, it does not yet elastically scale by itself in a fully automatic way.
Recent work with Solr Cloud (covered in chaper 13), utilizing Apache Zookeeper for cluster management, is a great first step in this direction, but there are still many features to be worked out, such as automatic content resharding and pluggable sharding strategies which are currently being discussed but are not yet fully implemented.
In this chapter, we’ve discussed the key search concepts that serve as the foundation for most of Solr’s search capabilities.
We discussed the structure of Solr’s inverted index and how it maps terms to documents in a way that allows quick execution of complex Boolean.
We also discussed how fuzzy queries and phrase queries use position information to match misspellings and variations of terms and phrases in the Solr index.
We took a deep dive into Solr relevancy, laying out the default relevancy formula Solr uses and explaining conceptually how each piece of relevancy scoring works and why it exists.
We then provided a brief overview of the concepts of Precision and Recall, which serve as two opposing forces within the field of information retrieval and provide us with a good conceptual framework with which to judge whether or not our search results are meeting our goals.
Finally, we discussed key concepts for how Solr scales, including discussions of content denormalization within documents and federated distributed searching to ensure query.
Please post comments or corrections to the Author Online forum:
At this point, you should have all of the conceptual background necessary to understand the core features of Solr throughout the rest of this book and should have a solid grasp the most important search concepts for building a killer search application.
In the next chapter, we’ll begin digging into Solr’s key configuration settings, which will enable more fine-grained control over many of the features discussed in this chapter.
Please post comments or corrections to the Author Online forum:
Up to this point, you've taken much of what has been presented on faith without learning how Solr actually works.
We'll change that in this chapter and the next by looking under-thehood to learn how Solr is configured and how configuration settings impact Solr's behavior.
As we learned in chapter 2, Solr works out-of-the-box without making any configuration changes.
But, at some point, you're going to need to make some configuration changes to optimize Solr for your specific search application requirements.
Broadly speaking, most of the configuration you'll do with Solr focuses around three main XML files:
In chapter 5, we'll learn all about schema.xml, which drives how your index is structured.
As most of Solr's configuration is specified in XML documents, this chapter contains numerous code listings showing XML snippets from solrconfig.xml and solr.xml.
However, our main focus is on the concepts behind the configuration settings rather than the specific XML syntax, which is mostly self-explanatory.
Please post comments or corrections to the Author Online forum:
To begin let's see what happens from a configuration perspective when you start the Solr server.
Recall from chapter 2 that Solr runs as a Java Web application in Jetty.
The Solr Web application uses a global Java system property (solr.solr.home) to identify the root directory from which to look for configuration files.
Figure 4.1 depicts how solr.xml and solrconfig.xml are used during the Solr initialization process.
Figure 4.1 Depiction of how solr.xml and solrconfig.xml are used to configure Solr during initialization.
During initialization, Solr locates solr.xml in the top-level Solr home directory; in the example server this is $SOLR_INSTALL/example/solr/solr.xml.
The solr.xml file identifies one or more cores to initialize.
Please post comments or corrections to the Author Online forum:
The initial configuration only has a single core named "collection1", but in general there can be many cores defined in solr.xml.
Now that we've seen how Solr identifies configuration files during startup, let's turn our attention to understanding the main sections of the solrconfig.xml, as that will give you an idea of what's to come in the rest of this chapter.
To illustrate the concepts in solrconfig.xml, we'll build upon the work done in chapter 2 by using the pre-configured example server and the Solritas example search UI.
To begin, we recommend that you start up the example server we used in chapter 2 using:
This will display the active solrconfig.xml for the collection1 core running on your computer.
Listing 4.2 shows a condensed version of the solconfig.xml to give you an idea of the main elements.
Please post comments or corrections to the Author Online forum:
As you can see, solrconfig.xml has a number of complex sections.
The good news is that you don’t have to worry about these until you encounter a specific need.
On the other hand, we do think it is a good idea to make a mental note of what is in solrconfig.xml as it shows how flexible Solr is and what types of behavior you can control and extend.
When organizing this chapter, we chose to present the configuration settings in an order that builds on previous sections rather than following the order of elements in the XML document.
For example, we present Solr's request handling framework before we discuss caching even though cache-related settings come before request handler settings in solrconfig.xml.
We took this approach because you should understand how requests are handled before you worry about optimizing a specific type of request with caching.
Please post comments or corrections to the Author Online forum:
However, we save a discussion of index-related settings for the next chapter when you can learn about them after gaining a basic understanding of the indexing process.
Specifically, you can ignore the following elements until chapter 5:
As you work through solrconfig.xml, you will encounter common XML elements that Solr uses to represent various data structures and types.
Table 4.1 provides a brief description and example of the types of elements Solr uses throughout the solrconfig.xml document.
You will also encounter these elements in XML search results, so please spend a minute getting familiar with this Solr-specific syntax.
Table 4.1 Solr's XML elements for data structures and typed values.
Please post comments or corrections to the Author Online forum:
Learning about configuration is not the most exciting of tasks so to help keep you interested, we recommend that you experiment with configuration changes as you work through this chapter.
However, your changes won't be applied until you reload the Solr core.
In other words, Solr doesn't "watch" for changes to solrconfig.xml and apply them automatically; you have to take an explicit action to apply configuration changes.
For now, the easiest way to apply configuration changes is to use the Reload button from the Core Admin page of the administration console as shown in figure 4.2
Figure 4.2 Reload a core to apply configuration changes from the Core Admin page.
Please post comments or corrections to the Author Online forum:
If you're running Solr locally, then go ahead and click on the Reload button for the collection1 core just to verify the functionality works.
Also, we'll see another way to reload cores programmatically using the Core Admin API at the end of this chapter.
Now that we've covered some of the configuration background, let's start our tour of solrconfig.xml by looking at some miscellaneous settings for the Solr server.
Listing 4.3 shows the configuration settings we'll discuss in this section.
Listing 4.3 Global settings near the top of solrconfig.xml ...
The <luceneMatchVersion> element controls the version of Lucene your index is based on.
If you're just starting out, then use the version that is specified in the example server, such as:
Now imagine that after running Solr for several months and indexing millions of.
When you start the updated Solr server, it uses the <luceneMatchVersion> to understand which version your index is based on and whether to disable any Lucene features that depend on a later version than what is specified.
You will be able to run the upgraded version of Solr against your older index, but at some point you may need to raise the <luceneMatchVersion> to take advantage of new features and bug fixes in Lucene.
In this case, you can either re-index all your documents or use Lucene's built-in index upgrade tool1
As that is a problem for the future, we'll refer you to the JavaDoc for instructions on how to run the upgrade tool.
Refer to the Lucene JavaDoc for the org.apache.lucene.index.IndexUpgrader class for usage instructions.
Please post comments or corrections to the Author Online forum:
Each <lib> element identifies a directory and a regular expression to match files in the directory.
Notice that the dir attribute uses relative paths, which are evaluated from the core directory root, commonly referred to as the core instanceDir.
Consequently, the two example <lib> elements shown above result in the following JAR files being added to Solr's classpath:
Note that the version number for the apache-solr-langid JAR file may be different depending on the exact version of Solr 4 you are using.
Alternatively, you can use the path attribute to identify a single JAR file, such as:
Alternatively, you can also put JAR files for plug-ins in the $SOLR_HOME/lib directory,
In a nutshell, an MBean is a Java object that exposes configuration properties and statistics using the Java Management Extensions (JMX) API.
This allows Solr to be integrated into your existing system administration infrastructure.
However, you don't need an external JMX-enabled monitoring tool to see Solr's MBeans in action.
The Solr administration console provides access to all of Solr's MBeans.
Please post comments or corrections to the Author Online forum:
We will see a few more examples of inspecting Solr's MBeans from the administration console throughout this chapter.
For now, let's move on to learning how Solr processes requests.
Solr's main purpose is to search, so it follows that handling search requests is one of the most important processes in Solr.
In this section, you'll learn how Solr processes search requests and how to customize request handling to better fit your specific search requirements.
For example, if you want to query Solr, then you send an HTTP GET request.
Alternatively, if you want to index a document in Solr, you use an HTTP POST request.
Listing 4.4 shows a HTTP GET request to query the example Solr server.
Please post comments or corrections to the Author Online forum:
You can either input this URL into a Web browser, use a command-line tool like cURL, or our recommended approach is to use the example driver application that comes with the book.
To execute this request using the example driver, you simply do:
The output when running this utility for listing 4.4 is:
Please post comments or corrections to the Author Online forum:
The http utility provides a couple of other options to allow you to override the address of.
To see a full list of options, simply do: java -jar target/sia-examples.jar http -h.
Figure 4.4 shows the sequence of events and main components involved in handling this Solr request.
Figure 4.4 Sequence of events to process a request to the /select request handler.
A client application sends an HTTP GET request to in the query string of the GET request.
Jetty accepts the request and routes it to Solr's unified request dispatcher using the /solr context in the request path.
In technical terms, the unified request dispatcher is a Java servlet filter mapped to /* for the solr Web application, see org.apache.solr.servlet.SolrDispatchFilter.
Please post comments or corrections to the Author Online forum:
Solr's request dispatcher uses the "collection1" part of the request path to determine the core name.
Next, the dispatcher locates the /select request handler registered in solrconfig.xml for the collection1 core.
The /select request handler processes the request using a pipeline of search components (covered in section 4.2.4 below)
After the request is processed, results are formatted by a response writer component and returned to the client application, by default the /select handler returns results as XML.
The main purpose of the request dispatcher is to locate the correct core to handle the request, such as collection1, and then route the request to the appropriate request handler registered in the core, in this case /select.
In practice, the default configuration for the request dispatcher is sufficient for most applications.
On the other hand, it is common to define a custom search request handler or to customize one of the existing handlers, such as /select.
Let's dig into how the /select handler works to gain a better understanding of how to customize a request handler.
Listing 4.5 shows the definition of the /select request handler from solrconfig.xml.
Behind the scenes, all request handlers are implemented by a Java class, in this case solr.SearchHandler.
At runtime, solr.SearchHandler resolves to the built-in Solr class: org.apache.solr.handler.component.SearchHandler.
In general, anytime you see "solr." as a prefix of a class in solrconfig.xml, then you know this translates to the fully qualified Java package: org.apache.solr.handler.component.
This shorthand notation helps reduce clutter in Solr's configuration documents.
In Solr, there are two main types of request handlers:
Please post comments or corrections to the Author Online forum:
We'll learn more about update handlers in the next chapter when we cover indexing.
For now, let's concentrate on how search request handlers process queries, as depicted in figure 4.5
Figure 4.5 Search request handler made up of parameter decoration (defaults, appends, invariants), firstcomponents, components, and last-components.
The search handler structure depicted in figure 4.5 is designed to make it easy for you to adapt Solr's query processing pipeline for your application.
For example, you can define your own request handler or, more commonly, add a custom search component to an existing request handler, such as /select.
In general, a search handler is comprised of the following phases, where each phase can be customized in solrconfig.xml:
Please post comments or corrections to the Author Online forum:
A request handler does not need to define all phases in solrconfig.xml.
As you can see from listing 4.5, the /select only defines the defaults section.
This means that all other phases are inherited from the base solr.SearchHandler implementation.
In practice, customized request handlers are commonly used to simplify client applications.
For instance, the Solritas example we introduced in chapter 2 uses a custom request handler /browse to power a feature-rich search experience while keeping the client-side code for Solritas very simple.
Hiding complexity from client code is at the heart of Web services and object-oriented design.
Solr adopts this proven design pattern by allowing you to define a custom search request handler for your application, which allows you to hide complexity from your Solr client.
For example, rather than requiring every query to send the correct parameters to enable spell correction, you can use a custom request handler that has spell correction enabled by default.
The Solr example server comes pre-configured with a great example of this design pattern at work to support the Solritas example application.
Listing 4.6 shows an abbreviated definition of the /browse request handler from solrconfig.xml.
Please post comments or corrections to the Author Online forum:
We recommend that you take a minute to go through all the sections of the /browse request handler in the actual solrconfig.xml file.
One thing that should stand out to you is that a great deal of effort was put into configuring this handler, in order to demonstrate many of the great features in Solr.
When starting out with Solr, you definitely do not need to configure something similar for your application all at once.
In other words, you can build up a custom request handler over time as you gain experience with Solr.
Let's see the /browse request handler in action using the Solritas example.
With the example Solr server running, direct your browser to http://localhost:8983/solr/collection1/browse.
Enter "iPod" into the search box as shown in Figure 4.6
Please post comments or corrections to the Author Online forum:
Figure 4.6 Screen shot of Solritas example powered by the /browse request handler.
Take a moment to scan over figure 4.6 to see all the search features activated for this simple query.
Behind the scenes, the Solritas search form submits a query to the /browse request handler.
That's an these features are enabled using default parameters in the /browse request handler.
For example, the default value for the response writer type parameter "wt" is.
Please post comments or corrections to the Author Online forum:
From the log message shown above, the only parameter sent by the form was "q", so all other parameters are set by defaults.
Let's do a little experiment to see the actual query that gets processed.
Instead of using response writer type "velocity", let's set the wt parameter to "xml" so we can see the response in raw form without the HTML decoration provided by Velocity.
Also, in order to see all the query parameters, we need to set the echoParams value to "all"
This is a good example of overriding default values by explicitly passing parameters from the client.
Notice how the number of parameters actually sent to the /browse request handler is quite large.
Please post comments or corrections to the Author Online forum:
A Many more default parameters in this request not shown here From looking at listing 4.7, it should be clear that parameter decoration for a search.
Specifically, the defaults list provides two main benefits to your application.
First, helps simplify client code by establishing sensible defaults for your application in one place.
For instance, setting the response writer type "wt" to "velocity" means that client applications do not need to worry about setting this parameter.
Moreover, if you ever swap out Velocity for another templating engine, your client code does not need to change!
Second, as you can see from listing 4.7, the actual request includes a number of complex parameters needed to configure search components used by Solritas.
For example, there are over twenty different parameters to configure the faceting component for Solritas.
By preconfiguring complex components like faceting, you can establish consistent behavior for all queries while keeping your client code simple.
The /browse handler serves as a good example of what is possible with Solr query processing, but it's also unlikely that it can be used by your application because the default parameters are tightly coupled to the Solritas data model.
For example, range faceting is configured for the price, popularity, and manufacturedate_dt fields.
Consequently, you should treat the /browse handler as an example and not a 100% reusable solution when designing your own application-specific request handler.
Please post comments or corrections to the Author Online forum:
From listing 4.6, notice that the /browse request handler specifies:
This configuration means that the default set of search components is applied and then the spellcheck component is applied.
This is a very common design pattern for search request handlers.
In fact, you'll be hard-pressed to come up with an example of where you need to redefine the <components> phase for a search handler.
At a high-level, the query component parses and executes queries using the active searcher, which is discussed in section 4.3 below.
The specific query parsing strategy is controlled by the "defType" parameter.
For instance, the /browse request handler uses the edismax query parser.
The query component identifies all documents in the index that match the query.
The set of matching documents can then be used by other components in the query processing chain, such as the facet component.
The query component is always enabled and all other components need to be explicitly enabled using query parameters.
The key take-away for now is that faceting is built-in to every search request and it just needs to be enabled with query request parameters.
For /browse, faceting is enabled using default parameter: <str MORELIKETHIS COMPONENT Given a result set created by the query component, the More Like This component, if enabled, identifies other documents that are similar to the documents in search results.
To see an example of the More Like This component in action, search for "hard drive" in the.
Please post comments or corrections to the Author Online forum:
Figure 4.8 Example of similar items found by the More Like This search component.
We cover the More Like This component in chapter 10
To see an example of what the stats component produces, execute GET request as shown in listing 4.8:
Please post comments or corrections to the Author Online forum:
A Request statistics for the price field #B Summary statistics returned for the price field.
The parsed query value is returned to help you track down query formulation issues.
To see the debug component at work, direct your browser to the following URL:
You should notice that this is the exact same query that we executed from the Solritas.
Please post comments or corrections to the Author Online forum:
Notice how a single term query "iPod" entered by the user results in a fairly complex query composed of many different boosts on numerous fields.
The more complex query is created by the edismax query parser, which is enabled by the defType parameter under defaults.
Listing 4.10 shows the definition of the spellcheck component from solrconfig.xml:
Listing 4.10 Define a search component to do spell checking ...
Notice that the name of the component "spellcheck" matches what is listed in the <lastcomponents> section of the /browse request handler.
The key take-away at this point is seeing how a search component is added to the search request-handling pipeline using.
At this point, you should have a solid understanding of how Solr processes query.
Before we move on to another configuration topic, you should be aware that the Solr administration console provides access to all active search request handlers under Plugins / Stats > QUERYHANDLER.
Figure 4.9 shows properties and statistics for the /browse search handler, which as you might have guessed is just another MBean.
Please post comments or corrections to the Author Online forum:
Now let's turn our attention to configuration settings that help optimize query performance.
The <query> element contains settings that allow you to optimize query performance using techniques like caching, lazy field loading, and new searcher warming.
It goes without saying that designing for optimal query performance from the start is critical to the success of your search application.
In this section, you'll learn about managing searchers, which is one of the most important techniques for optimizing query performance.
In Solr, queries are processed by a component called a searcher.
There is only one "active" searcher in Solr at any given time.
All query components for all search request handlers execute queries against the active searcher.
The active searcher has a read-only view of a snapshot of the underlying Lucene index.
It follows that if you add a new document to Solr, then it is not visible in search results from.
Please post comments or corrections to the Author Online forum:
This raises the question of how do new documents become visible in search results? The answer, of course, is to close the current searcher and open a new one that has a read-only view of the updated index.
This is what it means to commit documents to Solr.
The actual commit process in Solr is more complicated but we'll save a thorough discussion of the nuances of commits for the next chapter.
For now you can think of a commit as a black-box operation that makes new documents and any updates to your existing index visible in search results by opening a new searcher.
On the CORE page, take note of the searcherName property (in the diagram it is "Searcher@25082661 main")
Let's trigger the creation of a new searcher by re-sending all the example documents to your server as we did in section 2.1.4 using:
Please post comments or corrections to the Author Online forum:
A new searcher was created because the post.jar command sent a commit after adding the example documents.
So now that we know a commit creates a new searcher to make new documents and updates visible, let's think about the implications of creating a new searcher.
However, there could be queries currently executing against the old searcher so Solr must wait for all in-progress queries to complete.
Also, any cached objects that are based on the current searcher's view of the index must be invalidated.
We'll learn more about Solr cache management in the next section.
For now, think about a cached result set from a specific query.
As some of the documents in the cached results may have been deleted and new documents may now match the query, it should be clear that the cached result set is not valid for the new searcher.
Because pre-computed data, such as a cached query result set, must be invalidated and re-computed, it stands to reason that opening a new searcher on your index is potentially an expensive operation.
When the user requests the next page, all of the previously computed filters and cached documents are no longer valid.
Without some care, the user is likely to experience some slowness, especially if their query is complex.
The good news is that Solr has a number of tools to help alleviate this situation.
First and foremost, Solr supports the concept of warming a new searcher in the background and keeping the current searcher active until the new one is fully warmed.
Solr takes the approach that it is better to serve stale results for a short period of time rather than allowing query performance to slow down significantly.
This means that Solr does not close the current searcher until a new searcher is warmed up and ready to execute queries with optimal performance.
Warming a new searcher is much like a sprinter in track and field.
Before a sprinter goes full speed in a race, she makes sure her muscles are warmed up and ready perform at full speed when the gun fires to start the race.
Just as a sprinter wouldn't start a race with cold muscles, nor should Solr activate a "cold" searcher.
We'll learn more about autowarming caches in the next section when we dig into Solr's cache management features.
A cache-warming query is a pre-configured query (in solrconfig.xml) that gets executed against a new searcher in order to populate the new searcher's caches.
Listing 4.11 shows the configuration of cache warming queries for the example server.
Please post comments or corrections to the Author Online forum:
Also, note that the actual queries are commented out! This is intentional because there is a cost to executing warming queries and the Solr developers wanted to ensure you configure warming queries explicitly for your application.
In other words, the cache warming queries are application specific so the out-of-the-box defaults are strictly for example purposes.
As a rule of thumb, warming queries should contain query parameters (q, fq, sort, etc.) that are used frequently by your application.
Since we haven't covered Solr query syntax yet, we'll table the discussion of creating warming queries in until chapter 7
For now, it's sufficient to make a mental note that you need to revisit this topic once you have a more thorough understanding of Solr query construction.
We should also mention that you do not need to have any warming queries for your application.
If query performance begins to suffer after commits, then you'll know it is time to consider using warming queries.
Each query takes time to execute so having many warming queries configured can lead to long delays in opening new searchers.
Thus, it's best to keep the list of warming queries to the minimal set of the most.
You might be wondering what the problem with a new searcher taking a long time to warm-up is.
It turns out that warming too many searchers in your application concurrently can consume too many resources (CPU and memory), thus leading to a degraded search experience.
Please post comments or corrections to the Author Online forum:
We'll leave it as an exercise for the reader to determine if there's value in configuring warming queries for the first searcher.
Most Solr users just use the same queries for warming new and first searchers.
As a brief aside, Solr supports using XInclude to pull XML elements from other files into solrconfig.xml.
For example, rather than duplicating your list of warming queries for new and first searchers, you can maintain the list in a separate file and XInclude it in both.
The <useColdSearcher> element covers the case where a new search request is made and there is no currently registered searcher.
If completed executing all warming queries; this is the default configuration for the example Solr server: <useColdSearcher>false</useColdSearcher>
On the other hand, if <useColdSearcher> is true, then Solr will immediately register the warming searcher regardless of how "warm" it is.
Returning to our track-and-field analogy, false would mean the starting official waits to start the race until our sprinter is fully warmed-up, regardless of how long that takes.
Conversely, a true value means that the race will start immediately regardless of how warmed-up our sprinter is.
This is especially true if your searchers take considerable time to warm-up.
The <maxWarmingSearchers> element allows you to control the maximum number of searchers that can be warming up in the background concurrently.
Once this threshold is reached, new commit requests will fail, which is a good thing because allowing too many warming searchers to run in the background can quickly eat up memory and CPU resources on your server.
Solr ships with a default of 2, which is a good value to start with: If you find your server is reaching the maximum threshold too often, then revisit your warming logic to see if new searcher warming is taking too long.
Please post comments or corrections to the Author Online forum:
So hopefully you now have a good sense for what a searcher is and how to configure Solr to manage searchers correctly for your application.
Now let's look at more ways to optimize query performance using caching.
Solr provides a number of built-in caches to improve query performance.
Before we get into the details of specific Solr caches, it's important to understand cache management fundamentals in Solr.
There are four main concerns when working with Solr caches:
Broadly speaking, proper cache management in Solr is not a set-it and forget-it type process.
You will need to keep an eye on your caches and fine-tune them based on actual usage of Solr.
Remember that the Solr administration console is your friend when it comes to monitoring important components like caches and searchers.
Solr keeps all cached objects in memory and does not overflow to disk, as is possible with some caching frameworks.
Consequently, Solr requires you to set an upper limit on the number of objects in each cache.
Solr will evict objects when the cache reaches the upper limit using either a Least Recently Used (LRU) or Least Frequently Used (LFU) eviction policy.
Least Recently Used (LRU) evicts objects when a cache reaches its maximum threshold based on the time when an object was last requested from the cache.
When a cache is full and a new object is added, the LRU policy will remove the oldest entry where age is determined by the last time each object in the cache was requested.
Solr also provides a Least Frequently Used (LFU) policy that evicts objects based on how frequently they are requested from the cache.
This is beneficial for applications that want to give priority to more popular items in the cache, rather than just those that have been used recently.
For example, Solr's filter cache is a good candidate for using the LFU eviction policy because filters are typically expensive to create and store so you want to keep the filter cache small and give priority to the most popular filters in your application.
We'll learn more about the filter cache in the next section.
Please post comments or corrections to the Author Online forum:
A common misconception with cache sizing is to make your cache sizes really large if you have the memory available.
The problem with this approach is that once a cache becomes invalidated after a commit, there can be many objects that need to be garbage collected by the JVM.
Without proper tuning of garbage collection, this can lead to long pauses in your server caused by full garbage collection.
We'll learn more about tuning garbage collection parameters for Solr in chapter 12
For now, the important lesson is to avoid defining overly large caches and let some objects in the cache be evicted periodically.
Hit ratio indicates how much benefit your application is getting from its cache.
Conversely, a low hit ratio is an indication that Solr is not benefiting from caching.
Eviction count shows how many objects have been evicted from the cache based on the eviction policy described above.
It follows that having a large number of evictions is an indication that the maximum size of your cache may be too small for your application.
Also, eviction count and hit ratio are interrelated, as a high eviction count will lead to a suboptimal hit ratio.
However, in Solr, this is not a concern because all objects in a cache are linked to a specific searcher instance and are immediately invalidated when a searcher is closed.
Recall that a searcher is a read-only view of a snapshot of your index; consequently all cached objects remain valid until the searcher is closed.
AUTO-WARMING NEW CACHES As we discussed in section 4.3, Solr creates a new searcher after a commit but it does not close the old searcher until the new searcher is fully warmed.
It turns out that some of the keys in the soon-to-be-closed searcher's cache can be used to populate the new searcher's cache, a process known as "auto-warming" in Solr.
Note that auto-warming a cache is different than using a warming query to populate a cache, as we discussed above in section 4.3.2
Every Solr cache supports an autowarmCount attribute that indicates either the maximum number of objects or a percentage of the old cache size to auto-warm.
How the objects are actually auto-warmed depends on the specific cache.
The key point for now is that you can configure Solr's caches to refresh a subset of cached objects when opening a new searcher, but as with any optimization technique, you need to be careful to not over do it.
Please post comments or corrections to the Author Online forum:
At this point you should have a basic understanding of cache management concepts in Solr.
Now, let's learn about the specific types of caches Solr uses to optimize query performance, starting with one of the most important caches: filter cache.
In Solr, a filter restricts search results to documents that meet the filter criteria but does not affect scoring.
A Filter query "fq" on the manu field When Solr executes this query, it computes and caches an efficient data structure that indicates which documents in your index match the filter.
For the example server, there are 2 documents that match this filter.
Please post comments or corrections to the Author Online forum:
Figure 4.11 Execute a query with a filter query "fq" clause to see the filter cache in action.
Next, navigate to the to the Plugins / Stats page for the collection1 and click on the CACHE link.
Figure 4.12 shows the properties and statistics for the filterCache MBean.
Reexecute the same query several times and you will see the filterCache statistics change.
Please post comments or corrections to the Author Online forum:
Of course, it's difficult to fully appreciate the value of caching filters for a small index, but imagine an index with millions of documents and you can see how caching filters can really help optimize query performance.
In fact, using filters to optimize queries is one of the most powerful features in Solr, mainly because filters are re-usable across queries.
For now, we'll save a deeper discussion of filters for chapter 7 and turn our focus to how the filter cache is configured in solrconfig.xml.
Listing 4.13 shows the default configuration for the filter cache.
Listing 4.13 Initial settings for the filter cache in the example server.
Please post comments or corrections to the Author Online forum:
AUTO-WARMING THE FILTER CACHE A filter can be a powerful tool for optimizing queries, but you can also get into trouble if you don't manage cached filters correctly.
Filters can be expensive to create and store in memory if you have a large number of documents in your index, or if the filter criteria is complex.
If a filter is generic enough to apply to multiple queries in your application, then it makes sense to cache the resulting filter.
In addition, you probably want to auto-warm some of the cached filters when opening a new searcher.
Let's look under the hood of the filter cache to understand what happens during autowarming of objects in the filter cache.
By now, you should know that objects cannot just be moved from the old cache to the new cache because the underlying index has changed, thus invalidating cached objects like filters.
Of course each object in the cache has a key.
For the filter cache, the key is the filter query, such as manu:Belkin.
To warm the new cache, a subset of keys are pulled from the old cache and then executed against the new searcher, which re-computes the filter.
In other words, auto-warming the filter cache requires Solr to re-execute the filter query with the new searcher.
Consequently, auto-warming the filter cache can be a source of performance and resource utilization problems in Solr.
Imagine the scenario where you have hundreds of filters cached and your autowarmCount is set to 100
When warming the new searcher, Solr must execute 100 filter queries.
Under this scenario, you'll quickly run into problems where you are warming too many searchers in the background.
We recommend that you enable auto-warming for the filter cache but set the autowarmCount attribute to a small number, less than 10 to start out.
In addition, we think the LFU eviction policy is more appropriate for the filter cache because it allows you to keep the filter cache small and give priority to the most popular filters in your application.
Here are the recommended configuration settings for the filter cache:
Of course, you need to do some experimentation with these parameters depending on how many filters your application uses and how frequently you commit against your index.
In terms of memory usage per cached filter, Solr has different filter representations based on the size of the matching document set.
As an upper limit, you can figure that any filter that matches many documents in your index will require MaxDoc bits of memory.
Please post comments or corrections to the Author Online forum:
The query result cache holds result sets for a query.
If you execute this query more than once, then subsequent results are served from the query result cache rather than re-executing the same query against the index.
This can be a powerful solution for reducing the cost of computationally expensive queries.
Behind the scenes, the query result cache holds a query as the key and a list of internal.
Internal Lucene document IDs can change from one searcher to the next, so the cached values must be recomputed when warming the query result cache.
To auto-warm the query result cache, Solr needs to re-execute queries, which can be expensive.
So the same advice we gave about keeping the autowarmCount attribute small for the filter cache applies the query result cache.
That said, we do recommend setting the autowarmCount attribute for this cache to something other than the default zero so that you get some benefit from auto-warming recent queries.
Beyond sizing, Solr provides a few miscellaneous settings to help you fine-tune your usage of the query result cache.
The <queryResultWindowSize> element allows you to prepare additional pages when you execute a query.
For example, imagine your application shows 10 documents per page and that in most cases your users only look at the first and second pages.
However, if you set it too large, then every query is paying the price of loading more documents than you are showing to the user.
If your users rarely go beyond page 1, then it is better to set this element to the page size.
You can imagine a result set holding millions of documents in the cache would greatly impact available memory in Solr.
The <queryResultMaxDocsCached> element allows you to limit the number of documents cached for each entry in the query result cache.
Please post comments or corrections to the Author Online forum:
However, the documents in the index have many more fields, such as category, popularity, manufacture date, etc.
If your application adopts this common design pattern, then you want to set example documents do not have many fields so it's easy to overlook the benefit of this setting.
In practice, most documents have many fields so it is a good idea to load fields lazily.
The query result cache holds a list of internal document IDs that match a query, so even if the query results are cached, Solr still needs to load the documents from disk to produce search results.
The document cache is used to store documents loaded from disk in memory keyed by their internal document ID.
It follows that the query result cache uses the document cache to find cached versions of documents in the cached result set.
This raises the question whether it makes sense to auto-warm the document cache? There's actually a good argument to be made against auto-warming this cache because there's no way to ensure the documents you are warming have any relation to queries and filters being auto-warmed from the query result and filter caches.
In other words, you could be spending time re-creating documents that may not actually benefit your warmed filter and query result caches.
The last cache we'll mention is the field value cache, which is strictly used by Lucene and is not managed by Solr.
The field value cache provides fast access to stored field values by internal document ID.
The field value cache is used during sorting and when building documents for the response.
As this is an advanced topic, we'll refer you to the Lucene JavaDoc3 for more information.
At this point, you know how Solr processes queries using a request handling pipeline and.
Let's finish our tour of solrconfig.xml by learning how to control what gets returned from a query.
Solr provides great flexibility in what is returned to a client in the response.
In this section, we'll cover query response writers and document transformers.
Please post comments or corrections to the Author Online forum:
After query processing completes, Solr passes the results to a response writer based on the "wt" parameter in the request.
A response writer transforms the Java objects into a serialized form to be returned to the client application.
As we've seen in numerous examples, the default value for "wt" is XML.
Let's take a look at a few of the response writer definitions from solrconfig.xml to see how they work.
Listing 4.14 provides the definition of the JSON response writer.
The Solritas example uses the Velocity writer to demonstrate how Solr can be configured to produce a full Web-based UI using a custom response writer.
The actual definition of the Velocity response writer is quite simple:
Of course, all of the actual work is in the Velocity templates! If you're interested in how.
Let's move on to a new feature in Solr 4 that allows you to enrich or otherwise transform.
A document transformer allows you to add fields dynamically to documents in the response.
For instance, you could use a document transformer to enrich a document with a value from an external database.
Let's work with one of the built-in document transformers to see how they work.
Specifically, we'll use Solr's ExplainAugmenterFactory to add the Lucene explanation of the document score to each result.
This could be useful for a search performance monitoring application to track the relationship between user clicks and document ranking.
First, you need to activate the transformer by adding the following configuration to solrconfig.xml:
A The name of the field to produce in response documents #B Return explanation as a named-list.
Please post comments or corrections to the Author Online forum:
Next, you need to reload the core to pick-up this configuration change, see section 4.1.2
Table 4.2 provides a summary of Solr's built-in document transformers:
Before we wrap up this chapter, we want to introduce the Core Admin API, which allows you to programmatically create, update, reload, rename, swap, and unload cores.
You've already seen how to reload a core from the Solr administration panel in section 4.1.2
Please post comments or corrections to the Author Online forum:
To begin, let's use the STATUS action to get some basic status information about the collection1 core.
As you might have guessed the Core Admin API is based on HTTP so requesting status is as simple as sending the following HTTP GET request as in listing 4.16
The general format for a Core Admin API request is to specify an action and a core name.
To begin, make sure the example server is running and enter the following URL into your browser:
Please post comments or corrections to the Author Online forum:
A HTTP GET request to create a new core named "SolrInAction" #B Expected error message because the sia directory does not exist.
The operation should have failed because there is no directory named "sia" under Solr home.
To resolve this problem, open a command-line and recursively copy the collection1 directory to sia; on Unix/Linux based systems, you can do:
Now try the URL again and you should see a response message that looks like: The core admin API returns the response as XML but you can control that using the "wt"
Verify that your new SolrInAction core is active by going to the Solr administration.
You should now see SolrInAction on the left navigation panel below collection1
Also, take a peek at the solr.xml and you should see something like:
Next, execute the find all documents query (*:*) for the SolrInAction core.
Of course in practice you probably don't want two cores with the same data but there's no harm in having the same data for now.
Now, let's use the API to reload the SolrInAction core after making a configuration change.
For the sake of this exercise, change the autowarmCount to 10 for each of the caches defined in solrconfig.xml for the SolrInAction core at:
Please post comments or corrections to the Author Online forum:
After making your changes, you need to reload the core to apply the configuration changes.
This is done using the RELOAD action in the Core Admin API as shown in listing 4.18
If you go to the plug-in stats page in the admin console for the SolrInAction core, you'll.
Definitely give yourself a pat on the back after working through this long chapter! We know learning about configuration is not the most interesting of topics.
At this point you should have a solid understanding of how to configure Solr, especially for optimizing queryprocessing performance.
Specifically, you learned that Solr's request processing pipeline is composed of a unified request dispatcher and a highly configurable request handler.
We saw how a search request handler has four phases and you can customize each phase.
The /browse handler for the Solritas application served as a good example of using default parameters and custom components (spellcheck) to enable a feature-rich search experience while simplifying client application code.
We also learned how Solr processes query requests using a read-only view of the index with a component called a searcher.
There is only one active searcher in Solr at any point in time and a new searcher needs to be created before updates to the index are visible.
Closing the currently active searcher and opening a new one can be an expensive operation that affects query performance.
To minimize impact on query performance, Solr allows you to configure static queries to warm-up a new searcher.
Properly managing the new searcher warm-up process is one of the most important configuration tasks you'll need to do for your search application.
Solr also provides a number of important caches that need to be fine-tuned for your application.
We looked at the filter, query result, document, and field value caches.
For each cache, you need to set a maximum size and eviction policy based on actual usage of your application.
The Solr administration console provides key statistics, such as the hit ratio, to help you determine if your caches are sized correctly.
Please post comments or corrections to the Author Online forum:
Caches can be auto-warmed when creating a new searcher, which also helps optimize query performance.
For example, you can use cache auto-warming to pre-populate Solr's filter cache with the most popular filters used by queries in your application.
Cache autowarming, while powerful, can also lead to large wait times waiting for a new searcher to warm-up.
So we advised to start with small autowarmCount values and monitor searcher warm-up time closely.
Next, we covered how to control the format of responses using a response writer.
The default response writer produces XML, but you can easily request other formats such as JSON using the "wt" query parameter.
We closed the chapter with a quick overview of the Core Admin API.
Specifically, we showed you how to request the status for an existing core, create a new core, and reload a core using simple HTTP GET requests.
As we mentioned above, we chose to skip over the index-related settings in solrconfig.xml until we covered the basics of indexing.
In the next chapter, you will learn about the Solr indexing process and index-related configuration settings.
Please post comments or corrections to the Author Online forum:
Field types for structured data like dates and language codes.
How to index XML, JSON, CSV and other document types.
In Chapter 3, we learned how Solr finds documents using an inverted index, which in its simplest form, is a dictionary of terms and a list of documents where each term occurs.
Solr uses this index to match terms in a user’s query with documents where those terms occur.
In this chapter, we learn how Solr processes documents to build the index.
In this chapter we’ll focus on the indexing process and non-text fields, saving a detailed discussion of text analysis until Chapter 6
At the end of this chapter, you’ll know how to get documents indexed in Solr and will understand key concepts like fields, field types, and schema design.
As a prerequisite, this chapter will be easier to work through if you have the Solr example server running locally, which was covered in chapter 2
On the other hand, you will still be able to follow along with most examples without actually running Solr if you prefer to read this chapter and then come back to doing the hands-on activities another time.
Please post comments or corrections to the Author Online forum:
Throughout this chapter and the next, we will design and implement an indexing and text analysis solution for searching micro-blog content from popular social media sites like Twitter.
We use the term "micro-blog" as a generic term for the short, informal messages and other medium people share with each other on social networks.
Examples of micro-blogs are tweets on Twitter, Facebook posts, and check-ins on Foursquare.
In this chapter, we define the fields and field types to represent micro-blogs in Solr and learn how to add documents to Solr.
In chapter 6, we learn how to do text analysis on micro-blog content using built-in Solr tools.
Let’s get started by looking at the type of documents we will be working with in this example and how users might want to search them.
To begin, table 5.1 shows some fields from a fictitious tweet that we’ll use throughout this chapter to learn about indexing documents in Solr.
Even if you are not interested in analyzing social media content, the lessons we learn by working through this example have broad applicability for most search applications.
Each document in a Solr index is made up of fields, where each field has a specific type that determines how it is stored, searched, and analyzed.
Take a moment to think about how a user might find microblogs using these fields.
For a thorough discussion of all the available fields in a tweet, we recommend reading Map of a Tweet: www.slaw.ca/wp-content/uploads/2011/11/map-of-a-tweet-copy.pdf.
Please post comments or corrections to the Author Online forum:
Of course you could just index all these fields but if you are developing a large-scale system to support millions of documents and high query volumes, then you only want to include the fields that will actually be searched by your users.
For example, the user_id field is an internal identifier for Twitter so it’s unlikely users will ever want to search on this field.
In general, each field increases the size of your index so you should only include fields that add value for users.
The favourites_count field is the number of favorites the author of the tweet has, not the number of favorites for the tweet.
This field is interesting because it has useful information from a user interface perspective but doesn’t seem like a good candidate as a parameter to a search query.
We’ll address how to handle these display-oriented fields in section 5.2 when we discuss stored vs.
Now, let’s think about how users might build a query using these fields, as that will help us decide how to represent these fields in our Solr index.
Figure 5.1 depicts a fictitious search form based on the fields for our example micro-blog search application.
Each field that we identified as being useful from a search perspective is represented on the form.
This is a key point in designing your search application in that you need to think about how users will search a specific field in your index as that will help determine how the field is defined in Solr.
Please post comments or corrections to the Author Online forum:
The favourites_count field is used for displaying results but is not used for searching.
So now we have a conceptual understanding of the fields in our example application and an idea of how users will search for documents using these fields.
Next, let’s get a high-level understanding of how to add documents to Solr.
At a high-level, the Solr indexing process distills down to three key tasks:
Figure 5.2 provides a high-level overview of these three basic steps to getting your document indexed in Solr.
Please post comments or corrections to the Author Online forum:
Next, in step 2, we send the XML document to Solr’s document update service using HTTP POST.
In step 3, each field is analyzed based on the configuration defined in schema.xml before being added to the index.
Solr supports several formats for indexing your document including XML, JSON, and CSV.
In Figure 5.2, we chose XML because its self-describing format makes it easy to understand.
Here is how our example tweet would look using the Solr XML format:
Listing 5.1 XML document used to index the example tweet in Solr Grecco in SF's historic North Beach...
Please post comments or corrections to the Author Online forum:
This is because you define how fields are analyzed in the schema.xml document depicted in the diagram.
Recall from our discussion in Chapter 2 that Solr provides a simple HTTP-based interface to all of its core services, including a document update service for adding and updating documents.
At the top left of the diagram in Figure 5.2, we depict sending the XML for our example tweet using an HTTP POST to a document update service in Solr.
We’ll go into more details about how to add specific document types, such as XML, JSON, and CSV later in the chapter.
For now, think of the document update service as an abstract component that validates the contents of each field in a document and then invokes the text analysis process.
After each field is analyzed, the resulting text is added to the index, thus making the document available for search.
We will spend more time on how the actual indexing process works in section 5.5 below.
A high-level overview of the indexing process is sufficient for now, as we need to focus on more foundational concepts first.
Specifically, we need to understand how Solr uses the schema.xml depicted in figure 5.2 to drive the indexing process.
The schema.xml defines the fields and field types for your documents.
For simple applications, the fields to search and their types may be obvious.
In general, though, it helps to do some up-front planning about your schema.
With our example micro-blog search application, we dove right in and defined what a document is and which fields we want to index.
In practice, this process is not always obvious for a real application, so it helps to do some up-front design and planning work.
In this section, we learn about some key design considerations for search applications.
Specifically, we’ll learn to answer the following key questions about your search application:
What are the fields in your document that can be searched by users?
Which fields should be displayed to users in the search results?
Let’s begin by determining the appropriate granularity of a document in your search application as that impacts how you answer the other questions.
Determining what is a document in your Solr index drives the entire schema design process.
In some cases it is obvious, such as with our tweet example, where the text content is typically short, so each tweet will be a document.
However, if the content you want to index is very large, such as a technical computer book, you may want to treat sub-sections of a large document as the indexed unit.
The key is to think about what your users will want to.
Please post comments or corrections to the Author Online forum:
Let’s look at a different example to help you think about what is a document for your index.
Imagine searching for "text analysis" on Web site that sells technical computer books.
If the site treated each book as a single document, then the user would see Solr in Action in the search results, but would then need to page through the table of contents or index to find specific places where "text analysis" occurs in the book.
In Figure 5.3, the left-side image depicts how search results might look when an entire book is indexed as a single document.
Figure 5.3 Comparison of search results when indexing entire book as document vs.
On the other hand, if the site treated individual chapters in each book as documents in the index, then the search results might show the user the Text Analysis chapter in Solr in Action as the top result, as seen on the right in Figure 5.3
However, since text analysis is a central concept in search, most of the other chapters in this book and other search books would be included as highly relevant results as well.
So being too granular can overwhelm users with too many results to wade through.
Please post comments or corrections to the Author Online forum:
You may also need to consider the type of content you are indexing, as splitting a technical computer book by chapter seems to make sense but splitting a fiction novel by chapter doesn’t seem like a good approach.
In the end, it’s your choice on what makes a document in your index, but definitely consider how document granularity impacts user experience.
In general, you want your documents to be as granular as possible without causing your users to miss the forest for the trees.
As a quick aside, we should note that Solr offers a feature called hit highlighting that allows you to highlight relevant sections of longer documents in search results.
This is useful when you cannot break long documents up into smaller units but still want to help your users quickly navigate to highly relevant sections in large documents.
For example, we could use hit highlighting to show the first paragraph of Chapter 6 in this book in the search results of a query for "text analysis"
Once you’ve identified what a document is, you should determine how to uniquely identify each document in the index.
For a chapter, it might be the ISBN number plus the chapter number.
Solr does not require a unique identifier for each document but if supplied, Solr uses it to avoid duplicating documents in your index.
For those with a database background, the unique identifier is similar to a primary key for a row in a table.
If a document with the same unique key is added to the index, then Solr overwrites the existing record with the latest document.
We’ll return to the discussion of unique keys when we discuss distributed search later in the book.
For our example micro-blog search application from section 5.1, the tweet already includes a unique identifier field: id.
However, if we index content from a variety of social media sources, then we would probably add something like "twitter:" as a prefix to differentiate this document from a Facebook post with the same numeric id value.
Once you have determined what a document is for your index and how to uniquely identify each document, the next step is to determine the indexed fields in the document.
For example, every book has a title and an author.
When searching for books, people generally expect to find books of interest based on title and author, so these fields should be indexed.
On the other hand, although every book has an editor, users typically do not search.
Please post comments or corrections to the Author Online forum:
Thus, editor name would need to not be an indexed field.
Conversely, if you were building a search index for the book publishing industry, then it’s very likely that your users would want to search by editor name so you would include that as an indexed field.
Determining which fields to include in the index is specific to every search application.
Take a moment to think about the indexed fields for your documents.
Keep these fresh in your mind, as you’ll need to refer to them as you work through the rest of this chapter.
As we already discussed, the screen_name, type, timestamp, lang, and text should be indexed for our micro-blog example.
The id and user_id fields are used internally by Twitter and are unlikely to be missed if you don’t allow users to search by these fields.
Although users probably won’t search by editor name to find a book to read, we may still want to display the editor’s name in the search results.
In general, your documents may contain fields that are not very useful from a search perspective but are still useful for displaying search results.
The favourites_count field is a good example of a stored field that is not indexed but is useful for display purposes.
You can imagine users would find it useful to see which authors have more favorites than others in search results but it’s unlikely that users would want to search by this field.
In addition to displaying stored fields, you can also sort by them, such as to see tweets from authors with more favorites at the top of search results.
Of course a field may be indexed and stored, such as the screen_name, timestamp, and text fields in our micro-blog search application.
Each of these fields can be searched and displayed in results.
As a search application architect, one of your goals should be to minimize the size of your index.
If you’re considering Solr, then most likely you have an application that needs to scale to handle large volumes of documents and users.
Each stored field in your index consumes disk space and requires CPU and I/O resources to read the stored value for returning in search results.
Thus, you should choose your stored fields wisely especially for large-scale applications.
At this point you should have a good idea about the types of questions you need to.
Once you’ve settled on a plan, it’s time to rollup your sleeves and work with Solr’s schema.xml to implement your design.
As we saw in figure 5.2, schema.xml is the main configuration document Solr uses to understand how to index your documents.
Let’s take a quick preview of the main sections of schema.xml so we have an idea of what’s in-store for us over the next couple of sections in this chapter.
In the next few sections, we build a valid schema.xml document for our example micro-blog search application.
The schema.xml file is in the conf directory for your Solr core.
For instance, the schema.xml for the example Solr server is in:
Please post comments or corrections to the Author Online forum:
Listing 5.2 is a condensed version of the example schema.xml provided with Solr to give a feel for the XML syntax and.
To see the full example schema, click on the [SCHEMA] link from the Solr administration page, which loads the schema.xml into your browser.
Please post comments or corrections to the Author Online forum:
At a quick glance, it’s easy to be overwhelmed by all the details in this document.
By the end of this chapter, you’ll have a clear understanding of all these details and will be wellequipped to craft your own schema.xml.
For now, notice that there are three main sections of the schema.xml document:
Field Types under the <types> element determine how dates, numbers, and text fields are handled in Solr.
Solr uses the field definitions from schema.xml to build the internal structure of the index for your documents.
In this section, we learn how to define fields, dynamic fields, and copy fields in schema.xml.
Listing 5.3 defines the indexed and stored fields for our example application.
Listing 5.3 Field elements for our example micro-blog search application.
Please post comments or corrections to the Author Online forum:
With these field definitions, Solr knows how to index micro-blog documents so they can be searched using a form similar to figure 5.1
When defining a field in schema.xml, there are a few required attributes you must provide to Solr.
Each field has a unique name that is used when constructing queries.
In layman's terms, this definition means that the screen_name field is indexed and.
Each field must define a type attribute that identifies the field must also define whether it is indexed and/or stored.
As we discussed in section 5.2, indexed fields can be searched and stored fields can be returned in search results for display and sorting purposes.
Of course, a field can be both, as is the case for most of the fields in our example.
Also, notice that there are no "nested" fields in Solr; all fields are siblings of each other in schema.xml implying a flat document structure.
As we discussed in Chapter 3, documents in Solr need to be de-normalized into a flat structure and must contain all the fields needed to support your search requirements.
In other words, there is no relational structure that allows you to join with other documents to pull in additional information to service queries or generate results.
You’ll undoubtedly encounter content on the Web about doing document joins in Solr.
For now, it’s important to understand that Solr joins are more like sub-queries in SQL than joins.
A typical use case is to find the parent documents of child documents that match your search criteria using Solr joins.
For example, in our example micro-blog application, we could use Solr joins to bring back the original post instead of re-tweets.
One thing that can be confusing is that when a field is stored, Solr stores the original value and not the analyzed value.
For example, in listing 5.3, we declared the text field with and you can return the original text in search results.
Of course, if you don’t return a field in search results, then it doesn’t need to be stored.
Please post comments or corrections to the Author Online forum:
There's also a case to be made for storing all fields for a document.
If you plan to update documents after they are indexed, then you will need to store all fields.
We'll learn more about updating documents in section 5.6.3 later in the chapter.
So far, our micro-blog search application only uses a small number of simple fields.
Let’s add a few more fields into the mix to exercise some of Solr’s strengths in dealing with more complex document structures.
Specifically, let’s add support for a "links" field that contains zero or more links associated with each document.
As you’ve probably seen on Twitter, users can embed links to related content, such as a photo or article on the Web.
Here’s an example of another fictitious tweet with two links embedded:
The links in this document are shortened URLs provided by bitly.com that resolve to the following Web sites shown in table 5.2:
Table 5.2 Actual URLs for shortened links in the example tweet.
For example, you can imagine users wanting to find all tweets that link to the Solr In Action MEAP page on manning.com.
Since this example contains two links, we need a way to encode two values for one field.
In Solr, fields that can have more than one value per document are called multi-valued fields.
When adding a document that has multiple links, you simply add multiple "link" fields in the XML document, as is depicted in listing 5.4
Listing 5.4 Representing multi-valued fields in XML during indexing ...
Please post comments or corrections to the Author Online forum:
A Reuse the same field name to populate a multi-valued field during indexing When searching, you can query for link:"http://manning.com/" and Solr will look.
So far, our micro-blog documents have only a small number of fields, which made it easy.
In practice, not all documents are so simple or sparse.
Let’s look at another type of field, called dynamic fields, that help deal with larger and more complex document structures.
In other words, dynamic fields uses a special naming scheme to apply the same field definition to any fields that match a glob-style pattern.
Dynamic fields help address a few common problems that occur when building search applications, including:
Let’s look at each one of these use cases to get a good feel for what you can do with dynamic fields.
However, you should note that you do not need to use dynamic fields with Solr.
It’s perfectly acceptable to not use dynamic fields if none of these use cases applies to your application.
Also, Solr ignores the dynamic field definitions in your schema.xml until you start indexing documents that make use of them.
Thus, in practice, most Solr users keep the extensive list of dynamic fields provided with the Solr example schema so they are there when you need them, but are simply ignored otherwise.
Moreover, each of these fields are both stored and indexed.
That is, other than the name, each field definition is exactly the same.
Now imagine that in addition to these three fields, we have dozens of "string" fields that are also stored and indexed.
Of course you can type in an explicit definition for each field or you can define a single <dynamicField> element to account for all of these string fields using a suffix pattern on the field name:
With this glob pattern, any field with the name ending in "_s" will inherit this field.
Please post comments or corrections to the Author Online forum:
You can also use dynamic fields for multi-valued fields, such as our links field in the.
Listing 5.5 Using dynamic fields to represent multi-valued fields during indexing ...
We think it’s more intuitive and maintainable to handle these source-specific fields as dynamic fields.
For example, instead of defining many fields for each source as in the example below:
A Many Facebook specific string fields that are stored and indexed #B Many Twitter specific string fields that are stored and indexed.
You can accomplish the sample using a single "string" dynamic field with the "*_s" suffix pattern as the name:
When indexing, you need to send fields with the "_s" suffix, such as:
Listing 5.6 Include source-specific fields during indexing using dynamic fields.
Please post comments or corrections to the Author Online forum:
New social networks seem to come online everyday so we wouldn’t want to re-work our schema.xml just to handle documents from these new sources.
With dynamic fields, you can include new fields introduced by your new document source without making any changes to the schema.xml.
For example, suppose you wanted to add support for a new social network that includes a field that captures the phase of the moon when the content was posted to the network (perhaps it’s a dating site)
With dynamic fields, you can just include this field as a string in crescent</field>
Lastly, although dynamic fields can be a handy feature on the indexing side, there’s no real magic on the query side.
When querying for documents that were indexed with dynamic fields, you must use the full field name in the query.
In other words, you cannot formulate queries to find a match in all string fields by querying with a prefix or suffix pattern: *_s:coffee.
However, if you want to find matches in more than one field, dynamic or static, then Solr provides a clever way to do that with copy fields.
In Solr, copy fields allow you to populate one field from one or more other fields.
Specifically, copy fields support two use cases that are common in most search applications:
The intent of this approach is to help your users find documents quickly without having to fill-out a complicated form; think about how successful a simple search box has been for Google.
In our tweet example, you might think it’s an easy decision—just search the tweet text and you’re done.
However, with this approach, users would not find our example tweet if they searched for "@thelabdude" because that information is contained in the.
Please post comments or corrections to the Author Online forum:
In addition, if a tweet contains shortened bit.ly-style URLs, then searches in the text field for the actual "resolved" URL will not match as those are stored in the links field.
What we really want here is a catch-all search field that contains the text from the screen_name, text, and resolved link fields.
Thankfully, Solr makes it easy to create a single catch-all search field from many other fields in your document using the.
A Catch all field should not be stored as it is populated from another field #B Destination field must be multivalued if any of the source fields are multi-valued.
This looks like any other field except there are two important aspects to the definition.
In fact, even if you wanted to do this, you can’t because there really is no original value for Solr to return for copy fields.
Remember that Solr returns the original value for a stored field.
Second, if any of the source fields are multi-valued, then the destination field must be a must define our copy field as multi-valued as well.
Now that we’ve defined the destination field, we need to tell Solr which fields to copy from using the <copyField> directive.
Take a moment to think about why this is the case.
The best way to make sense of this is that you must define the source and destination fields first and then you connect them with the copy field directive after all fields have been defined.
Please post comments or corrections to the Author Online forum:
For instance, as we’ll see in chapter 6, stemming is a technique that transforms terms into a common base form, known as a "stem", in order to improve recall.
With stemming, the terms "fishing", "fished" and "fishes" all have a common stem of "fish"
Thus, stemming can help your users find documents without having to think about all the possible linguistic forms of a word, which is a good approach for general text search fields.
On the other hand, consider how stemming would affect a type-ahead suggestion box (auto-suggest)
In this case, stemming would work against your users in that you could only suggest the stemmed values and not the full terms.
For example, with stemming enabled, your search application would not be able to suggest "humane" or "humanities" when the user started typing "human" in the auto-suggest box.
Solr copy fields give you the flexibility to enable or disable certain text analysis features like stemming without having to duplicate storage in your index.
Listing 5.8 Using copyField to apply different text analysis to the same text.
In this case, the text field has field type "stemmed_text" which presumably means the text is stemmed.
Under the covers, Solr sends the raw, un-analyzed contents of the text field to the auto_suggest field, which allows a different text analysis strategy.
To reiterate, you cannot return the original value of the auto_suggest field in search.
In section 5.2.2, we discussed how it is a good idea to make your documents uniquely identifiable in the index using some unique ID value.
To recap, if you provide a unique identifier field for each of your documents, then Solr will avoid creating duplicates during indexing.
In addition, if you plan to distribute your Solr index across multiple servers, then you must provide a unique identifier for your documents.
For these reasons, we recommend defining a unique identifier for your documents from the start.
Please post comments or corrections to the Author Online forum:
In fact, we’ve seen instances where Solr does not return results correctly if you don’t use "string" as the type for text-based keys.
So save yourself some trouble and just use string or one of the other primitive field types for your unique key field.
At this point, we’ve covered all the basic aspects of defining fields in schema.xml.
It’s now time to dig into the next major section of Solr’s schema.xml to learn how to define field types.
In this section, we learn to define field types for handling structured data like dates, language codes, and usernames.
In chapter 6, we learn how to define field types for text fields like the body text of our example tweet.
In general, Solr provides a number of built-in field types for structured data, such as numbers, dates, and geo-location fields.
Figure 5.4 shows a class diagram of some of the more commonly used field types in Solr.
Please post comments or corrections to the Author Online forum:
Figure 5.4 Class diagram of commonly used field types from the org.apache.solr.schema Java package.
Let’s begin our discussion of field types for non-text data by looking at one of the most common field types: string.
For our example tweet, in addition to the text field, we decided the screen_name, type, timestamp, and lang should also be indexed fields.
Now we need to decide the appropriate type for each field.
It turns out that each of these fields contains structured data that does not need to be analyzed.
For example, the lang field contains a standard ISO-639-1 language code used to identify the language of the tweet, such as "en"
Users can query the lang field to find English tweets as shown in Figure 5.5
Please post comments or corrections to the Author Online forum:
Figure 5.5 Mirco-blog search application Web form used to find documents using structured "non-text" fields.
Since the language code is already standardized, we don’t want Solr to make any changes to it during indexing and query processing.
Solr provides the string field type for fields that contain structured values that should not be altered in any way.
Here is how the string field type is defined in schema.xml:
Listing 5.10 Field type definition for string fields in schema.xml.
Behind the scenes, all field types are implemented by a Java class, in this case.
At runtime, "solr.StrField" resolves to the built-in Solr class: org.apache.solr.schema.StrField.
Anytime you see "solr." as a prefix of a class in schema.xml you know this translates to the fully qualified Java package: org.apache.solr.schema.
The sortMissingLast and omitNorms attributes are advanced options that we’ll discuss in more detail in section 5.4.4
Please post comments or corrections to the Author Online forum:
If we use the string field type for our lang field, Solr will take the value "en" from our document and store it in the index unaltered as "en" during indexing.
At query time, you also need to pass the exact value "en" to match English documents.
In Figure 5.5, the user selected "English" which needs to be translated into "en" when processing the form.
A common approach to searching on date fields is to allow users to specify a date range.
On the query side, this would be a range query on the timestamp field:
Listing 5.11 Field type definition for date fields in schema.xml.
Again, the solr.TrieDateField is Solr’s short-hand notation for specifying the Java class name that implements this field type, which in this case is: org.apache.solr.schema.TrieDateField.
A "trie" is an advanced tree-based data structure that allows for efficient searching for numeric and date values by varying degrees of precision.
During indexing, Solr needs to know how to parse a date.
Recall from section 5.1, the example XML document we sent to Solr for indexing included the timestamp field as:
In schema.xml, the timestamp field is configured to use the tdate type:
Please post comments or corrections to the Author Online forum:
If you send Solr a date in some other format, you will get a validation error during indexing and the document will be rejected.
This goes back to understanding how your users need to query using dates.
For example, if your users only expect to query for documents by day, then there’s no point in indexing a date with second or millisecond precision.
On the other hand, if you need to sort documents by date, then hour-level granularity may be too coarse, in which case you may want to do minute-level granularity.
During indexing, Solr supports Date Math operations to help you achieve the correct precision for a date field with little effort.
For example, let’s say you decide that you only need to index your dates at the hour-level of granularity.
This saves space in your index, but also means that users cannot get more specific than hour ranges when searching.
When indexing, you can send your date with /HOUR on the end; the / tells Solr to "round-down" to a specific granularity.
Let’s see how we can index our example tweet with hour granularity:
In the index, the value of the timestamp field for the example document will be equivalent to: 2012-05-22T09:00:00Z.
In addition to specifying the date/time exactly, Solr also supports the NOW keyword to represent the current system time on your Solr server.
You can combine specific dates or the NOW keyword with Solr’s Date Math operations to accomplish very powerful date calculations.
For example, NOW/DAY evaluates to midnight of the current day and NOW/DAY+1DAY evaluates to midnight tomorrow.
We’ll dig more into the details of Solr’s Date Math in chapter 7 when we discuss range queries.
Lastly, it should be noted that the tdate field is a good choice for fields that you need to do date range queries on but it comes at the cost of requiring more space in your index because more tokens are stored per date value.
We dive into the details of how to choose the right precisionStep in section 5.4.4 below.
For the most part, numeric fields behave as you would expect in Solr.
This is not an intuitive field to search by but is useful from a display and sorting perspective.
In other words, you can imagine users wanting to sort matching tweets by this field to see content from more popular authors.
Please post comments or corrections to the Author Online forum:
Because we don’t need to support range queries on this field, we chose costs associated with a higher precision step used for faster range queries.
Also, note that you shouldn’t index a numeric field that you need to sort by as a string field because Solr will do a lexical sort instead of a numeric sort if the underlying type is string-based.
Up to this point, we’ve discussed the main concepts for indexing fields that contain.
We’ll return to specific cases for these types of non-text fields in later chapters.
For now, let’s wrap-up this section with a short discussion of some of the advanced configuration options for field types.
Solr supports optional attributes for field types to enable advanced behavior.
Table 5.4 Overview of advanced attributes for field type elements in schema.xml.
Let's take a closer look at precisionStep as that is a common source of confusion for.
You can safely skip the following discussion and come back to this after you have your search application implemented and are looking for ways to improve performance of sorting and range queries.
Please post comments or corrections to the Author Online forum:
As we discussed in section 5.4.3, Solr uses a trie data structure to support efficient range queries and sorting of numeric and date values.
Let's learn how to choose the best value for precisionStep to support range queries and sorting in your Solr instance.
First, decide if you even need to worry about precisionStep by asking whether you have any numeric or date fields in your index that users would like to find documents across a range of values in those fields.
For example, consider a Solr index for finding houses for sale across the United States.
Homebuyers typically search for houses in a specific area and price range.
Home price seems like a good example of a field that needs to support efficient range searches.
Next, we need to decide on the best field type for listing price.
You can imagine that most house prices will be rounded to the nearest dollar, as it's rare to see a home price with cents, so an int or long field should suffice.
Keep in mind that you want to be as frugal with your field types as possible, i.e.
This reduces the size of your index on disk and reduces memory usage during searching and sorting.
We can define a field for home listing price as:
To support range queries, the field must be indexed and since we want to display the.
In the Solr example schema.xml, the "tint" field type is defined as:
Let's take a look at what gets indexed for a home price of $327,500 using a TrieInt field multiple terms for each value in the field, where each term has less precision.
In other words, two different home prices will have overlapping terms at lower precisions.
Lucene does this to reduce the number of terms that have to be matched to satisfy a range query.
Please post comments or corrections to the Author Online forum:
Table 5.6 shows the terms that would be indexed for price.
Table 5.6 shows that using a smaller precision step equates to more terms being indexed,
In general, a smaller precision step leads to more terms being indexed per value, which increases the size of your index.
However, more terms also equates to faster range queries because Lucene can narrow the search space quicker with more terms.
The intuition here is that Lucene can search the center of a range using the lowest possible precision in the trie.
Please post comments or corrections to the Author Online forum:
Notice that the index sizes differ by about 8-bytes per document, which agrees with table.
To summarize, when selecting a precision step, you have to balance space considerations with range query performance.
For our TrieInt listing price field, a precision step of 4 leads to more terms being indexed per unique price but slightly faster range searches, especially when the cardinality of a field is large (many unique values)
We now have enough background on the indexing process to begin adding documents to Solr.
In this section we learn how to send documents to Solr for indexing and get a glimpse of what happens behind the scenes.
At the end of this section, you’ll be able to start indexing documents to Solr from your application.
Let’s begin by learning how to index the example tweets we’ve been working with in this chapter.
As we touched on in section 5.1, Solr allows you to add documents using a simple XML document structure.
Listing 5.12 shows this structure for the two example tweets we used previously in this chapter.
However, in this case, we changed the names of the fields to use dynamic fields.
We do this for convenience as you can add these documents to the example Solr server without making any changes to schema.xml.
If you were building a real application, then you may want to declare the fields from listing 5.3 explicitly in your schema.xml, but using dynamic fields works fine for this example.
Please post comments or corrections to the Author Online forum:
Listing 5.12 XML document used to index example tweets in Solr using dynamic fields historic North Beach...
Let’s send this XML document to Solr to index these two tweets.
The Solr example includes a simple command-line application that allows you to post XML documents into the example server.
Open a command-line on your workstation and execute the following commands as seen in listing 5.13
The two example tweets should now be indexed in your example Solr server.
Please post comments or corrections to the Author Online forum:
Behind the scenes, the post.jar application sent the XML document over HTTP to Solr’s update handler at URL: http://localhost:8983/solr/collection1/update.
The update handler supports adding, updating, and deleting documents; we cover the update request handler in more detail in section 5.6 below.
Beyond XML, Solr’s update request handler also supports the popular JSON (JavaScript Object Notation) and CSV (comma-separated values) data formats.
For example, instead of indexing our example tweets using XML, we could have used JSON as shown in listing 5.14
Please post comments or corrections to the Author Online forum:
We’ll use the post.jar utility to send the JSON to Solr, but since XML is the default type, we have to explicitly tell the application that we are sending JSON by setting the "type" system property to "application/json":
If you walked through both exercises of adding the XML and JSON documents, then you might think there are 4 documents in your index.
However, because we are using the "id" field in our documents, there will only be 2 documents in the index.
Verify this by yourself by re-issuing the type_s:post query as before.
This demonstrates how Solr will update an existing document using the unique key field, which in the example schema.xml is "id"
If you look closely at the output from the post.jar application, you’ll notice that it also sends a commit to Solr after POSTing the documents.
Regardless of how you send documents to Solr, they are not searchable until they are committed.
The commit process is quite involved and is covered in detail in section 5.6 below.
Let’s continue our discussion of how to index documents by learning about a popular Java-based client for Solr called SolrJ.
SolrJ is a Java-based client library provided with the core Solr project to communicate with your Solr server from a Java application.
In this section, we’ll implement a simple SolrJ client to send documents using Java.
If you’re not a Java developer or your application is not written in Java, then you’ll be happy to know there are many other Solr client libraries.
Please post comments or corrections to the Author Online forum:
For a complete list of client libraries, see the Integrating Solr page in the Solr wiki3
Listing 5.15 provides a simple example using SolrJ to add our two example tweet documents to the index and then doing a hard commit.
After committing, the example code sends the match all docs query (*:*) to Solr to get the documents we indexed back in the search results.
Please post comments or corrections to the Author Online forum:
As you can see from this basic example, the SolrJ API makes it very easy to connect to Solr, add documents, send queries, and process results.
To begin, all you need is the URL of the Solr server, which in our example was http://localhost:8983/solr/collection1
Behind the scenes SolrJ uses the Apache HttpComponents Client library to communicate with the Solr server using HTTP.
In section 5.5.1, we saw how Solr supports XML and JSON, so you may be wondering if SolrJ is using one of those formats to connect to Solr.
It turns out that SolrJ actually uses an internal binary protocol called javabin by default.
When doing Java-to-Java communication, the javabin protocol is more efficient than using XML or JSON.
Lastly, for better scalability for handling applications that need to index many documents very quickly, SolrJ provides a ConcurrentUpdateSolrServer that is more efficient for doing bulk adds and updates.
In listing 5.15, each invocation of the add method resulted in a.
Please post comments or corrections to the Author Online forum:
In contrast, if we used the ConcurrentUpdateSolrServer, then add requests would be buffered on the client and sent to Solr in a bulk transfer.
However, while the ConcurrentUpdateSolrServer is well suited for efficiently indexing many documents, it should not be used to make queries to Solr.
Looking ahead, we’ll see another type of SolrServer provided by SolrJ called CloudSolrServer when we learn about SolrCloud in chapter 13
We’ve seen how we can send documents to Solr using basic HTTP POST using the post.jar application and using the popular SolrJ client from Java.
These are not the only ways to get your documents into Solr.
Being a mature, widely-adopted open source technology, Solr offers a number of powerful utilities for adding documents from other systems.
In this section we introduce you to three popular tools available for populating your Solr index:
Each of these tools is powerful and could easily justify taking an entire chapter to describe each tool.
So for now, we just want to give brief mention of these tools so that you are aware of these options for populating your index.
At a high-level, you provide the database connection parameters and a SQL query to Solr and the DIH component queries the database and transforms the results into documents.
We cover the DIH in detail in chapter 12, so for now let’s look at another tool for indexing rich binary documents like PDF and MS Word documents.
Behind the scenes, Solr Cell uses the Apache Tika project to do the extraction.
Specifically, Tika provides components that know how to detect the type of document and then parse the binary documents to extract text and metadata.
For example, you can send a PDF document to the ExtractingRequestHandler and it will automatically populate fields like title, subject, keywords, and body text in your Solr index.
We will also work through an example using the ExtractingRequestHandler in chapter 12
Please post comments or corrections to the Author Online forum:
Thus, if your application needs to crawl hyper-linked pages on a massive scale, then Nutch is probably a good place for you to start.
Now that you've seen how to send documents to Solr for indexing, let's learn how those request are actually processed in Solr using a component called the update handler.
In the previous section, we sent new documents to Solr using HTTP POST requests.
The request to "add" these new documents was handled by Solr's update handler.
In general, the update handler processes all updates to your index as well as commit and optimize requests.
Table 5.8 provides an overview of common request types supported by the update handler.
Table 5.8 Overview of common requests processed by the update handler.
Add Add one or more documents to the index, see listing 5.12 for a full example.
Delete Delete a document by ID, such as deleting document.
Delete by Query Delete documents that match a Lucene query, such as deleting all micro-blog documents from user with.
Atomic Update Update one or more fields of an existing document using optimistic locking; see section 5.6.4 below.
Commit Commits documents to the index with options to do a soft or hard commit and whether to block on the client until the new searcher is open and warmed.
Please post comments or corrections to the Author Online forum:
Optimize Optimize the index by merging segments and removing deletes.
Although table 5.8 shows examples of update requests using XML, the update request handler supports other formats, such as JSON, CSV, and javabin.
Behind the scenes, the update request handler looks at the Content-Type HTTP header to determine the format of the request, such as Content-Type: text/xml.
Listing 5.16 shows the configuration of the update handler in solrconfig.xml:
Listing 5.16 Configuration elements for the update handler in solrconfig.xml ...
One of the most important tasks performed by the update handler is to process requests to commit documents to the index to make them visible in search results.
In this section, we dig into the details of how Solr makes documents available for searching by committing them to the index.
When a document is added to Solr, it will not be returned in search results until it is committed to the index.
In other words, from a query perspective, a document is not visible until it is committed.
In Solr 4, there are two types of commits: soft and normal or sometimes called "hard" commits.
Let’s look at how normal commits work as that will help you understand soft commits.
For our purposes, you can think of a searcher as a read-only view of all committed documents in the index.
Refer to chapter 4.3 for a detailed discussion of how a searcher works.
For now, let it suffice to say that a hard commit can be an.
Please post comments or corrections to the Author Online forum:
After a normal commit succeeds, the newly committed documents are safely persisted to durable storage and will survive server restarts due to normal maintenance operations or a server crash.
For high availability, you still need to have a solution to fail-over to another server if the disk fails.
We discuss near-real-time searching in more depth in chapter 13
For now, you can think of a soft commit as a mechanism to make documents searchable in near real-time by skipping the costly aspects of hard commits, such as flushing to durable storage and warming a new searcher.
As soft commits are less expensive, you can issue a soft commit every second to make newly indexed documents searchable within about a second of adding them to Solr.
However, keep in mind that you still need to do a hard commit at some point to ensure documents are eventually flushed to durable storage.
To summarize, a hard commit makes documents searchable but is expensive because it has to flush documents to durable storage and warm-up a new searcher.
In contrast, a soft commit also makes documents searchable but they are not flushed to durable storage and a new searcher is not warmed up.
AUTO-COMMIT For either normal or soft commits, you can configure Solr to automatically commit documents using one of three strategies.
Commit all documents once a user-specified threshold of uncommitted documents is reached.
Solr's auto-commit behavior for hard and soft commits is configured in solrconfig.xml.
When performing an auto-commit, the normal behavior is to open a new searcher.
However, Solr does allow you to disable this behavior by specifying disk, but will not be visible in search results.
Please post comments or corrections to the Author Online forum:
In this scenario, it may make sense to only pay the penalty of warming up a new searcher once, after all documents are indexed rather than warming up a new searcher 100 times.
Of course, your client application can also send an intermittent hard commit request every 1M documents so that some documents are visible in search results sooner.
The main point is that you want to think about whether you need to open a new searcher after every auto-commit.
If the number of documents you are indexing is much larger than your auto-commit threshold, then you might consider setting documents are indexed.
A new searcher is always opened and warmed when you send a <commit> request; the waitSearcher attribute indicates whether your client code should block until the new searcher is fully warmed up.
As you learned in chapter 4, warming a new searcher can take a long time so.
You can also configure Solr to do soft-commits automatically using the values for soft commits, such as every second (1000 ms) as shown in the XML snippet below.
A Do a soft commit every second (1000 ms) Now let's turn our attention to another powerful feature of the update handler that helps.
Solr uses a transaction log to ensure updates accepted by Solr are saved on durable storage until they are committed to the index.
Imagine the scenario where your client application sends a commit every 10,000 documents.
If Solr crashes after the client sends some documents to be indexed but before your client sends the commit, then without a transaction log, these un-committed documents will be lost.
Please post comments or corrections to the Author Online forum:
The transaction log is configured for a core in solrconfig.xml:
A Default location is tlog sub-directory of data Every update request is logged to the transaction log.
The transaction log continues to grow until you issue a commit.
During a commit, the active transaction log is processed and then a new transaction log file is opened.
Figure 5.7 illustrates the steps involved in processing an update request.
Figure 5.7 Sequence of events and main components used to process update requests, such as adding a new document.
A few of the components in figure 5.7 should be familiar to you, such as the request dispatcher and response writer.
These are the same components we discussed in chapter 4
Please post comments or corrections to the Author Online forum:
Let's walk through the sequence of events in figure 5.7 to highlight some of the important concepts:
The client can send the request as JSON, XML, or Solr's internal binary javabin format.
We saw an example client built using SolrJ in listing 5.15
Solr's request dispatcher uses the "collection1" part of the request path to determine the core name.
Next, the dispatcher locates the /update request handler registered in solrconfig.xml for the collection1 core.
When adding or updating documents, the update handler uses schema.xml to process each field in each document in the request.
In addition, the request handler invokes a configurable chain of update request processors to perform additional work on each document during indexing.
We'll see an example of this in chapter 6 where we use an update request processor to do language detection during indexing.
Once the update request is securely saved to durable storage, a response is sent to the client application using a response writer.
At this point, the client application knows the update request is successful and can continue processing.
With the transaction log, your main concern is balancing the trade-off between the length of your transaction log, i.e.
If your transaction log grows very large, then a restart may take a long time to process the updates, thus delaying your recovery process.
In contrast, committing micro-blog documents like we used in this chapter every 100,000 documents would not be a problem.
The key take-away is that you need to consider the size of your transaction log when configuring your auto-commit settings.
As you learned in the previous section, you can issue a hard commit without However, this implies that at some point, your client application must issue a full hard commit to make all updates visible in search results.
You can update existing documents in Solr by sending a new version of the document.
However, unlike a database where you can update a specific column in a row, with Solr you must update the entire document.
Behind the scenes, Solr deletes the existing document and creates a new one; this occurs whether you change one field or all fields.
Please post comments or corrections to the Author Online forum:
From a client perspective, your application must send a new version of the document in its entirety.
For some applications where documents can be created from other sources, this is not such a big deal.
For others that use Solr as a primary data store, re-creating a document in its entirety just to update a single field can be problematic.
In practice, this requires users to query for the entire document, apply the updates, and then send the fully specified document back to Solr.
This pattern of requesting all fields for an existing document, updating a subset of fields, and then sending the new version to Solr is very common in practice.
Consequently, atomic updates are new feature in Solr that allow you to send updates to only the fields you want to change.
This brings Solr more inline with how database updates work.
Solr will still delete and create a new document, but this is transparent to your client application code.
FIELD-LEVEL UPDATES Returning to our micro-blog search example, let's imagine that we want to index a new field on existing documents that holds the number of times the tweet has been re-tweeted.
We'll use this new field as an indication of popularity of a tweet.
To keep things simple, we'll update this field once a day.
You can imagine a daily volume statistic would also be useful but we'll just stick with an aggregated value to keep things simple.
Let's name our new field retweet_count_ti, which indicates we are using a dynamic field so we don't have to update the schema.xml to add this new field.
The "_ti" suffix applies the following dynamic field (from schema.xml):
Here's an example request to update the retweet_count_ti field using XML:
A Slightly un-intuitive but updates must be wrapped in an <add> element.
It follows that all fields must be stored for this to work because the client application is only sending the id field and the new field.
All other fields must be pulled from the existing document.
Please post comments or corrections to the Author Online forum:
In a nutshell, we'll pay users to classify each document as either being positive, neutral, or negative.
The sentiment field could be useful for allowing users to find negative information about a product or restaurant.
Once classified, each micro-blog document needs to be updated in Solr with the sentiment label.
In our re-tweet count example, we updated the retweet_count_ti field once a day using an automated process.
However, with sentiment classification, updates to the sentiment_s field can happen at anytime.
Thus, it's conceivable that two users will attempt to update the sentiment label on same document at the same time.
Of course we could implement some cumbersome process that requires users to explicitly lock a document before labeling, but that would slow them down unnecessarily.
Also, we probably don't want to pay for a document to be classified twice.
Hence, we need some way to guard against concurrent updates to the same document—enter optimistic concurrency control.
To avoid conflicts, Solr supports optimistic concurrency control using a special version tracking field, named _version_
The special version field should be defined in your schema.xml as:
A With Solr 4, you should not change or remove this field from your schema.xml.
When a new document is added, Solr assigns a unique version number automatically.
When you need to guard against concurrent updates, you simply include the exact version the update is based on in the update request.
Consider the following update request that includes a specific _version_:
When Solr processes this update, it will compare the _version_ value in the request with the latest version of the document, pulled from either the index or the transaction log.
If they do not match, then the update request fails and an error is returned to the user.
A client application can handle the error response to let the user know the document was already classified by another user.
This approach is called "optimistic" because it assumes that most updates will work on the initial attempt and that conflicts are rare.
Please post comments or corrections to the Author Online forum:
Real-time get returns the latest version of a document regardless of whether it is.
Consequently, real-time get and atomic updates rely on the transaction log being enabled for your index.
Solr gives you a few other options for handling concurrent updates with the _version_ field.
As illustrated by this simple example, atomic updates are a powerful new addition to.
With Solr 4, you can now update existing documents simply by sending the fields that need to be updated along with the unique identifier of the document to update.
In chapter 4, we delayed a discussion of index management settings in solrconfig.xml until you had more background with Solr indexing.
You are now ready to tackle Solr's index management settings.
In this section, we focus on the index-related settings that you are most likely to need to change, beginning with how indexed documents are stored.
It should be said that most of the index-related settings in Solr are for expert use only.
What this really means is that you should take caution when making changes and that the default settings are appropriate for most Solr installations.
When documents are committed to the index, they are written to durable storage using a component called a Directory.
The Directory component provides the following key benefits to Solr:
Hides details of reading from and writing to durable storage, such as using JDBC to store documents in a database.
Please post comments or corrections to the Author Online forum:
Implements a storage-specific locking mechanism to prevent index corruption, such as OS-level locking for file system based storage.
Enables extending the behavior of a base Directory implementation to support specific use cases like near real-time search.
Solr provides several different Directory implementations and there is no one best Directory implementation for all Solr installations! Thus, you will need to do some research to decide on the best implementation for your specific application of Solr.
In practice, this depends on your operating system, JVM type, and use cases.
However, as you learned in chapter 4, Solr tries to be well-configured out-of-the-box.
Let's dig into how Solr's index storage is configured by default, which will help you decide if you need to change the default configuration.
For instance, the example server stores its index in $SOLR_INSTALL/example/solr/collection1/data.
The location of the data directory is controlled by the <dataDir> element in solrconfig.xml:
A Default setting from solrconfig.xml, resolves to collection1/data for the example server.
The solr.data.dir property defaults to "data" but can be overridden in solr.xml for each core, such as:
A Directory to store data for the collection1 core The first thing you need to consider is whether the data directory for your index has.
Also, it's important that your data directory supports fast reads and writes, with a little more priority given to read performance.
Strategies for optimizing disk I/O is beyond the scope of this book, but here are some simple pointers to keep in mind:
Each core should not have to compete for the disk with other processes.
If you have multiple cores on the same server, it's a good idea to use separate physical disks for each index.
Use high quality, fast disks or even better, consider using Solid State Drives (SSD) if your budget allows.
Spend some quality time with your system administrators to discuss RAID options for your servers.
Please post comments or corrections to the Author Online forum:
At runtime, the StandardDirectoryFactory selects a specific Directory implementation based on your operating system and JVM type, as depicted in figure 5.8 below.
Figure 5.8 Solr Directory implementation selected at runtime depending on your specific operation system version and JVM type.
Based on figure 5.8, there are three possible file system based Directory options for Solr:
MMapDirectory: Uses memory-mapped I/O when reading the index; best option for installations on 64-bit Windows, Solaris, or Linux operating systems with the Sun JVM.
SimpleFSDirectory: Uses a Java RandomAccessFile; should be avoided unless you are running on 32-bit Windows.
Please post comments or corrections to the Author Online forum:
NIOFSDirectory: Uses java.nio optimizations to avoid synchronizing reads from the same file; should be avoided on Windows due to a long-standing JVM bug.
You can determine which Directory implementation is enabled for your Solr server using the Core Admin page on the Solr administration console.
You can override the default selection by explicitly setting the directory factory in solrconfig.xml.
For example, in figure 5.9, Solr is using the NRTCachingDirectory implementation.
If we want to change that to use MMapDirectory, then solrconfig.xml should be changed to:
Please post comments or corrections to the Author Online forum:
A segment is a self-contained, read-only sub-set of a full Lucene index; once a segment is flushed to durable storage, it is never altered.
When new documents are added to your index, they are written to a new segment.
Consequently, there can be many active segments in your index.
Each query must read data from all segments to get a complete results set.
At some point, having too many small segments can negatively impact query performance.
Combining many smaller segments into fewer larger segments is commonly known as segment merging.
Let it suffice to say that optimize can be a very expensive operation in terms of memory, CPU, and disk I/O in Solr, especially for large indexes; it is not uncommon for a full optimization to take hours for a large index.
One of the most common questions on the Solr user mailing list is whether to "optimize" your index.
This is understandable because who doesn't want an "optimized" index? However, current wisdom in the Solr community suggests that rather than optimizing your index, it is better to fine-tune Solr's segment merge policy.
Moreover, having an optimized index doesn't mean that a slow query will suddenly become fast.
Conversely, you may find that query performance is acceptable with an un-optimized index.
EXPERT-LEVEL MERGE SETTINGS By default, all the segment-merging settings are commented out in solrconfig.xml.
This is by design because the default settings should work for most installations, especially when you're just getting started.
You should also notice that each element is labeled as an "expert" level setting.
Table 5.10 provides an overview of segment merge related elements from solrconfig.xml.
This should not be confused with commits, which force all buffered documents to be written to durable storage.
Increase this value to buffer more documents in memory and reduce disk I/O during indexing.
Please post comments or corrections to the Author Online forum:
Default is the TieredMergePolicy, see JavaDoc for org.apache.lucene.index.TieredMergePolicy for more information.
Determining the best value for your index depends on average document size, available RAM, and desired indexing throughput.
To be clear, just because these expert-level settings are commented out, segment merging is still enabled in your index, running in the background5
For now, we recommend that you avoid optimizing and just use the default configuration for segment merging until you have a good reason to change these settings.
It's very likely that "doing nothing" when it comes to segment merging is the right approach for your server6
If indexing throughput becomes an issue for your application, then you can revisit these settings.
Unfortunately, we're not able to be more specific when it comes to tuning the merge process because it depends on so many environment specific factors.
This implies that deletes do not actually delete documents from existing segments.
It turns out that deleted documents are not removed from your index until segments containing deletes are merged.
In a nutshell, Lucene keeps track of deletes in a separate data structure and then applies the deletes when merging.
For the most part, you don't have to worry about how this works.
At this point you should have a good understanding of the Solr indexing process.
To recap, we began the chapter by learning about the schema design process.
Specifically, we discussed considerations about document granularity, document uniqueness, and how to determine if a field should be indexed, stored, or both.
Next, we learned how to define fields in schema.xml, including multi-valued and dynamic fields.
We saw how dynamic fields are useful for supporting documents with many fields and documents coming from diverse sources.
You also learned how to use Solr’s <copyField> directive in order to populate a catch-all text search field or to apply different text analysis to the same text during indexing.
Please post comments or corrections to the Author Online forum:
Next, we saw how to work with structured data using Solr’s support for strings, dates, and numeric field types.
We used Solr’s round-down operator (/) to index date values at different precisions, such as hour-level precision using /HOUR.
We also learned that Solr provides Trie-based fields to support efficient range queries and sorting on numeric and date fields.
We saw how to use the precisionStep attribute for numeric and date fields to balance the trade-off between having a larger index size and range query performance.
Armed with an understanding of schema.xml, we learned how to send XML and JSON documents to Solr using HTTP and SolrJ.
We briefly introduced some additional tools provided by Solr for importing documents from a relational database (DIH) and indexing rich binary documents like PDF and MS Word using the extracting request handler.
After documents are processed, they need to be committed before they can be searched using either normal commits or soft commits for near-real-time search.
We showed how Solr uses a transaction log to avoid losing un-committed updates.
Beyond adding new documents, you learned how to update existing documents using Solr's atomic update support.
You can guard against concurrent updates using optimistic concurrency control using the special _version_ field.
We closed out this chapter by returning to a discussion of index-related settings from solrconfig.xml.
Specifically, we showed you where and how Solr stores the index using a Directory component.
You also learned about segment merging and that it is a good idea to avoid optimizing your index or changing segment merge settings until you have a better understanding of your indexing throughput requirements.
In the next chapter, we continue learning about the indexing process by diving deep into text analysis.
After finishing chapter 6, you will have a solid foundation for designing and implementing a powerful indexing solution for your application.
Please post comments or corrections to the Author Online forum:
In Chapter 5, we learned how the Solr indexing process works and learned to define nontext fields in schema.xml.
In this chapter, we get a little deeper into the indexing process by learning about text analysis.
When done correctly, text analysis allows your users to query using natural language without having to think about all the possible forms of their search terms.
Allowing users to find information they seek using natural language is fundamental to providing a good user experience.
Given the broad adoption and sophistication of Google and similar search engines, users are conditioned to expect search engines to be very intelligent and intelligence in search starts with great text analysis!
Please post comments or corrections to the Author Online forum:
The state-of-the-art of text analysis goes well beyond removing superficial differences between terms to address more complex issues like language-specific parsing, part-ofspeech tagging, and lemmatization.
Don’t worry if you’re not familiar with some these terms as we’ll cover them in more detail below.
What’s important is that Solr has an extensive framework for doing basic text analysis tasks, such as removing very common words, known as stop words, as well as doing more complex analysis tasks.
To accommodate such power and flexibility, Solr’s text analysis framework can seem overly complex and daunting to new users.
As we like to say, Solr makes solving very difficult text analysis problems possible and simple tasks a little too cumbersome.
However, after working through this chapter, we’re confident that you’ll be able to harness this powerful framework to analyze most any content you’ll encounter.
The main goal of this chapter is demonstrate how Solr approaches text analysis and to help you think about how to construct analysis solutions for your documents.
To this end, we’ll tackle a somewhat complex text analysis problem to demonstrate the mechanics and strategies you need to be successful.
Specifically, we’ll cover these fundamental components of text analysis with Solr:
Basic elements of text analysis in Solr: analyzer, tokenizer, and chain of token filters.
Defining a custom field type in schema.xml to analyze text during indexing and query processing.
Common text analysis strategies such as removing stop words, case-folding, synonym expansion, and stemming.
Once we have a solid understanding of the basic building blocks, we’ll tackle a harder analysis problem to exercise some of the more advanced features Solr provides for text analysis.
Specifically, we’ll see how to analyze micro-blog content from sites like Twitter.
Tweets present some unique challenges that require us to think hard about how users will use our search solution.
Collapse repeated characters down to a maximum of two in terms like “yummm”
Use a custom token filter to resolve shortened bit.ly style URLs.
Let’s continue with the example micro-blog search application we introduced in chapter 5
To recap, we’re designing and implementing a solution to search micro-blogs from popular social media sites like Twitter.
Since the main focus of this chapter is on text analysis, let’s take a closer look at the text field in our example micro-blog document.
Please post comments or corrections to the Author Online forum:
As was discussed in the introduction above, a primary goal of text analysis is to allow your users to search using natural language without having to worry about all the possible forms of their search terms.
We assert that our example document should be a strong match for this query because of the relationships between terms used in the user’s search query and terms in the document shown in table 6.1:
Please post comments or corrections to the Author Online forum:
Table 6.1 Query terms that match terms in our example tweet.
Coffee latte, Caffé So the task ahead of us is to use Solr’s text analysis framework to transform the tweet.
Figure 6.2 shows the transformations we’ll make to the text using Solr’s text analysis framework, which for now you can think of as a black box.
In the remaining sections of this chapter, we’ll open up the black box to see how it works.
Figure 6.2 Tweet text transformed into a more optimal form for searching.
Each box indicates a unique term in the Solr index after text analysis process is applied to the text.
The extra space between some terms indicates stop words that were excluded from the index.
Notice that each transformation taken individually is usually quite simple but collectively they make a big difference in improved user experience with your search application.
Please post comments or corrections to the Author Online forum:
Table 6.2 Overview of the transformations made to the micro-blog text using Solr’s text analysis tools.
Notice how all transformations are performed using built-in Solr text analysis tools, i.e.
SF’s sf, san francisco Apostrophe s (‘s) removed by the WordDelimiterFilterFactory and sf mapped as a synonym of san francisco using the SynonymFilterFactory.
Caffé caffe Diacritic é transformed to e using the ASCIIFoldingFilterFactory.
Yummm #yumm Collapse repeated letters down to a maximum of two using the PatternReplaceCharFilterFactory.
Don’t worry if some of the Solr class names look a bit daunting as we’ll cover each one of.
For now, let’s consider a few of the interesting transformations occurring on this text, all of which are provided by built-in Solr tools.
Of course “iPad” is the correct form, but with search, you want to be as accommodating of simple variations of terms as possible.
Solr also allows you to replace characters and terms using regular expressions.
For instance, repeating letters in a word is very common in social media content like tweets in.
Please post comments or corrections to the Author Online forum:
Later in the chapter, we’ll see how to use regular expressions with Solr to make this transformation.
It’s worth highlighting that all of the transformations shown above were provided by built-in Solr tools, meaning that we only had to configure them and didn’t write any Java code.
While Solr’s built-in arsenal is powerful, sometimes you need to extend its capabilities.
We’ll see how to do this with Solr’s Plug-In framework to deal with bit.ly-style shortened URLs in social media content in section 6.4.3
At this point, you should have a good feel for where we are headed with text analysis in Solr and might be wondering how to get started.
In other words, now that we know Solr provides all these great tools to transform text, how do we actually apply these tools to our documents during indexing? In the next section, we’ll start to work with field types for doing text analysis.
The example schema provided with Solr defines an extensive list of field types applicable for most search applications.
Of course, if none of the pre-defined Solr field types meets your needs, then you can build your own field type using the Solr Plug-In framework.
We’ll see an example of the Solr Plug-In framework in the last section of this chapter.
If all your fields contained structured data like language codes and timestamps, you actually wouldn’t need to use Solr since a relational database is very efficient at indexing and searching “structured” data.
Consequently, the example Solr schema pre-defines a number of powerful field types for analyzing text.
Please post comments or corrections to the Author Online forum:
A Use a short, descriptive name based on the type of data #B Define the analyzer to use for indexing documents.
At the top, you define a attribute to be solr.TextField.
This tells Solr that you want the text to be analyzed.
Also, you should use a name that gives a clue about the type of text that will be analyzed using this field type; for example, text_general is a good all-purpose type for when you don’t know the language of the text you are analyzing.
In practice, it’s common to define two separate when searching; the text_general field type uses this approach.
Take a moment to think about why you might use different analyzers for indexing and querying.
It turns out that you often need to do some additional analysis for processing queries than is needed for indexing a document.
For example, adding synonyms is typically done during query text analysis only to avoid inflating the size of your index and to make it easier to manage synonyms.
Although you can define two separate analyzers, the analysis applied to query terms must be compatible with how the text was analyzed during indexing.
Technically, there is also a pre-processing phase before tokenization where you can apply character filters.
We’ll discuss character filtering in more detail in section 6.3.1, so for now let’s concentrate on tokenization and token filters.
In the tokenization phase, text is split into a stream of tokens using some form of parsing.
The most basic tokenizer is a WhitespaceTokenizer that splits text on whitespace only.
More common is the StandardTokenizer, which performs intelligent parsing to split.
Please post comments or corrections to the Author Online forum:
To define a tokenizer, you need to specify the Java implementation class of the factory.
For example, to use the common StandardTokenizer, you specify solr.StandardTokenizerFactory.
In Solr, you must specify the factory class instead of the underlying Tokenizer implementation class because most tokenizers do not provide a default no-arg constructor.
By using the factory approach, Solr gives you a standard way to define any tokenizer in XML.
Behind the scenes, each factory class knows how to translate the XML configuration properties to construct an instance of the specific tokenizer implementation class.
All tokenizers produce a stream to tokens that can be processed by zero or more filters that perform some sort of transformation of the token.
Transformation – Changing the token to a different form such as lowercasing all letters or stemming.
Token injection – Adding a token to the stream, as is done with the synonym filter.
Token removal – Removing terms as is done by the stop word filter.
Filters can be chained together to apply a series of transformations on each token.
The order of the filters is important as you wouldn’t want to have a filter that depended on the case of your tokens listed after a filter that lowercases all tokens.
Let’s see this process in action to process our example tweet text, starting with the StandardTokenizer.
At this point, you should have a good understanding of the schema design process and.
Let’s put this knowledge to work to do some basic text analysis of our example tweet text from chapter 5
The first step in basic text analysis is to determine how to parse the text into a stream of tokens using a tokenizer.
Let’s start by using the StandardTokenizer, which has been the classic go-to solution for many Solr and Lucene projects because it does a great job of splitting text on whitespace and punctuation but also handles acronyms and contractions with ease.
To see this tokenizer in action, let’s use it to parse our example tweet:
Please post comments or corrections to the Author Online forum:
Splits on whitespace and standard punctuation characters such as period, comma, semi-colon, etc.
Preserves Internet domain names and email addresses as a single token.
Supports a configurable maximum token length attribute, default is 255
Next, let’s look at several common token filters provided by Solr to do basic text analysis.
Returning to our example tweet, there are a number of issues with the token stream that should be addressed before this text is added to the index.
Please post comments or corrections to the Author Online forum:
Solr’s StopFilterFactory removes stop words from the token stream during analysis as.
Removing stop words during indexing helps reduce the size of your index and can improve search performance as it reduces the number of documents Solr has to rank for queries that contain stop words.
To analyze our example tweet, we defined the StopFilterFactory in listing 6.1 as:
Here are the English stop words included in the example Solr server:
Solr provides custom stop word lists for many languages in the lang sub-directory.
Google owns a patent on its approach to handling stop words where they include all stop words during indexing and selectively remove stop words from queries based on comparing sets of documents retrieved with and without using the stop words, see: http://www.google.com/patents/US7945579
Google’s patented approach to stop words is a great example of how search providers use advanced text analysis to achieve competitive advantage.
Even with something as simple as removing stop words, there is not a one-size fits all solution.
As with stop words, it’s not always clear whether to apply the lowercase filter to all terms.
For example, terms starting with a capital letter in the middle of a sentence typically indicate a proper noun that can greatly improve precision of your results if users seek the proper.
Please post comments or corrections to the Author Online forum:
However, using a more nuanced approach to lowercasing terms assumes that your users use the right case when searching.
In most cases, you’ll want apply the lowercase filter but still need to determine where in the filter chain to apply it.
If you have synonym list in all lowercase, then you’ll need to apply the lowercase filter before your synonym filter.
Figure 6.4 shows the resulting text after applying the stop word and lowercase filters to the example tweet:
Figure 6.4 Resulting text to be indexed after splitting using the StandardTokenizer and applying the stop word and lowercase filter.
The terms with red (x) are the stop words and would not be included in your index.
So now we have a basic text analysis solution for our sample tweet.
Let’s apply what we learned so far to see some actual text analysis in action.
Please post comments or corrections to the Author Online forum:
The form also allows you to see if a query would match a sample document without having to actually index the document.
The easiest way to get started is to use Solr’s admin console, which provides a simple Web form to analyze your content.
Figure 6.5 How to find the link to the Analysis form from the Solr admin panel.
Please post comments or corrections to the Author Online forum:
Once you enter the field type and text to analyze, click on the Analyse Values button to see the result.
Below the form, Solr reports the steps it takes to analyze the text for indexing using the text_general field type.
Notice how the text is first parsed with the StandardTokenizer, abbreviated as ST and then each token passes through the StopFilter (SF) and LowercaseFilter (LCF)
Please post comments or corrections to the Author Online forum:
Although this is a nonsensical query, we would expect our sample document to be a match.
We’re using this nonsensical query to demonstrate that seemingly small differences in terms can lead to highly relevant documents being missed by your users.
Case in point, none of the query terms match the example document! The problem of course is that while it’s easy for a human to see the similarity between the terms in the query and the example document, to Solr, the terms have no relation to each other! We will use better text analysis to overcome this mismatch.
Overall, as a first pass, we resolved a number of text parsing and basic analysis issues with very little effort.
On the other hand, there are a number of outstanding issues that will make finding this tweet difficult for your users.
Please post comments or corrections to the Author Online forum:
Figure 6.8 Remaining text analysis issues with our sample tweet after applying the lowercase and stop filters.
Unless you are indexing tweets or content from other social media sources, you may not encounter any of the analysis issues shown in figure 6.8
In general, the main point is that you need to study a representative sample of the documents in your index to determine the type of analysis needed as we’ve done here.
At this point, it should be clear that the basic text analysis provided by the text_general general field type is not sufficient to meet our needs.
Consequently, we need to implement a new custom field type building on the tools we’ve already learned about as well as learning a few new ones.
We left off having made good progress with analyzing social media text, but there were a.
In this section, we address these remaining issues by introducing a few more of Solr’s built-in text analysis tools.
To begin, since none of the predefined field types meet all of our needs, we will define a new custom field type in schema.xml.
Listing 6.2 Custom field type for analyzing micro-blog text ...
Please post comments or corrections to the Author Online forum:
Please post comments or corrections to the Author Online forum:
You should recognize the structure and some of the elements in this field type definition,
Table 6.3 provides an overview of the new tools we’ll cover in this section.
Table 6.3 List of additional Solr text analysis tools needed to analyze micro-blog text.
KStemFilterFactory Stemming on English text; less aggressive than the Porter stemmer.
Don’t worry if this list of complicated names looks overwhelming, as we’ll work through each of these tools in the sections below.
Let’s start with a solution for removing repeated letters from words like “yummm” using a regular expression.
In Solr, a CharFilter (or character filter) is a pre-processor on an incoming stream of.
Much like token filters, CharFilters can be chained together to add, change, or remove characters from text.
As with most Solr features, you can implement your own using the Plug-In framework.
Of the three filters, the PatternReplaceCharFilterFactory seems most appropriate for our current text analysis needs as tweets typically do not have HTML embedded in them and we do not need to map any characters.
Consequently, we won’t show how to use the MappingCharFilterFactory or HTMLStripCharFilterFactory filters in these chapters, so please see the Solr wiki for more details.
Let’s see how to apply the PatternReplaceCharFilterFactory to address a few of the issues with our example tweet.
Please post comments or corrections to the Author Online forum:
The solr.PatternReplaceCharFilterFactory is used to filter characters using regular expressions.
To configure this factory, you need to define two attributes: pattern and replacement.
The pattern is a regular expression that identifies the characters we want to replace in our text.
The replacement attribute specifies the value you want to replace the matched characters with.
Don’t worry if you’re not a regular expression expert, in most cases you can find the expression you need online using Google or similar search engine.
Let’s use this <charFilter> to solve those pesky terms with repeated letters like yummm.
In regex speak, the [a-zA-Z] is a character class that identifies a single letter in lower or uppercase.
The parenthesis around the character class identifies the matching letter as a captured group.
Listing 6.3 Define a charFilter to collapse repeated letters using  regular expression.
It turns out that a few of the issues we are having with the example tweet are caused by.
Unfortunately, the means that searches for “iPad” won’t match our example document as we saw in the previous section.
In terms of hashtags and mentions, you might wonder why we care about preserving those special characters on the front?
Specifically, @Manning used in a social context has a very special meaning in that it identifies a specific social account, in this case the one used by Manning Publishers.
This is very different than a tweet about Peyton Manning.
Please post comments or corrections to the Author Online forum:
For example, #fail is a hashtag commonly used to denote a person’s dissatisfaction with another person, place or thing such as a brand.
Now we just need to figure out how to do it.
First, let’s make sure we understand how they are being removed.
If you are a Java developer, then your first inclination might be to extend StandardTokenizer and override the behavior that strips off these two special characters.
Unfortunately, the StandardTokenizer is not easy to extend and more importantly, we can do this without writing any custom code! And, we get a chance to learn about two more text analysis tools in Solr in the process, namely the WhitespaceTokenizerFactory and the WordDelimiterFilterFactory.
In general, we want to preserve hashtags and mentions so that we have flexibility to differentiate between documents with #fail and documents with just fail.
The general lesson here is that sometimes you need to preserve context about special terms during text analysis.
If you recall, the StandardTokenizer split our example tweet into 23 separate tokens.
In contrast the whitespace tokenizer produces a different set of tokens, as depicted in figure 6.9:
Please post comments or corrections to the Author Online forum:
So we solved our hashtag, mention, and hyphenated term issue but we introduced a few more issues in the process.
Luckily, with Solr, these issues are easily resolved using the WordDelimiterFilterFactory.
WordDelimiterFilterFactory offers a powerful solution to resolving most issues caused by splitting on whitespace.
At a high-level, this filter splits a token into sub-words using various parsing rules.
First let’s see how this filter helps us preserve the special characters on our hashtags and mentions.
For our example, we configured the WordDelimiterFilterFactory using the following options shown in listing 6.4:
Please post comments or corrections to the Author Online forum:
The leading backslash on the hash sign is so that Solr won’t interpret that line as a comment when reading the wdfftypes.txt file.
With this simple mapping, hashtags and mentions are preserved in our text.
The WordDelimiterFilter also handles hyphenated terms in a robust manner.
Think for a minute about what needs to happen during indexing and query analysis to ensure all three forms produce matches.
This is the exact behavior you get from the WordDelimiterFilter when.
In general, the WordDelimiterFilter provides a number of options used to fine-tune the transformations it makes on your tokens.
Table 6.4 gives you an overview of how each of these options works.
Please post comments or corrections to the Author Online forum:
It may take a little experimentation to get a feel for how the WordDelimiterFactory works with your content.
We recommend using Solr’s analysis form to experiment with these settings as we did in section 6.2.4
For now, we’ve solved the problems with hashtags, mentions, and hyphenated terms, so let’s turn our attention to how to handle terms with accent marks like caffé.
Please post comments or corrections to the Author Online forum:
In search, it’s often the case that doing simple things can make a big difference.
In most cases, you can’t be sure users will type characters with the diacritical mark when searching, so Solr provides the ASCIIFoldingFilterFactory to transform characters into their ASCII equivalent, if available.
In schema.xml, you can include this filter in your analyzer by adding:
It’s best to list this filter after the lowercase filter so that you only have to work with.
Stemming transforms words into a base form using language-specific rules.
For now, we’ll use a filter based on the Krovetz Stemmer: solr.KStemFilterFactory.
This stemmer is less aggressive in its transformations than other popular stemmers like the PorterStemmer.
We’ll discuss stemming and lemmatization in more depth in chapter 18
Table 6.5 shows some examples of stemming applied to terms using the KStemFilterFactory and the PorterStemFilterFactory.
Table 6.5 Comparing stems produced by the KStemmer and Porter algorithms.
It should be clear from these few examples that the Porter stemming algorithm is much.
The problem with being too aggressive is that your search application may end up matching documents that have little to do with a user’s query.
Please post comments or corrections to the Author Online forum:
In general, a stemmer expands the set of documents that match a query but can negatively impact precision of the results.
This filter injects synonyms for important terms into the token stream.
In most cases, synonyms are injected only during query time analysis.
This helps reduce the size of your index and is easier to maintain changes to the synonym list.
If you inject synonyms during indexing and you discover a new synonym for one of your terms, then you will have to re-index all of your documents to apply the change.
On the other hand, if you only inject synonyms during query processing, new synonyms can be introduced without re-indexing.
This filter is best applied to the query analyzer only.
To determine this, you need to think about what transformations need to take place before you match synonyms for terms.
We think it makes the most sense to list this filter last for your query analyzer so that your synonym list can assume all other transformations have already taken place on a token.
Consider if we apply the ASCIIFoldingFilter after our synonym filter; this means that our synonym list will need to include the diacritics, such as caffé.
In our example, there are a few tokens that could benefit from synonyms including: SF, latte, and caffe.
To map synonyms for these terms in Solr, add the following to the synonyms.txt file identified in your filter definition:
There are some solutions available as community extensions to Solr, such as a utility to convert the WordNet database of English synonyms into Solr’s format.
After applying these additional filters to our example tweet, we are left with the following.
Please post comments or corrections to the Author Online forum:
So now let’s return to the Solr Analysis form to see if the previous query we tried is a match.
Please post comments or corrections to the Author Online forum:
The query terms shown in table 6.6 are now matches to terms in our example tweet based on text analysis.
Table 6.6 Query terms matching our example tweet based on text analysis.
Please post comments or corrections to the Author Online forum:
Of course this query is a bit contrived for example purposes.
The key take-away is that Solr provides a wealth of built-in text analysis tools that allow you to implement flexible solutions to handle complex text.
The ultimate goal being a search application that makes it easier for your users to find relevant documents using natural language without having to think about linguistic differences in text.
After applying the schema.xml, restart your Solr server and then use the post.sh script to add the tweets.xml document to your server.
After indexing the documents, you can search for catch_all:@thelabdude and you.
Feel free to experiment with various queries to see Solr text analysis in action.
Let’s now turn our attention to more advanced topics in text analysis.
Throughout this chapter we built a solution to parse and analyze very complex social.
We covered the main tools Solr provides but in reality we’ve only touched on the most common tools available.
Before we wrap up this chapter, we want to provide you with an overview of some of the advanced techniques you can use to fine-tune your analysis.
Let’s begin by looking at some of the more advanced attributes you can apply to text fields in schema.xml.
Please post comments or corrections to the Author Online forum:
We think it is important to be aware of these options, as you will undoubtedly encounter them when looking at the Solr example schema.xml.
Also, you should be aware that each of these advanced attributes only apply to text fields and do not impact non-text fields like dates and strings.
Table 6.7 Overview of advanced attributes for field elements in schema.xml.
Norms help give shorter documents a little boost during relevance scoring.
Thus, if most of your documents are of similar size, then you could consider omitting norms to help reduce the size of your index.
However, you must not on a field as Solr encodes the boost into the norm value.
Norms are omitted by default for primitive types such as dates, strings, and numeric fields.
The MoreLikeThis feature requires term vectors to be enabled so that Solr can compute a similarity measure between two documents.
Storing term vectors can be expensive for large indexes, so only enable these if you really need them.
As we discussed in chapter 3, a norm is floating-point value (Java float) based on a document length norm, document boost, and field boost.
Under the covers, Lucene encodes this floating point value into a single byte, which if you think about it is pretty cool.
The document length norm is used to boost smaller documents.
Without going into too much detail, Lucene gives smaller documents a slight boost over longer documents to improve relevance scoring.
Conceptually, if a query term matches a short document and a long document, both containing the term once, Lucene considers the.
Please post comments or corrections to the Author Online forum:
In this case, term weight is just the term frequency (1) divided by the total number of terms in the document (N)
Thus, Lucene gives the shorter document a slight boost, which is encoded in the norm.
It stands to reason that if your documents are of similar length and you are not using memory during searching.
However, when just starting out, we recommend that you use the length normalization.
Let’s take a quick look at another advanced attribute for fields, which is used to improve the performance of document similarity calculations.
Solr also provides a feature to compute a similarity between documents, commonly known as More Like This.
The More Like This feature in Solr finds documents in the index that are very similar to a specific document.
Under the covers, this feature uses document term vectors to compute the similarity.
The term vector for any document can be computed at query time using information stored in the index.
However, for better performance, term vectors can be pre-computed and stored during indexing.
The optional termVectors attribute allows you to enable term vectors to be stored for each document during indexing.
Thus, if you plan to make heavy use of the More Like This you decide to enable this feature after indexing documents, then you will need to need to reindex all documents.
We will revisit these attributes throughout the rest of the book, when applicable.
For example, we’ll look closely at the termPositions and termOffsets attributes in Chapter 9 when we discuss hit highlighting.
For now, let’s turn our attention to another advanced text analysis topic—namely multi-lingual analysis and language detection.
In other words, the solution we built for analyzing English micro-blog content won’t work very well for German or French tweets.
Each language will have its own parsing rules for tokenizing, stop words list, and stemming rules.
In general, you will need to develop a specific <fieldType> for each language you want to analyze for your index.
That said many of the techniques you learned in this chapter are still applicable for analyzing languages other than English.
This raises the question of how to select the right text analyzer during indexing? Assuming you want to index all your documents regardless of language in the same index, a simple solution would be to use a unique field for each language.
Please post comments or corrections to the Author Online forum:
StandardTokenizer is used for tokenizing because it works well with most Latinbased languages, but if you wanted to preserve hashtags and mentions you would need to switch over to using the WhitespaceTokenizer.
Of course, if you know a document is French ahead of time, then you can manually populate the text_fr field when constructing your document to be indexed.
For example, assume we have the following tweet, which is a famous quote from Voltaire:
Le vrai philosphe n'attend rien des hommes, et il leur fait tout le bien don't il est capable.
The Solr XML document to index this French tweet is shown in listing 6.6:
Please post comments or corrections to the Author Online forum:
Listing 6.6 Example of a French tweet leur fait tout le bien don't il est capable.
BUILT-IN LANGUAGE DETECTION Solr language detection solution was designed to work with documents that are primarily.
In other words, your mileage may vary if you send a document that contains a mixture of languages.
Mixed language documents aside, let’s apply Solr’s language detection solution to our social media search application to see how it works.
Solr’s solution should work well for our social media search application in that blogs, tweets, and comments are typically written in a single language.
What we hope to see is that we can send a document, such as a tweet, in any major language, have Solr detect the language, and then apply the correct text analysis based on the language.
Continuing with our Voltaire quote, we want Solr to determine the document is French and then populate our lang field with the value “fr”
Once the language is known, Solr will look for a field named “text_fr” to populate so that this text gets analyzed using the correct field type designed for French text.
To get started, you need to enable the language detection component on Solr’s updateRequestHandler defined in solrconfig.xml, as shown in listing 6.7
Listing 6.7 Solr’s update request handler with language detection enabled.
A activates the language detection component during indexing This activates the “langid” updateRequestProcessorChain when documents are.
Next, you need to configure the langid updateRequestProcessorChain, which is located near the bottom of solrconfig.xml in the Solr example server; in your text editor, search for “langid”
Solr provides two options for detecting language: 1) Tika-based language.
Please post comments or corrections to the Author Online forum:
Also, LangDetect is an open-source project released under the commercial friendly Apache 2.0 license; it’s hosted at http://code.google.com/p/language-detection/
Listing 6.8 shows how to configure language detection in solrconfig.xml.
During indexing, Solr passes each document through the “langid” updateRequestProcessorChain.
The processor, an instance of LangDetectLanguageIdentifierUpdateProcessor, uses the value of the langid.fl parameter to determine which fields in the document contain text that should be used to detect the language.
The processor pulls the text and determines the language using statistical analysis on the patterns in the text.
Once the language is detected, the processor populates the language code in the field specified in the langid.langField parameter, which in our example is lang.
Thus, for our example, the processor will map French text to the text_fr field.
With these settings in place, you can now send documents containing text in any of the 53 languages supported by LangDetect and Solr will do the right thing.
Of course, you still need to define the fields and field types to handle specific languages as we did for French.
We’ll wrap-up this chapter with an answer to the question of what do you do when Solr.
Please post comments or corrections to the Author Online forum:
Consequently, it will be rare to encounter text analysis requirements that cannot be addressed with one of the built-in tools.
The Solr PlugIn framework can be used to develop extensions for a number of Solr components beyond text analysis.
Thus, we’ll only touch on the basics as they relate to text analysis.
We’ll use the Plug-In framework to extend other components of Solr in later chapters.
To begin, we need a requirement that Solr can’t solve with built-in tools.
Recall from our previous discussion of multi-valued fields where we indexed zero or more URLs into the “links” field.
For tweets, any links in the text are going to be shortened links provided by an online service like bitly.com.
For instance, the shortened bit.ly URL for the Solr home page http://lucene.apache.org/solr/ is http://bit.ly/3ynriE.
Thus, during indexing, we need to extract the shortened URL and replace it with the resolved URL.
To get the resolved URL, we can use an HTTP HEAD request and follow redirects until we reach the resolved URL or, in the case of bit.ly, we can use their Web service API.
Now that we know the problem to solve and have a basic understanding of how we want to solve it, we need to determine where in the Solr text analysis process to plug-in our solution.
In other words, we need to determine the type of plug-in we need to build.
Recall that an analyzer brings together a tokenizer and chain of token filters into a single component.
That feels a little heavy-handed since we want to utilize our existing tokenizer and chain of filters.
Our solution involves replacing one token, a shortened URL, with a different token, a fully-resolved URL.
Thus, for this requirement, it makes sense to build a custom TokenFilter, which in Solr is the most common and easiest way to customize text analysis.
That said, you can also build your own analyzer, tokenizer, or char filter using a similar process to what we illustrate below.
To create a custom TokenFilter, you need to develop two concrete Java classes: one that extends Lucene’s org.apache.lucene.analysis.TokenFilter class to perform the filtering and a factory for your custom filter that extends Lucene’s org.apache.lucene.analysis.util.TokenFilterFactory.
The factory class is needed so that Solr can instantiate configured instances of your TokenFilter using configuration supplied in the schema.xml file.
As our main focus here is to learn how to.
Please post comments or corrections to the Author Online forum:
Listing 6.9 shows a skeleton of the custom TokenFilter class for resolving shortened URLs:
If you choose to actually implement this filter, we encourage you to consider the impact your solution will have on indexing performance.
Please post comments or corrections to the Author Online forum:
A rough outline of a solution would use a distributed caching solution, such as memcached, to avoid resolving links more than once and would take full advantage of Web service APIs provided by URL shortening services like bitly.com.
Typically, the API-based approach will allow for batching up many shortened URLs into a single request, which will be more efficient than using HTTP HEAD requests to resolve the links by following redirects.
The Factory is responsible for taking attributes specified in schema.xml and converting them into parameters needed to create the TokenFilter.
The factory uses the shortenedUrlPattern attribute to compile a Java Pattern object for matching shortened URLs you want to resolve during text analysis.
The example shown here only supports bit.ly URLs so in a real application, you would want to extend this regular expression to support all possible shortened URL sources.
Next, take a moment to think about where in the chain of filters in the text_microblog field type you should put this filter.
We think it should go immediately after the WhitespaceTokenizer (before the WordDelimiterFilter), as you don’t want to perform any transformations on the shortened URL before trying to resolve it.
The Java implementation for our factory is shown in listing 6.10
Please post comments or corrections to the Author Online forum:
Only a few lines of code are needed to plug-in a custom TokenFilter! The key take-away here is that Solr uses your factory class as the intermediary between the filter definition in schema.xml and a configured instance of your custom TokenFilter used during text analysis.
The last thing you need to do is to place a JAR containing your Plug-In classes in a location where Solr can locate them during initialization.
When the server starts up, it makes all JAR files in the plugins directory available to the Solr ClassLoader.
If Solr complains about not being able to find your custom classes during initialization, try putting the full path to your plugins directory.
To recap, we began the chapter by learning how to define field types to do basic text analysis.
This is where we learned that field types for unstructured text-based fields are composed of either one analyzer for both indexing and query processing or two separate, but compatible analyzers for indexing and query processing.
Each analyzer is made up of a tokenizer and chain of token filters.
To test our simple analysis solution, we used Solr’s Analysis form to see an example document pass through the StandardTokenizer and chain of simple filters to remove stop words and lowercase terms.
Our testing demonstrated that the basic text analysis solution was not sufficient to deal with all the nuances of our micro-blog content.
Consequently, we leveraged more built-in Solr tools to tackle these complex requirements.
The WordDelimiter also proved useful for handling hyphenated terms in a more robust manner.
We also saw how to use stemming and synonym expansion to improve our search application’s matching capabilities.
Overall, we developed a powerful solution for analyzing micro-blog content using only built-in tools and not a single line of custom code.
Please post comments or corrections to the Author Online forum:
Finally, we finished our tour of text analysis by looking at some advanced concepts.
Next, we saw how to use Solr’s built-in solution for language detection to handle documents in other languages.
Lastly, we developed a custom TokenFilter to resolve shortened URLs.
The key take-away was that Solr makes it easy to implement a custom analyzer, tokenizer, token filter, or char filter to implement exotic requirements.
In the next chapter, we learn how to query Solr and process results.
Please post comments or corrections to the Author Online forum:
An introduction to faceting and an overview of common use cases.
Field Facets and the ability to see the top values in any field per query.
Using bucketized Range Facets to understand number and date ranges values in your search documents.
Getting matched document counts for any number of arbitrarily complex queries by utilizing Query Facets.
Implementing multi-select faceting and even faceting upon values not included in your search results.
An introduction to advanced faceting topics covered in later chapters.
Faceting is one of the most powerful features of Solr, as compared to traditional databases and other NoSQL data stores.
Faceted search, also called faceted navigation or faceted browsing, is a capability which allows those running searches to see a high-level breakdown of their search results based upon one or more aspects (facets) of their documents, allowing them to select filters to drill into those search results.
When running a search on a news site, you would expect to see options to filter your results by timeframe (last hour, last 24 hours, last week) or by category (politics, technology, local, business)
When searching on a job search site, you would likewise expect to see options to filter results by city, job category, industry, or even company name.
These filtering options generally display not only the available values for each of these facets, but also a count of the total search results matching each of those values.
In fact, since only a limited number of values can be displayed on the screen for each facet, search engines often sort the values for each facet based upon the most prevalent values (those matching the.
Please post comments or corrections to the Author Online forum:
This allows users to quickly see a birds-eye-view of their results set, without having to actually look through every single search result.
Faceting in Solr enables this kind of dynamic metadata to be brought back with each set of search results.
While the most basic form of faceting simply shows a breakdown of unique values within a field shared across many documents (a list of categories, for example), Solr also provides many advanced faceting capabilities.
These include faceting based upon the resulting values of a function, based upon ranges of values, or even based upon arbitrary queries.
Solr also allows for hierarchical and multi-dimensional faceting and multi-select faceting, which allows returning facet counts for documents that may have actually been filtered out a search result.
We will investigate each of these capabilities in this chapter, leaving you with the knowledge to implement your own world-class faceted search experience.
A basic understanding of how the Lucene index stores terms (covered in chapter 3) is also useful, but not required.
Let us begin with some examples demonstrating how faceting in Solr enables navigating through and visualizing search results at a glance.
In this section, we you will see a high-level overview of faceting, including several.
At this point, you may be wondering exactly what a facet looks like when it is brought back from the search engine.
Please post comments or corrections to the Author Online forum:
This navigational element provides a clear visual demonstration of what faceting is, and it also provides an initial glimpse into the power faceting provides.
You can think of each facet as a slice of information that describes the results of a search.
It is important to note that, while the information in figure 8.1 is useful, there are many potentially better way to visualize these facets.
One downside of including each of the values above is that you can only display a few at a time for each facet.
Figure 8.2 demonstrates an alternate way to view the State facet.
This visualization allows all 50 States within the United States to be displayed in the user interface at one time, preventing the user from being overwhelmed by information.
As you can see, by utilizing modern visualization techniques, it is possible to utilize facets as representing important meta-data about a user’s search results to provide an enhanced searching experience.
Figure 8.3 illustrates a similarly appropriate visualization for the Restaurant Type facet.
Please post comments or corrections to the Author Online forum:
Figure 8.3  Visualization representing the Restaurant Type facet as a pie chart.
While geographic maps and pie charts may be useful for demonstrating discrete values, representing continuous values such as numbers and dates can often be best represented through line graphs, as demonstrated in figure 8.4
Figure 8.4  The price range facet is visually represented as a continuous line graph, allowing users to interpret  all of the values in the facet with one quick glance.
The line graph demonstrating the Price Range facet is particularly interesting because this visualization can be used to represent any range of values which can plotted in a continuous.
Please post comments or corrections to the Author Online forum:
This is not a chapter on data visualization, so we will not belabor the point here, but it is important to take away from this section that faceting provides an incredibly powerful ability to generate real-time analytics on user searches which can greatly enhance your users’ search experiences.
Before we dive into the mechanics of implementing facets in Solr, it is also important to note that facets are calculated in real-time for each set of search results.
The count is NOT how many times the value exists across all documents (as a value may exist multiple times within a single document), it is only a count of the number of documents matched.
The fact that all facet values are calculated based upon a search result set means that every time a new search is fired, different facet values and counts will be returned for each facet.
This allows users to run a search, see the facets that are returned from the search result, and then perform another search which filters on any values for which they want to limit the next set of search results.
Notice that all cities in the City facet are now located in California since all documents within our search results must now be in California.
While figure 8.5 only filters on a single value (the state of California), you can actually apply as many filters as you want on any given search, and each facet will calculate its values based upon all of the filters being applied.
If you explore the Price Range facet for the first search (nation-wide) and compare them to the results of the second search (state of California), you would be able to spot a noticeable price difference between California and the rest of the United States, as figure 8.6 will visually demonstrate.
Please post comments or corrections to the Author Online forum:
A noticeable trend is evident when the facet values are compared side-by-side: Restaurants in California, on the whole, are more expensive than restaurants in the rest of the country.
You will see in section 8.5 how to apply these filters to facet values, but the key take-away from this section is that facets can provide rich insights into the results of any given search, allowing your users to easily measure the quality of their search and drill-in to the aspects of the results they find most interesting.
Before we dive into the mechanics of faceting, we need to load up some sample data into.
Solr that will be used for the examples in rest of this chapter.
Our sample data will be a small subset of documents similar to our restaurants example from section 8.1
This section will get you up and running with a handful of restaurant documents upon which you will be able to facet throughout the rest of this chapter.
Please post comments or corrections to the Author Online forum:
The schema in listing 8.1 defines each of the fields upon which we will attempt to facet in.
Each of the documents upon which we will facet is contained in listing 8.2
Please post comments or corrections to the Author Online forum:
Please post comments or corrections to the Author Online forum:
You can also find the example schema.xml (called restaurants_schema.xml) and the.
After downloading or recreating files from the above listings in your current home directory, it is now time to start Solr and index the example restaurant documents.
The last line above actually indexes the restaurants from a text file using the post.jar utility that comes with Solr.
If everything is successful, you should see an output like the following:
Once you have successfully indexed the restaurant documents, you should be able to hit the standard Solr search handler and verify that your documents in the engine, along with all of the fields from the example documents, using the following url:
Please post comments or corrections to the Author Online forum:
Please note that all listings and other Solr responses in this chapter are in JSON format and not XML.
Even though XML is Solr’s default response type, JSON is a more humanreadable format which is more compact and is thus easier to use for demonstration response type from XML to JSON format.
If you have not changed your default response parameter to all of the Solr requests in this chapter to return them in the same format.
You will also notice that all Solr responses in this chapter appear nicely indented.
This this indented response format is not recommended for a production application (it works, but adds unnecessary extra processing time to your application), but it does make Solr responses more human-readable, which is why it has been used for all the examples in this chapter.
The response you receive from Solr should include all twenty of the example restaurant.
With the twenty example documents now searchable, we are ready to begin running faceted searches.
We will begin with the most common form of faceting: faceting upon each of the unique values within a field.
Please post comments or corrections to the Author Online forum:
Field faceting is the most common form of faceting: requesting the unique values found in a particular field back, along with the number of documents in which they are found, when a search is performed.
In this section, you will learn how to construct a Solr query requesting a Field Facet, and you will learn the various faceting parameters that allow you to tweak how the facet values are calculated and returned from Solr.
For demonstration purposes for the rest of the chapter, we are going to be faceting upon the documents we indexed in section 8.2
Listing 8.3  Facet results for a Field Facet on a single-valued field.
This example demonstrates the most basic form of faceting in Solr: Field Faceting on a.
In this kind of faceting, each unique value is examined, along with a count of documents in which that value is found.
Since there is only one value per document, the sum of all of the counted values (company names in this case) will also equal the total number of documents.
Please post comments or corrections to the Author Online forum:
Not all fields in Solr contain a single value, however.
The tags field is an example of a multi-valued field.
Let us see what happens when we try to facet upon the tags field in listing 8.4
This is because two of the restaurants, Starbucks and McDonalds, fell into both of these categories.
If you were to search for breakfast or for coffee, this would become readily apparent from the search results returned from Solr.
It is also important to note that sum of the counts for each of the tags facet values is much greater than 20, which is the total number of documents in the search engine.
This is because each document contains more than one term, which means that many of the individual terms map to the same document (and thus most of the documents are counted more than once)
In section 8.1 we discussed several methods for visualizing facets.
This tag faceting example lends itself to what is probably a distinctly obvious kind of visualization: a tag cloud.
Please post comments or corrections to the Author Online forum:
Figure 8.7 demonstrates mapping the facet values from this multi-valued Field Facet result on the tags field.
Figure 8.7  A tag cloud representation of a Field Facet on a multi-valued tags field.
The size of the text is relative to the number of documents in which the phrase was found.
This demonstrates again that there are many ways to visualize facets.
Tag clouds are common ways for users to see a high-level overview of the results of their search from a categorical standpoint.
In addition to using tags like in this example, it is also common to facet upon a raw content field containing everyday language to glean these kinds of insights (such as the full-text of a restaurant’s description in this case)
An additional important item to keep in mind when you are faceting is that the values returned for a field facet are based upon the indexed values for a field.
Thus, unless you want to actually bring back facet counts for each of those terms individually and lowercased, you have to consider how you may want to facet on a field when you create the field definition in Solr’s schema.xml.
It is fairly standard for Solr developers to create a separate field into which they will put a duplicate copy of certain content for the sole purpose of faceting (so that the original text can be preserved in a facetable form.
At this point you should have a solid grasp of what a facet is, and you should also have a good feel for requesting a facet on either a single-valued field or a multi-valued field.
Thusfar, however, you have only been exposed to the default settings for bringing back a facet.
Faceting seems easy when you are only dealing with twenty documents, but what happens when there are thousands or millions of unique terms that would come back from Solr on a faceting request?  Fortunately, Solr has many faceting options which allow finetuning how facets are returned on a per-query basis.
This list of Field Facet options is given in table 8.1
Please post comments or corrections to the Author Online forum:
Table 8.1  A listing of the Field Faceting parameters which can be specified on the Solr url to modify faceting behavior.
One important take-away from table 8.1 is that multiple facets can be requested by specifying the facet.field parameter multiple times.
This parameter may be specified multiple times to return multiple facets.
By default, terms with zero matches in the current search will be included in facet results, so it is common to set facet.mincount to at least 1
The fc (field cache) method loops over the documents that match the query and finds the terms within those documents.
The fc method is faster for fields which contain many unique values, whereas the enum method is faster for fields which contain few values.
The fc method is the default for all fields except Boolean fields.
The default is 0, meaning that the filterCache should always be used.
Please post comments or corrections to the Author Online forum:
Listing 8.5  Mixing Field Faceting parameters on a field-by-field basis.
Please post comments or corrections to the Author Online forum:
As you can see, this listing combines multiple facet options on multiple fields in a custom.
While the query required many parameters, the flexibility these faceting options provide can be well worth the additional complexity.
At this point you have learned how to request Field Facets back in Solr so that you can see how many documents match each unique value in any of your indexed fields.
While this is a powerful feature, faceting in Solr is actually much more powerful.
While it is great to be able to return the top values within any indexed field as a facet, as discussed in the last section, it can also be extremely useful to bring back counts for arbitrary sub-queries so that you know how many results might match a future search.
The best way to demonstrate this capability is through an example.
You could certainly accomplish this by running three different queries, as indicated in listing 8.6
Listing 8.6  Running multiple queries to obtain document counts for sub-queries ...
Please post comments or corrections to the Author Online forum:
While the pain may not seem enormous in this small, contrived example, it is nevertheless unnecessary.
Listing 8.7 demonstrates how such a query can be easily combined into a single query using Query Facets.
Listing 8.7  Running a single Query Facet to obtain document counts for sub-queries.
As you can see from listing 8.7, multiple sub-queries can be combined into a single request to Solr through the use of Query Facets.
The above example is very specific to our data set (since it requires knowing all possible values at query time), but you have already seen an example in section 8.1 which is a great use-case for Query Facets: faceting upon price, where the price ranges are not evenly spaced out.
We can re-create this example using our test data, as indicated in listing 8.8
Please post comments or corrections to the Author Online forum:
This example demonstrates how Query Facets can effectively be used to create new buckets of information at query time in any Solr query.
In reality, you could have just as easily created a new field in Solr called “pricerange” which contained each of these bucketized values.
Had you done so, you could have just performed a field facet upon the new pricerange field to pull back each of the five bucketized values.
Of course, this would also required you to map these bucketizing rules at index time when you are first feeding your content to Solr, a process which can be painful, especially as your amount of content in Solr begins to grow.
Query Facets provide a nice alternative which allows you complete flexibility at query time to specify and re-define which buckets should be calculated and returned.
While the examples in this section have been simple, they demonstrate the complete flexibility that faceting upon any arbitrary query provides.
Because Solr provides many powerful query capabilities including nested query handlers and function queries, the possibilities for advanced query-based faceting are only limited by one’s imagination.
The ability to extend faceting in this way is tremendously powerful - anything you can search upon, you can facet upon.
While Query Facets are incredibly flexible, they can become burdensome at times to request from Solr, as every single value upon which you want to generate facet counts must be explicitly specified.
As we will see in the next section, Solr also provides a convenient Range Faceting capability that makes faceting upon numeric and date values much easier in this regard.
Range faceting, as its name implies, provides the ability to bucketize numeric and date field values into ranges such that the ranges (and their counts) get returned from Solr as a facet.
This can be particularly useful as a replacement for creating many different Query Facets to represent multiple ranges.
In the last section, a Query Facet was demonstrated based upon the price field in our example data from section 8.2
Had the values needed from the search engine been evenly spread out, similar facet counts could have been returned from Solr using a Range Facet, as indicated in listing 8.9
Please post comments or corrections to the Author Online forum:
First, Range Faceting returns counts for every value falling between the start facet.range.start and facet.range.end parameters on the query, even those ranges containing no documents.
Second, unlike the Query Facet example in listing 8.7, the buckets in range faceting are equally spaced out based upon the facet.range.gap parameter.
You can adjust the gap to create larger or more granular buckets based upon the needs of your application.
This is a great time saver if you want to bring back all of the range buckets within your range, as it prevents you from writing countless facet.query parameters manually to accomplish a similar effect.
In addition to the Range Faceting options already described, several additional range faceting parameters are available, including facet.range.hardened, facet.range.other, and facet.range.include.
Table 8.2 is included as a reference for each available Range Faceting parameter and its available options.
Table 8.2  A listing of the Range Faceting parameters which can be specified on the Solr url to modify faceting behavior.
Determines which field a Range Facet should be calculated upon.
This parameter may be specified multiple times to return multiple facets.
Please post comments or corrections to the Author Online forum:
The numerical or date value at which the first range should begin.
No value lower than this will be included in the counts for this facet.
The numerical or date value at which the first range should end.
No value higher than this will be included in the counts for this facet.
To create the ranges, this gap will be added to the lower bound (facet.range.start) successively until the upper bound (facet.range.end) is reached.
If hardened is set to “true” then the final range will stop at the upper bound, leaving a potentially smaller final bucket.
If hardened is false, then final bucket size will be increased above the upper bound such that its size it is the same size as the other buckets (the size of the gap)
Indicates additional ranges that should be included in the ranges.
The “before” option creates a bucket for all values prior to the lower bound.
The “after” option creates a bucket for all values after the upper bound.
The between option creates a bucket for all values between the lower and upper bounds.
This parameter may be specified multiple times to include multiple values.
If the “none” option is present, it will override any other parameters which are specified.
Please post comments or corrections to the Author Online forum:
This parameter may be specified multiple times to include multiple values.
The Solr parameters in table 8.2 demonstrate the rich options available when performing Range Faceting in Solr.
As with Field Faceting, several of the Range Faceting parameters can be specified on a per-field basis utilizing the.
Range Faceting often provides a more convenient and succinct query syntax than Query Faceting when faceting upon ranges of number or date values.
Query Faceting can alternatively be used when the range queries become overly complicated, allowing for some very powerful queries, as we saw in section 8.5
Of the three types of faceting we discussed, Field Faceting is the most widely used and simplest to use.
For each of these three faceting types, we have explored how you can requests facets back from Solr.
What we have yet to discuss is how you would go about refining your subsequent search once a facet is selected, which will be the topic of the following section.
Returning facets from Solr is the first step toward allowing your users to refine their search results.
Once you’ve shown the breakdown of faceted values to your users, however, the next step is to allow them to actually click on one or more facet values to apply that value as a filter.
In this section, we will discuss the best approaches for applying these filters.
At the most basic level, applying filters upon a facet is no more difficult than just adding.
Our initial query and results would look similar to listing 8.10
Listing 8.10  Faceting upon the tags field in the example restaurant data.
Please post comments or corrections to the Author Online forum:
Search results will also be returned for this query (not shown in listing 8.10), but your.
Please post comments or corrections to the Author Online forum:
First, they run a base search bringing back facets, and then they select a facet to run a subsequent search and narrow down their search results with a filter.
This could continue, with the user running a third search, such as in listing 8.12
As the user drills down even further, applying both a filter of state:California and.
It is worth noting that each of the examples you have seen thus far operate on a field containing only one value.
As such, as soon as you filter upon that value, no documents.
Please post comments or corrections to the Author Online forum:
Any field which is marked as multivalued in Solr’s schema.xml file or that is of a field type which is analyzed into multiple tokens may actually contribute multiple terms to a facet.
We can see this in action by utilizing the multivalued “tags” field in the example Solr index.
Listing 8.13  Applying several filters on a faceted field containing multiple values.
Please post comments or corrections to the Author Online forum:
From this example, you can see that fields containing multiple values will allow facets to.
It is also worth noting that, even though these examples specify each selected filter value as its own fq parameter on the Solr url, there is no requirement that this be done.
In fact, the filters applied in the last search of listing 8.13 could easily be converted from (or any logically equivalent Boolean expression)
This will require less lookups in the Solr Filter Cache (discussed in chapter 13), and will also provide more control over how your filter values interact.
For example, there is nothing requiring you to “AND” together each of the facets selected.
It could be a perfectly valid use case for you to “OR” values together, such as allowing users to select multiple cities in their facet while filtering to only the cities selected.
The discussion of multi-select faceting in the following section will highlight how you might accomplish the display of such a facet with multiple filters selected at once, even on a single-valued field.
Please post comments or corrections to the Author Online forum:
All of our examples of applying facet filters thus far have applied filters based upon a.
In reality, the terms brought back in facets may be more challenging to deal with, such as multi-word terms.
In order to allow for phrases separated by a space, most Solr developers decide to quote all terms they facet.
Unfortunately, there is even a problem with just blindly quoting the terms you are filtering upon: if the term has quotes within it, the syntax will break unless you escape it.
As you saw in chapter 5, text analysis upon a field can be defined differently for content indexing vs.
Thus, if any kind of mis-match occurs (which is generally a bad sign, of course), it is possible that a value you are trying to filter upon does not actually match the same number of documents as reported by the facet.
Thankfully, there is a fairly simple solution to ensuring the values returned by a facet and matched by a subsequent filter find the exact same documents: utilizing Solr’s Term Query Parser (TermQParserPlugin), which was discussed in chapter 7
One of the benefits of this query parser is that it bypasses the defined text analysis chain for your field and instead matches the term passed in directly against the Solr index.
This saves text-processing time, and it also prevents the other quoting and escaping logic discussed in the previous paragraphs.
The syntax using the Term query parser for the Los Angeles example would be.
The one downside of using the Term query parser for all of your facet filters is that this query parser does not support Boolean syntax, so if you want to combine multiple facet values together in a filter, you must utilize the Nested query parser and its syntax.
Listing 8.14 demonstrates utilizing both approaches: using a separate filter for each facet term and also combining multiple facet terms into a single filter utilizing the Term query parser.
Listing 8.14  Utilizing the Term query parser to filter on facet values.
Please post comments or corrections to the Author Online forum:
If you end up utilizing the Nested query parser syntax in approach 2, you will still need to escape quotes within your terms (since the whole nested query is in quotes), but this is a minor inconvenience if you want the capability to still use Boolean logic in your facet filters.
In this section, you have seen that applying filters to your facets is no different than applying any other filter in Solr, while seeing that it is possible to apply a separate filter per facet value or one filter for multiple facet terms.
You also saw that it is possible to use the Term query parser to bypass text processing and avoid difficult-to-handle character escaping when applying a faceting filter, since you already know the exact term you need to match from the Solr index since facets are pulled directly from the index.
At this point, you should be able to request and filter upon all of the basic facet types, but there is still more to explore.
In the next section, you will uncover some useful ways to re-name facets for display purposes and to even bring back facet counts for documents which have already been filtered out.
When requesting a facet back from Solr, the name of the facet is not always the most useful for purposes of displaying results back to the user or even handling them within your application stack.
Solr actually provides a very convenient ability to rename facets when they are returned, making facets much more user friendly for many use cases.
Solr also provides the ability to bring back facet counts for documents that have been filtered out.
This can be incredibly useful for implementing multi-select faceting – an ability to filter search results but still see the number of documents that would have matched had the filter not been applied.
In this section, you will be introduced to the concepts of the keys, tags, and excludes local params (local params were introduced in chapter 7), which enable these useful facet re-naming and multi-select capabilities.
All facets have a name which allows developers to distinguish them from each other.
Using the key local param, however, it is easy to rename any facet, as demonstrated in listing 8.15
Please post comments or corrections to the Author Online forum:
Please post comments or corrections to the Author Online forum:
The ability to rename a facet demonstrated in listing 8.15 can be very useful in many.
It allows your search application to request Query Facets, for example, without requiring the application to interpret the queries from the result set during a post-processing stage.
It also allows for user-friendly names to be assigned to facets regardless of the underlying field or query associated with the facet, which can make displaying the results in a user interface more straightforward.
Additionally, by enabling each facet to be assigned a unique name, this capability to specify keys allows for more than one facet to be defined on the same field (which can be useful for Field Facets or Range Facets), each coming back under a unique name.
One last advantage of this approach is that it allows multiple fields to be mapped into the same name depending upon query-time rules.
With such a setup, you could create a facet at query time which either specified layer of indirection can by handy in many scenarios, including any time you may want to redefine a facet to point to a different field with minimal changes to your application stack.
In addition to renaming facets, Solr also provides the ability to tag certain filters so that you can control their interaction with other Solr features.
When filters are applied to a Solr query request, the results must include every single filter.
One of the problems this presents, however, is that often times it is useful to see facet counts for values which have been already been excluded from the query.
Please post comments or corrections to the Author Online forum:
By default, after filtering upon a facet, the facet values which are returned for that facet no longer include the documents which were filtered out.
This is problematic if you want to allow your user to select multiple values to include in the search, as they will never be able to “OR” any additional facet filters for the values since they are no longer visible as options.
Even though you would expect your search results to only display documents in California given the above user interface, you would also expect the facets to continue displaying the other states so that you could expand your query.
The same principle applies for price ranges and cities – it is silly to only allow your users to search for one value per facet at a time.
Fortunately, Solr has a solution to this problem through a feature called Facet Exclusions.
By adding back the removed documents to the facet counts, you can make facet counts on each facet effectively ignore any filters applied based upon that facet.
Listing 8.16  Using tags and excludes to implement multi-select faceting.
Please post comments or corrections to the Author Online forum:
In terms of the mechanics of the query, a tag of “tagForState” was applied on the filter of for the state of California.
This was the state facet was requested, it was told to exclude all filters tagged with “tagForState”
Even though you cannot see the actual search results in listing 8.16, it is important to keep in mind that the documents returned from Solr are still constrained to the state of California (because the filter was applied to the query), even though state facet is not limited by that filter.
It is also important to understand that all of the other facets not tagged with the “tagForState” tag are constrained by still constrained to the state of California for the same reason.
You may also note that each of the other requested facets (on city and price) also contained exclusion tags, but that those exclusion tags did not correspond with any tagged filters.
While these exclusion tags were unnecessary, they did not cause any problems and.
Please post comments or corrections to the Author Online forum:
If someone were to re-run the query and add a filter on one of the additional facet values, the currently unused exclusion tag would kick into effect.
Whether you actually choose to apply exclusion tags on facets prior to the existence of any filters containing the excluded tags is up to you, but the point here is that doing so, while possibly wasteful syntax-wise, will not negatively impact the returned results.
It is possible to build some very interesting user interfaces and data analytics capabilities by mixing and matching tags and facets, but those use cases go well beyond what this chapter can cover.
You should feel free to experiment with these capabilities in Solr if you want to learn more.
At this point you have seen all of the major use cases for facets, including Field Faceting, Query Faceting, and Range Faceting.
There are several additional aspects of faceting that have not yet been discussed.
In the next section, we will touch on some of the more advanced topics related to faceting which will be discussed in later chapters.
This chapter provides a solid overview of the most used faceting capabilities in Solr, but this is not the last time you will see faceting discussed.
Faceting makes heavy use of Solr’s caches, so you will need to optimize your use of Solr’s built in caches in order to maximize the performance of your faceting requests.
Working with caches will be discussed in-depth in chapter 13
In addition to performance tuning, you will also see some more advanced forms of faceting in chapter 14
One of these advanced faceting capabilities is called Pivot Faceting, and it provides the ability for you to facet in many dimensions.
For example, say it was not sufficient for your application to just know the top values from the “tags” field, and that you really needed to know the top tags per city.
How would you go about accomplishing this? You could run a first search and get a facet for all the cities, and then run subsequent searches for each city to get its tags facet.
Unfortunately, this approach does not scale very well and can easily result in you having to run dozens or hundreds of searches as your document set gets larger.
Pivot facets, however, allow you to facet across multiple dimensions to pull back these kinds of calculations in a single search.
If you would like to learn more, please checkout the Advanced Faceting section of chapter 14
Congratulations on wrapping up an in-depth chapter on one of Solr’s most powerful features.
As you have seen, faceting provides a fast way to allow users to see a high-level overview of what kinds of documents their queries match.
You would be hard pressed to find a major search-powered website today which does not provide some form of faceting to allow users to drill down and explore their search results.
Using Solr, you have the ability to bring back the top values within each field using Field Facets, to bring back bucketed ranges of numbers or date values utilizing Range Facets, or to bring back the counts of any number of arbitrarily complex queries by utilizing Query Faceting.
Please post comments or corrections to the Author Online forum:
You also saw that it is possible to utilize keys to rename facets as they are being returned, learned how to use tags and Facet Excludes to implement multi-select faceting which returns counts even for documents which are filtered out by a query, and explored multiple ways of applying filters to a query once facets are clicked upon by your users.
At this point, you should be able to implement some fairly sophisticated search application utilizing all but the most complex forms of faceting available in Solr.
In the next chapter, you will learn how to use another very common Solr feature, Hit Highlighting, which allows the snippets of text matched in each document during a search to be returned for display in your list of search results, providing a potentially important insight to your users as to whether a document in your search results is worth exploring.
Please post comments or corrections to the Author Online forum:
Returning multiple groups of query results in a single search request.
Ensuring variety in search results by ensuring multiple categories are represented.
Result Grouping is a useful capability in Solr for ensuring an optimal mix of search results is returned for a user’s query.
Result Grouping, also commonly referred to as Field Collapsing, is the ability to ensure only one document (or some limited number) is returned for each unique value (actual or dynamically computed) within a field.
You have probably seen implementations of this capability when using your favorite web search engine at some point.
If you have ever seen search results telling you that many results matched your query, but only one (or the top few) were being displayed, you have encountered Field Collapsing.
Often times, such a message will provide a link users can click to see the fully-expanded list of search results, along with a count of how many additional documents would have been returned had the search results not been collapsed.
In addition to collapsing search results to remove duplicate documents, the Result Grouping functionality in Solr provides several other useful features.
In many ways, you can view Result Grouping in Solr as a more verbose form of Faceting.
Instead of only returning separate Facet sections along with the counts for each value, however, Result Grouping actually returns the unique values and their counts (like Faceting) plus some number of.
Please post comments or corrections to the Author Online forum:
One way in which grouping is very different than Faceting, however, is that it returns the requested groups within the search results section, which means the groups and values within the groups are sorted based upon the sorts specified for the documents in the query.
Grouping can be performed based upon field values, functions, or queries, making its applications numerous.
While this may sound complex initially, we will cover several use cases to demonstrate this incredibly useful and (mostly) straightforward feature.
One question commonly asked is why the Result Grouping functionality in Solr is referred.
Field Collapsing is the more commonly used term for this functionality in other search engines, referring to the act of returning a normal set of results with duplicate values removed.
Let’s say you are running an online e-commerce website that sells user-posted items.
While on the one hand it may be nice to run a search for an item and see thousands of copies of identical products, you may actually provide a much better user experience if instead you only show one document per unique item (possibly along with a count of how many of each item exists)
This will allow some diversity in the search results, just in case the item that shows up at the top is not exactly what the user was trying to find.
Please post comments or corrections to the Author Online forum:
In order to demonstrate this capability in action, some example product documents have been added in the source code that accompanies this book.
After following the steps in Appendix A to obtain a clean version of Solr, you can start up Solr and add the example documents using the commands in Listing 11.1
With our e-commerce search engine up and running, we can now demonstrate Solr’s Result Grouping capabilities in action.
Please post comments or corrections to the Author Online forum:
The results from listing 11.2 almost certainly represent a bad user experience, as they seem to present every format of the same product as separate products, listing it multiple times.
By turning Grouping on, the results appear much more diverse, providing a cleaner user experience, as demonstrated in Listing 11.3
Listing 11.3  Grouped search results collapsing on the product field.
Please post comments or corrections to the Author Online forum:
Please post comments or corrections to the Author Online forum:
First, notice that group.limit parameter was set to 1, indicating that only one document should be returned for each unique value within a Group.
You will also notice that the results format in listing 11.3 is substantially different than the default Solr results format.
This more verbose format is necessary to communicate all of the information associated with grouped search results – the name of the field that is grouped upon, the unique terms within that field (groupValue) which defines each group, and the total number of results (matches) before collapsing occurred in each group.
Unfortunately, it can often be inconvenient to support parsing out two separate Solr results formats to handle grouping.
Thankfully, if you are looking to remove duplicates and do not need all of this additional grouping information, Solr provides a group.main parameter which, if set to true, will merge the results from each group back into a flat list and return it in the main results format.
Please post comments or corrections to the Author Online forum:
Listing 11.4  Flattening grouped results into the main search results format.
Please post comments or corrections to the Author Online forum:
The main disadvantages of using the group.main option, of course, is that you lose access to the total number of un-collapsed results within each group, but if this is not important in your search application then this may be a fair trade-off in return for not having to handle two different search results formats.
You also lose the name of the group in this simple format, but this can often be derived from the results by returning the field in each document that you grouped upon (assuming it is a single-valued field)
The other disadvantage of using the group.main format is that it only supports a single group being but Solr actually supports returning multiple groups.
For example, you could ask for both a return the last group that you specify in the Solr url.
There is also another option in-between the advanced grouping format (the default) and the group.main format: the simple grouping format.
By specifying grouping format) while still returning the results for each group request in a flat list like the the last specified group in the main results list.
Please post comments or corrections to the Author Online forum:
You have seen throughout this section how to collapse the results of a query into groups so as to remove duplicate documents.
You have also seen three formats in which grouped search results can be returned: the default advanced grouping format, the simple grouping format, and the returning of a single collapsed group in the main search results.
Each of these formats strikes a balance between backwards compatibility with the standard search results format and the richness of information available to describe the identified groups.
Please post comments or corrections to the Author Online forum:
Throughout the rest of this chapter, we will see some more advanced use cases for the more generic grouping capabilities available in Solr.
The next section will begin by demonstrating how to request multiple documents per group in a single query.
Collapsing to a single document per unique field value is not the only practical use-case for Solr’s Grouping functionality, of course.
Returning to our e-commerce search engine example from the previous section, imagine that if instead of only collapsing duplicate documents, we could actually guarantee that we would return a fixed number of results from each product category.
If you still have your Solr instance up and running from listing 11.1 (if not, you can easily go back and restart it), we can run this query.
The example documents contain a type field that we can group upon, requesting a limit of three documents per group.
Additionally, let us request a maximum of 5 total groups be returned.
Please post comments or corrections to the Author Online forum:
Please post comments or corrections to the Author Online forum:
Even though the group.limit is set to 3, this only sets the upper limit, as any group without three is controlling not how many documents are returned, but instead it is controlling how many groups are returned.
In the default advanced grouping format, both the rows parameter and the start parameter apply to the groups instead of to the documents within each group.
That is, rows controls how many groups are returned, while group.limit controls how many documents are returned within each group.
Likewise, the start parameter controls the group offset for paging through groups, while the group.offset parameter controls the document offset for paging through the documents within each group.
One final very important aspect of grouping can be gleaned from listing 11.6: the way sorting interacts with grouping.
Conceptually, all groups are sorted based upon the order in which their top document is sorted.
What this means is that, assuming groupings were not enabled, all of the documents would appear in their sorted order (by relevancy score by default)
Inside of each group, the documents are also sorted, by default, in the order of their first appearance.
This means that since “Movies” is the top category group and since three movies are requested to appear within the Movies category group, that the second and third movie are likely to be less relevant (sorted lower) in an absolute sense even though they show up higher in the results because they get promoted up to the top group of “Movies.” For this reason, it is often uncommon for someone to request a group.limit of greater order of results may appear quite strange without the structure provided by the advanced grouping format to distinguish when one group ends and another begins.
While grouping on the values within a field provides so very useful search capabilities, as shown in the above examples, much more is possible with Solr’s Result Grouping capabilities.
The next section will begin by demonstrating the ability to group on more than just field values - you will see how to group by arbitrary queries and functions, as well.
Please post comments or corrections to the Author Online forum:
In addition to supporting grouping by unique field values, Solr also supports two additional grouping use cases.
The first of these is similar to grouping on a field, but it instead allows grouping by dynamically computed values by utilizing function queries.
The second additional use case is Solr’s query grouping capability, which essentially allows multiple queries to be run concurrently and returned as separate result sets.
Grouping based upon functions is accomplished using the group.func parameter.
In this case, the function is trying to group the search results into three groups by popularity.
Please post comments or corrections to the Author Online forum:
As you can see in listing 11.7, grouping by a function is conceptually the same as.
Functions can be nested, as you see in this case which nests three map functions, which means you have full control over the values that are computed if you want to manipulate them by combining multiple functions together.
If function grouping is too limiting for your use case, it is also possible to group by a query so that you can specify your own arbitrary values upon which to group.
In section 11.3, you saw that it is possible to collapse search results so that no more than a few documents are returned matching a particular value within a field (using the group.field parameter)
In addition to grouping on pre-defined field values, it can also be useful to dynamically group on arbitrary queries.
For example, a customer-centric user experience could be highly customized to return three sets of search results: those within 50 kilometers of the user, those within the customer’s price range, and those within the customer’s favorite category.
Please post comments or corrections to the Author Online forum:
Please post comments or corrections to the Author Online forum:
This is actually true for any kind of grouping query (group.field, group.func, or group.query) – any number of groups can be returned from Solr within a single request.
Second, a query group is essentially a way to perform multiple sub-searches of the original search.
In this case, the initial query was a wide-open single request, each returning separate sets of results.
Third, while it is true that a document will only appear once with a grouped result set, it is important to note that when multiple group parameters are used in a Solr request, each set of grouped results can contain that document again.
In other words, it is as if you are literally running multiple searches within the same request, because each of the separately requested query groups can contain the same documents if the query in their corresponding group.query parameter matches those documents.
With this ability to run multiple sub-searches in a single Solr request, some interesting interactions take place between Solr’s grouping functionality and the results paging and document sorting which also occur during the request.
Grouping, because of its richer search results structure, introduces some additional complexity when paging through and sorting search results.
You will recall from chapter 7 that Solr utilizes the rows parameter to determine how many documents to return from Solr for a standard search query.
Please post comments or corrections to the Author Online forum:
The rows parameter determines how many groups to return, the start parameter controls paging through available groups, and the sort parameter controls how groups are sorted (based upon their top document) as opposed to how documents are sorted across groups.
Should you additionally need to increase the number of documents per group, page through the results within a group, or sort the results within the groups differently, Solr’s Result Grouping functionality has separate parameters to control this.
As indicated in section 11.3, the group.limit parameter specifies the maximum number of results to return per group, performing the behavior that the rows parameter performs on a non-grouped search.
The group.offset parameter allows you to page through the results within a group, performing the behavior that the start parameter provides on a non-grouped search.
Finally, the group.sort parameter allows you to re-sort the documents within your groups, even though they have already been sorted initially by the sort parameter to determine the order in which the groups will appear.
Some very interesting uses for this two-pass sorting could be implemented, with one phase finding the relevant groups of documents and the other phase sorting the documents within that group based upon some other business need.
At the end of the day, perhaps the easiest way to think of paging, sorting, and limiting the results in a grouped request is to think of the groups as the actual documents Solr is returning.
Solr then provides the additional group parameters (group.limit, group.offset, and group.sort) to help you refine the documents within the groups for display.
While Solr’s Grouping capabilities prove useful for many use-cases, there are a few aspects of this functionality that can be a bit tricky to navigate.
Understanding these details is your query performance will be impacted.
By default, Facet counts are based upon the original query results, not the grouped results.
This means that whether you turn grouping on for a query or not, the facet counts will be the same.
It is possible, however, to only return facet counts for “collapsed” results by setting.
If you were to turn faceting on and facet on the type field using the ecommerce data for this chapter, you would get the results in listing 11.9
Please post comments or corrections to the Author Online forum:
The first characteristic of listing 11.9 that you should notice is that while eleven documents matched the filter for type:Movies, only six documents were returned in the grouped search results when the results were collapsed by the product field.
This is because several of the movies contained duplicate documents corresponding with different movie formats (DVD, Blue-ray, and even VHS), so the duplicate documents with the same product name were were collapsed out.
The second characteristic you may notice is that the facet counts.
Please post comments or corrections to the Author Online forum:
From a customer’s perspective, however, there are not eleven unique movies upon which they should be able to facet.
The customer most likely does not care about all the format variations and simply wants to see that there are six unique movies.
Please post comments or corrections to the Author Online forum:
This ability to group on a field and then facet on the collapsed group can come in very handy is turned on or off for all requested facets), and it cannot be applied to multiple requested results groups in the current version of Solr.
As such, if you do turn on grouped faceting, you should be aware that it only applies to the first requested result grouping.
One very important consideration when utilizing Solr’s Result Grouping functionality is how it interacts with distributed search.
Unlike standard searches, Result Grouping cannot be said to fully work in distributed mode… instead, it is more accurate to say that it works in a pseudo-distributed mode.
Grouping does return aggregated results in distributed mode, but the results are only the aggregates of the groups calculated locally on each Solr core.
Why does this matter? It matters because if the values you are grouping on are randomly distributed across Solr cores then the counts of grouped documents are going to be inaccurate.
If you were grouping a query for products by a field containing the product manufacturer’s name (to see all the unique products for that manufacturer), your total count of groups would be roughly the sum of the group counts for each Solr core you search against in a distributed search.
If and only if your documents are partitioned into separate shards by manufacturer name would you get the correct group count, since each group is guaranteed to only exist on one shard.
This is an important consideration to keep in mind if you plan on using Solr’s grouping functionality and require that the count of groups are grouping on and you are performing a distributed search, the count of groups that is returned will merely be an upper limit.
In addition to these data partitioning limitations, a few of the grouping parameters simply do not currently work in a distributed mode: group.truncate and group.func, so be careful using these if you think your data will grow beyond what one Solr core can handle.
You saw in section 11.2 how to return grouped results in the simple (flat) grouped format.
While both of these options are useful, you should consider wisely whether you can live without the extra information provided by the advanced grouping format.
Without the advanced format, you cannot request the number for groups, for example.
If you are using grouping to collapse documents, this means that you will not know how many unique values were found without using the advanced format.
Because it can often be challenging to change the response format in your application down the line, you should carefully consider which format to use during your initial application development.
Please post comments or corrections to the Author Online forum:
While grouping is a very powerful feature, it is considerably slower than a standard Solr query.
Based upon unscientific measurements, some have seen an average query take about three times longer to execute when grouping was requested for the purpose of collapsing down to one unique value (sometimes more, sometimes less depending upon the complexity of the query)
In order to help speed up grouping, a cache can be turned on for your grouping queries utilizing the group.cache.percent query parameter.
Because Result Grouping is internally implemented as two actual searches, turning this cache on can increase the speed of queries by caching one of those searches.
It is an advanced feature, but it has been demonstrated to improve the performance of Boolean queries, wildcard queries, and fuzzy queries.
Be careful, though, as it has also been shown to decrease performance on very simple queries such as term queries and match all (*:*) queries.
You will need to measure the performance impact for your own search application, including whether to turn group caching on or not, but a query using Result Grouping is likely to demonstrate a performance slowdown relative to a non-grouped query.
You should certainly take the performance impact into consideration when determining whether Solr’s grouping capabilities make sense for your use case.
Result Grouping can also be used to modify the results used for faceting, essentially preventing duplicate documents from being considered each time for facet counts.
You also saw how to return multiple documents per group and how to group on fields, functions, and queries.
Finally, you got to see some of the trickier gotchas associated with using Solr’s Result Grouping functionality, and you saw both the performance impacts of Result Grouping as well as the formats in which grouped results can be returned.
The last five chapters have covered many of Solr’s key search features, and by now you should feel prepared to build a world-class search application based upon Solr.
Taking that search application to production will require some additional work, however, and that effort is the subject of our next chapter.
