Programming Pig, the image of a domestic pig, and related trade dress are trademarks of O’Reilly Media, Inc.
Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks.
Where those designations appear in this book, and O’Reilly Media, Inc.
While every precaution has been taken in the preparation of this book, the publisher and author assume no responsibility for errors or omissions, or for damages resulting from the use of the information contained herein.
To my wife, Barbara, and our boys, Adam and Joel.
Their support, encouragement, and sacrificed Saturdays have made this book possible.
Our ability to collect and store data has grown massively in the last several decades.
Yet our appetite for ever more data shows no sign of being satiated.
Scientists want to be able to store more data in order to build better mathematical models of the world.
Marketers want better data to understand their customers’ desires and buying habits.
Financial analysts want to better understand the workings of their markets.
And everybody wants to keep all their digital photographs, movies, emails, etc.
The computer and Internet revolutions have greatly increased our ability to collect and store data.
Before these revolutions, the US Library of Congress was one of the largest collections of data in the world.
It is estimated that its printed collections contain approximately 10 terabytes (TB) of information.
Today large Internet companies collect that much data on a daily basis.
And it is not just Internet applications that are producing data at prodigious rates.
For example, the Large Synoptic Survey Telescope (LSST) planned for construction in Chile is expected to produce 20 TB of data every day.
Part of the reason for this massive growth in data is our ability to collect much more data.
Every time someone clicks on a website’s links, the web server can record information about what page the user was on and which link he clicked.
Every time a car drives over a sensor in the highway, its speed can be recorded.
But much of the reason is also our ability to store that data.
Ten years ago, telescopes took pictures of the sky every night.
But they could not store it at the same detail level that will be possible when the LSST is operational.
The extra data was being thrown away because there was nowhere to put it.
The ability to collect and store vast quantities of data only feeds our data addiction.
One of the most commonly used tools for storing and processing data in computer systems over the last few decades has been the relational database management system (RDBMS)
But as data sets have grown large, only the more sophisticated (and hence more expensive) RDBMSs have been able to reach the scale many users now desire.
At the same time, many engineers and scientists involved in processing the data have realized that they do not need everything offered by an RDBMS.
The high cost and unneeded features of RDBMSs have led to the development of many alternative data-processing systems.
Hadoop is an open source project started by Doug Cutting.
Over the past several years, Yahoo! and a number of other web companies have driven the development of Hadoop, which was based on papers published by Google describing how their engineers were dealing with the challenge of storing and processing the massive amounts of data they were collecting.
Hadoop is installed on a cluster of machines and provides a means to tie together storage and processing in that cluster.
The development of new data-processing systems such as Hadoop has spurred the porting of existing tools and languages and the construction of new tools, such as Apache Pig.
Tools like Pig provide a higher level of abstraction for data users, giving them access to the power and flexibility of Hadoop without requiring them to write extensive data-processing applications in low-level Java code.
Those who have never used Pig will find introductory material on how to run Pig and to get them started writing Pig Latin scripts.
For seasoned Pig users, this book covers almost every feature of Pig: different modes it can be run in, complete coverage of the Pig Latin language, and how to extend Pig with your own User Defined Functions (UDFs)
Even those who have been using Pig for a long time are likely to discover features they have not used before.
Being a relatively young project, Pig has changed and grown significantly over the last four years.
However, the rest of the book will still be applicable.
Some knowledge of Hadoop will be useful for readers and Pig users.
Appendix B provides an introduction to Hadoop and how it works.
These sections will be helpful for those not already familiar with Hadoop.
Small snippets of Java, Python, and SQL are used in parts of this book.
Knowledge of these languages is not required to use Pig, but knowledge of Python and Java will be necessary for some of the more advanced features.
Conventions Used in This Book The following typographical conventions are used in this book: Italic.
Indicates new terms, URLs, email addresses, filenames, and file extensions.
Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.
Constant width bold Shows commands or other text that should be typed literally by the user.
Constant width italic Shows text that should be replaced with user-supplied values or by values determined by context.
Code Examples in This Book Many of the example scripts, User Defined Functions (UDFs), and data used in this book are available for download from my GitHub repository.
Each example script in the text that is available on GitHub has a comment at the beginning that gives the filename.
Pig Latin and Python script examples are organized by chapter in the examples directory.
UDFs, both Java and Python, are in a separate directory, udfs.
For brevity, each script is written assuming that the input and output are in the local directory.
Therefore, when in local mode, you should run Pig in the directory that the input data is in.
When running on a cluster, you should place the data in your home directory on the cluster.
The three data sets used in the examples are real data sets, though quite small.
This data was trimmed to include only stock symbols, starting with C from the year 2009, to make the data small enough to download easily.
If you want to download the entire data set and place it on a cluster (only a few nodes would be necessary), it would be a more realistic demonstration of Pig and Hadoop.
Instructions on how to download the data are in the README files.
The third data set is a very brief web crawl started from Pig’s web page.
Using Code Examples This book is here to help you get your job done.
In general, you may use the code in this book in your programs and documentation.
You do not need to contact us for permission unless you’re reproducing a significant portion of the code.
For example, writing a program that uses several chunks of code from this book does not require permission.
Selling or distributing a CD-ROM of examples from O’Reilly books does require permission.
Answering a question by citing this book and quoting example code does not require permission.
Incorporating a significant amount of example code from this book into your product’s documentation does require permission.
An attribution usually includes the title, author, publisher, and ISBN.
With a subscription, you can read any page and watch any video from our library online.
Access new titles before they are available for print, and get exclusive access to manuscripts in development and post feedback for the authors.
Copy and paste code samples, organize your favorites, download chapters, bookmark key sections, create notes, print out pages, and benefit from tons of other time-saving features.
To have full digital access to this book and others on similar topics from O’Reilly and other publishers, sign up for free at http://my.safaribooksonline.com.
How to Contact Us Please address comments and questions concerning this book to the publisher:
We have a web page for this book, where we list errata, examples, and any additional information.
Much of the glory goes to the quarterback or a running back.
But if the team has a bad offensive line, the quarterback never gets the chance to throw the ball.
Receivers must be able to catch, and the defense must be able to prevent the other team from scoring.
In short, the whole team must play well in order to win.
And behind those on the field there is an array of coaches, trainers, and managers who prepare and guide the team.
But without the amazing group of developers, researchers, testers, documentation writers, and users that contribute to the Pig project, there would be nothing worth writing about.
In particular, I would like to acknowledge Pig contributors and users for their contributions and feedback on this book.
Corrine Chandel deserves special mention for reviewing the entire book.
Her feedback has added greatly to the book’s clarity and correctness.
Thanks go to Tom White for encouraging me in my aspiration to write this book, and for the sober warnings concerning the amount of time and effort it would require.
Douglas of the Hadoop project provided me with very helpful feedback on the sections covering Hadoop and MapReduce.
I would also like to thank Mike Loukides and the entire team at O’Reilly.
They have made writing my first book an enjoyable and exhilarating experience.
What Is Pig? Pig provides an engine for executing data flows in parallel on Hadoop.
It includes a language, Pig Latin, for expressing these data flows.
Pig Latin includes operators for many of the traditional data operations (join, sort, filter, etc.), as well as the ability for users to develop their own functions for reading, processing, and writing data.
It handles breaking the files into large blocks and distributing them across different machines, including making multiple copies of each block so that if any one machine fails no data is lost.
By default, Pig reads input files from HDFS, uses HDFS to store intermediate data between MapReduce jobs, and writes its output to HDFS.
As you will see in Chapter 11, it can also read input from and write output to sources other than HDFS.
Every job in MapReduce consists of three main phases: map, shuffle, and reduce.
In the map phase, the application has the opportunity to operate on each record in the input separately.
Many maps are started at once so that while the input may be gigabytes or terabytes in size, given enough machines, the map phase can usually be completed in under one minute.
Part of the specification of a MapReduce job is the key on which data will be collected.
For example, if you were processing web server logs for a website that required users to log in, you might choose the user ID to be your key so that you could see everything done by each user on your website.
In the shuffle phase, which happens after the map phase, data is collected together by the key the user has chosen and distributed to different machines for the reduce phase.
Every record for a given key will go to the same reducer.
In the reduce phase, the application is presented each key, together with all of the records containing that key.
After processing each group, the reducer can write its output.
See the next section for a walkthrough of a simple MapReduce program.
Consider a simple MapReduce application that counts the number of times each word appears in a given text.
In this example the map phase will read each line in the text, one at a time.
It will then split out each word into a separate string, and, for each word, it will output the word and a 1 to indicate it has seen the word one time.
The shuffle phase will use the word as the key, hashing the records to reducers.
The reduce phase will then sum up the number of times each word was seen and write that together with the word as output.
Mary had a little lamb its fleece was white as snow and everywhere that Mary went the lamb was sure to go.
Let’s assume that each line is sent to a different map task.
In reality, each map is assigned much more data than this, but it makes the example easier to follow.
The data flow through MapReduce is shown in Figure 1-1
Once the map phase is complete, the shuffle phase will collect all records with the same word onto the same reducer.
For this example we assume that there are two reducers: all words that start with A-L are sent to the first reducer, and M-Z are sent to the second reducer.
The reducers will then output the summed counts for each word.
Pig uses MapReduce to execute all of its data processing.
It compiles the Pig Latin scripts that users write into a series of one or more MapReduce jobs that it then executes.
There is no need to be concerned with map, shuffle, and reduce phases when using Pig.
It will manage decomposing the operators in your script into the appropriate MapReduce phases.
This means it allows users to describe how data from one or more inputs should be read, processed, and then stored to one or more outputs in parallel.
These data flows can be simple linear flows like the word count example given previously.
They can also be complex workflows that include points where multiple inputs are joined, and where data is split into multiple streams to be processed by different operators.
To be mathematically precise, a Pig Latin script describes a directed acyclic graph (DAG), where the edges are data flows and the nodes are operators that process the data.
This means that Pig Latin looks different from many of the programming languages you have seen.
There are no if statements or for loops in Pig Latin.
This is because traditional procedural and object-oriented programming languages describe control flow, and data flow is a side effect of the program.
For information on how to integrate the data flow described by a Pig Latin script with control flow, see Chapter 9
After a cursory look, people often say that Pig Latin is a procedural version of SQL.
It allows users to describe what question they want answered, but not how they want it answered.
In Pig Latin, on the other hand, the user describes exactly how to process the input data.
Another major difference is that SQL is oriented around answering one question.
When users want to do several data operations together, they must either write separate queries, storing the intermediate data into temporary tables, or write it in one query using subqueries inside that query to do the earlier steps of the processing.
However, many SQL users find subqueries confusing and difficult to form properly.
Also, using subqueries creates an inside-out design where the first step in the data pipeline is the innermost query.
Pig, however, is designed with a long series of data operations in mind, so there is no need to write the data pipeline in an inverted set of subqueries or to worry about storing data in temporary tables.
Consider a case where a user wants to group one table on a key and then join it with a second table.
Because joins happen before grouping in a SQL query, this must be expressed either as a subquery or as two queries with the results stored in a temporary table.
Example 1-3 will use a temporary table, as that is more readable.
Furthermore, each was designed to live in a different environment.
Pig is designed for the Hadoop data-processing environment, where schemas are sometimes unknown or inconsistent.
Data may not be properly constrained, and it is rarely normalized.
As a result of these differences, Pig does not require data to be loaded into tables first.
It can operate on data as soon as it is copied into HDFS.
My wife and I have been to France together a couple of times.
But because English is the language of commerce (and probably because Americans and the British like to vacation in France), there is enough English spoken in France for me to get by.
She can explore the parts of France that are not on the common tourist itinerary.
Her experience of France is much deeper than mine because she can speak the native language.
It has the nice feature that everyone and every tool knows it, which means the barrier to adoption is very low.
Our goal is to make Pig Latin the native language of parallel data-processing systems such as Hadoop.
It may take some learning, but it will allow users to utilize the power of Hadoop much more fully.
The first line of this program loads the file users and declares that this data has two fields: name and age.
Now the data has only records of users in the age range we are interested in.
The second load statement loads pages and names it Pages.
It declares its schema to have two fields, user and url.
The line Jnd = join joins together Fltrd and Pages using Fltrd.name and Pages.user as the key.
After this join we have found all the URLs each user has visited.
The line Grpd = group collects records together by URL.
The next line then counts how many records are collected together for each URL.
So after this line we now know, for each URL, how many times it was visited by users aged 18–25
The next thing to do is to sort this from most visits to least.
The line Srtd = order sorts on the count value from the previous line and places it in desc (descending) order.
Finally, we need only the top five pages, so the last line limits the sorted results to only five records.
The results of this are then stored back to HDFS in the file top5sites.
In Pig Latin this comes to nine lines of code and took about 15 minutes to write and debug.
The same code in MapReduce (omitted here for brevity) came out to about 170 lines of code and took me four hours to get working.
The Pig Latin will similarly be easier to maintain, as future developers can easily understand and modify this code.
It is possible to develop algorithms in MapReduce that cannot be done easily in Pig.
A good engineer can always, given enough time, write code that will out perform a generic system.
Basically this is the same situation as choosing to code in Java versus a scripting language such as Python.
Java has more power, but due to its lower-level nature, it requires more development time than scripting languages.
Developers will need to choose the right tool for each job.
What Is Pig Useful For? In my experience, Pig Latin use cases tend to fall into three separate categories: traditional extract transform load (ETL) data pipelines, research on raw data, and iterative processing.
A common example is web companies bringing in logs from their web servers, cleansing the data, and precomputing common aggregates before loading it into their data warehouse.
It is also used to join web event data against user databases so that user cookies can be connected with known user information.
Another example of data pipelines is using Pig offline to build behavior prediction models.
Pig is used to scan through all the user interactions with a website and split the users into various segments.
Then, for each segment, a mathematical model is produced that predicts how members of that segment will respond to types of advertisements or news articles.
In this way the website can show ads that are more likely to get clicked on, or offer news stories that are more likely to engage users and keep them coming back to the site.
Traditionally, ad-hoc queries are done in languages such as SQL that make it easy to quickly form a question for the data to answer.
However, for research on raw data, some users prefer Pig Latin.
Because Pig can operate in situations where the schema is unknown, incomplete, or inconsistent, and because it can easily manage nested data, researchers who want to work on data before it has been cleaned and loaded into the warehouse often prefer Pig.
Researchers who work with large data sets often use scripting languages such as Perl or Python to do their processing.
Users with these backgrounds often prefer the dataflow paradigm of Pig over the declarative query paradigm of SQL.
Users building iterative processing models are also starting to use Pig.
Consider a news website that keeps a graph of all news stories on the Web that it is tracking.
In this graph each news story is a node, and edges indicate relationships between the stories.
For example, all stories about an upcoming election are linked together.
Every five minutes a new set of stories comes in, and the data-processing engine must integrate them into the graph.
Some of these stories are new, some are updates of existing stories, and some supersede existing stories.
Some data-processing steps need to operate on this entire graph of stories.
For example, a process that builds a behavioral targeting model needs to join user data against this entire graph of stories.
Rerunning the entire join every five minutes is not feasible because it cannot be completed in five minutes with a reasonable amount of hardware.
But the model builders do not want to update these models only on a daily basis, as that means an entire day of missed serving opportunities.
To cope with this problem, it is possible to first do a join against the entire graph on a regular basis, for example, daily.
Then, as new data comes in every five minutes, a join can be done with just the new incoming data, and these results can be combined with the results of the join against the whole graph.
This combination step takes some care, as the five-minute data contains the equivalent of inserts, updates, and deletes on the entire graph.
It is possible and reasonably convenient to express this combination in Pig Latin.
One point that is implicit in everything I have said so far is that Pig (like MapReduce) is oriented around the batch processing of data.
If you need to process gigabytes or terabytes of data, Pig is a good choice.
But it expects to read all the records of a file and write all of its output sequentially.
For workloads that require writing single or small groups of records, or looking up many different records in random order, Pig (like MapReduce) is not a good choice.
Pig Philosophy Early on, people who came to the Pig project as potential contributors did not always understand what the project was about.
They were not sure how to best contribute or which contributions would be accepted and which would not.
So, the Pig team produced a statement of the project’s philosophy that summarizes what Pig aspires to be: Pigs eat anything.
Pig can operate on data whether it has metadata or not.
It can operate on data that is relational, nested, or unstructured.
And it can easily be extended to operate on data beyond files, including key/value stores, databases, etc.
Pigs live anywhere Pig is intended to be a language for parallel data processing.
It has been implemented first on Hadoop, but we do not intend that to be only on Hadoop.
Pigs are domestic animals Pig is designed to be easily controlled and modified by its users.
Pig allows integration of user code wherever possible, so it currently supports user defined field transformation functions, user defined aggregates, and user defined conditionals.
These functions can be written in Java or in scripting languages that can compile down to Java (e.g., Jython)
It supports external executables via its stream command and MapReduce JARs via its mapreduce command.
It allows users to provide a custom partitioner for their jobs in some circumstances, and to set the level of reduce parallelism for their jobs.
Pig has an optimizer that rearranges some operations in Pig Latin scripts to give better performance, combines MapReduce jobs together, etc.
However, users can easily turn this optimizer off to prevent it from making changes that do not make sense in their situation.
We want to consistently improve performance, and not implement features in ways that weigh Pig down so it can’t fly.
So, a team of development engineers was assembled to take the research prototype and build it into a production-quality product.
About this same time, in fall 2007, Pig was open sourced via the Apache Incubator.
The first Pig release came a year later in September 2008
Later that same year, Pig graduated from the Incubator and became a subproject of Apache Hadoop.
Early in 2009 other companies started to use Pig for their data processing.
Amazon also added Pig as part of its Elastic MapReduce service.
In 2010, Pig adoption continued to grow, and Pig graduated from a Hadoop subproject, becoming its own top-level Apache project.
Off the top of his head, one researcher suggested Pig, and the name stuck.
While some have hinted that the name sounds coy or silly, it has provided us with an entertaining nomenclature, such as Pig Latin for a language, Grunt for a shell, and Piggybank for a CPAN-like shared repository.
Downloading and Installing Pig Before you can run Pig on your machine or your Hadoop cluster, you will need to download and install it.
You can download Pig as a complete package or as source code that you build.
You can also get it as part of a Hadoop distribution.
It comes packaged with all of the JAR files needed to run Pig.
It can be downloaded by going to Pig’s release page.
Pig does not need to be installed on your Hadoop cluster.
It runs on the machine from which you launch Hadoop jobs.
Though you can run Pig from your laptop or desktop, in practice, most cluster owners set up one or more machines that have access to their Hadoop cluster but are not part of the cluster (that is, they are not data nodes or task nodes)
This makes it easier for administrators to update Pig and associated tools, as well as to secure access to the clusters.
You will need to install Pig on these gateway machines.
If your Hadoop cluster is accessible from your desktop or laptop, you can install Pig there as well.
Also, you can install Pig on your local machine if you plan to use Pig in local mode.
The core of Pig is written in Java and is thus portable across operating systems.
The shell script that starts Pig is a bash script, so it requires a Unix environment.
Hadoop, which Pig depends on, even in local mode, also requires a Unix environment for its filesystem operations.
In practice, most Hadoop clusters run a flavor of Linux.
For future versions, check the download page for information on what version(s) of Hadoop they require.
The correct version of Hadoop is included with the Pig download.
If you plan to use Pig in local mode or install it on a gateway machine where Hadoop is not currently installed, there is no need to download Hadoop separately.
Once you have downloaded Pig, you can place it anywhere you like on your machine, as it does not depend on being in a certain location.
To install it, place the tarball in the directory of your choosing and type:
The only other setup in preparation for running Pig is making sure that the environment variable JAVA_HOME is set to the directory that contains your Java distribution.
Pig will fail immediately if this value is not in the environment.
You can set this in your shell, specify it on the command line when you invoke Pig, or set it explicitly in your copy of the Pig script pig, located in the bin directory that you just unpacked.
You can find the appropriate value for JAVA_HOME by executing which java and stripping the bin/java from the end of the result.
Downloading Pig from Cloudera In addition to the official Apache version, there are companies that repackage and distribute Hadoop and associated tools.
Currently the most popular of these is Cloudera, which produces RPMs for Red Hat–based systems and packages for use with APT on Debian systems.
It also provides tarballs for other systems that cannot use one of these package managers.
The upside of using a distribution like Cloudera’s is that all of the tools are packaged and tested together.
The downside is that you are constrained to move at the speed of your distribution provider.
There is a delay between an Apache release of Pig and its availability in various distributions.
For complete instructions on downloading and installing Hadoop and Pig from Cloudera, see Cloudera’s download site.
Note that you have to download Pig separately; it is not part of the Hadoop package.
Downloading Pig Artifacts from Maven In addition to the official release available from Pig’s Apache site, it is possible to download Pig from Apache’s Maven repository.
This site includes JAR files for Pig, for the source code, and for the Javadocs, as well as the POM file that defines Pig’s dependencies.
Development tools that are Maven-aware can use this to pull down Pig’s source and Javadoc.
If you use maven or ant in your build process, you can also pull the Pig JAR from this repository automatically.
Downloading the Source When you download Pig from Apache, you also get the Pig source code.
This enables you to debug your version of Pig or just peruse the code to see how it works.
But if you want to live on the edge and try out a feature or a bug fix before it is available in a release, you can download the source from Apache’s Subversion repository.
You can also apply patches that have been uploaded to Pig’s issue-tracking system but that are not yet checked into the code repository.
Information on checking out Pig using svn or cloning the repository via git is available on Pig’s version control page.
Running Pig You can run Pig locally on your machine or on your grid.
Running Pig Locally on Your Machine Running Pig locally on your machine is referred to in Pig parlance as local mode.
Local mode is useful for prototyping and debugging your Pig Latin scripts.
In versions 0.6 and earlier, Pig executed scripts in local mode itself.
Starting with version 0.7, it uses the Hadoop class LocalJobRunner that reads from the local filesystem and executes MapReduce jobs locally.
This has the nice property that Pig jobs run locally in the same way as they will on your cluster, and they all run in one process, making debugging much easier.
Another reason for switching to MapReduce for local mode was that as Pig added features that took advantage of more advanced MapReduce features, it became difficult or impossible to replicate those features in local mode.
Thus local mode and MapReduce mode were diverging in their feature set.
This matches the schema we declared in our Pig Latin script.
The first field is the exchange this stock is traded on, the second field is the stock ticker symbol, the third is the date the dividend was paid, and the fourth is the amount of the dividend.
Remember that to run Pig you will need to set the JAVA_HOME environment variable to the directory that contains your Java distribution.
You can then run this example on your local machine by entering:
The result should be a lot of output on your screen.
But some of it is Pig telling you how it will execute the script, giving you the status as it executes, etc.
Near the bottom of the output you should see the simple message Success!
The script stores its output to average_dividend, so you might expect to find a file by that name in your local directory.
Because Hadoop is a distributed system and usually processes data in parallel, when it outputs data to a “file” it creates a directory with the file’s name, and each writer creates a separate part file in that directory.
In this case we had one writer, so we have one part file.
We can look in that part file for the results by entering:
All of the parsing, checking, and planning is done locally.
Usually this will be one or more machines that have access to your Hadoop cluster.
However, depending on your configuration, it could be your local machine as well.
The only thing Pig needs to know to run on your cluster is the location of your cluster’s NameNode and JobTracker.
The NameNode is the manager of HDFS, and the JobTracker coordinates MapReduce jobs.
In Hadoop 0.18 and earlier, these locations are found in your hadoop-site.xml file.
In Hadoop 0.20 and later, they are in three separate files: core-site.xml, hdfs-site.xml, and mapred-site.xml.
If you are already running Hadoop jobs from your gateway machine via MapReduce or another tool, you most likely have these files present.
If not, the best course is to copy these files from nodes in your cluster to a location on your gateway machine.
This guarantees that you get the proper addresses plus any site-specific settings.
If, for whatever reason, it is not possible to copy the appropriate files from your cluster, you can create a hadoop-site.xml file yourself.
You will need to find the names and ports for your NameNode and JobTracker from your cluster administrator.
Once you have located, copied, or created these files, you will need to tell Pig the directory they are in by setting the PIG_CLASSPATH environment variable to that directory.
Note that this must point to the directory that the XML file is in, not the file itself.
Pig will read all XML and properties files in that directory.
Let’s run the same script on your cluster that we ran in the local mode example (Example 2-1)
If you are running on a Hadoop cluster you have never used before, you will most likely need to create a home directory.
If you are using 0.5 or earlier, change fs -mkdir to mkdir.
Remember, you need to set JAVA_HOME before executing any Pig commands.
In order to run this example on your cluster, you first need to copy the data to your cluster:
If you are running Pig 0.5 or earlier, change fs -copyFromLocal to copyFromLocal.
Now you are ready to run the Pig Latin script itself:
The first few lines of output will tell you how Pig is connecting to your cluster.
After that it will describe its progress in executing your script.
It is important for you to verify that Pig is connecting to the appropriate filesystem and JobTracker by checking that these values match the values for your cluster.
If the filesystem is listed as file:/// or the JobTracker says localhost, Pig did not connect to your cluster.
You will need to check that you entered the values properly in your configuration files and properly set PIG_CLASSPATH to the directory that contains those files.
Near the end of the output there should be a line saying Success!
However, in this example I ran cat directly on average_dividend.
If you list average_dividend, you will see that it is still a directory in this example, but in Pig, cat can operate on directories.
Running Pig in the Cloud Cloud computing† along with the software as a service (SaaS) model have taken off in recent years.
This has been fortuitous for hardware-intensive applications such as Hadoop.
Setting up and maintaining a Hadoop cluster is an expensive proposition in terms of hardware acquisition, facility costs, and maintenance and administration.
Many users find that it is cheaper to rent the hardware they need instead.
Rather than allowing customers to rent machines for any type of process (like Amazon’s Elastic Cloud Computing [EC2] service and other cloud services), EMR allows users to rent virtual Hadoop clusters.
These clusters read data from and write data to Amazon’s Simple Storage Service (S3)
This means users do not even need to set up their own Hadoop cluster, which they would have to do if they used EC2 or a similar service.
However, I suggest beginning with this nice tutorial, which will introduce you to the service.
Command-Line and Configuration Options Pig has a number of command-line options that you can use with it.
You can see the full list by entering pig -h.
Most of these options will be discussed later, in the sections that cover the features these options control.
In this section I discuss the remaining miscellaneous options: -e or -execute.
For example, pig -e fs -ls will list your home directory.
Being the current flavor of the month, the term cloud computing is being used to describe just about anything that takes more than one computer and is not located on a person’s desktop.
In this chapter I use cloud computing to mean the ability to rent a cluster of computers and place software of your choosing on those computers.
The entire list can be printed out with pig -h properties.
Specific properties are discussed later in sections that cover the features they control.
Hadoop also has a number of Java properties it uses to determine its behavior.
In Pig version 0.8 and later, these can be passed to Pig, and then Pig will pass them on to Hadoop when it invokes Hadoop.
In earlier versions, the properties had to be in hadoop-site.xml so that the Hadoop client itself would pick them up.
When placed on the command line, these property definitions must come before any Pig-specific commandline options (such as -x local)
Finally, you can specify a separate properties file by using -P.
If properties are specified on both the command line and in a properties file, the command-line specification takes precedence.
Return Codes Pig uses return codes, described in Table 2-1, to communicate success or failure.
ParseException thrown (can happen after parsing if variable substitution is being done)
It enables users to enter Pig Latin interactively and provides a shell for users to interact with HDFS.
To enter Grunt, invoke Pig with no script or command to run.
This gives you a Grunt shell to interact with your local filesystem.
If you omit the -x local and have a cluster configuration set in PIG_CLASSPATH, this will put you in a Grunt shell that will interact with HDFS on your cluster.
As you would expect with a shell, Grunt provides command-line history and editing, as well as Tab completion.
It does not provide filename completion via the Tab key.
That is, if you type kil and then press the Tab key, it will complete the command as kill.
But if you have a file foo in your local directory and type ls fo, and then hit Tab, it will not complete it as ls foo.
This is because the response time from HDFS to connect and find whether the file exists is too slow to be useful.
Although Grunt is a useful shell, remember that it is not a full-featured shell.
It does not provide a number of commands found in standard Unix shells, such as pipes, redirection, and background execution.
To exit Grunt you can type quit or enter Ctrl-D.
This can be particularly useful for quickly sampling your data and for prototyping new Pig Latin scripts.
Pig will not start executing the Pig Latin you enter until it sees either a store or dump.
However, it will do basic syntax and semantic checking to help you catch errors quickly.
If you do make a mistake while entering a line of Pig Latin in Grunt, you can reenter the line using the same alias, and Pig will take the last instance of the line you enter.
In versions 0.5 and later of Pig, all hadoop fs shell commands are available.
The dash (-) used in the hadoop fs is also required:
A number of the commands come directly from Unix shells and will operate in ways that are familiar: chgrp, chmod, chown, cp, du, ls, mkdir, mv, rm, and stat.
A few of them either look like Unix commands you are used to but behave slightly differently or are unfamiliar, including: cat filename.
You can apply this command to a directory and it will apply itself in turn to each file in the directory.
In versions of Pig before 0.5, hadoop fs commands were not available.
Instead, Grunt had its own implementation of some of these commands: cat, cd, copyFromLocal, copy ToLocal, cp, ls, mkdir, mv, pwd, rm (which acted like Hadoop’s rmr, not Hadoop’s rm), and rmf.
As of Pig 0.8, all of these commands are still available.
In version 0.8, a new command was added to Grunt: sh.
This command gives you access to the local shell, just as fs gives you access to HDFS.
Simple shell commands that do not involve pipes or redirects can be executed.
It is better to work with absolute paths, as sh does not always properly track the current working directory.
Controlling Pig from Grunt Grunt also provides commands for controlling Pig and MapReduce: kill jobid.
The output of the pig command that spawned the job will list the ID of each job it spawns.
You can also find the job’s ID by looking at Hadoop’s JobTracker GUI, which lists all jobs currently running on the cluster.
If your Pig job contains other MapReduce jobs that do not depend on the killed MapReduce job, these jobs will still continue.
If you want to kill all of the MapReduce jobs associated with a particular Pig job, it is best to terminate the process running Pig, and then use this command to kill any MapReduce jobs that are still running.
Make sure to terminate the Pig process with a Ctrl-C or a Unix kill, not a Unix kill -9
The latter does not give Pig the chance to clean up temporary files it is using, which can leave garbage in your cluster.
This command is useful for testing your Pig Latin scripts while inside a Grunt session.
Thus all aliases referenced in script are available to Grunt, and the commands in script are accessible via the shell history.
This is another option for testing Pig Latin scripts while inside a Grunt session.
Before we take a look at the operators that Pig Latin provides, we first need to understand Pig’s data model.
This includes Pig’s data types, how it handles concepts such as missing data, and how you can describe your data to Pig.
Types Pig’s data types can be divided into two categories: scalar types, which contain a single value, and complex types, which contain other types.
Scalar Types Pig’s scalar types are simple types that appear in most programming languages.
With the exception of bytearray, they are all represented in Pig interfaces by java.lang classes, making them easy to work with in UDFs: int.
Constant integers are expressed as integer numbers, for example, 42
Constant longs are expressed as integer numbers with an L appended, for example, 5000000000L.
Note that because this is a floating-point number, in some calculations it will lose precision.
For calculations that require no loss of precision, you should use an int or long instead.
Note that because this is a floatingpoint number, in some calculations it will lose precision.
For calculations that require no loss of precision, you should use an int or long instead.
Constant chararrays are expressed as string literals with single quotes, for example, 'fred'
Unicode characters can be expressed as \u followed by their four-digit hexadecimal Unicode value.
For example, the value for Ctrl-A is expressed as \u0001
Bytearrays are represented in interfaces by a Java class DataByteArray that wraps a Java byte[]
Complex Types Pig has three complex data types: maps, tuples, and bags.
All of these types can contain data of any type, including other complex types.
So it is possible to have a map where the value field is a bag, which contains a tuple where one of the fields is a map.
A map in Pig is a chararray to data element mapping, where that element can be any Pig type, including a complex type.
The chararray is called a key and is used as an index to find the element, referred to as the value.
Because Pig does not know the type of the value, it will assume it is a bytearray.
If you do not cast the value, Pig will make a best guess based on how you use the value in your script.
If the value is of a type other than bytearray, Pig will figure that out at runtime and handle it.
By default there is no requirement that all values in a map must be of the same type.
It is legitimate to have a map with two keys name and age, where the value for name is a chararray and the value for age is an int.
Beginning in Pig 0.9, a map can declare its values to all be of the same type.
This is useful if you know all values in the map will be of the same type, as it allows you to avoid the casting, and Pig can avoid the runtime type-massaging referenced in the previous paragraph.
Map constants are formed using brackets to delimit the map, a hash between keys and values, and a comma between key-value pairs.
The first value is a chararray, and the second is an integer.
A tuple is a fixed-length, ordered collection of Pig data elements.
Tuples are divided into fields, with each field containing one data element.
These elements can be of any type—they do not all need to be the same type.
A tuple is analogous to a row in SQL, with the fields being SQL columns.
A tuple can, but is not required to, have a schema associated with it that describes each field’s type and provides a name for each field.
This allows Pig to check that the data in the tuple is what the user expects, and it allows the user to reference the fields of the tuple by name.
Tuple constants use parentheses to indicate the tuple and commas to delimit fields in the tuple.
For example, ('bob', 55) describes a tuple constant with two fields.
Bag is the one type in Pig that is not required to fit into memory.
As you will see later, because bags are used to store collections when grouping, bags can become quite large.
Pig has the ability to spill bags to disk when necessary, keeping only partial sections of the bag in memory.
The size of the bag is limited to the amount of local disk available for spilling the bag.
Memory Requirements of Pig Data Types In the previous sections I often referenced the size of the value stored for each type (four bytes for integer, eight bytes for long, etc.)
This tells you how large (or small) a value those types can hold.
However, this does not tell you how much memory is actually used by objects of those types.
Because Pig uses Java objects to represent these values internally, there is an additional overhead.
This overhead depends on your JVM, but it is usually eight bytes per object.
It is even worse for chararrays because Java’s String uses two bytes per character rather than one.
So, if you are trying to figure out how much memory you need in Pig to hold all of your data (e.g., if you are going to do a join that needs to hold a hash table in memory), do not count the bytes on disk and assume that is how much memory you need.
The multiplication factor between disk and memory is dependent on your data, whether your data is compressed on disk, your disk storage format, etc.
As a rule of thumb, it takes about four times as much memory as it does disk to represent the uncompressed data.
Nulls Pig includes the concept of a data element being null.
It is important to understand that in Pig the concept of null is the same as in SQL, which is completely different from the concept of null in C, Java, Python, etc.
In Pig a null data element means the value is unknown.
This might be because the data is missing, an error occurred in processing it, etc.
In most procedural languages, a data value is said to be null when it is unset or does not point to a valid address or object.
This difference in the concept of null is important and affects the way Pig treats null data, especially when operating on it.
Unlike SQL, Pig does not have a notion of constraints on the data.
In the context of nulls, this means that any data element can always be null.
As you write Pig Latin scripts and UDFs, you will need to keep this in mind.
Schemas Pig has a very lax attitude when it comes to schemas.
This is a consequence of Pig’s philosophy of eating anything.
If a schema for the data is available, Pig will make use of it, both for up-front error checking and for optimization.
But if no schema is available, Pig will still process the data, making the best guesses it can based on how the script treats the data.
First, we will look at ways that you can communicate the schema to Pig; then, we will examine how Pig handles the case where you do not provide it with the schema.
The easiest way to communicate the schema of your data to Pig is to explicitly tell Pig what it is when you load the data:
If it has more, it will truncate the extra ones.
If it has less, it will pad the end of the record with nulls.
It is also possible to specify the schema without giving explicit data types.
In this case, the data type is assumed to be bytearray:
You would expect that this also would force your data into a tuple with four fields, regardless of the number of actual input fields, just like when you specify both names and types for the fields.
But in 0.8 and earlier versions it does not; no truncation or null padding is done in the case where you do not provide explicit types for the fields.
Also, when you declare a schema, you do not have to declare the schema of complex types, but you can if you want to.
For example, if your data has a tuple in it, you can declare that field to be a tuple without specifying the fields it contains.
You can also declare that field to be a tuple that has three columns, all of which are integers.
Table 4-1 gives the details of how to specify each data type inside a schema declaration.
This declares all values in the map to be of this type.
It makes it easy for users to operate on data without having to first load it into a metadata system.
It also means that if you are interested in only the first few fields, you only have to declare those fields.
But for production systems that run over the same data every hour or every day, it has a couple of significant drawbacks.
One, whenever your data changes, you have to change your Pig Latin.
To address these issues, there is another way to load schemas in Pig.
If the load function you are using already knows the schema of the data, the function can communicate that to Pig.
In this case, you do not have to declare the schema as part of the load statement.
And you can still refer to fields by name because Pig will fetch the schema from the load function before doing error checking on your script:
If they are not identical, Pig will determine whether it can adapt the one returned by the loader to match the one you gave.
For example, if you specified a field as a long and the loader said it was an int, Pig can and will do that cast.
However, if it cannot determine a way to make the loader’s schema fit the one you gave, it will give an error.
Now let’s look at the case where neither you nor the load function tell Pig what the data’s schema is.
In addition to being referenced by name, fields can be referenced by position, starting from zero.
The syntax is a dollar sign, then the position: $0 refers to the first field.
So it is easy to tell Pig which field you want to work with.
But how does Pig know the data type? It does not, so it starts by assuming everything is a bytearray.
Then it looks at how you use those fields in your script, drawing conclusions about what you think those fields are and how you want to use them.
But should it treat them as integers or floating-point numbers? Here Pig plays it safe and guesses that they are floating points, casting them to doubles.
This is the safer bet because if they actually are integers, those can be represented as floating-point numbers, but the reverse is not true.
However, because floatingpoint arithmetic is much slower and subject to loss of precision, if these values really are integers, you should cast them so that Pig uses integer types in this case.
There are also cases where Pig cannot make any intelligent guess:
In this case, it treats these fields as if they were bytearrays, which means it will do a byte-to-byte comparison of the data in these fields.
Pig also has to handle the case where it guesses wrong and must adapt on the fly.
By the rules laid out previously, Pig will assume they are doubles.
But let’s say they actually turn out to be represented internally as integers.* In that case, Pig will need to adapt at runtime and convert what it thought was a cast from bytearray to double into a cast from int to double.
Note that it will still produce a double output and not an int output.
It should be noted that in Pig 0.8 and earlier, much of this runtime adaption code was shaky and often failed.
But if you are using an older version of Pig, you might need to cast the data explicitly to get the right results.
Finally, Pig’s knowledge of the schema can change at different points in the Pig Latin script.
In all of the previous examples where we loaded data without a schema and then passed it to a foreach statement, the data started out without a schema.
Similarly, Pig can start out knowing the schema, but if the data is mingled with other data without a schema, the schema can be lost.
In this example, because Pig does not know the schema of daily, it cannot know the schema of the join of divs and daily.
Casts The previous sections have referenced casts in Pig without bothering to define how casts work.
The syntax for casts in Pig is the same as in Java—the type name in parentheses before the value:
The syntax for specifying types in casts is exactly the same as specifying them in schemas, as shown previously in Table 4-1
Table 4-2 describes which casts are allowed between scalar types.
Casts to bytearrays are never allowed because Pig does not know how to represent the various data types in binary format.
Casts to and from complex types currently are not allowed, except from bytearray, although conceptually in some cases they could be.
For that to be the case, you would need to use a loader that did load the bat map with these values as integers.
Values with precision beyond what float can represent will be truncated.
One type of casting that requires special treatment is casting from bytearray to other types.
Because bytearray indicates a string of bytes, Pig does not know how to convert its contents to any other type.
The casts in the script indicate that you want them treated as ints.
Pig does not know whether integer values in baseball are stored as ASCII strings, Java serialized values, binary-coded decimal, or some other format.
So it asks the load function, because it is that function’s responsibility to cast bytearrays to other types.
In general this works nicely, but it does lead to a few corner cases where Pig does not know how to cast a bytearray.
In particular, if a UDF returns a bytearray, Pig will not know how to perform casts on it because that bytearray is not generated by a load function.
Before leaving the topic of casts, we need to consider cases where Pig inserts casts for the user.
These casts are implicit, compared to explicit casts where the user indicates the cast.
In this case, Pig will change the second line to (float)volume * close to do the operation without losing precision.
In general, Pig will always widen types to fit when it needs to insert these implicit casts.
So, int and long together will result in a long; int or long and float will result in a float; and int, long, or float and double will result in a double.
There are no implicit casts between numeric types and chararrays or other types.
How Strongly Typed Is Pig? In a strongly typed computer language (e.g., Java), the user must declare up front the type for all variables.
In weakly typed languages (e.g., Perl), variables can take on values of different type and adapt as the occasion demands.
So which is Pig? For the most part it is strongly typed.
If you describe the schema of your data, Pig expects your data to be what you said.
But when Pig does not know the schema, it will adapt to the actual types at runtime.
In this example, remember we are pretending that the values for base_on_balls and ibbs turn out to be represented as integers internally (that is, the load function constructed them as integers)
If Pig were weakly typed, the output of unintended would be records with one field typed as an integer.
As it is, Pig will output records with one field typed as a double.
Pig will make a guess and then do its best to massage the data into the types it guessed.
The downside here is that users coming from weakly typed languages are surprised, and perhaps frustrated, when their data comes out as a type they did not anticipate.
However, on the upside, by looking at a Pig Latin script it is possible to know what the output data type will be in these cases without knowing the input data.
This chapter provides you with the basics of Pig Latin, enough to write your first useful scripts.
More advanced features of Pig Latin are covered in Chapter 6
Each processing step results in a new data set, or relation.
In input = load 'data', input is the name of the relation that results from loading the data set data.
It is possible to reuse relation names; for example, this is legitimate:
It looks here as if you are reassigning A, but really you are creating new relations called A, losing track of the old relations called A.
Pig is smart enough to keep up, but it still is not a good practice.
It leads to confusion when trying to read your programs (which A am I referring to?) and when reading error messages.
In addition to relation names, Pig Latin also has field names.
In the previous snippet of Pig Latin, dividends and symbol are examples of field names.
These are somewhat like variables in that they will contain a different value for each record as it passes through the pipeline, but you cannot assign values to them.
Both relation and field names must start with an alphabetic character, and then they can have zero or more alphabetic, numeric, or _ (underscore) characters.
Keywords in Pig Latin are not case-sensitive; for example, LOAD is equivalent to load.
Input and Output Before you can do anything of interest, you need to be able to add inputs and outputs to your data flows.
Load The first step to any data flow is to specify your input.
In Pig Latin this is done with the load statement.
By default, load looks for your data on HDFS in a tab-delimited file using the default load function PigStorage.
By default, your Pig jobs will run in your home directory on HDFS, /users/yourlogin.
Unless you change directories, all relative paths will be evaluated from there.
In practice, most of your data will not be in tab-separated text files.
You also might be loading data from storage systems other than HDFS.
Pig allows you to specify the function for loading your data with the using clause.
For example, if you wanted to load your data from HBase, you would use the loader for HBase:
If you do not specify a load function, the built-in function PigStorage will be used.
You can also pass arguments to your load function via the using clause.
For example, if you are reading comma-separated text data, PigStorage takes an argument to indicate which character to use as a separator:
The load statement also can have an as clause, which allows you to specify the schema of the data you are loading.
When specifying a “file” to read from HDFS, you can specify directories.
In this case, Pig will find all files under the directory you specify and use them as input for that load statement.
So, if you had a directory input with two datafiles today and yesterday under it, and you specified input as your file to load, Pig will read both today and yesterday as input.
If the directory you specify has other directories, files in those directories will be included as well.
PigStorage and TextLoader, the two built-in Pig load functions that operate on HDFS files, support globs.* With globs, you can read multiple files that are not under the same directory or read some but not all files under a directory.
Be aware that glob meaning is determined by HDFS underneath Pig, so the globs that will work for you depend on your version of HDFS.
Also, if you are issuing Pig Latin commands from a Unix shell command line, you will need to escape many of the glob characters to prevent your shell from expanding them.
The first character must be lexicographically less than or equal to the second character.
The ^ character must occur immediately to the right of the opening bracket.
The ^ character must occur immediately to the right of the opening bracket.
Any loader that uses FileInputFormat as its InputFormat will support globs.
Most loaders that load data from HDFS use this InputFormat.
Store After you have finished processing your data, you will want to write it out somewhere.
In many ways it is the mirror image of the load statement.
By default, Pig stores your data on HDFS in a tab-delimited file using PigStorage:†
Pig will write the results of your processing into a directory processed in the directory /data/examples.
If you do not specify a store function, PigStorage will be used.
You can specify a different store function with a using clause:
For example, if you want to store your data as comma-separated text data, PigStorage takes an argument to indicate which character to use as a separator:
But how many part files will be created? That depends on the parallelism of the last job before the store.
If it has reduces, it will be determined by the parallel level set for that job.
If it is a map-only job, it will be determined by the number of maps, which is controlled by Hadoop and not Pig.
Dump In most cases you will want to store your data somewhere when you are done processing it.
But occasionally you will want to see it on the screen.
It can also be useful for quick ad hoc jobs.
A single function can be both a load and store function, as PigStorage is.
Relational Operations Relational operators are the main tools Pig Latin provides to operate on your data.
They allow you to transform it by sorting, grouping, joining, projecting, and filtering.
What is covered here will be enough to get you started programming in Pig Latin.
From these expressions it generates new records to send down the pipeline to the next operator.
For those familiar with database terminology, it is Pig’s projection operator.
For example, the following code loads an entire record, but then removes all but the user and id fields from each record:
Field references can be by name (as shown in the preceding example) or by position.
Positional style references are useful in situations where the schema is unknown or undeclared.
In addition to using names and positions, you can refer to all fields using * (asterisk), which produces a tuple that contains all the fields.
Beginning in version 0.9, you can also refer to ranges of fields using ..
This is particularly useful when you have many fields and do not want to repeat them all in your foreach command:
The unary negative operator (-) is also supported for both integers and floating-point numbers.
Pig also provides a binary condition operator, often referred to as bincond.
It begins with a Boolean test, followed by a ?, then the value to return if the test is true, then a :, and finally the value to return if the test is false.
Both value arguments of the bincond must return the same type:
To extract data from complex types, use the projection operators.
For maps this is # (the pound or hash), followed by the name of the key as a string.
Keep in mind that the value associated with a key may be of any type.
If you reference a key that does not exist in the map, the result is a null:
As with top-level records, the field can be referenced by name (if you have a schema for the tuple) or by position.
Referencing a nonexistent positional field in the tuple will return null.
Referencing a field name that does not exist in the tuple will produce an error:
Bag projection is not as straightforward as map and tuple projection.
Bags do not guarantee that their tuples are stored in any order, so allowing a projection of the tuple inside the bag would not be meaningful.
Instead, when you project fields in a bag, you are creating a new bag with only those fields:
This will produce a new bag whose tuples have only the field x in them.
You can project multiple fields in a bag by surrounding the fields with parentheses and separating them by commas:
This seemingly pedantic distinction that b.x is a bag and not a scalar value has consequences.
It is clear what the programmer is trying to do here.
But because A.y and B.y are bags and the addition operator is not defined on bags, this will produce an error.‡ The correct way to do this calculation in Pig Latin is:
Because they are part of a foreach statement, these UDFs take one record at a time and produce one output.
Keep in mind that either the input or the output can be a bag, so this one record can contain a bag of records:
In addition, eval funcs can take * as an argument, which passes the entire record to the function.
They can also be invoked with no arguments at all.
For a complete list of UDFs that are provided with Pig, see Appendix A.
The result of each foreach statement is a new tuple, usually with a different schema than the tuple that was an input to foreach.
Pig can infer the data types of the fields in this schema from the foreach statement.
But it cannot always infer the names of those fields.
For fields that are simple projections with no other operators applied, Pig keeps the same name as before:
In designing the language, we thought it better to be consistent and always say that bags could not be added rather than allow it in some instances and not others.
Once any expression beyond simple projection is applied, Pig does not assign a name to the field.
If you do not explicitly assign a name, the field will be nameless and will be addressable only via a positional parameter, for example, $0
Notice that in foreach the as is attached to each expression.
This is different than load, where it is attached to the entire statement.
Filter The filter statement allows you to select which records will be retained in your data pipeline.
If that predicate evaluates to true for a given record, that record will be passed down the pipeline.
These comparators can be used on any scalar data type.
To use these with two tuples, both tuples must have either the same schema or no schema.
None of the equality operators can be applied to bags.
Pig Latin follows the operator precedence that is standard in most programming languages, where arithmetic operators have precedence over equality operators.
For chararrays, users can test to see whether the chararray matches a regular expression:
This format requires the entire chararray to match, not just a portion as in Perl-style regular expressions.
You can find chararrays that do not match a regular expression by preceding the test with not:
You can combine multiple predicates into one by using the Boolean operators and and or, and you can reverse the outcome of any predicate by using the Boolean not operator.
As is standard, the precedence of Boolean operators, from highest to lowest, is not, and, or.
Thus a and b or not c is equivalent to (a and b) or (not c)
If the first (left) predicate of an and is false, the second (right) will not be evaluated.
Similarly, if the first predicate of an or is true, the second predicate will not be evaluted.
Thus x == null results in a value of null, not true (even when x is null also) or false.
The way to look for null values is to use the is null operator, which returns true whenever the value is null.
To find values that are not null, use is not null.
Likewise, null neither matches nor fails to match any regular expression value.
Just as there are UDFs to be used in evaluation expressions, there are UDFs specifically for filtering records, called filter funcs.
These are eval funcs that return a Boolean value and can be invoked in the filter statement.
Group The group statement collects together records with the same key.
It is the first operator we have looked at that shares its syntax with SQL, but it is important to understand that the grouping operator in Pig Latin is fundamentally different than the one in SQL.
In SQL the group by clause creates a group that must feed directly into one or more aggregate functions.
In Pig Latin there is no direct connection between group and aggregate functions.
Instead, group does exactly what it says: collects all records with the same value for the provided key together into a bag.
You can then pass this to an aggregate function if you want or do other things with it:
That example groups records by the key stock and then counts them.
It is just as legitimate to group them and then store them for processing at a later time:
You can also group on multiple keys, but the keys must be surrounded by parentheses.
In this case, the group field is a tuple with a field for each key:
The record coming out of group all has the chararray literal all as a key.
Usually this does not matter because you will pass the bag directly to an aggregate function such as COUNT.
But if you plan to store the record or use it for another purpose, you might want to project out the artificial key first.
Grouping means collecting all records where the key has the same value.
If the pipeline is in a map phase, this will force it to shuffle and then reduce.
If the pipeline is already in a reduce, this will force it to pass through map, shuffle, and reduce phases.
This is unfortunate and confusing, but also hard to change now.
Because grouping collects all records together with the same value for the key, you often get skewed results.
That is, just because you have specified that your job have 100 reducers, there is no reason to expect that the number of values per key will be distributed evenly.
They might have a Gaussian or power law distribution.‖ For example, suppose you have an index of web pages and you group by the base URL.
Certain values such as yahoo.com are going to have far more entries than most, which means that some reducers get far more data than others.
Because your MapReduce job is not finished (and any subsequent ones cannot start) until all your reducers have finished, this skew will significantly slow your processing.
In some cases it will also be impossible for one reducer to manage that much data.
Pig has a number of ways that it tries to manage this skew to balance out the load across your reducers.
This does not remove all skew, but it places a bound on it.
And because for most jobs the number of mappers will be at most in the tens of thousands, even if the reducers get a skewed number of records, the absolute number of records per reducer will be small enough that the reducers can handle them quickly.
Unfortunately, not all calculations can be done using the combiner.
Calculations that can be decomposed into any number of steps, such as sum, are called distributive.
Calculations that can be decomposed into an initial step, any number of intermediate steps, and a final step are referred to as algebraic.
Count is an example of such a function, where the initial step is a count and the intermediate and final steps are sums.
Distributive is a special case of algebraic, where the initial, intermediate, and final steps are all the same.
Session analysis, where you want to track a user’s actions on a website, is an example of a calculation that is not algebraic.
You must have all the records sorted by timestamp before you can start analyzing their interaction with the site.
Pig’s operators and built-in UDFs use the combiner whenever possible, because of its skew-reducing features and because early aggregation greatly reduces the amount of data shipped over the network and written to disk, thus speeding performance significantly.
UDFs can indicate when they can work with the combiner by implementing the Algebraic interface.
Also, keep in mind that when using group all, you are necessarily serializing your pipeline.
That is, this step and any step after it until you split out the single bag now containing all of your records will not be done in parallel.
In my experience, the vast majority of data tracking human activity follows a power law distribution.
Finally, group handles nulls in the same way that SQL handles them: by collecting all records with a null key into the same group.
Order by The order statement sorts your data for you, producing a total order of your output data.
Total order means that not only is the data sorted in each partition of your data, it is also guaranteed that all records in partition n are less than all records in partition n - 1 for all n.
When your data is stored on HDFS, where each partition is a part file, this means that cat will output your data in order.
You indicate a key or set of keys by which you wish to order your data.
One glaring difference is that there are no parentheses around the keys when multiple keys are indicated in order:
It is also possible to reverse the order of the sort by appending desc to a key in the sort.
In order statements with multiple keys, desc applies only to the key it immediately follows.
Data is sorted based on the types of the indicated fields: numeric values are sorted numerically, chararray fields are sorted lexically, and bytearray fields are sorted lexically, using byte values rather than character values.
For all data types, nulls are taken to be smaller than all possible values for that type, and thus will always appear first (or last when desc is used)
This affects order just as it does group, causing some reducers to take significantly longer than others.
It does this by first sampling the input of the order statement to get an estimate of the key distribution.
For example, suppose you are ordering on a chararray field with the values a, b, e, e, e, e, e, e, m, q, r, z, and you have three reducers.
In practice, we rarely see variance in reducer time exceed 10% when using this algorithm.
An important side effect of the way Pig distributes records to minimize skew is that it breaks the MapReduce convention that all instances of a given key are sent to the same partition.
If you have other processing that depends on this convention, do not use Pig’s order statement to sort data for it.
Also, Pig adds an additional MapReduce job to your pipeline to do the sampling.
Because this sampling is very lightweight (it reads only the first record of every block), it generally takes less than 5% of the total job time.
It works only on entire records, not on individual fields:
Because it needs to collect like records together in order to determine whether they are duplicates, distinct forces a reduce phase.
It does make use of the combiner to remove any duplicate records it can delete in the map phase.
The use of distinct shown here is equivalent to select distinct x in SQL.
Join join is one of the workhorses of data processing, and it is likely to be in many of your Pig Latin scripts.
When those keys are equal,# the two rows are joined.
Like foreach, join preserves the names of the fields of the inputs passed to it.
It also prepends the name of the relation the field came from, followed by a ::
Adding describe jnd; to the end of the previous example produces:
The daily:: prefix needs to be used only when the field name is no longer unique in the record.
In this example, you will need to use daily::date or divs::date if you wish to refer to one of the date fields after the join.
But fields such as open and divs do not need a prefix because there is no ambiguity.
In outer joins, records that do not have a match on the other side are included, with null values being filled in for the missing fields.
A left outer join means records from the left side will be included even when they do not have a match on the right side.
Likewise, a right outer joins means records from the right side will be included even when they do not have a match on the left side.
A full outer join means records from both sides are taken even when they do not have matches:
Actually, joins can be on any condition, not just equality, but Pig only supports joins on equality (called equijoins)
Unlike some SQL implementations, full is not a noise word.
Outer joins are supported only when Pig knows the schema of the data on the side(s) for which it might need to fill in nulls.
Thus for left outer joins, it must know the schema of the right side; for right outer joins, it must know the schema of the left side; and for full outer joins, it must know both.
This is because, without the schema, Pig will not know how many null values to fill in.* As in SQL, null values for keys do not match anything, even null values from the other input.
So, for inner joins, all records with null key values are dropped.
For outer joins, they will be retained but will not match any records from the other input.
Pig can also do multiple joins in a single operation, as long as they are all being joined on the same key(s)
It seems like this ought to work, since Pig could split the divs1 data set and send it to join twice.
But the problem is that field names would be ambiguous after the join, so the load statement must be written twice.
The next best thing would be for Pig to figure.
You may object that Pig could determine this by looking at other records in the join and inferring the correct number of fields.
First, when no schema is present, Pig does not enforce a semantic that every record has the same schema.
So, assuming Pig can infer one record from another is not valid.
Second, there might be no records in the join that match, and thus Pig might have no record to infer from.
Pig does these joins in MapReduce by using the map phase to annotate each record with which input it came from.
It then uses the join key as the shuffle key.
Once all of the records with the same value for the key are collected together, Pig does a cross product between the records from both inputs.
To minimize memory usage, it has MapReduce order the records coming into the reducer using the input annotation it added in the map phase.
Thus all of the records for the left input arrive first.
All of the records for the right input arrive second.
As each of these records arrives, it is crossed with each record from the left side to produce an output record.
In a multiway join, the left n - 1 inputs are held in memory, and the nth is streamed through.
It is important to keep this in mind when writing joins in your Pig queries if you know that one of your inputs has more records per value of the chosen key.
Placing that input on the right side of your join will lower memory usage and possibly increase your script’s performance.
Limit Sometimes you want to see only a limited number of results.
Note that for all operators except order, Pig does not guarantee the order in which records are produced.
Putting an order immediately before the limit will guarantee that the same results are returned every time.
It does optimize this phase by limiting the output of each map and then applying the limit again in the reducer.
In the case where limit is combined with order, the two are done together on the map and reduce.
That is, on the map side, the records are sorted by MapReduce and the limit applied in the combiner.
They are sorted again by MapReduce as part of the shuffle, and Pig applies the limit again in the reducer.
One possible optimization that Pig does not do is terminate reading of the input early once it has reached the number of records specified by limit.
So, in the example, if you hoped to use this to read just a tiny slice of your input, you will be disappointed.
Sample sample offers a simple way to get a sample of your data.
It reads through all of your data but returns only a percentage of rows.
Obviously this is nondeterministic, so results of a script with sample will vary with every run.
Also, the percentage will not be an exact match, but close.
There has been discussion about adding more sophisticated sampling techniques, but it has not been done yet.
Parallel One of Pig’s core claims is that it provides a language for parallel data processing.
The parallel clause can be attached to any relational operator in Pig Latin.
However, it controls only reduce-side parallelism, so it makes sense only for operators that force a reduce phase.
Operators marked with an asterisk have multiple implementations, some of which force a reduce and some which do not.
For details on this and on operators not covered in this chapter, see Chapter 6
In this example, parallel will cause the MapReduce job spawned by Pig to have 10 reducers.
So if this group were followed by an order, parallel would need to be set for that order separately.
Most likely the group will reduce your data size significantly and you will want to change the parallelism:
If, however, you do not want to set parallel separately for every reduce-invoking operator in your script, you can set a script-wide value using the set command:
In this script, all MapReduce jobs will be done with 10 reduces.
When you set a default parallel level, you can still add a parallel clause to any statement to override the default value.
Thus it can be helpful to set a default value as a base to use in most cases, and specifically add a parallel clause only when you have an operator that needs a different value.
Using parameter substitution, you can write your parallel clauses with variables, providing values for those variables at runtime.
So far we have assumed that you know what your parallel value should be.
The MapReduce default parallelism is controlled by your cluster configuration.
The installation default value is one, and most people do not change that.
This most likely means that you will be running with only one reducer.
To avoid this situation, Pig added a heuristic in 0.8 to do a gross estimate of what the parallelism should be set to if it is not set.
It looks at the initial input size, assumes there will be no data size changes, and then allocates a reducer for every 1G of data.
It must be emphasized that this is not a good algorithm.
It is provided only to prevent mistakes that result in scripts running very slowly, and, in some extreme cases, mistakes that cause MapReduce itself to have problems.
What about map parallelism? MapReduce only allows users to set reduce parallelism: it controls map parallelism itself.
Because Pig cannot control map parallelism, it cannot expose that to its users either.
In MapReduce, data is read using a class called InputFormat.
Part of InputFormat’s purpose is to tell MapReduce how many map tasks to run.
Although Pig cannot give you direct control over how many map tasks to run, it does let you build and run your own InputFormat as part of building your own load function.
See Chapter 11 for details on how to do this.
User Defined Functions Much of the power of Pig lies in its ability to let users combine irs operators with their own or others’ code via UDFs.
As of version 0.8, UDFs can also be written in Python.
Prior to version 0.8, this was a very limited set, including only the standard SQL aggregate functions and a few others.
In 0.8, a large number of standard string-processing, math, and complex-type UDFs were added.
Piggybank is a collection of user-contributed UDFs that is packaged and released along with Pig.
Piggybank UDFs are not included in the Pig JAR, and thus you have to register them manually in your script.
Of course you can also write your own UDFs or use those written by other users.
For details of how to write your own, see Chapter 10
Finally, you can use some static Java functions as UDFs as well.
Registering UDFs When you use a UDF that is not already built into Pig, you have to tell Pig where to look for that UDF.
Pig opens all of the registered JARs, takes out the files, and places them in the JAR that it sends to Hadoop to run your jobs.
In this example, we have to give Pig the full package and class name of the UDF.
The second option is to include a set of paths on the command line for Pig to search when looking for UDFs.
Using yet another property, we can get rid of the register command as well.
In many cases it is better to deal with registration and definition issues explicitly in the script via the register and define commands than use these properties.
Otherwise, everyone who runs your script has to know how to configure the command line.
However, in some situations your scripts will always use the same set of JARs and always look in the same places for them.
For instance, you might have a set of JARs used by everyone in your company.
In this case, placing these properties in a shared properties file and using that with your Pig scripts will make sharing those UDFs easier and assure that everyone is using the correct versions of them.
In 0.8 and later versions, the register command can also take HDFS paths.
In this case you do not register a JAR, but rather a Python script that contains your UDF.
The important differences here are the using jython and as bballudfs portions of the register statement.
Pig does not know where on your system the Jython interpreter is, so you must include jython.jar in your classpath when invoking Pig.
This can be done by setting the PIG_CLASSPATH environment variable.
Each Python file you load should be given a separate namespace.
This avoids naming collisions when you register two Python scripts with duplicate function names.
One caveat: Pig does not trace dependencies inside your Python scripts and send the needed Python modules to your Hadoop cluster.
You are required to make sure the modules you need reside on the task nodes in your cluster and that the PYTHONPATH environment variable is set on those nodes such that your UDFs will be able to find them for import.
This issue has been fixed after 0.9, but as of this writing is not yet released.
It can also be used to provide constructor arguments to your UDFs.
Eval and filter functions can also take one or more strings as constructor arguments.
If you are using a UDF that takes constructor arguments, define is the place to provide those arguments.
For example, consider a method CurrencyConverter that takes two constructor arguments, the first indicating which currency you are converting from and the second which currency you are converting to:
Calling Static Java Functions Java has a rich collection of utilities and libraries.
Because Pig is implemented in Java, some of these functions can be exposed to Pig users.
Starting in version 0.8, Pig offers invoker methods that allow you to treat certain static Java functions as if they were Pig UDFs.
Any public static Java function that takes no arguments or some combination of int, long, float, double, String, or arrays thereof,‡ and returns int, long, float, double, or String can be invoked in this way.
Because Pig Latin does not support overloading on return types, there is an invoker for each return type: InvokeForInt, InvokeForLong, InvokeForFloat, InvokeForDouble, and InvokeForString.
You must pick the appropriate invoker for the type you wish to return.
The first is the full package, classname, and method name.
The second is a space-separated list of parameters the Java function expects.
If the parameter is an array, [] (square brackets) are appended to the type name.
If the method takes no parameters, the second constructor argument is omitted.
For example, if you wanted to use Java’s Integer class to translate decimal values to hexadecimal values, you could do:
If your method takes an array of types, Pig will expect to pass it a bag where each tuple has a single field of that type.
For int, long, float, and double, invoker methods can call Java functions that take the scalar types but not the associated Java classes (so int but not Integer, etc.)
Invokers do not use the Accumulator or Algebraic interfaces, and are thus likely to be much slower and to use much more memory than UDFs written specifically for Pig.
This means that before you pass an array argument to an invoked method, you should think carefully about whether those inefficiencies are acceptable.
Invoking Java functions in this way does have a small cost because reflection is used to find and invoke the methods.
You should place a filter before the invocation to prevent this.
In the previous chapter we worked through the basics of Pig Latin.
In this chapter we will plumb its depths, and we will also discuss how Pig handles more complex data flows.
Finally, we will look at how to use macros and modules to modularize your scripts.
Advanced Relational Operations We will now discuss the more advanced Pig Latin operators, as well as additional options for operators that were introduced in the previous chapter.
Now we will look at ways it can explode the number of records in your pipeline, and also how it can be used to apply a set of operations to each record.
Sometimes you have data in a bag or a tuple and you want to remove that level of nesting.
Because a player can play more than one position, position is stored in a bag.
This allows us to still have one entry per player in the baseball file.* But when you want to switch around your data on the fly and group.
Those with database experience will notice that this is a violation of the first normal form as defined by E.
This intentional denormalization of data is very common in OLAP systems in general, and in large data-processing systems such as Hadoop in particular.
In systems such as Hadoop, where storage is cheap and joins are expensive, it is generally better to use nested data structures to avoid the joins.
To do this, Pig provides the flatten modifier in foreach:
A foreach with a flatten produces a cross product of every record in the bag with all of the other expressions in the generate statement.
Looking at the first record in baseball, we see it is the following (replacing tabs with commas for clarity):
If there is more than one bag and both are flattened, this cross product will be done with members of each bag as well as other expressions in the generate statement.
So rather than getting n rows (where n is the number of records in one bag), you will get n * m rows.
One side effect that surprises many users is that if the bag is empty, no records are produced.
So if there had been an entry in baseball with no position, either because the bag is null or empty, that record would not be contained in the output of flatten.pig.
The record with the empty bag would be swallowed by foreach.
One, since Pig may or may not have the schema of the data in the bag, it might have no idea how to fill in nulls for the missing fields.
Two, from a mathematical perspective, this is what you would expect.
Crossing a set S with the empty set results in the empty set.
If you wish to avoid this, use a bincond to replace empty bags with a constant bag:
In this case, it does not produce a cross product; instead, it elevates each field in the tuple to a top-level field.
If the fields in a bag or tuple that is being flattened have names, Pig will carry those names along.
As with join, to avoid ambiguity, the field name will have the bag’s name and :: prepended to it.
As long as the field name is not ambiguous, you are not required to use the bagname:: prefix.
If you wish to change the names of the fields, or if the fields initially did not have names, you can attach an as clause to your flatten, as in the preceding example.
If there is more than one field in the bag or tuple that you are assigning names to, you must surround the set of field names with parentheses.
Finally, if you flatten a bag or tuple without a schema and do not provide an as clause, the resulting records coming out of your foreach will have a null schema.
This is because Pig will not know how many fields the flatten will result in.†
So far, all of the examples of foreach that we have seen immediately generate one or more lines of output.
It can also apply a set of relational operations to each record in your pipeline.
This is referred to as a nested foreach, or inner foreach.
One example of how this can be used is to find the number of unique entries in a group.
For example, to find the number of unique stock symbols for each exchange in the NYSE_daily data:
In versions 0.8 and earlier, there is a bug where this flatten is assigned a schema of one field, which is a bytearray, instead of causing the schema to be null.
One way to think about this is that the assignment operator inside foreach can be used to take an expression and create a relation, as happens in this example.
The last line in a nested foreach must always be generate.
This tells Pig how to take the results of the nested operations and produce a record to be put in the outer relation (in this case, uniqcnt)
So, generate is the operator that takes the inner relations and turns them back into expressions for inclusion in the outer relation.
Theoretically, any Pig Latin relational operator should be legal inside foreach.
However, at the moment, only distinct, filter, limit, and order are supported.
Let’s look at a few more examples of how this feature can be useful, such as to sort the contents of a bag before the bag is passed to a UDF.
This is convenient for UDFs that require all of their input to come in a certain order.
Consider a stock-analysis UDF that wants to track information about a particular stock over time.
Doing the sorting in Pig Latin, rather than in your UDF, is important for a couple of reasons.
One, it means Pig can offload the sorting to MapReduce.
MapReduce has the ability to sort data by a secondary key while grouping it.
So, the order statement in this case does not require a separate sorting operation.
Two, it means that your UDF does not need to wait for all data to be available before it starts processing.
This feature can be used to find the top k elements in a group.
The following example will find the top three dividends payed for each stock:
Currently, these nested portions of code are always run serially for each record handed to them.
Of course the foreach itself will be running in multiple map or reduce tasks, but each instance of the foreach will not spawn subtasks to do the nested operations in parallel.
But each group of stocks would be sorted and the top three records taken serially within one of those 10 reducers.
There is, of course, no requirement that the pipeline inside the foreach be a simple linear pipeline.
For example, if you wanted to calculate two distinct counts together, you could do the following:
For simplicity, Pig actually runs this pipeline once for each expression in generate.
Here this has no side effects because the two data flows are completely disjointed.
However, if you constructed a pipeline where there was a split in the flow, and you put a UDF in the shared portion, you would find that it was invoked more often than you expected.
However, Pig offers multiple join implementations, which we will discuss here.
In RDBMS systems, traditionally the SQL optimizer chooses a join implementation for the user.
This is nice as long as the optimizer chooses well, which it does in most cases.
In the Pig team we like to say that our optimizer is located between the user’s chair and keyboard.
We empower the user to make these choices rather than having Pig make them.
So for operators such as join where there are multiple implementations, Pig lets the user indicate his choice via a using clause.
Also, as a relatively new product, Pig has a lot of functionality to add.
It makes more sense to focus on adding implementation choices and letting the user choose which ones to use, rather than focusing on building an optimizer capable of choosing well.
A common type of join is doing a lookup in a smaller input.
For example, suppose you were processing data where you needed to translate a US ZIP code (postal code) to the state and city it referred to.
As there are at most 100,000 zip codes in the US, this translation table should easily fit in memory.
Rather than forcing a reduce phase that will sort your big file plus this tiny zip code translation file, it makes sense instead to send the zip code file to every machine, load it into memory, and then do the join by streaming through the large file and looking up each record in the zip code file.
This is called a fragment-replicate join (because you fragment one file and replicate the other):
The using 'replicated' tells Pig to use the fragment-replicate algorithm to execute this join.
Because no reduce phase is necessary, all of this can be done in the map task.
The second input listed in the join (in this case, divs) is always the input that is loaded into memory.
Pig does not check beforehand that the specified input will fit into memory.
If Pig cannot fit the replicated input into memory, it will issue an error and fail.
Due to the way Java stores objects in memory, the size of the data on disk will not be the size of the data in memory.
You will need more memory for a replicated join than you need space on disk to store the replicated input.
It cannot do a right outer join, because when a given map task sees a record in the replicated input that does not match any record in the fragmented input, it has no idea whether it would match a record in a different fragment.
So, it does not know whether to emit a record.
If you want a right or full outer join, you will need to use the default join operation.
Fragment-replicate join can be used with more than two tables.
In this case, all but the first (left-most) table are read into memory.
Pig implements the fragment-replicate join by loading the replicated input into Hadoop’s distributed cache.
The distributed cache is a tool provided by Hadoop that preloads a file onto the local disk of nodes that will be executing the maps or reduces for that job.
Second, if multiple map tasks are located on the same physical machine, the files in the distributed cache are shared between those instances, thus reducing the number of times the file has to be copied.
Pig runs a map-only MapReduce job to preprocess the file and get it ready for loading into the distributed cache.
If there is a filter or foreach between the load and join, these will be done as part of this initial job so that the file to be stored in the distributed cache is as small as possible.
The join itself will be done in a second map-only job.
As we have seen elsewhere, much of the data you will be processing with Pig has significant skew in the number of records per key.
For example, if you were building a map of the Web and joining by the domain of the URL (your key), you would expect to see significant skew for values such as yahoo.com.
Pig’s default join algorithm is very sensitive to skew, because it collects all of the records for a given key together on a single reducer.
In many data sets, there are a few keys that have three or more orders of magnitude more records than other keys.
This results in one or two reducers that will take much longer than the rest.
Skew join works by first sampling one input for the join.
In that input it identifies any keys that have so many records that skew join estimates it will not be able to fit them all into memory.
Then, in a second MapReduce job, it does the join.
For all records except those identified in the sample, it does a standard join, collecting records with the same key onto the same reducer.
Based on how many records were seen for a given key, those records are split across the appropriate number of reducers.
The number of reducers is chosen based on Pig’s estimate of how wide the data must be split such that each reducer can fit its split into memory.
For the input to the join that is not split, those keys that were split are then replicated to each reducer that contains that key.‡ For example, let’s look at how the following Pig Latin script would work:
Let’s further assume that Pig determined that it could fit 75,000 records into memory on each reducer.
When this data was joined, New York would be identified as a key that needed to be split across reducers.
During the join phase, all records with keys other than New York would be treated as in a default join.
Records from users with New York as the key would be split between.
Records from cityinfo with New York as a key would be duplicated and sent to both of those reducers.
The second input in the join, in this case users, is the one that will be sampled and have its keys with a large number of values split across reducers.
The first input will have records with those values replicated across reducers.
If both inputs have skew, this algorithm will still work, but it will be slow.
Much of the motivation behind this approach was that it guarantees the join will still finish, given time.
Before Pig introduced skew join in version 0.4, data that was skewed on both sides could not be joined in Pig because it was not possible to fit all the records for the high-cardinality key values in memory for either side.
Skew join can be done on inner or outer joins.
Multiway joins must be broken into a series of joins if they need to use skew join.
Since data often has skew, why not use skew join all of the time? There is a small performance penalty for using skew join, because one of the inputs must be sampled first to find any key values with a large number of records.
This usually adds about 5% to the time it takes to calculate the join.
If your data frequently has skew, it might be worth it to always use skew join and pay the 5% tax in order to avoid failing or running very slowly with the default join and then needing to rerun using skewed join.
As stated earlier, Pig estimates how much data it can fit into memory when deciding which key values to split and how wide to split them.
For the purposes of this calculation, Pig looks at the record sizes in the sample and assumes it can use 30% of the JVM’s heap to materialize records that will be joined.
In your particular case you might find you need to increase or decrease this size.
You should decrease the value if your join is still failing with out-of-memory errors even when using skew join.
This indicates that Pig is estimating memory usage improperly, so you should tell it to use less.
If profiling indicates that Pig is not utilizing all of your heap, you might want to increase the value in order to do the join more efficiently; the less ways the key values are split, the more efficient the join will be.
For example, if you wanted it to use Pig command line or define the value in your properties file.
Like order, skew join breaks the MapReduce convention that all records with the same key will be processed by the same reducer.
This means records with the same key might be placed in separate part files.
If you plan to process the data in a way that depends on all records with the same key being in the same part file, you cannot use skew join.
A common database join strategy is to first sort both inputs on the join key and then walk through both inputs together, doing the join.
In MapReduce, because a sort requires a full MapReduce job, as does Pig’s default join, this technique is not more efficient than the default.
However, if your inputs are already sorted on the join key, this approach makes sense.
The join can be done in the map phase by opening both files and walking through them.
Pig refers to this as a merge join because it is a sort-merge join, but the sort has already been done:
This sample builds an index that tells Pig the value of the join keys, symbol in the first record in every input split (usually each HDFS block)
Because this sample reads only one record per split, it runs very quickly.
Pig will then run a second MapReduce job that takes the first input, NYSE_daily_sorted, as its input.
When each map reads the first record in its split of NYSE_daily_sorted, it takes the value of symbol and looks it up in the index built by the previous job.
It looks for the last entry that is less than its value of symbol.
Once it finds a match, it collects all the records with that value into memory and then does the join.
If the key is the same, it again does the join.
If the value is greater, it advances the first input and continues.
Because both inputs are sorted, it never needs to look in the index after the initial lookup.
All of this can be done without a reduce phase, and so it is more efficient than a default join.
This algorithm, which was introduced in version 0.4, currently supports only twoway inner joins.
Instead of collecting records of one input based on a key, it collects records of n inputs based on a key.
The result is a record with a key and one bag for each input.
Each bag contains all records from that input that have the given value for the key:
Another way to think of cogroup is as the first half of a join.
The keys are collected together, but the cross product is not done.
In fact, cogroup plus foreach, where each bag is flattened, is equivalent to a join—as long as there are no null values in the keys.
That is, all records with a null value in the key will be collected together.
For example, Pig Latin does not have a semi-join operator, but you can do a semi-join:
Because cogroup needs to collect records with like keys together, it requires a reduce phase.
If you had two files you wanted to use for input and there was no glob that could describe them, you could do the following:
Unlike union in SQL, Pig does not require that both inputs share the same schema.
If both do share the same schema, the output of the union will have that schema.
If one schema can be produced from another by a set of implicit casts, the union will have that resulting schema.
If neither of these conditions hold, the output will have no schema (that is, different records will have different fields)
This schema comparison includes names, so even different field names will result in the output having no schema.
You can get around this by placing a foreach before the union that renames fields.
If you have data you collect every month, you might add a new column this month.
Now you are prevented from using union because your schemas do not match.
If you want to union this data and force your data into a common schema, you can add the keyword onschema to your union statement:
It also requires that a shared schema for all inputs can be produced by adding fields and implicit casts.
If a shared schema cannot be produced by this method, an error is returned.
When the data is read, nulls are inserted for fields not present in a given input.
Given inputs with n and m records respectively, cross will produce output with n x m records.
It does this by generating a synthetic join key, replicating rows, and then doing the cross as a join.
In 0.9, this is changed to be the square root of the parallel factor, rounded up.
When these records are flattened, four copies of each input record will be created in the map.
For every record in each input, it is guaranteed that there is one and only one instance of the artificial keys that will match and produce a record.
Because the random numbers are chosen differently for each record, the resulting joins are done on an even distribution of the reducers.
However, it creates a burden on the shuffle phase by increasing the number of records in each input being shuffled.
Also, no matter what you do, cross outputs a lot of data.
Writing all of this data to disk is expensive, even when done in parallel.
This is not to say you should not use cross.
Pig’s join operator supports only equi-joins, that is, joins on an equality condition.
Because general join implementations (ones that do not depend on the data being sorted or small enough to fit in memory) in MapReduce depend on collecting records with the same join key values onto the same reducer, non-equi-joins (also called theta joins) are difficult to do.
They can be done in Pig using cross followed by filter:
Fuzzy joins could also be done in this manner, where the fuzzy comparison is done after the cross.
However, whenever possible, it is better to use a UDF to conform fuzzy values to a standard value and then do a regular join.
The most obvious way Pig does that is through its UDFs.
But it also allows you to directly integrate other executables and MapReduce jobs.
You may want to do this when you have a legacy program that you do not want to modify.
You can also use stream when you have a program you use frequently, or one you have tested on small data sets and now want to apply to a large data set.
Let’s look at an example where you have a Perl program highdiv.pl that filters out all stocks with a dividend below $1.00:
But Pig has no idea what the executable will return, so if you do not provide the as clause, the relation highdivs will have no schema.
The executable highdiv.pl is invoked once on every map or reduce task.
Pig instantiates the executable and keeps feeding data to it via stdin.
It also keeps checking stdout, passing any results to the next operator in your data flow.
The executable can choose whether to produce an output for every input, only every so many inputs, or only after all inputs have been received.
The preceding example assumes that you already have highdiv.pl installed on your grid, and that it is runnable from the working directory on the task machines.
If that is not the case, which it usually will not be, you can ship the executable to the grid.
Now in stream we refer to highdiv.pl by the alias we gave it, hp, rather than referring to it directly.
Second, it tells Pig to pick up the file ./highdiv.pl and ship it to Hadoop as part of this job.
This file will be picked up from the specified location on the machine where you launch the job.
It will be placed in the working directory of the task on the task machines.
So, the command you pass to stream must refer to it relative to the current working directory, not via an absolute path.
If your executable depends on other modules or files, they can be specified as part of the ship clause as well.
For example, if highdiv.pl depends on a Perl module called Financial.pm, you can send them both to the task machines:
Many scripting languages assume certain paths for modules based on their hierarchy.
However, the ship clause always puts files in your current working directory, and it does not take directories, so you could not ship Acme.
The workaround for this is to create a TAR file and ship that, and then have a step in your executable that unbundles.
You then need to set your module include path (for Perl, -I or the PERL LIB environment variables) to contain.
But sometimes the file you want is already in the grid.
If you have a grid file that will be accessed by every map or reduce task in your job, the proper way to access it is via the distributed cache.
The distributed cache is a mechanism Hadoop provides to share files.
It reduces the load on HDFS by preloading the file to the local disk on the machine that will be executing the task.
You can use the distributed cache for your executable by using the cache clause in define:
The string after the # is the name of the file as viewed by the executable.
So far we have assumed that your executable takes data on stdin and writes it to stdout.
If your executable needs a file to read from, write to, or both, you can specify that with the input and output clauses in the define command.
Again, file locations are specified from the working directory on the task machines.
Again, the executable will be invoked only once per map or reduce task, so Pig will first write out all the input to the file.
This is convenient if you have processing that is better done in MapReduce than Pig but must be integrated with the rest of your Pig data flow.
It can also make it easier to incorporate legacy processing written in MapReduce with newer processing you want to write in Pig Latin.
MapReduce jobs expect to read their input from and write their output to a storage device (usually HDFS)
So to integrate them with your data flow, Pig first has to store the data, then invoke the MapReduce job, and then read the data back.
You also provide Pig with the name of the JAR that contains the code for your MapReduce job.
As an example, let’s continue with the blacklisting of URLs that we considered in the previous section.
Only now let’s assume that this is done by a MapReduce job instead of a Python script:
It uses load and store phrases to specify how data will be moved from Pig’s data pipeline to the MapReduce job.
Notice that the input alias is contained in the store clause.
As with stream, the output of mapreduce is opaque to Pig, so if we want the resulting relation goodurls to have a schema, we have to tell Pig what it is.
Any arguments you wish to pass to the invocation of the Java command that will run the MapReduce task can be put in backquotes after the load clause:
The string in the backquotes will be passed directly to your MapReduce job as is.
So if you wanted to pass Java options, etc., you can do that as well.
The load and store clauses of the mapreduce command have the same syntax as the load and store statements, so you can use different load and store functions, pass constructor arguments, and so on.
Nonlinear Data Flows So far our examples have been linear data flows or trees.
In a linear data flow, one input is loaded, processed, and stored.
We have looked at operators that combine multiple data flows: join, cogroup, union, and cross.
With these you can build tree structures where multiple inputs all flow to a single output.
But in complex data-processing situations, you often also want to split your data flow.
That is, one input will result in more than one output.
You might also have diamonds, places where the data flow is split and eventually joined back together.
Splits in your data flow can be either implicit or explicit.
In an implicit split, no specific operator or syntax is required in your script.
You might, for example, want to analyze players by position and by team at the same time:
The pwithba relation is referred to by the group operators for both the byteam and bypos relations.
Pig builds a data flow that takes every record from pwithba and ships it to both group operators.
Splitting data flows can also be done explicitly via the split operator, which allows you to split your data flow as many ways as you like.
Let’s take an example where you want to split data into different files depending on the date the record was created:
At first glance, split looks like a switch or case statement, but it is not.
A single record can go to multiple legs of the split since you use different filters for each if clause.
In the preceding example, if a record were found with a date of 20110331, it would be dropped.
And there is no default clause—no way to send any leftover records to a particular alias.
In fact, Pig will internally rewrite the original script that has split in exactly this way.
Let’s take a look at how Pig executes these nonlinear data flows.
In cases where all operators will fit into a single map task, this is easy.
Pig creates separate pipelines inside the map and sends the appropriate records to each pipeline.
The example using split to store data by date will be executed in this way.
Pig can also combine multiple group operators together in many cases.
In the example given at the beginning of this section, where the baseball data is grouped by both team and position, this entire Pig Latin script will be executed inside one MapReduce job.
Pig accomplishes this by duplicating records on the map side and annotating each record with its pipeline number.
When the data is partitioned during the shuffle, the appropriate key is used for each record.
That is, records from the pipeline grouping by team will use team as their shuffle key, and records from the pipeline grouping by position will use position as their shuffle key.
This is done by declaring the key type to be tuple and placing the correct values in the key tuple for each record.
Once the data has been collected to reducers, the pipeline number is used as part of the sort key so that records from each pipeline and group are collected together.
In the reduce task, Pig instantiates multiple pipelines, one for each group operator.
It sends each record down the appropriate pipeline based on its annotated pipeline number.
In this way, input data can be scanned once but grouped many different ways.
An example of how one record flows through this pipeline is shown in Figure 6-1
Although this does not provide linear speedup, we find it often approaches it.
There are cases where Pig will not combine multiple operators into a single MapReduce job.
Pig does not use multiquery for any of the multiple-input operators: join, union,
Also, if it has multiple group statements and some would use Hadoop’s combiner and some would not, it combines only those statements that use Hadoop’s combiner into a multiquery.
This is because we have found that combining the Hadoop combiner and non-Hadoop combiner jobs together does not perform well.
Multiquery scripts tend to perform better than loading the same input multiple times, but this approach does have limits.
Because it requires replicating records in the map, it does slow down the shuffle phase.
Eventually the increased cost of the shuffle phase outweighs the reduced cost of rescanning the input data.
Pig has no way to estimate when this will occur.
Currently, the optimizer is optimistic and always combines jobs with multiquery whenever it can.
If it combines too many jobs and becomes slower than splitting some of the jobs, you can turn off multiquery or you can rewrite your Pig Latin into separate scripts so Pig does not attempt to combine them all.
To turn off multiquery, you can pass either -M or -no_multiquery on the command line or set the property opt.multiquery to false.
We must also consider what happens when one job in a multiquery fails but others succeed.
If all jobs succeed, Pig will return 0, meaning success.
If all of the jobs fail, Pig will return 2
If some jobs fail and some succeed, Pig will return 3
By default, if one of the jobs fails, Pig will continue processing the other jobs.
However, if you want Pig to stop as soon as one of the jobs fails, you can pass -F or -stop_on_failure.
In this case, any jobs that have not yet been finished will be terminated, and any that have not started will not be started.
Any jobs that are already finished will not be cleaned up.
Controlling Execution In addition to providing many relational and dataflow operators, Pig Latin provides ways for you to control how your jobs execute on MapReduce.
It allows you to set values that control your environment and details of MapReduce, such as how your data is partitioned.
Table 6-1 shows Pig-specific parameters that can be controlled via set.
By default the name is the filename of the script being run, or a randomly generated name for interactive sessions.
For example, to set the default parallelism of your Pig Latin script and set the job name to my_job:
In addition to these predefined values, set can be used to pass Java property settings to Pig and Hadoop.
Both Pig and Hadoop use a number of Java properties to control their behavior.
Consider an example where you want to turn multiquery off for a given script, and you want to tell Hadoop to use a higher value than usual for its map-side sort buffer:
You can also use this mechanism to pass properties to UDFs.
All of the properties are passed to the tasks on the Hadoop nodes when they are executed.
They are not set as Java properties in that environment; rather, they are placed in a Hadoop object called JobConf.
Thus, anything you set in the script can be seen by your UDFs.
This can be a convenient way to control UDF behavior.
Values that are set in your script are global for the whole script.
If they are reset later in the script, that second value will overwrite the first and be used throughout the whole script.
Setting the Partitioner Hadoop uses a class called Partitioner to partition records to reducers during the shuffle phase.
Pig does not override the default partitioner, except for order and skew join.
Beginning in version 0.8, Pig allows you to set the partitioner, except in the cases where it is already overriding it.
To do this, you need to tell Pig which Java class to use to partition your data.
Note that this is the newer (version 0.20 and later) map reduce API and not the older mapred:
These operators are cogroup, cross, distinct, group, and join (again, not in conjunction with skew join)
Starting with 0.9, it also provides inclusion of other Pig Latin scripts and function-like macro definitions, so that you can write Pig Latin in a modular way.
Parameter Substitution Pig Latin scripts that are used frequently often have elements that need to change based on when or where they are run.
A script that is run every day is likely to have a date component in its input files or filters.
Rather than edit and change the script every day, you want to pass in the date as a parameter.
Parameter substitution provides this capability with a basic string-replacement functionality.
Parameters must start with a letter or an underscore and can then have any amount of letters, numbers, or underscores.
Values for the parameters can be passed in on the command line or from a parameter file:
When you run daily.pig, you must provide a definition for the parameter DATE; otherwise, you will get an error telling you that you have undefined parameters:
You can repeat the -p command-line switch as many times as needed.
Parameters can also be placed in a file, which is convenient if you have more than a few of them.
The format of the file is parameter=value, one per line.
Comments in the file should be preceded by a #
You then indicate the file to be used with -m or -param_file:
Parameters passed on the command line take precedence over parameters provided in files.
This way, you can provide all your standard parameters in a file and override a few as needed on the command line.
So, for example, you could have the following parameter file:
The parameter file here would produce an error if the DAY line came after the DATE line.
The other caveat is that there is no special character to delimit the end of a parameter.
Any alphanumeric or underscore character will be interpreted as part of the parameter, and any other character will be interpreted as itself.
So, if you had a script that ran at the first of every month, you could not do the following:
This would try to resolve a parameter MONTH01 when you meant MONTH.
When using parameter substitution, all parameters in your script must be resolved after the preprocessor is finished.
If not, Pig will issue an error message and not continue.
You can see the results of your parameter substitution by using the -dryrun flag on the Pig command line.
Pig will write out a version of your Pig Latin script with the parameter substitution done, but it will not execute the script.
Consider a case where most of the time your script is run on one Hadoop cluster, but occasionally it is run on a different cluster with different hardware:
When running your script in the usual configuration, there is no need to set the parameter parallel_factor.
On the occasions it is run in a different setup, the parallel factor can be changed by passing a value on the command line.
Macros Starting in 0.9, Pig added the ability to define macros.
This makes it possible to make your Pig Latin scripts modular.
It also makes it possible to share segments of Pig Latin code among users.
This can be particularly useful for defining standard practices and making sure all data producers and consumers use them.
A macro takes a set of input parameters, which are string values that will be substituted for the parameters when the macro is expanded.
By convention, input relation names are placed first before other parameters.
It is also possible to have a macro that does not return a relation.
In this case, the returns clause of the define statement is changed to returns void.
This can be useful when you want to define a macro that controls how data is partitioned and sorted before being stored to a particular output, such as HBase or a database.
This is where an important difference between macros and functions becomes apparent.
Macros can invoke other macros, so a macro A can invoke a macro B, but A cannot invoke itself.
Parameters should be passed explicitly to macros, and parameter substitution should be used only at the top level.
You can use the -dryrun command-line argument to see how the macros are expanded inline.
When the macros are expanded, the alias names are changed to avoid collisions with alias names in the place the macro is being expanded.
If we take the previous example and use -dryrun to show us the resulting Pig Latin, we will see the following (reformatted slightly to fit on the page):
As you can see, the aliases in the macro are expanded with a combination of the macro name and the invocation number.
This provides a unique key so that if other macros use the same aliases, or the same macro is used multiple times, there is still no duplication.
Including Other Pig Latin Scripts For a long time in Pig Latin, the entire script needed to be in one file.
Starting in 0.9, the preprocessor can be used to include one Pig Latin script in another.
This means that all macros are in the same namespace, even when they have been imported from separate files.
Thus, care should be taken to choose unique names for your macros.
The last few chapters focused on Pig Latin the language.
Now we will turn to the practical matters of developing and testing your scripts.
This chapter covers helpful debugging tools such as describe and explain.
Information on how to make your scripts perform better will be covered in the next chapter.
Development Tools Pig provides several tools and diagnostic operators to help you develop your applications.
In this section we will explore these and also look at some tools others have written to make it easier to develop Pig with standard editors and integrated development environments (IDEs)
Syntax Highlighting and Checking Syntax highlighting often helps users write code correctly, at least syntactically, the first time around.
The packages listed in Table 7-1 were created and added at various times, so how their highlighting conforms with current Pig Latin syntax varies.
In addition to these syntax highlighting packages, Pig will also let you check the syntax of your script without running it.
If you add -c or -check to the command line, Pig will just parse and run semantic checks on your script.
The -dryrun command-line option will also check your syntax, expand any macros and imports, and perform parameter substitution.
This can be very helpful as you are developing your scripts.
It is especially useful as you are learning Pig Latin and understanding how various operators change the data.
So, in this example, the relation trimmed has two fields: symbol, which is a chararray, and dividends, which is a float.
Finally, in avgdiv there are two fields, group and a double, which is the result of the AVG function and is unnamed.
But sometimes you need to peek into the barn and see how Pig is compiling your script into MapReduce jobs.
It was written so that Pig developers could examine how Pig handled various scripts, thus its output is not the most user-friendly.
But with some effort, explain can help you write better Pig Latin.
You can explain any alias in your Pig Latin script, which will show the execution plan Pig would use if you stored that relation.
You can also take an existing Pig Latin script and apply explain to the whole script in Grunt.
One, you do not have to edit your script to add the explain line.
Two, it will work with scripts that do not have a single store, showing how Pig will execute the entire script:
This will produce a printout of several graphs in text format; we will examine this output momentarily.
When using explain on a script in Grunt, you can also have it print out the plan in graphical format.
To do this, add -dot -out filename to the preceding command line.
This prints out a file in DOT language containing diagrams explaining how your script will be executed.
Tools that can read this language and produce graphs can then be used to view the graphs.
For some tools, you might need to split the three graphs in the file into separate files.
Pig goes through several steps to transform a Pig Latin script to a set of MapReduce jobs.
After doing basic parsing and semantic checking, it produces a logical plan.
This plan describes the logical operators that Pig will use to execute the script.
For example, filters are pushed as far up* as possible in the logical plan.
The logical plan for the preceding example is shown in Figure 7-1
I have trimmed a few extraneous pieces to make the output more readable (scary that this is more readable, huh?)
If you are using Pig 0.9, the output will look slightly different, but close enough that it will be recognizable.
The flow of this chart is bottom to top so that the Load operator is at the very bottom.
Each of the four operators created by the script (Load, CoGroup, ForEach, and Store) can be seen.
Each of these operators also has a schema, described in standard schema syntax.
The CoGroup and ForEach operators also have expressions attached to them (the lines dropping down from those operators)
In the CoGroup operator, the projection indicates which field is the grouping key (in this case, field 1)
Notice how each of the Project operators has an Input field, indicating from which operator they are drawing their input.
Figure 7-2 shows how this plan looks when the -dot option is used instead.
Database textbooks usually talk of pushing filters down, closer to the scan.
Because Pig Latin scripts start with a load at the top and go down, we tend to refer to it as pushing filters up toward the load.
After optimizing the logical plan, Pig produces a physical plan.
This plan describes the physical operators Pig will use to execute the script, without reference to how they will be executed in MapReduce.
This looks like the logical plan, but with a few differences.
This example was run in local mode, so the paths are local files.
The other noticeable difference is that the CoGroup operator was replaced by three operators, Local Rearrange, Global Rearrange, and Package.
Local Rearrange is the operator Pig uses to prepare data for the shuffle by setting up the key.
Package sits in the reduce phase and directs records to the proper bag.
Finally, Pig takes the physical plan and decides how it will place its operators into one or more MapReduce jobs.
First, it walks the physical plan looking for all operators that require a new reduce.
After it has done this, it sees whether there are places that it can do physical optimizations.
For example, it looks for places the combiner can be used, and whether sorts can be avoided by including them as part of the sorting Hadoop does in the shuffle.
After all of this is done, Pig has a MapReduce plan.
Completing our example, the MapReduce plan is shown in Figure 7-5
The pipeline is now broken into three stages: map, combine, and reduce.
The Global Rearrange operator is gone because it was a stand-in for the shuffle.
The AVG UDF has been broken up into three stages: Initial in the map, Intermediate in the combiner, and Final in the reduce.
If there were multiple MapReduce jobs in this example, they would all be shown in this output.
But if you are using Pig, the odds are that you have a large data set.
If it takes several hours to process your data, this makes for a very long debugging cycle.
One obvious solution is to run your script on a sample of your data.
But sampling has another problem: it is not always trivial to pick a sample that will exercise your script properly.
For example, if you have a join, you have to be careful to sample records from each input such that at least some have the same key.
To address this issue, the scientists in Yahoo! Research built illustrate into Pig.
When necessary, it will manufacture records that look like yours (i.e., that have the same schema) but are not in the sample it took.
To use illustrate, apply it to an alias in your script, just as you would describe.
Figure 7-7 shows the results of illustrating the following script:
For each relation here, illustrate shows us records as they look coming out of the relation.
Like explain, illustrate can be given as a command-line option rather than modifying your script; for example, bin/pig -e 'illustrate -script illustrate.pig'
Pig Statistics Beginning in version 0.8, Pig produces a summary set of statistics at the end of every run:
Running stats.pig produces the statistics shown in Figure 7-8, reformatted slightly so it will fit on the page.
The first couple of lines give a brief summary of the job.
StartedAt is the time Pig submits the job, not the time the first job starts running the Hadoop cluster.
Depending on how busy your cluster is, these may vary significantly.
Similarly, FinishedAt is the time Pig finishes processing the job, which will be slightly after the time the last MapReduce job finishes.
The section labeled Job Stats gives a breakdown of each MapReduce job that was run.
This includes how many map and reduce tasks each job had, statistics on how long these tasks took, and a mapping of aliases in your Pig Latin script to the jobs.
This last feature is especially useful when trying to understand which operators in your script are running in which MapReduce job, which can be helpful when determining why a particular job is failing or producing unexpected results.
The statistics on spills record how many times Pig spilled records to local disk to avoid running out of memory.
In local mode the Counters section will be missing because Hadoop does not report counters in local mode.
The Job DAG section at the end describes how data flowed between MapReduce jobs.
MapReduce Job Status When you are running your Pig Latin scripts on your Hadoop cluster, finding the status and logs of your job can be challenging.
Logs generated by Pig while it plans and manages your query are stored in the current working directory.
You can select a different directory by passing -l logdir on the command line.
However, Hadoop does not provide a way to fetch back the logs from its tasks.
So, the logfile created by Pig contains only log entries generated on your machine.
Log entries generated during the execution, including those generated by your UDFs, stay on the task nodes in your Hadoop cluster.
All data written to stdout and stderr by map and reduce tasks is also kept in the logs on the task nodes.
The first step to locating your logs is to connect to the JobTracker’s web page.
This page gives you the status of all jobs currently running on your Hadoop cluster, plus the list of the last hundred or so finished jobs.
Figure 7-9 shows a sample page taken from a cluster running in pseudodistributed mode on a Linux desktop.
In this screenshot there, is only one job that has been run on the cluster recently.
The user who ran the job, the job ID, and the job name are all listed.
Jobs started by Pig are assigned the name of the Pig Latin script that you ran, unless you use the commandline option to change the job name.
All jobs started by a single script will share the same name.
In most cases you will have more than one MapReduce job resulting from your Pig job.
When you have multiple jobs with the same name, this will help you determine which MapReduce job you are interested in.
For the job in the screenshot shown in Figure 7-9, the relevant portions of the summary look like this:
Given this job ID, you now know which job to look at on the JobTracker page.
Note that jobs are shown on the JobTracker page only once they start to execute on your Hadoop cluster.
It takes Pig a few seconds to parse your script and plan the MapReduce jobs it will run.
It then takes a few seconds after Pig submits the first job before Hadoop begins running it.
Also, the necessary resources might not be available, in which case your job will not appear until it has been assigned resources.
Clicking on the job ID will take you to a screen that summarizes the execution of the job, including when the job started and stopped, how many maps and reduces it ran, and the results of all of the counters, as shown in Figure 7-10
Let’s say you want to look at the logs for the single map task in this job.
In the table toward the top of the page that summarizes the results of the map and reduce tasks,
Selecting any particular task will show you the machine the task ran on, its status, its start and end times, and will then provide a link to its logfile.
Clicking on that link will (finally) allow you to see the log for that individual task.
Of course, in this example, finding the map task we wanted was easy because there was only one.
If your tasks are failing only periodically, you can examine the logs of the failing tasks.
If they are all failing, you should be able to pick any of them, since they are all running the same code.
If your job is running slower than it seems like it should, you can look for tasks that took much longer than others.
It is also often useful to look to see if all maps or all reduces take about the same amount of time.
Debugging Tips Beyond the tools covered previously, there are a few things I have found useful in debugging Pig Latin scripts.
First, if illustrate does not do what you need, use local mode to test your script before running it on your Hadoop cluster.
Two, the logs for your operations appear on your screen, instead of being left on a task node somewhere.
This means that you can attach a debugger to the process.
This is particularly useful when you need to debug your UDFs.
A second tip I have found useful is that sometimes you need to turn off particular features to see whether they are the source of your problem.
All of these are options that can be passed to Pig on the command line.
Command-line option What it does When you might want to turn it off.
Your foreach is not producing the rows or fields you expect.
Your load function is not returning the fields you expect.
Your limit is not returning the number of rows you expect.
If you find you are turning off a feature to avoid a bug, please file a JIRA ticket so that the problem can be fixed.
Command-line option What it does When you might want to turn it off.
Your foreach is not producing the rows or fields you expect.
Physical optimizations (such as use of combiner, multiquery, etc.) will still be done.
Your script is not producing the rows you expect and you want to understand whether the logical optimizer is part of the problem.
Scripts that worked in previous versions of Pig stop working in 0.8
Helps you check if your UDF has a problem in its Algebraic implementation, as this is called only when the combiner is used.
Your multiquery scripts are running out of memory, underperforming, or otherwise failing.
Some input formats, such as HBase, cannot have their splits combined.
In Pig 0.8.0, the logical optimizer and logical plan were completely rewritten.
The new optimizer and plan are used by default in 0.8.0, but old ones are available as a backup.
After releasing 0.8.0, a number of issues were found with the new optimizer and plan.
If upgrading is not an option, the workaround is to turn off the new logical plan as described in Table 7-2
In Pig 0.9, the old logical plan has been removed.
Testing Your Scripts with PigUnit As part of your development, you will want to test your Pig Latin scripts.
Even once they are finished, regular testing helps assure that changes to your UDFs, to your scripts, or in the versions of Pig and Hadoop that you are using do not break your code.
PigUnit provides a unit-testing framework that plugs into JUnit to help you write unit tests that can be run on a regular basis.
Let’s walk through an example of how to test a script with PigUnit.
This is not distributed as part of the standard Pig distribution, but you can build it from the source code included in your distribution.
To do this, go to the directory your distribution is in and type ant jar pigunit-jar.
Once this is finished, there should be two files in the directory: pig.jar and pigunit.jar.
You will need to place these in your classpath when running PigUnit tests.
You can use an existing input file, or you can manufacture some input in your test and run that through your script.
Finally, you need to write a Java class that JUnit can use to run your test.
Let’s start with a simple example that runs the preceding script:
You can also specify the input inline in your test rather than relying on an existing datafile:
It is also possible to specify the Pig Latin script in your test and to test the output against an existing file that contains the expected results:
Finally, let’s look at how to integrate PigUnit with parameter substitution, and how to specify expected output that will be compared against the stored result (rather than specifying an alias to check):
These examples can be run by using the build.xml file included in the examples from this chapter.
These examples are not exhaustive; see the code itself for a complete listing.
Who says Pigs can’t fly? Knowing how to optimize your Pig Latin scripts can make a significant difference in how they perform.
Pig is still a young project and does not have a sophisticated optimizer that can make the right choices.
Instead, consistent with Pig’s philosophy of user choice, it relies on you to make these choices.
Beyond just optimizing your scripts, Pig and MapReduce can be tuned to perform better based on your workload.
And there are ways to optimize your data layout as well.
This chapter covers a number of features you can use to help Pig fly.
Before diving into the details of how to optimize your Pig Latin, it is worth understanding what items tend to create bottlenecks in Pig jobs: Input size.
It does not seem that a massively parallel system should be I/O bound.
Hadoop’s parallelism reduces I/O bound but does not entirely remove it.
Additional maps take more time to start up, and MapReduce has to find more slots in which to run them.
If you have twice as many maps as you have slots to run them, it will take twice your average map time to run all of your maps.
Adding one more map in that case will actually make it worse because the map time will increase to three times the average.
Also, every record that is read might need to be decompressed and will need to be deserialized.
Shuffle size By shuffle size I mean the data that is moved from your map tasks to your reduce tasks.
All of this data has to be serialized, sorted, moved over the network, merged, and deserialized.
Every reducer has to go to every mapper, find the portion of the map’s output that belongs to it, and copy that.
So if there are m maps and r reduces, the shuffle will have m x r network connections.
Output size Every record written out by a MapReduce job has to be serialized, possibly compressed, and written to the store.
When the store is HDFS, it must be written to three separate machines before it is considered written.
Intermediate results size Pig moves data between MapReduce jobs by storing it in HDFS.
Thus the size of these intermediate results is affected by the input size and output size factors mentioned previously.
Memory Some calculations require your job to hold a lot of information in memory, for example, joins.
If Pig cannot hold all of the values in memory simultaneously, it will need to spill some to disk.
This causes a significant slowdown, as records must be written to and read from disk, possibly multiple times.
Writing Your Scripts to Perform Well There are a number of things you can do when writing Pig Latin scripts to help reduce the bottlenecks discussed earlier.
Filter Early and Often Getting rid of data as quickly as possible will help your script perform better.
Pushing filters higher in your script can reduce the amount of data you are shuffling or storing in HDFS between MapReduce jobs.
Pig’s logical optimizer will push your filters up whenever it can.
In cases where a filter has multiple predicates joined by and, and one or more of the predicates can be applied before the operator preceding the filter, Pig will split the filter at the and and push the eligible predicate(s)
This allows Pig to push parts of the filter when it might not be able to push the filter as a whole.
Table 8-1 describes when these filter predicates will and will not be pushed once they have been split.
This is done only after all filter pushing is complete.
Also, consider adding filters that are implicit in your script.
For example, all of the records with null values in the key will be thrown out by an inner join.
If you know that more than a few hundred of your records have null key values, put a filter input by key is not null before the join.
Project Early and Often For earlier versions of Pig, we told users to employ foreach to remove fields they were not using as soon as possible.
As of version 0.8, Pig’s logical optimizer does a fair job of removing fields aggressively when it can tell that they will no longer be used:
However, you are still smarter than Pig’s optimizer, so there are situations where you can tell that a field is no longer needed but Pig cannot.
It cannot see that COUNT does not need all of the fields in the bag it is being passed.
Whenever you pass a UDF the entire record (udf(*)) or an entire complex field, Pig cannot determine which fields are required.
In this case, you will need to put in the foreach yourself to remove unneeded data as early as possible.
Set Up Your Joins Properly Joins are one of the most common data operations, and also one of the costliest.
Choosing the correct join implementation can improve your performance significantly.
The flowchart in Figure 8-1 will help you make the correct selection.
Once you have selected your join implementation, make sure to arrange your inputs in the correct order as well.
For replicated joins, the small table must be given as the last input.
For skewed joins, the second input is the one that is sampled for large keys.
For the default join, the rightmost input has its records streamed through, whereas the other input(s) have their records for a given key value materialized in memory.
Thus if you have one join input that you know has more records per key value, you should place it in the rightmost position in the join.
For merge join, the left input is taken as the input for the MapReduce job, and thus the number of maps started are based on.
If one input is much larger than the other, you should place it on the left in order to get more map tasks dedicated to your jobs.
This will also reduce the size of the sampling step that builds the index for the right side.
Use Multiquery When Possible Whenever you are doing operations that can be combined by multiquery, such as grouping and filtering, these should be written together in one Pig Latin script so that Pig can combine them.
Although adding extra operations does increase the total processing time, it is still much faster than running jobs separately.
Choose the Right Data Type As discussed elsewhere, Pig can run with or without data type information.
In cases where the load function you are using creates data that is already typed, there is little you need to do to optimize the performance.
However, if you are using the default PigStorage load function that reads tab-delimited files, then whether you use types will affect your performance.
On the one hand, converting fields from bytearray to the appropriate type has a cost.
So, if you do not need type information, you should not declare it.
For example, if you are just counting records, you can omit the type declaration without affecting the outcome of your script.
On the other hand, if you are doing integer calculations, types can help your script perform better.
When Pig is asked to do a numeric calculation on a bytearray, it treats that bytearray as a double because this is the safest assumption.
But floating-point arithmetic is much slower than integer arithmetic on most machines.
For example, if you are doing a SUM over integer values, you will get better performance by declaring them to be of type integer.
Select the Right Level of Parallelism Setting your parallelism properly can be difficult, as there are a number of factors.
Before we discuss the factors, a little background will be helpful.
It would be natural to think more parallelism is always better; however, that is not the case.
Like any other resource, parallelism has a network cost, as discussed under the shuffle size performance bottleneck.
Second, increasing parallelism adds latency to your script because there is a limited number of reduce slots in your cluster, or a limited number that your scheduler will assign to you.
Because there is overhead in starting and stopping reduce tasks, and the shuffle gets less efficient as parallelism increases, it is often not efficient to select more reducers than you have slots to run them.
In fact, it is best to specify slightly fewer reducers than the number of slots that you can access.
This leaves room for MapReduce to restart a few failed reducers and use speculative execution without doubling your reduce time.
Also, it is important to keep in mind the effects of skew on parallelism.
MapReduce generally does a good job partitioning keys equally to the reducers, but the number of records per key often varies radically.
Thus a few reducers that get keys with a large number of records will significantly lag the other reducers.
Pig cannot start the next MapReduce job until all of the reducers have finished in the previous job.
So the slowest reducer defines the length of the job.
Increasing your parallelism will not help; it will just waste more cluster resources.
Instead, you need to use Pig’s mechanisms to handle skew.
Writing Your UDF to Perform Pig has a couple of features intended to enable aggregate functions to run significantly faster.
The Accumulator interface allows Pig to break a collection of records into several sets and give each set to the UDF separately.
This avoids the need to materialize all of the records simultaneously, and thus spill to disk when there are too many records.
Whenever possible, you should write your aggregate UDFs to make use of these features.
Pig also has optimizations to help loaders minimize the amount of data they load.
Pig can tell a loader which fields it needs and which keys in a map it needs.
Tune Pig and Hadoop for Your Job On your way out of a commercial jet airliner, have you ever peeked around the flight attendant to gaze at all the dials, switches, and levers in the cockpit? This is sort of what tuning Hadoop is like: many, many options, some of which make an important difference.
But without the proper skills, it can be hard to know which is the right knob to turn.
Table 8-2 looks at a few of the important features.
See those tables for a more complete list of parameters.
Increasing this will decrease the number of spills from the map and make the combiner more efficient, but will leave less memory for your map tasks.
There are a couple of memoryrelated parameters that will help ensure Pig uses its memory in the best way possible.
Once the bags fill up this amount, the data is spilled to disk.
Setting this to a higher value will reduce spills to disk during execution but increase the likelihood of a task running out of heap.
Setting this to a higher value will reduce the number of ways that large keys are split and thus how many times their records must be replicated, but it will increase the likelihood of a reducer running out of memory.
Using Compression in Intermediate Results As is probably clear by now, some of the biggest costs in Pig are moving data between map and reduce phases and between MapReduce jobs.
Compression can be used to reduce the amount of data to be stored to disk and written over the network.
By default, compression is turned off, both between map and reduce tasks and between MapReduce jobs.
You will also need to select a compression type to use.
Compressing data between MapReduce jobs can also have a significant impact on Pig performance.
This is particularly true of Pig scripts that include joins or other operators that expand your data size.
In the testing we did while developing this feature, we saw performance improvements of up to four times when using LZO, and slight performance degradation when using gzip.
To use it, you first need to build and install the LZO plug-in for Hadoop and configure your cluster to use it.
To download LZO, go to http://code.google.com/a/apache-extras.org/p/hadoop-gpl-com pression and click on the Downloads tab.
Then you will need to build the native LZO library on your system.
Be sure to do this build on a system that matches your grid machines, as this is C code and not portable.
Once you have built the native library, you need to install it on your cluster.
A number of fixes for bugs found in this tarball have been committed to GitHub.
You might want to clone and build this version if you have issues with the official tarball.
Data Layout Optimization How you lay out your data can have a significant impact on how your Pig jobs perform.
On the one hand, you want to organize your files such that Pig can scan the minimal set of records.
For example, if you have regularly collected data that you usually read on an hourly basis, it likely makes sense to place each hour’s data in a separate file.
On the other hand, the more files you create, the more pressure you put on your NameNode.
And MapReduce operates more efficiently on larger files than it does on files that are less than one HDFS block (64 MB by default)
You will need to find a balance between these two competing forces.
Beginning in 0.8, when your inputs are files and they are smaller than half an HDFS block, Pig will automatically combine the smaller sections when using the file as input.
This allows MapReduce to be more efficient and start fewer map tasks.
It is not always better for the performance of your individual query, however, because you will be losing locality of data reads for many of the combined blocks, and your map tasks may run longer.
Bad Record Handling When processing gigabytes or terabytes of data, the odds are overwhelming that at least one row is corrupt or will cause an unexpected result.
An example is division by zero, even though no records were supposed to have a zero in the denominator.
Causing an entire job to fail over one bad record is not good.
To avoid these failures, Pig inserts a null, issues a warning, and continues processing.
Warnings are aggregated and reported as a count at the end.
You should check the warnings to be sure that the failure of a few records is acceptable in your job.
If you need to know more details about the warnings, you can turn off the aggregation by passing -w on the command line.
Unlike general-purpose programming languages, it does not include control flow constructs such as if and for.
For many data-processing applications, the operators Pig provides are sufficient.
But there are classes of problems that either require the data flow to be repeated an indefinite number of times or need to branch based on the results of an operator.
Iterative processing, where a calculation needs to be repeated until the margin of error is within an acceptable limit, is one example.
It is not possible to know beforehand how many times the data flow will need to be run before processing begins.
Blending data flow and control flow in one language is difficult to do in a way that is useful and intuitive.
Building a general-purpose language and all the associated tools, such as IDEs and debuggers, is a considerable undertaking; also, there is no lack of such languages already.
If we turned Pig Latin into a general-purpose language, it would require users to learn a much bigger language to process their data.
For these reasons, we decided to embed Pig in existing scripting languages.
This avoids the need to invent a new language while still providing users with the features they need to process their data.* As with UDFs, we chose to use Python for the initial release of embedded Pig in version Python scripts that embed Pig.
In the future we hope to extend the system to other scripting languages that can access Java objects, such as JavaScript† and JRuby.
Of course, since the Pig infrastructure is all in Java, it is possible to use this same interface to embed Pig into Java scripts.
In some of the documentation, wiki pages, and issues on JIRA, embedded Pig is referred to as Turing Complete Pig.
This was what the project was called when it first started, even though we did not make Pig itself Turing complete.
There is already an experimental version of JavaScript in 0.9
This embedding is done in a JDBC-like style, where your Python script first compiles a Pig Latin script, then binds variables from Python to it, and finally runs it.
It is also possible to do filesystem operations, register JARs, and perform other utility operations through the interface.
Throughout this chapter we will use an example of calculating page rank from a web crawl.
You can find this example under examples/ch9 in the example code.
This code iterates over a set of URLs and links to produce a page rank for each URL.‡ The input to this example is the webcrawl data set found in the examples.
Each record in this input contains a URL, a starting rank of 1, and a bag with a tuple for each link found at that URL:
Even though control flow is done via a Python script, it can still be run using Pig’s bin/ pig script.
This allows you to use these scripts with systems that expect to invoke a Pig Latin script.
It also allows Pig to include UDFs from this file automatically and to give correct line numbers for error messages.
In order to use the Pig class and related objects, the code must first import them into the Python script:
Compile Calling the static method Pig.compile causes Pig to do an initial compilation of the code.
Because we have not bound the variables yet, this check cannot completely verify the script.
Type checking and other semantic checking is not done at this phase—only the syntax is checked.
The example code was graciously provided by Julien Le Dem.
The syntax for these parameters is the same as for parameter substitution.
However, Pig expects these to be supplied by the control flow script when bind is called.
There are three other compilation methods in addition to the one shown in this example.
This name can be used in other Pig Latin code blocks to import this block:
These take the same arguments as compile, but they expect the script argument to refer to a file containing the script, rather than the script itself.
Bind Once your script has been compiled successfully, the next step is to bind variables in the control flow to variables in Pig Latin.
In our example script this is done by providing a map to the bind call.
The keys are the name of the variables in Pig Latin.
The values in the following example are literal string values that are updated as the script progresses.
As is shown in this script, a single Pig object can be bound multiple times.
A compile is necessary only on the first pass, with different values being bound to it each time.
In our example, bind is given a mapping of the variables to bind.
If all of your Python variables and Pig Latin variables have the same name, you can call bind with no arguments.
This will cause bind to look in the Python context for variables of the same name as the parameters in Pig and use them.
If it cannot find appropriate variables, it will throw an error.
We could change our example script to look like this:
Binding Multiple Sets of Variables Our example page rank script binds its compiled Pig Latin to different variables multiple times in order to iterate over the data.
Each of these jobs is run separately, as is required by the iterative nature of calculating page rank.
However, sometimes you want to run a set of jobs together; for example, consider calculating census data from countries all over the world.
You want to run the same Pig Latin for each country, but you do not want to run them separately.
There is no point in having a massively parallel system.
You want to tell Pig to take your script and run it against input from all the countries at the same time.
There is a form of bind that provides this capability.
Instead of taking a map of parameters, it takes a list of maps of parameters.
It still returns a single BoundScript object, but when run is called on this object, all of the separate instantiations of the script will be run together:
Run Once we have our BoundScript object, we can call runSingle to run it.
This tells Pig to run a single Pig Latin script.
This is appropriate when you have bound your script to just one set of variables.
This object allows you to get your results and examine what happened in your script, including status, error codes and messages if there was an error, and statistics about the run itself.
Table 9-1 summarizes the more important methods available for PigStats.
This will try to pick the most relevant error message that was returned, most likely the last.
For example, if you wrote output to a file on HDFS, this will return the filename.
As seen in the example, the OutputStats object returned by result() can be used to get an iterator on the result set.
With this you can iterate through the tuples of your data, processing them in your Python script.
Standard Tuple methods such as get() can be used to inspect the contents of each record.
Based on the results read in the iterator, your Python script can decide whether to cease iteration and declare success, raise an error, or continue with another iteration.
For this iterator to work, the store function you use to store results from the alias must also be a load function.
Pig attempts to use the same class to load the results as was used to store it.
Running Multiple Bindings If you bound your Pig object to a list of maps of parameters, rather than call runSin gle, you should call run.
This will cause Pig to start a thread for each binding and run it.
All these jobs will be submitted to Hadoop at the same time, making use of Hadoop’s parallelism.
The PigStats objects are guaranteed to be in the same order in the list as in the maps of bound variables passed to bind.
Thus the results of the first binding map are in the first position of the PigStats list, etc.
Utility Methods In addition to the compile, bind, and run methods presented so far, there are also utility methods provided by Pig and BoundScript.
Filesystem operations can be done by calling the static method Pig.fs.
The string passed to it should be a valid string for use in the Grunt shell (see Chapter 3)
The return code from running the shell command will be returned.
You can use register, define, and set in your compiled Pig Latin statements as you do in nonembedded Pig Latin.
However, you might wish to register a JAR, define a function alias, or set a value that you want to be effective through all your Pig Latin code blocks.
In these cases you can use the static methods of Pig described in Table 9-2
The registers, defines, and sets done by these methods will affect all Pig Latin code compiled after they are called:
Once a script has been bound and a BoundScript returned, in addition to running the script you can also call describe, explain, or illustrate.
These do exactly what they would if they were in a nonembedded Pig Latin script.
However, they do not return the resulting output to your script; instead, it is dumped to the standard out.
These operators are intended for use in debugging rather than for returning data directly to your script.)
It is time to turn our attention to how you can extend Pig.
So far we have looked at the operators and functions Pig provides.
But Pig also makes it easy for you to add your own processing logic via User Defined Functions (UDFs)
It will also cover how to write filter functions, UDFs that can be used as part of filter statements.
UDFs are powerful tools, and thus the interfaces are somewhat complex.
In designing Pig, our goal was to make easy things easy and hard things possible.
So, the simplest UDFs can be implemented in a single method, but you will have to implement a few more methods to take advantage of more advanced features.
Throughout this chapter we will use several running examples of UDFs.
Some of these are built-in Pig UDFs, which can be found in your Pig distribution at src/org/apache/ pig/builtin/
The others can be found on GitHub with the other example UDFs, in the directory udfs.
Writing an Evaluation Function in Java Pig and Hadoop are implemented in Java, and so it is natural to implement UDFs in Java.
This allows UDFs access to the Hadoop APIs and to many of Pig’s facilities.
Before diving into the details, it is worth considering names.
Pig locates a UDF by looking for a Java class that exactly matches the UDF name in the script.
There is not an accepted standard on whether UDF names should be all uppercase, camelCased (e.g., MyUdf), or all lowercase.
In 0.9, eval funcs can also be written in JavaScript, though this is experimental and has not yet been fully tested.
Keep in mind that, whatever you choose, you and all of the users of your UDF will have a better user experience if you make the name short, easy to remember, and easy to type.
Where Your UDF Will Run Writing code that will run in a parallel system presents challenges.
A separate instance of your UDF will be constructed and run in each map or reduce task.
It is not possible to share state across these instances because they may not all be running at the same time.
There will be only one instance of your UDF per map or reduce task, so you can share state within that context.† When writing code for a parallel system, you must remember the power of parallelism.
Operations that are acceptable in serial programs may no longer be advisable.
Consider a UDF that, when it first starts, connects to a database server to download a translation table.
In a serial or low-parallelism environment, this is a reasonable approach.
But if you have 10,000 map tasks in your job and they all connect to your database at once, you will most likely hear from your DBA, and the conversation is unlikely to be pleasant.
In addition to an instance in each task, Pig will construct an instance of your UDF on the frontend during the planning stage.
One, it wants to test early that it can construct your UDF; it would rather fail during planning than at runtime.
Two, as we will cover later in this chapter, it will ask your UDF some questions about schemas and types it accepts as part of the execution planning.
It will also give your UDF a chance to store information it wants to make available to the instances of itself that will be run in the backend.
It is parameterized by the return type of your UDF.
It takes one record and returns one result, which will be invoked for every record that passes through your execution pipeline.
As input it takes a tuple, which contains all of the fields the script passes to your UDF.
For simple UDFs, this is the only method you need to implement.
The following code gives an example of a UDF that raises an integer to an integral power and returns a long result:
It can be used in a Pig Latin script as Pow(x, y), where x and y * are both expected to be ints.
Assuming there is one instance of your UDF in the script.
Each reference to a UDF in a script becomes a separate instance on the backend, even if they are placed in the same map or reduce task.
Because the group operator returns a record for each group, with a bag containing all the records in that group, your eval func still takes one record and returns one record.
As an example of this, let’s take a look at the implementation of exec in Pig’s COUNT function.
Some of the errorhandling code has been removed for ease of reading:
Just as UDFs can take complex types as input, they also can return complex types as output.
You could, for example, create a SetIntersection UDF that took two bags as input and returned a bag as output.
UDFs can also be handed the entire record by passing * to the UDF.
You might expect that in this case the input Tuple argument passed to the UDF would contain all the fields passed into the operator the UDF is in.
Instead, it contains one field, which is a tuple that contains all those fields.
In this case, myudf.exec will get a tuple with one field, which will be a tuple that will have three fields: x, y, and z.
Evaluation functions and other UDFs are exposed to the internals of how Pig represents data types.
For most of these types, you construct the appropriate Java objects in the normal way.
However, this is not the case for tuples and bags.
These are interfaces, and they do not have direct constructors.
Instead, you must use factory classes for each of these.
This was done so that users and developers could build their own implementations of tuple and bag and instruct Pig to use them.
TupleFactory is an abstract singleton class that you must use to create tuples.
You can also configure which TupleFactory is used, since users who provide their own tuples will need to provide their own factory to produce them.
You can now create new tuples with either newTuple() or newTuple(int size)
Whenever possible you should use the second method, which preallocates the tuple with the right number of fields.
This avoids the need to dynamically grow the tuple later and is much more efficient.
The method creates a tuple with size number of fields, all of which are null.
You can now set the fields using the Tuple’s set(int fieldNum, Object val) method.
As an example, we can look at how the example load function we will build in the next chapter creates tuples:
If you do not know the number of fields in the tuple when it is constructed, you can use newTuple()
You can then add fields using Tuple’s append(Object val) method, which will append the field to the end of the tuple.
To read data from tuples, use the get(int fieldNum) method.
This returns a Java Object because the tuple does not have a schema instance and does not know what type this field is.
Similar to tuples, BagFactory must be used to construct bags.
You can then add tuples to it as you construct them using DataBag’s add(Tuple t) method.
Again we can look at Json Loader to see an example of constructing bags:
To read data from a bag, use the iterator provided by iterator()
This also implements Java’s Iterable, so you can use the construct for (Tuple t : bag)
Bags make the assumption that once data is being read from them, no new data will be written to them.
Their implementation of how they spill and reread data depends on this assumption.
So once you call iterator, you should never call add again on the same bag.
Input and Output Schemas Pig typechecks a script before running it.
EvalFunc includes a method to allow you to turn on type checking for your UDF as well, both for input and output.
When your UDF returns a simple type, Pig uses Java reflection to determine the return type.
However, because exec takes a tuple, Pig has no way to determine what input you expect your UDF to take.
You can check this at runtime, of course, but your development and testing will go more smoothly if you check it at compile time instead.
For example, we could use the Pow UDF example in the previous section like this:
Runtime exceptions like this are particularly expensive in Hadoop, both because scheduling can take a while on a busy cluster and because each task is tried three times before the whole job is declared a failure.
Let’s fix this UDF so it checks up front that it was given reasonable input.
The method to declare the input your UDF expects is outputSchema.
The method is called this because it returns the schema that describes the UDF’s output.
If your UDF does not override this method, Pig will attempt to ascertain your return type from the return type of your implementation of EvalFunc, and pass your UDF whatever input the script indicates.
If your UDF does implement this method, Pig will pass it the schema of the input that the script has indicated to pass into the UDF.
This is also your UDF’s opportunity to throw an error if it receives an input schema that does not match its expectations.
An implementation of this method for Pow looks like this:
Pig’s Schema is a complicated class, and we will not delve into all its complexities here.
The following summary will be enough to help you build your own schemas for out putSchema.
At its core, Schema is a list of FieldSchemas and a mapping of aliases to FieldSchemas.
This member is nonnull only when the type is complex.
In the case of tuples, it defines the schema of the tuple.
In the case of bags, it defines the schema of the tuples in the bag.
Starting in 0.9, if a schema is present for a map, it indicates the data type of values in the map.
Map of alias names to field schemas, so that lookup can be done by alias.
If this is a tuple itself, it can have a schema.
As mentioned earlier, when your UDF returns a scalar type, Pig can use reflection to figure out that return type.
When your UDF returns a bag or a tuple, however, you will need to implement outputSchema if you want Pig to understand the contents of that bag or tuple.
Error Handling and Progress Reporting Our previous examples have given some hints of how to deal with errors.
When your UDF encounters an error, you have a couple of choices on how to handle it.
The most common case is to issue a warning and return a null.
This tells Pig that your UDF failed and its output should be viewed as unknown.‡ We saw an example of this when the Pow function detected overflow:
These warnings are aggregated by Pig and reported to the user at the end of the job.
Warning and returning null is convenient because it allows your job to continue.
When you are processing billions of records, you do not want your job to fail because one record out of all those billions had a chararray where you expected an int.
Given enough data, the odds are overwhelming that a few records will be bad, and most calculations will be fine if a few data points are missing.
For errors that are not tolerable, your UDF can throw an exception.
If Pig catches an exception, it will assume that you are asking to stop everything, and it will cause the task to fail.
If any particular task fails three times, Hadoop will not restart it again.
Instead, it will kill all the other tasks and declare the job a failure.
When you have concluded that you do need an exception, you should also issue a log message so that you can read the task logs later and get more context to determine what happened.
Hadoop prints any log messages into logfiles on the task machine, which are available from the JobTracker UI.
You can also print info messages into the log to help you with debugging.
In addition to error reporting, some UDFs will need to report progress.
Hadoop listens to its tasks to make sure they are making progress.
Recall that in Pig null means that the value is unknown, not that it is 0 or unset.
It then kills the task if it is still running, cleans up its resources, and restarts the task elsewhere.
However, if you have a UDF that is very compute-intensive and a single invocation of it might run for more than five minutes, you should also report progress.
Constructors and Passing Data from Frontend to Backend Our discussion so far assumes that your UDF knows everything it needs to know at development time.
Consider a UDF that needs to read a lookup table from HDFS.
You would like to be able to declare the filename when you use the UDF.
You can do that by defining a nondefault constructor for your UDF.
By default, EvalFuncs have a no-argument constructor, but you can provide a constructor that takes one or more String arguments.
As an example, we will look at a new UDF, MetroResolver.
This UDF takes a city name as input and returns the name of the larger metropolitan area that city is part of.
Based on which country the input cities are in, a different lookup table will be needed.
The name of a file in HDFS that contains this lookup table can be provided as a constructor argument.
The class declaration, members, and constructor for our UDF look like this:
The UDF can now be invoked in a Pig Latin script like this:
However, our UDF is not yet complete because we have not constructed the lookup table.
It does not make sense to construct it in the constructor, because the constructor will be invoked on both the frontend and backend.
There are forms of dark magic that will allow the UDF to figure out whether it is being invoked on the frontend or backend, but I cannot recommend them, because they are not guaranteed to work the same between releases.
It is much better to do the lookup table construction in a method that we know will be called only in the backend.
EvalFunc does not provide an initialize method that it calls on the backend before it begins processing.
You can work around this by keeping a flag to determine whether you have initialized your UDF in a given task.
The exec function for MetroResolver does this by tracking whether lookup is null:
This initialization section handles opening the file and reading it.
In order to open the file, it must first connect to HDFS.
This method in turn needs a JobConf object, which is where Hadoop stores all its job information.
The JobConf object can be obtained using UDFContext, which we will cover in more detail later.
Note that obtaining JobConf in this way works only on the backend, as no job configuration exists on the frontend.
Once we are connected to HDFS, we open the file and read it as we would any other file.
It is parsed into two fields and put into the hash table.
All subsequent calls to exec will just be lookups in the hash table.
Our MetroResolver UDF opens and reads its lookup file from HDFS, which you will often want.
However, having hundreds or thousands of map tasks open the same file on HDFS at the same time puts significant load on the NameNode and the DataNodes that host the file’s blocks.
To avoid this situation, Hadoop provides the distributed cache, which allows users to preload HDFS files locally onto the nodes their tasks will run on.
Let’s write a second version of MetroResolver that uses the distributed cache.
Beginning in version 0.9, EvalFunc provides a method getCacheFiles that is called on the frontend.
Your UDF returns a list of files from this method that it wants in the distributed cache.
You should place any files in your working directory rather than using an absolute path.
Constructor arguments work as a way to pass information into your UDF, if you know the data at the time the script is written.
But some information you want to pass from frontend to backend cannot be known when the script is run, or it might not be accessible in String form on the command line.
An example is collecting properties from the environment and passing them.
To allow UDFs to pass data from the frontend to the backend, starting in version 0.8, Pig provides a singleton class, UDFContext.
Your UDF obtains a reference to it by calling getUDFContext.
We have already seen that UDFs can use UDFContext to obtain a copy of the JobConf.
Beginning in version 0.9, UDFContext also captures the System properties on the client and carries them to the backend.
UDFContext also provides mechanisms for you to pass a properties object explicitly for your UDF.
You can either pass a properties object for all UDFs of the same class or pass a specific object for each instance of your UDF.
This will return a Prop erties object that is a reference to a properties object kept by UDFContext.
UDFContext will capture and transmit to the backend any changes made in this object.
You can call this in outputSchema, which is guaranteed to be called in the frontend.
When you want to read the data, call the same method again in your exec method.
When using the object in the exec method, keep in mind that any changes made to the returned Prop erties will not be transmitted to other instances of the UDF on the backend, unless you happen to have another instance of the same UDF in the same task.
This is a mechanism for sending information from the frontend to the backend, not between instances in the backend.
Sometimes you will want to transmit different data to different instances of the same UDF.
By different instances I mean different invocations in your Pig Latin script, not.
The constructor arguments to your UDF are a good candidate to be passed as the array of String.
This allows each instance of the UDF to differentiate itself.
If your UDF does not take constructor arguments, or all arguments have the same value, you can add one unused argument that is solely to distinguish separate instances of the UDF.
Consider a UDF that has its own properties file, which might be useful if you want to pass different properties to different UDFs, or if you have many UDF-specific properties that you want to change without changing your Pig properties file.
Let’s write a second version of the stock analyzer UDF that we used in Chapter 6:
Make sure the input isn't null and is of the right size.
Overloading UDFs Sometimes you want different UDF implementations depending on the data type the UDF is processing.
For example, MIN(long) should return a long, whereas MIN(int) should return an int.
If this method returns a null, Pig will use the current UDF.
To provide a list of alternate UDFs based on the input types, this function returns a list of FuncSpecs.
A FuncSpec is a Pig class that describes a UDF.
Each of these FuncSpecs describes a set of expected input arguments and the UDF, as a Java class, that should be used to handle them.
Pig’s typechecker will use this list to determine which Java class to place in the execution pipeline (more on this later)
Pig’s typechecker goes through a set of steps to determine which FuncSpec is the closest match, and thus which Java class it should place in this job’s execution pipeline.
At each step, if it finds a match, it uses that match.
If it finds more than one match at a given step, it will return an error that gives all the matching possibilities.
If it finds no match in the whole list, it will also give an error.
As an example of this, let’s consider another version of the Pow UDF we built above.
It takes either two longs or two doubles as input.
In the typechecker’s search for the best UDF to use, step one is to look for an exact match, where all of the expected input declared by the UDF is matched by the actual input passed in Pig Latin.
Step two is to look for bytearrays that are passed into the UDF and see whether a match can be made by inserting casts for those bytearrays.
For example, Pig will rewrite Pow(x, Pig when all arguments are bytearrays, because bytearrays can be cast to any type.
Pow(x, y), where both x and y are bytearrays, results in an error message:
Step three is to look for an implicit cast that will match one of the provided schemas.
Implicit casting of numeric types goes from int to long to float to double, and by closest I mean the cast that requires the least steps in that list.
Step four is to look for a working combination of steps two and three, bytearray casts plus implicit casts.
If after all these steps Pig still has not found a suitable method, it will fail and say it cannot determine which method to use.
Memory Issues in Eval Funcs Some operations you will perform in your UDFs will require more memory than is available.
As an example, you might want to build a UDF that calculates the cumulative sum of a set of inputs.
This will return a bag of values because, for each input, it needs to return the intermediate sum at that input.
Pig’s bags handle spilling data to disk automatically when they pass a certain size threshold or when only a certain amount of heap space remains.
Spilling to disk is expensive and should be avoided whenever possible.
But if you must store large amounts of data in a bag, Pig will manage it.
Bags are the only Pig data type that know how to spill.
Bags that are too large to fit in memory can still be referenced in a tuple or a map; this will not be counted as those tuples or maps not fitting into memory.
Algebraic Interface I have already mentioned in a number of other places that there are significant advantages to using Hadoop’s combiner whenever possible.
It lowers skew in your reduce tasks, as well as the amount of data sent over the network between map and reduce tasks.
Use of the combiner is interesting when you are working with sets of data, usually sets you intend to aggregate and then compute a single or small set of values for.
There are two classes of functions that fit nicely into the combiner: distributive and algebraic.
A function is said to be algebraic if it can be divided into initial, intermediate, and final functions (possibly different from the initial function), where the initial function is applied to subsets of the input set, the intermediate function is applied to results of the initial function, and the final function is applied to all of the results of the intermediate function.
A distributive function is a special case of an algebraic function, where the initial, intermediate, and final functions are all identical to the original function.
An EvalFunc can declare itself to be algebraic by implementing the Java interface Alge braic.
Algebraic provides three methods that allow your UDF to declare Java classes that implement its initial, intermediate, and final functionality.
Each of these methods returns a name of a Java class, which should itself implement EvalFunc.
Pig will use these UDFs to rewrite the execution of your script.
The execution pipeline for this script would initially look like: Map.
After being rewritten to use the combiner, it would look like: Map.
As an example, we will walk through the implementation for COUNT.
Each of these referenced classes is a static internal class in COUNT.
Even though the initial function is guaranteed to receive only one record in its input, that record will match the schema of the original function.
So, in the case of COUNT, it will be a bag.
Thus, this initial method determines whether there is a nonnull record in that bag.
The return type of the initial function is a tuple.
The contents of that tuple are entirely up to you as the UDF implementer.
In this case, the initial returns a tuple with one long field.
The input to the intermediate function is a bag of tuples that were returned by the initial function.
The intermediate function may be called zero, one, or many times.
So, it needs to output tuples that match the input tuples it expects.
As we now want to sum the previous counts, this function implements SUM rather than COUNT.
The final function is called in the reducer and is guaranteed to be called only once.
Its input type is a bag of tuples that both the initial and intermediate implementations return.
Its return type needs to be the return type of the original UDF, which in this case is long.
In COUNT’s case, this is the same operation as the intermediate because it sums the intermediate sums:
Implementing Algebraic does not guarantee that the algebraic implementation will always be used.
Pig chooses the algebraic implementation only if all UDFs in the same foreach statement are algebraic.
This is because our testing has shown that using the combiner with data that cannot be combined significantly slows down the job.
And there is no way in Hadoop to route some data to the combiner (for algebraic functions) and some straight to the reducer (for nonalgebraic)
This means that your UDF must always implement the exec method, even if you hope it will always be used in algebraic mode.
An additional motivation is to implement algebraic mode for your UDFs when possible.
Accumulator Interface Some calculations cannot be done in an algebraic manner.
In particular, any function that requires its records to be sorted before beginning is not algebraic.
But many of these methods still do not need to see their entire input at once; they can work on subsets of the data as long as they are guaranteed it is all available.
This means Pig does not have to read all of the records into memory at once.
Instead, it can read a subset of the records and pass them to the UDF.
To handle these cases, Pig provides the Accumu lator interface.
Rather than calling a UDF once with the entire input set in one bag, Pig will call it multiple times with a subset of the records.
When it has passed all the records in, it will then ask for a result.
Finally, it will give the UDF a chance to reset its state before passing it records for the next group:
As mentioned earlier, one major class of methods that can use the accumulator are those that require sorted input, such as session analysis.
Usually such a UDF will want records within the group sorted by timestamp.
As an example, let’s say you have log data from your web servers that includes the user ID, timestamp, and the URL the user viewed, and you want to do session analysis on this data:
Pig can move the sort done by the order statement to Hadoop, to be done as part of the shuffle phase.
Thus, Pig is still able to read a subset of records at a time from Hadoop and pass those directly to SessionAnalysis.
This important optimization allows accumulator UDFs to work with sorted data.
Whenever possible, Pig will choose to use the algebraic implementation of a UDF over the accumulator.
This is because the accumulator helps avoid spilling records to disk, but it does not reduce network cost or help balance the reducers.
If all UDFs in a foreach implement Accumulator and at least one does not implement Algebraic, Pig will use the accumulator.
If at least one does not use the accumulator, Pig will not use the accumulator.
This is because Pig already has to read the entire bag into memory to pass to the UDF that does not implement the accumulator, so there is no longer any value in the accumulator.
Python UDFs Pig and Hadoop are implemented in Java, so Java is a natural choice for UDFs as well.
For simple UDFs of only a few lines, the cycle of write, compile, package into a JAR, and deploy is an especially heavyweight process.
To allow users to write UDFs in scripting languages, we added support for UDFs in Python to Pig 0.8
We did it in such a way that supporting any scripting language that compiles down to the JVM requires only a few hundred lines of code.
We hope to keep expanding the supported languages in the future.
Python UDFs consist of a single function that is used in place of the exec method of a Java function.
The benefit is that Python UDFs can be compiled to Java bytecode and run with relatively little performance penalty.
In this section we will focus on writing the UDFs themselves.
Let’s take a look at the production UDF we used in that earlier section:
For these cases you can provide a schema function that will define your schema.
Let’s write a Python UDF that squares a number, always returning a number of the same type:
The input to the schema function is in the same format as the one specified in @output Schema: colname:type.
Its output is expected to be in the same format.
Because there will be no load function for the value, Pig will not be able to cast it to any other type, so it will be worthless for anything but store or dump.
In order to pass data between Java and Python, Pig must define a mapping of types.
Table 10-1 describes the mapping between Pig and Python types.
Writing Filter Functions Filter functions are evaluation functions that return a Boolean value.
Pig does not support Boolean as a full-fledged type, so filter functions cannot appear in statements such as foreach where the results are output to another operator.
Code has been checked in that allows Pig to determine the dependency tree for your Python code, fetch all the needed modules, and ship them as part of the job.
As of this writing, it has not yet been released.
We will now consider some of the more complex and most critical parts of Pig: data input and output.
Hadoop’s massive parallelism and movement of processing to the data mitigates but does not remove this.
Having efficient methods to load and store data is therefore critical.
Pig provides default load and store functions for text data and for HBase, but many users find they need to write their own load and store functions to handle the data formats and storage mechanisms they use.
As with evaluation functions, the design goal for load and store functions was to make easy things easy and hard things possible.
Also, we wanted to make load and store functions a thin wrapper over Hadoop’s InputFormat and OutputFormat.
The intention is that once you have an input format and output format for your data, the additional work of creating and storing Pig tuples is minimal.
In the same way evaluation functions were implemented, more complex features such as schema management and projection push down are done via separate interfaces to avoid cluttering the base interface.
This chapter will cover only the interfaces for 0.7 and later releases.
One other important design goal for load and store functions is to not assume that the input sources and output sinks are HDFS.
In the examples throughout this book, A = load 'foo'; has implied that foo is a file, but there is no need for that to be the case.
It could be an HDFS file, an HBase table, a database JDBC connection string, or a web service URL.
Because reading from HDFS is the most common case, many defaults and helper functions are provided for this case.
In this chapter we will walk through writing a load function and a store function for JSON data on HDFS, JsonLoader and JsonStorage, respectively.
They use the Jackson JSON library, which is included in your Pig distribution.
However, the Jackson JAR is not shipped to the backend by Pig, so when using these UDFs in your script, you will need to register the Jackson JAR in addition to the acme examples JAR:
These UDFs will serve as helpful examples, but they will not cover all of the functionality of load and store functions.
For those sections not shown in these examples, we will look at other existing load and store functions.
Load Functions Pig’s load function is built on top of a Hadoop InputFormat, the class that Hadoop uses to read data.
InputFormat serves two purposes: it determines how input will be split between map tasks, and it provides a RecordReader that produces key-value pairs as input to those map tasks.
The load function takes these key-value pairs and returns a Pig Tuple.
This is an abstract class, which allows it to provide helper functions and default implementations.
Load functions’ operations are split between Pig’s frontend and backend.
On the frontend, Pig does job planning and optimization, and load functions participate in this in several ways that we will discuss later.
On the backend, load functions get each record from the RecordReader, convert it to a tuple, and pass it on to Pig’s map task.
Load functions also need to be able to pass data between the frontend and backend invocations so they can maintain state.
Pig needs to know which InputFormat to use for reading your input.
It calls getInput Format to get an instance of the input format.
It gets an instance rather than the class itself so that your load function can control the instantiation: any generic parameters, constructor arguments, etc.
For our example load function, this method is very simple.
It uses TextInputFormat, an input format that reads text data from HDFS files:
Pig communicates the location string provided by the user to the load function via setLocation.
This method is called on both the frontend and backend, possibly multiple times.
Thus you need to take care that this method does not do anything that will cause problems if done more than one time.
Your load function should communicate the location to its input format.
For example, JsonLoader passes the filename via a helper method on FileInputFormat (a superclass of TextInputFormat):
The Hadoop Job is passed along with the location because that is where input formats usually store their configuration information.
For MapReduce jobs, which always have only one input, this works.
For Pig jobs, where the same input format might be used to load multiple different inputs (such as in the join or union case), one instance of the input path will overwrite another in the Job object.
To work around this, Pig remembers the location in an input-specific parameter and calls set Location again on the backend so that the input format can get itself set up properly before reading.
For files on HDFS, the location provided by the user might be relative rather than absolute.
To deal with this, Pig needs to resolve these to absolute locations based on the current working directory at the time of the load.
But Pig cannot assume it understands how to turn a relative path into an absolute path, because it does not know what that input is.
It could be an HDFS path, a database table name, etc.
Before calling setLocation, Pig passes the location string to relative ToAbsolutePath to do any necessary conversion.
Because most loaders are reading from HDFS, the default implementation in LoadFunc handles the HDFS case.
If your loading will never need to do this conversion, it should override this method and return the location string passed to it.
Some Pig functions, such as PigStorage and HBaseStorage, load data by default without understanding its type information, and place the data unchanged in DataByteArray objects.
At a later time, when Pig needs to cast that data to another type, it does not.
Therefore, it relies on the load function to provide a method to cast from bytearray to the appropriate type.
Pig determines which set of casting functions to use by calling getLoadCaster on the load function.
This should return either null, which indicates that your load function does not expect to do any bytearray casts, or an implementation of the LoadCaster interface, which will be used to do the casts.
Our example loader returns null because it provides typed data based on the stored schema and, therefore, does not expect to be casting data.
Any bytearrays in its data are binary data that should not be cast.
Passing Information from the Frontend to the Backend As with evaluation functions, load functions can make use of UDFContext to pass information from frontend invocations to backend invocations.
One significant difference between using UDF Context in evaluation and load functions is determining the instance-specific signature of the function.
In evaluation functions, constructor arguments were suggested as a way to do this.
For load functions, the input location usually will be the differentiating factor.
However, LoadFunc does not guarantee that it will call setLocation before other methods where you might want to use UDFContext.
It provides an instance-unique signature that you can use when calling getUDFProperties.
This method is guaranteed to be called before any other methods on LoadFunc in both the frontend and backend.
Your UDF can then store this signature and use it when getting its property object:
It is therefore the best candidate for storing needed information to UDF Context.
You might need to check that the data you are writing is available and nonnull to avoid overwriting your values when setLocation is called on the backend.
Backend Data Reading On the backend, your load function takes the key-value pairs produced by its input format and produces Pig Tuples.
Before reading any data, Pig gives your load function a chance to set itself up by calling prepareToRead.
This is called in each map task and passes a copy of the RecordReader, which your load function will need later to read records from the input.
RecordReader is a class that InputFormat uses to read records from an input split.
Pig obtains the record reader it passes to prepareToRead by calling getRecordReader on the input format that your store function returned from getInputFormat.
Pig also passes an instance of the PigSplit that contains the Hadoop InputSplit corresponding to the partition of input this instance of your load function will read.
If you need split-specific information, you can get it from here.
Our example loader, beyond storing the record reader, also reads the schema file that was stored into UDFContext in the frontend so that it knows how to parse the input file.
Finally, it creates a JsonFactory object that is used to generate a parser for each line:
Now we have reached the meat of your load function, reading records from its record reader and returning tuples to Pig.
Pig will call getNext and place the resulting tuple into its processing pipeline.
It will keep doing this until getNext returns a null, which indicates that the input for this split has been fully read.
Pig does not copy the tuple that results from this method, but instead feeds it directly to its pipeline to avoid the copy overhead.
This means this method cannot reuse objects, and instead must create a new tuple and contents for each record it reads.
On the other hand, record readers may choose to reuse their key and value objects from record to.
So, before writing a loader that tries to be efficient and wraps the keys and values from the record reader directly into the tuple to avoid a copy, you must make sure you understand how the record reader is managing its data.
Our sample load function’s implementation of getNext reads the value from the Hadoop record (the key is ignored), constructs a JsonParser to parse it, parses the fields, and returns the resulting tuple.
If there are parse errors, it does not throw an exception.
Instead, it returns a tuple with null fields where the data could not be parsed.
This prevents bad lines from causing the whole job to fail.
Warnings are issued so that users can see which records were ignored:
Additional Load Function Interfaces Your load function can provide more complex features by implementing additional interfaces.
Many data storage mechanisms can record the schema along with the data.
Pig does not assume the ability to store schemas, but if your storage can hold the schema, it can be very useful.
This frees script writers from needing to specify the field names and types as part of the load operator in Pig Latin.
This is user-friendly and less error-prone, and avoids the need to rewrite scripts when the schema of your data changes.
If Pig understands this partitioning, it can load only those partitions that are needed for a particular script.
Both of these functions are enabled by implementing the LoadMetadata interface.
It is passed the location string the user provides as well as the Hadoop Job object, in case it needs information in this object to open the schema.
It is expected to return a ResourceSchema, which represents the data that will be returned.
Resource Schema is very similar to the Schema class used by evaluation functions.
Our implementation of getSchema reads this file and also serializes the schema into UDFContext so that it is available on the backend:
Once your loader implements getSchema, load statements that use your loader do not need to declare their schemas in order for the field names to be used in the script.
For example, if we had data with a schema of user:chararray, age:int, gpa:double, the following Pig Latin will compile and run:
Pig does not yet make use of statistics in job planning; this method is for future use.
A file in the same directory that is not a part file.
MapReduce’s FileInputFormat knows to ignore them when reading input for a job.
Some types of storage partition their data, allowing you to read only the relevant sections for a given job.
The LoadMetadata interface also provides methods for working with partitions in your data.
In order for Pig to request the relevant partitions, it must know how the data is partitioned.
If this returns a null or the LoadMetadata interface is not implemented by your loader, Pig will assume it needs to read the entire input.
Pig expects getPartitionKeys to return an array of strings, where each string represents one field name.
Those fields are the keys used to partition the data.
Pig will look for a filter statement immediately following the load statement that includes one or more of these fields.
If such a statement is found, it will be passed to setPartitionFilter.
Pig will call getPartitionKeys, and HCatLoader will return two key names, date and colo.
The one exception to this is fields used in eval funcs or filter funcs.
Pig assumes that loaders do not understand how to invoke UDFs, so Pig will not push these expressions.
Our example loader works on file data, so it does not implement getPartitionKeys or setPartitionFilter.
This is possible when the expressions are connected by and but not when they are connected by or.
Often a Pig Latin script will need to read only a few fields in the input.
Some types of storage formats store their data by fields instead of by records (for example, Hive’s RCFile)
For these types of formats, there is a significant performance gain to be had by loading only those fields that will be used in the script.
Even for record-oriented storage formats, it can be useful to skip deserializing fields that will not be used.
As part of its optimizations, Pig analyzes Pig Latin scripts and determines what fields in an input it needs at each step in the script.
It uses this information to aggressively drop fields it no longer needs.
If the loader implements the LoadPushDown interface, Pig can go a step further and provide this information to the loader.
Once Pig knows the fields it needs, it assembles them in a RequiredFieldList and passes that to pushProjection.
In the load function’s reply, it indicates whether it can meet the request.
If the Boolean is true, Pig will assume that only the required fields are being returned from getNext.
If it is false, Pig will assume that all fields are being returned by getNext, and it will handle dropping the extra ones itself.
The RequiredField class used to describe which fields are required is slightly complex.
Beyond allowing a user to specify whether a given field is required, it provides the ability to specify which subfields of that field are required.
For example, for maps, certain keys can be listed as required.
For tuples and bags, certain fields can be listed as required.
Load functions that implement LoadPushDown should not modify the schema object returned by getSchema.
This should always be the schema of the full input.
Pig will manage the translation between the schema having all of the fields and the results of getNext having only some.
Store Functions Pig’s store function is, in many ways, a mirror image of the load function.
It takes Pig Tuples and creates key-value pairs that its associated output format writes to storage.
StoreFunc is an abstract class, which allows it to provide default implementations for some methods.
However, some functions implement both load and store functionality; PigStorage is one example.
Because Java does not support multiple inheritance, the interface StoreFuncInterface is provided.
These dual load/store functions can implement this interface rather than extending StoreFunc.
Store function operations are split between the frontend and backend of Pig.
Store functions have an opportunity at this time to check that a valid schema is being used and set up the storage location.
On the backend, store functions take a tuple from Pig, convert it to a key-value pair, and pass it to a Hadoop RecordWriter.
Store functions can pass information from frontend invocations to backend invocations via UDFContext.
Store Function Frontend Planning Store functions have three tasks to fulfill on the frontend:
Pig calls getOutputFormat to get an instance of the output format that your store function will use to store records.
This method returns an instance rather than the classname or the class itself.
This allows your store function to control how the class is instantiated.
This is an output format that stores text data in HDFS.
We have to instantiate this with a key of LongWrita ble and a value of Text to match the expectations of TextInputFormat:
Pig calls setStoreLocation to communicate the location string the user provides to your store function.
Given the Pig Latin store Z into 'output';, “output” is the location string.
This method, called on both the frontend and the backend, could be called multiple times; consequently, it should not have any side effects that will cause a problem if this happens.
Your store function will need to communicate the location to its output format.
Our example store function uses the FileOutputFormat utility function setOutputPath to do this:
The Hadoop Job is passed to this function as well.
Most output formats store the location information in the job.
Pig calls setStoreLocation on both the frontend and backend because output formats usually store their location in the job, as we see in our example store function.
This works for MapReduce jobs, where a single output format is guaranteed.
But due to the split operator, Pig can have more than one instance of the same store function in a job.
Pig avoids this by keeping output-specific information and calling setStoreLocation again on the backend so that it can properly configure the output format.
For HDFS files, the user might provide a relative path.
Pig needs to resolve these to absolute paths using the current working directory at the time the store is called.
For store functions writing to HDFS, the default implementation in StoreFunc handles the conversion.
If you are writing a store function that does not use file paths (e.g., HBase), you should override this method to return the string it is passed.
As part of frontend planning, Pig gives your store function a chance to check the schema of the data to be stored.
If you are storing data to a system that expects a certain schema.
Oddly enough, this method returns a void rather than a Boolean.
So if you detect an issue with the schema, you must throw an IOException.
Our example store function does not have limitations on the schemas it can store.
However, it uses this function as a place to serialize the schema into UDFContext so that it can be used on the backend when writing data:
Our example store function stores the signature in a member variable for later use:
Writing Data During backend processing, the store function is first initialized, and then takes Pig tuples and converts them to key-value pairs to be written to storage.
Pig calls your store function’s prepareToWrite method in each map or reduce task before writing any data.
This call passes a RecordWriter instance to use when writing data.
RecordWriter is a class that OutputFormat uses to write individual records.
Pig will get the record writer it passes to your store function by calling getRecordWriter on the output format your store function returned from getOutputFormat.
Your store function will need to keep this reference so that it can be used in putNext.
The example store function JsonStorage also uses this method to read the schema out of the UDFContext.
Finally, it creates a Json Factory for use in putNext:
Store the record writer reference so we can use it when it's time // to write tuples.
Pig calls this method for every tuple it needs to store.
Your store function needs to take these tuples and produce the key-value pairs that its output format expects.
JsonStorage encodes the contents of the tuple in JSON format and writes the resulting string into the value field of TextOutputFormat.
Failure Cleanup When jobs fail after execution has started, your store function may need to clean up partially stored results.
Pig will call cleanupOnFailure to give your store function an opportunity to do this.
It passes the location string and the job object so that your store function knows what it should clean up.
You need to implement this method only if you are storing data somewhere other than HDFS.
Storing Metadata If your storage format can store schemas in addition to data, your store function can implement the interface StoreMetadata.
This provides a storeSchema method that is called by Pig as part of its frontend operations.
Pig passes storeSchema a Resource Schema, the location string, and the job object so that it can connect to its storage.
In ResourceField Schema, the schema object associated with a bag always has one field, which is a tuple.
The example store function JsonStorage stores the schema in a side file named _schema in the same directory as the data.
The schema is stored as a string, using the toString method provided by the class:
StoreMetadata also has a storeStatistics function, but Pig does not use this yet.
The community of applications that run on Hadoop has grown significantly as the adoption of Hadoop has increased.
Many (but not all) of these applications are Apache projects.
It can be confusing, especially for those new to Hadoop, to understand how these different applications integrate, complement, and overlap.
In this chapter we will look at the different projects from a Pig perspective, focusing on how they complement, integrate, or compete with Pig.
It takes SQL queries and translates them to MapReduce jobs, much in the same way that Pig translates Pig Latin.
It stores data in tables and keeps metadata concerning those tables, such as partitions and schemas.
Since both provide a way for users to operate on data stored in Hadoop without writing Java code, this is a natural conclusion.
Because Hive provides SQL, it is a better tool for doing traditional data analytics.
Most data analysts are already familiar with SQL, and business intelligence tools expect to speak to data sources in SQL.
Pig Latin is a better choice when building a data pipeline or doing research on raw data.
Cascading Another data-processing framework available for Hadoop is Cascading, available at http://www.cascading.org.
The goal of Cascading is similar to Pig in that it enables users to build data flows on Hadoop.
Rather than presenting a new language, Cascading data flows are written in Java.
This allows users more control but requires more low-level coding.
NoSQL Databases Over the last few years a number of NoSQL databases have arisen.
These databases break one or more of the traditional rules of relational database systems.
Instead, the data accessed by a single application lives in one large table so that few or no joins are necessary.
Many of these databases do not implement full ACID semantics.* Like MapReduce, these systems are built to manage terabytes of data.
Unlike MapReduce, they are focused on random reads and writes of data.
Where MapReduce and technologies built on top of it (such as Pig) are optimized for reading vast quantities of data very quickly, these NoSQL systems optimize for finding a few records very quickly.
This different focus does not mean that Pig does not work with these systems.
Users often want to analyze the data stored in these systems.
Also, because these systems offer good random lookup, certain types of joins could benefit from having the data stored in these systems.
HBase Apache HBase is a NoSQL database that uses HDFS to store its data.
Reads in HBase are done by a key, a range of keys, or a bulk scan.
Users can also update or insert individual rows by keys.
In addition to a key, rows in HBase have column families, and all rows in a table share the same column families.
There is no constraint that each row have the same columns as any other row in a given column family.
Thus an HBase table T might have one column family F, which every row in that table would share, but a row with key x could have columns a, b, c in F, while another row with key y has columns a, b, d in F.
HBase keeps a configurable number of versions, so users can access the most recent version or previous versions of a column value.
All keys and column values in HBase are arrays of bytes.
Pig provides HBaseStorage to read data from and write data to HBase tables.
Bulk reads from HBase are slower than scans in HDFS.
However, if the data is already in HBase, it is faster to read it directly than it is to extract it, place it in HDFS, and then read it.
See http://en.wikipedia.org/wiki/ACID for a discussion of these properties in relational databases.
When loading from HBase, you must tell Pig what table to read from and what column families and columns to read.
You can read individual columns or, beginning in version 0.9, whole column families.
Because column families contain a variable set of columns and their values, they must be cast to Pig’s map type.
As an example, let’s say we have an HBase table users that stores information on users and their links to other users.
The user_info column family has columns such as name, email, etc.
The links column family has a column for each user that the user is linked to.
The column name is the linked user’s ID, and the value of these columns is the type of the link—friend, relation, colleague, etc.:
The appropriate HBase client configuration must be present on your machine to allow the HBase client to determine how to connect to the HBase server.
The first tells it which column families and columns to read, and the second passes a set of options.
When you want to extract a whole column family, you give the column family and an asterisk, for example, links:*
You can also get a subset of the columns in a column family.
The map that contains a column family has the HBase column names as keys and the column values as values.
This can be used to control whether the key is loaded, which rows are loaded, and other features.
All of these options are placed in one string, separated by spaces.
The Java class to use to do casting between Pig types and the bytes that HBase stores.
It is not possible to cast to maps using this converter, so you cannot read entire column families.
As of the time of this writing, Pig is able to read only the latest version of a column value.
There have been discussions about what the best interface and data type mapping would be to enable Pig to read multiple versions.
This feature will most likely be added at some point in the future.
When storing data, you specify the table name as the location string, just as in load.
The constructor arguments are also similar to the load case.
The row key is not referenced in this argument, but it is assumed to be the first field in the Pig tuple.
The only valid option in the optional second argument in the store case is -caster.
Assume at the end of processing that our Pig data has a schema of id: long, name:char array, email:chararray, links:map.
Storing into our example HBase table we used earlier looks like this:
Cassandra Apache Cassandra is another scalable database used for high-volume random reading and writing of data.
Whereas HBase guarantees consistency between its servers, Cassandra has an eventual consistency model, meaning that servers might have different values for the same data for some period of time.
Cassandra comes with support for Pig, which means that you can load data from and store data to Cassandra column families.
This works just as it does with any other storage mechanism that is used with Pig, such as HDFS.
Pig and Cassandra can be used together in a number of ways.
Pig can be used to do traditional analytics while Cassandra performs real-time operations.
Because Pig and MapReduce can be run on top of Cassandra, this can be done without moving data between Cassandra and HDFS.
It can be used to populate the data store with new data as new tables or column families are added.
The Pygmalion project was written to ease development when using Pig with data stored in Cassandra.
It includes helpful UDFs to extract column values from the results, marshal the data back to a form that Cassandra accepts, and others.
In order to properly integrate Pig workloads with data stored in Cassandra, the Cassandra cluster needs to colocate the data with Hadoop task trackers.
This allows the Hadoop job tracker to move the data processing to the nodes where the data resides.
Traditionally, Cassandra is used for heavy writes and real-time, random-access queries.
Heavy Hadoop analytic workloads can be performed on Cassandra without degrading the performance of real-time queries by splitting the cluster by workload type.
A set of nodes is dedicated to handling analytic batch processing and another set is dedicated to handling real-time queries.
Cassandra’s cross-datacenter replication copies data transparently between these sections of the cluster so that manual copying of data is never required, and the analytic section always has updated data.
Metadata in Hadoop Apache HCatalog provides a metadata and table management layer for Hadoop.
As a consequence of this abstraction, Pig users do not need to be concerned with where a file is located, which load and store function should be used, and whether the file is compressed.
It also makes it much easier for Pig, MapReduce, and Hive users to share data because HCatalog provides a single schema and data type model for all of these tools.
That data type model, taken from Hive, varies slightly from Pig’s, but the load and store functions take care of mapping between the models.
The location string for HCatLoader is the name of the table.
It implements LoadMetadata, so you do not need to specify the schema as part of your load statement; Pig will get it from HCatLoader.
Also, because it implements this interface, Pig can work with HCatalog’s partitioning.
If you place the filter statement that describes which partitions you want to read immediately after the load, Pig will push that into the load so that HCatalog returns only the relevant partitions.
As with the load function, the location string indicates the table to store records to.
The store function also requires a constructor argument to indicate the partition key values for this store.
At this time (version 0.1) only one partition can be written to in a single store.
There are plans to allow writing to multiple partitions in version 0.2
HCatStorer expects the schema of the alias being stored to match the schema of the table that records are being stored to.
A Pig Latin script to do this processing would look like the following:
This appendix covers UDFs that come as part of the Pig distribution, including builtin UDFs and user-contributed UDFs in Piggybank.
Built-in UDFs Pig comes prepackaged with many UDFs that can be used directly in Pig without using register or define.
HBaseStorage HBase table The first argument is a string describing column family and column to Pig field mapping.
PigStorage HDFS file The first argument is a field separator (optional; defaults to Tab)
Reads lines of text, each line as a tuple with one chararray field.
HBaseStorage HBase table The first argument is a string describing Pig field to HBase column family and column mapping.
PigStorage HDFS file The first argument is a field separator (optional; defaults to Tab)
Returns: Euler’s number (e) raised to the power of input.
Returns: Average of all values in input; nulls are ignored.
Returns: Average of all values in input; nulls are ignored.
Returns: Average of all values in input; nulls are ignored.
Returns: Average of all values in input; nulls are ignored.
Returns: Average of all bytearrays, cast to doubles, in input; nulls are ignored.
Returns: Number of all records in input, including null values.
Returns: Maximum of all bytearrays, cast to doubles, in input; nulls are ignored.
Returns: Minimum of all bytearrays, cast to doubles, in input; nulls are ignored.
Returns: Sum of all values in the bag; nulls are ignored.
Returns: Sum of all values in the bag; nulls are ignored.
Returns: Sum of all values in the bag; nulls are ignored.
Returns: Sum of all values in the bag; nulls are ignored.
Returns: Sum of all bytearrays, cast to doubles, in input; nulls are ignored.
Parameters: source: the chararray to search in search: the chararray to search for.
Returns: Index of the first instance of search in source; -1 if search is not in source.
Parameters: source: the chararray to search in search: the chararray to search for.
Returns: Index of the last instance of search in source; -1 if search is not in source.
Parameters: source: the chararray to search in regex: the regular expression to search for n: take the nth match, counting from 0
Returns: nth subset of the source matching regex; null if there are no matches.
Parameters: source: the chararray to search in regex: the regular expression to search for.
Returns: Tuple containing all subsets of source matching regex; null if there are no matches.
Parameters: source: the chararray to search in toReplace: the chararray to be replaced newValue: the new chararray to replace it with.
Returns: source with all instances of toReplace changed to newValue.
Returns: Tuple with one field for each section of source.
Returns: Tuple with one field for each section of source.
Returns: Tuple with one field for each section of source; if there are more than one maxsplits sections, only the first maxsplits sections will be in the tuple.
Returns: Subchararray; error if any input value has a length shorter than start.
Returns: input split on whitespace, with each resulting value being placed in its own tuple and all tuples placed in the bag.
Returns: If all inputs have the same schema, the resulting bag will have that schema, else it will have a null schema; if the parameters are tuples, all schemas must have the same field names in addition to types.
Returns: Input parameters are paired up and placed in a map as key/value, key/value; all keys must be chararrays; an odd number of arguments will result in an error.
Parameters: numRecords: the number of records to return field: the field to sort on source: the bag to return records from.
Returns: A tuple with all of the fields passed in as arguments.
Piggybank functions are distributed as part of the Pig distribution, but they are not built in.
At the time of writing, there is no central website or set of documentation for Piggybank.
To find out what is in there, you will need to browse through the code.
You can see all of the included functions by looking in your distribution under contrib/piggybank/
Piggybank does not yet include any Python functions, but it is set up to allow users to contribute functions in languages other than Java, so hopefully this will change in time.
This appendix gives a brief overview of Hadoop, focusing on elements that are of interest to Pig users.
MapReduce MapReduce is the framework for running jobs in Hadoop.
It provides a simple and powerful paradigm for parallelizing data processing.
The JobTracker is the central coordinator of jobs in MapReduce.
It controls which jobs are being run, which resources they are assigned, etc.
On each node in the cluster there is a TaskTracker that is responsible for running the map or reduce tasks assigned to it by the JobTracker.
When reading from HDFS, a record is usually a single line of text.
There is no requirement that data be sorted by key or that the keys must be unique.
Similarly, MapReduce produces a set of records, each with a key and value.
Every job has one input and one output.* MapReduce breaks each job into a series of tasks.
These tasks are of two primary types: map and reduce.
It is possible to bend this rule, as Pig and many other applications do.
For example, the one input can be a concatenation of multiple input files, and files can be opened on the side in tasks and written to or read from.
But, conceptually, each job has one primary input and one primary output.
Map Phase In the map phase, MapReduce gives the user an opportunity to operate on every record in the data set individually.
This phase is commonly used to project out unwanted fields, transform fields, or apply filters.
Certain types of joins and grouping can also be done in the map (e.g., joins where the data is already sorted or hash-based aggregation)
There is no requirement that for every input record there should be one output record.
Maps can choose to remove records or explode one record into multiple records.
This class is responsible for determining how data is split across map tasks and for providing a RecordReader.
In order to specify how data is split across tasks, an InputFormat divides the input data into a set of InputSplits.
In addition to information on what to read, the InputSplit includes a list of nodes that should be used to read the data.
In this way, when the data resides on HDFS, MapReduce is able to move the computation to the data.
The RecordReader provided by an InputFormat reads input data and produces key-value pairs to be passed into the map.
This class controls how data is decompressed (if necessary), and how it is converted to Java types that MapReduce can work with.
Combiner Phase The combiner gives applications a chance to apply their reducer logic early on.
As the map phase writes output, it is serialized and placed into an in-memory buffer.
When this buffer fills, MapReduce will sort the buffer and then run the combiner if the application has provided an implementation for it.
The resulting output is then written to local disk, to be picked up by the shuffle phase and sent to the reducers.
MapReduce might choose not to run the combiner if it determines it will be more efficient not to.
After the shuffle, each reducer will have one input for each map.
The reducer needs to merge these inputs in order to begin processing.
It is not efficient to merge too many inputs simultaneously.
Thus, if the number of inputs exceeds a certain value, the data will be merged and rewritten to disk before being given to the reducer.
During this merge, the combiner will be applied in an attempt to reduce the size of the input data.
See Hadoop’s documentation for a discussion of how and when this prereduce merge is triggered.
Because the combine phase will be run zero, one, or multiple times, the input and output keys and values of the combiner must be of the same type.
Shuffle Phase During the shuffle phase, MapReduce partitions data among the various reducers.
MapReduce uses a class called Partitioner to partition records to reducers during the shuffle phase.
An implementation of Partitioner takes the key and value of the record, as well as the total number of reduce tasks, and returns the reduce task number that the record should go to.
By default, MapReduce uses HashPartitioner, which calls hashCode() on the key and returns the result modulo of the number of reduce tasks.
MapReduce users can override this default to use their own implementation of Partitioner.
Data arriving on the reducer has been partitioned and sorted by the map, combine, and shuffle phases.
By default, the data is sorted by the partition key.
For example, if a user has a data set partitioned on user ID, in the reducer it will be sorted by user ID as well.
It is possible to specify additional sort keys beyond the partition key.
So, for example, the user could choose to partition by user ID and also sort by timestamp.
This feature is useful, as the user does not have to implement her own sorting on the reduce data.
Reduce Phase The input to the reduce phase is each key from the shuffle plus all of the records associated with that key.
Because all records with the same value for the key are now collected together, it is possible to do joins and aggregation operations such as counting.
MapReduce jobs that do not require a reduce phase can set the reduce count to zero.
Output Phase The reducer (or map in a map-only job) writes its output via an OutputFormat.
Output Format is responsible for providing a RecordWriter, which takes the key-value pairs produced by the task and stores them.
This includes serializing, possibly compressing, and writing them to HDFS, HBase, etc.
The OutputFormat is also responsible for providing the OutputCommitter, which is used to do post-output operations such as cleaning up after failure and indicating to the storage medium that data is available (e.g., a database commit)
Distributed Cache Sometimes all or many of the tasks in a MapReduce job will need to access a single file or a set of files.
For example, when joining a large file with a small file, one approach is to open the small file as a side file (that is, open it directly in your map task rather than specify it as an input to your MapReduce job), load it into memory, and do the.
When thousands of map or reduce tasks attempt to open the same HDFS file simultaneously, this puts a large strain on the NameNode and the DataNodes storing that file.
These files are then copied onto the local disk of the task nodes as part of the task initiation.
Map or reduce tasks can then read these as local files.
Handling Failure Part of the power of MapReduce is that it handles failure and retry for the user.
If you have a MapReduce job that involves 10,000 map tasks (not an uncommon situation), the odds are reasonably high that at least one machine will fail during that job.
Rather than trying to remove failure from the system, MapReduce is designed with the assumption that failure is common and must be coped with.
When a given map or reduce task fails, MapReduce handles spawning a replacement task to do the work.
Sometimes it does not even wait for tasks to fail.
When a task is slow, it might spawn a duplicate to see if it can get the task done sooner.
After a task fails a certain number of times (four by default), MapReduce gives up and declares the task and the job a failure.
It handles breaking the files into large blocks and distributing them across different machines.
It also makes multiple copies of each block so that if any one machine fails, no data is lost or unavailable.
By default it makes three copies of each block, though this value is configurable.
One copy is always written locally to the node where the write is executed.
If your Hadoop cluster is spread across multiple racks, HDFS will write one copy of the block on the same rack as the machine where the write is happening, and one copy on a machine in a different rack.
When a machine or disk dies or blocks are corrupted, HDFS will handle making another copy of the lost blocks to ensure that the proper number of replicas are maintained.
Storing data in large blocks works well for MapReduce’s batch model, where it is assumed that every job will read all of the records in a file.
Modern disks are much faster at sequential read than seek.
Thus for large data sets, if you require more than a few records, sequentially reading the entire data set outperforms random reads.
The three-way duplication of data, beyond obviously providing fault tolerance, also serves MapReduce because it gives the JobTracker more options for locating map tasks on the same machine as one of the blocks.
It is responsible for maintaining the master list of files in HDFS, and it handles the mapping of filenames to blocks, knowing where each block is stored, and making sure each block is replicated the appropriate number of times.
Each DataNode is colocated with a TaskTracker to allow moving of the computation to data.
We’d like to hear your suggestions for improving our indexes.
About the Author Alan Gates is an original member of the engineering team that took Pig from a Yahoo! Labs research project to a successful Apache open source project.
In that role, he oversaw the implementation of the language, including programming interfaces and the overall design.
He has presented Pig at numerous conferences and user groups, universities, and companies.
Alan is a member of the Apache Software Foundation and a cofounder of Hortonworks.
Colophon The animal on the cover of Programming Pig is a domestic pig (Sus scrofa domesticus or Sus domesticus)
While the larger pig family is naturally distributed in Africa, Asia, and Europe, domesticated pigs can now be found in nearly every part of the world that people inhabit.
In fact, some pigs have been specifically bred to best equip them for various climates; for example, heavily coated varieties have been bred in colder climates.
People have brought pigs with them almost wherever they go for good reason: in addition to their primary use as a source of food, humans have been using the skin, bones, and hair of pigs to make various tools and implements for millennia.
Domestic pigs are directly descended from wild boars, and evidence suggests that there have been three distinct domestication events; the first took place in the Tigris River Basin as early as 13,000 BC, the second in China, and the third in Europe, though the last likely occurred after Europeans were introduced to domestic pigs from the Middle East.
Despite the long history, however, taxonomists do not agree as to the proper classification for the domestic pig.
Some believe that domestic pigs remain simply a subspecies of the larger pig group including the wild boar (Sus scrofa), while others insist that they belong to a species all their own.
In either case, there are several hundred breeds of domestic pig, each with its own particular characteristics.
Perhaps because of their long history and prominent role in human society, and their tendency toward social behavior, domestic pigs have appeared in film, literature, and other cultural media with regularity.
Additionally, domestic pigs have recently been recognized for their intelligence and their ability to be trained (similar to dogs), and have consequently begun to be treated as pets.
Relational Operations foreach Expressions in foreach UDFs in foreach Naming fields in foreach.
